{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b8a18c36-7705-4cf3-a9a5-358a4a93c041",
   "metadata": {
    "id": "cac470df-29e7-4148-9bbd-d8b9a32fa570",
    "tags": []
   },
   "source": [
    "---\n",
    "title: \"**[CAM]**Grad CAM\"\n",
    "author: \"SEOYEON CHOI\"\n",
    "date: \"2023-09-16\"\n",
    "categories:\n",
    "  - CAM\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19445d6e-56f3-4636-bb6c-fc7c9eb1b857",
   "metadata": {},
   "source": [
    "https://github.com/jacobgil/pytorch-grad-cam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f89ca2-3f7b-472a-a7ea-ab75389a3c6b",
   "metadata": {},
   "source": [
    "https://hygradcam.blogspot.com/2020/11/blog-post.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20db569f-1ea7-4c7e-8a13-cb83f181ec74",
   "metadata": {},
   "source": [
    "https://haystar.tistory.com/72"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f718f2d4-de20-4b35-8e6c-b52b5dc83e00",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a54acc8-552c-4ad5-b2e8-5ce7648e42d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from fastai.vision.all import *\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageFile\n",
    "from PIL import Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from torchvision.utils import save_image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac0f99bc-6b90-4400-89a4-753c981bf3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_func(f):\n",
    "    if f[0].isupper():\n",
    "        return 'cat' \n",
    "    else: \n",
    "        return 'dog' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a27e091-0d72-4ed5-97fb-77267e95282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=Path('original_pet') \n",
    "files=get_image_files(path)\n",
    "dls=ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a242847-a29d-480d-9515-9e8fd43e043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_r=Path('random_pet_one')   #랜덤박스넣은사진\n",
    "files_r=get_image_files(path_r)\n",
    "dls_r=ImageDataLoaders.from_name_func(path_r,files_r,label_func,item_tfms=Resize(512)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc0dbe-3325-402f-a2e9-4416123b4d6a",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c731e1a1-9d0a-49e8-abbd-4e6efd76def9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/fastai/vision/learner.py:288: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n",
      "  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n",
      "/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.151488</td>\n",
       "      <td>0.016599</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>30:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='75' class='' max='92' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      81.52% [75/92 39:47&lt;09:01 0.0289]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrnr=cnn_learner(dls,resnet34,metrics=error_rate)\n",
    "lrnr.fine_tune(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88fff60-1256-457c-8a5f-3799430b9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr_r=cnn_learner(dls_r,resnet34,metrics=error_rate)\n",
    "lrnr_r.fine_tune(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f33f682-c635-403a-b11e-c3eda351fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "net1=lrnr.model[0]\n",
    "net2=lrnr.model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1cef2e-ef90-4bc4-808b-828e128f9398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.AdaptiveAvgPool2d(output_size=1), \n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(512,out_features=2,bias=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76bd2f4-3604-4628-b299-1d3e999e3910",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=torch.nn.Sequential(net1,net2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fc0ae4-4468-4662-b561-99a3328d386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr2=Learner(dls,net,metrics=accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b21ff-f9d3-496d-9a64-e043f4387f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr2.fine_tune(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6635340f-84dc-4c27-99ba-91a8d5f3c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(lrnr2)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b960250-95ea-4912-8df1-d1e9eab06eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "net1_r=lrnr_r.model[0]\n",
    "net2_r=lrnr_r.model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6a29c1-0459-4159-a333-e64a61ba3afd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net2_r = torch.nn.Sequential(\n",
    "    torch.nn.AdaptiveAvgPool2d(output_size=1), \n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(512,out_features=2,bias=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3525e4e6-aeba-4b22-bd0c-d64cd8db75e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_r=torch.nn.Sequential(net1_r,net2_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fae2d6-a25e-46c8-b644-477451a10606",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr2_r=Learner(dls_r,net_r,metrics=accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7662715-1b93-498d-98bc-713873744996",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr2_r.fine_tune(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38747f15-8578-4c1e-8146-6576b4c8777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_r = ClassificationInterpretation.from_learner(lrnr2_r)\n",
    "interp_r.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b181c05-2bc7-4675-9a79-edafa3eb8117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradcam(learn, img):    \n",
    "    with hook_output(lrnr2.model[0]) as hook_a, hook_output(lrnr2.model[0], grad=True) as hook_g:\n",
    "        preds = lrnr2.model.eval()(x) # 모델 평가 모드 전환 -> 활성화 맵과 그레디언트를 변수로 저장하자.\n",
    "        preds[0, preds.argmax()].backward() # 모델이 평가 모드로 전환되면, 드롭아웃이 비활성화되고 배치 정규화의 이동 평균과 이동 분산이 업데이트되지 않는다.\n",
    "        # 이 과정은 그레디언트 계산할 층 선택하는 과정\n",
    "    \n",
    "    activation = hook_a.stored[0].cpu() # shape = torch.Size([1, 512, 16, 16]) -> torch.Size([512, 16, 16])\n",
    "    gradient = hook_g.stored[0][0].cpu() # 값, 빈 값 -> 값의 shape = torch.Size([1, 512, 16, 16]) -> torch.Size([512, 16, 16])\n",
    "    \n",
    "    grad_cam = (activation * gradient).mean(0) # shape = torch.Size([16, 16]) 0번 축에 따라 mean 값 계산\n",
    "    grad_cam = np.maximum(grad_cam, 0) # shape = torch.Size([16, 16]) 0과 grad_cam 중 큰 것을 구별\n",
    "    grad_cam /= grad_cam.max() # \\=은 나눗셈 후에 결과를 원래 변수에 할당하는 축약형 연산자, [0,1] 범위로 정규화하기 위함\n",
    "    \n",
    "    return grad_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d917e0e9-c28b-448f-9a74-e2e0bf22a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, = first(dls.test_dl([PILImage.create(get_image_files(path)[7])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03eb58d-31a9-4622-afc7-1f82bcece5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam = compute_gradcam(lrnr,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242b1d6d-7528-4348-853c-c4b54c16ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1,1) \n",
    "dls.train.decode((x,))[0].squeeze().show(ax=ax1)\n",
    "ax1.imshow(grad_cam,alpha=0.5,extent=(0,511,511,0),interpolation='spline36',cmap='magma')\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136257a6-9b59-4397-b1a7-6b51b2a1245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,5) \n",
    "k=0 \n",
    "for i in range(5):\n",
    "    for j in range(5): \n",
    "        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n",
    "        dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n",
    "        ax[i][j].imshow(grad_cam,alpha=0.5,extent=(0,511,511,0),interpolation='spline36',cmap='magma')\n",
    "        k=k+1 \n",
    "fig.set_figwidth(16)            \n",
    "fig.set_figheight(16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d1c1e-9b74-4615-ad8f-05ac01838136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecaad3d-297a-497f-8b8f-3753f7aec3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c9aed6-45c4-4932-bbf5-13247edc7ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9ac84-b3c9-44ff-a578-04f9924dbb44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc3cf33-0a2a-4e83-aeaf-d1ef11586a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7021ec-8d78-4ab5-a414-902631bffd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,5) \n",
    "k=0 \n",
    "for i in range(5):\n",
    "    for j in range(5): \n",
    "        x, = first(dls_r.test_dl([PILImage.create(get_image_files(path_r)[k])]))\n",
    "        camimg = torch.einsum('ij,jkl -> ikl', net_2[2].weight, net_1(x).squeeze())\n",
    "        a,b = net_r(x).tolist()[0]\n",
    "        catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) ,  np.exp(b)/ (np.exp(a)+np.exp(b))\n",
    "        if catprob>dogprob: \n",
    "            test=camimg[0]-torch.min(camimg[0])\n",
    "            A1=torch.exp(-0.1*test)\n",
    "            X1=np.array(A1.to(\"cpu\").detach(),dtype=np.float32)\n",
    "            Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "            x1=x.squeeze().to('cpu')*Y1-torch.min(x.squeeze().to('cpu'))*Y1\n",
    "            (x1*0.25).squeeze().show(ax=ax[i][j])\n",
    "            ax[i][j].set_title(\"cat(%s)\" % catprob.round(5))\n",
    "        else: \n",
    "            test=camimg[1]-torch.min(camimg[1])\n",
    "            A1=torch.exp(-0.1*test)\n",
    "            X1=np.array(A1.to(\"cpu\").detach(),dtype=np.float32)\n",
    "            Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "            x1=x.squeeze().to('cpu')*Y1-torch.min(x.squeeze().to('cpu'))*Y1\n",
    "            (x1*0.25).squeeze().show(ax=ax[i][j])\n",
    "            ax[i][j].set_title(\"dog(%s)\" % dogprob.round(5))\n",
    "        k=k+1 \n",
    "fig.set_figwidth(16)            \n",
    "fig.set_figheight(16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fefe33-33e4-4631-89fc-422ac0280219",
   "metadata": {},
   "source": [
    "# Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5682e99-d98e-44ff-be51-e6ebab6435bb",
   "metadata": {},
   "source": [
    "# CAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cf7f52-3779-45ed-822f-cf64617b9a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, = first(dls_r.test_dl([PILImage.create(get_image_files(path_r)[2])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d9b901-9759-4ba1-8d70-a11bc34df1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "camimg = torch.einsum('ij,jkl -> ikl', net_2[2].weight, net_1(x).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3dd3a-ee55-4f31-8e09-bd9b8718fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebayesthresh = importr('EbayesThresh').ebayesthresh\n",
    "\n",
    "power_threshed=np.array(ebayesthresh(FloatVector(torch.tensor(camimg[0].detach().reshape(-1))**2)))\n",
    "ybar_threshed = np.where(power_threshed>2000,torch.tensor(camimg[0].detach().reshape(-1)),0)\n",
    "ybar_threshed = torch.tensor(ybar_threshed.reshape(16,16))\n",
    "\n",
    "power_threshed2=np.array(ebayesthresh(FloatVector(torch.tensor(camimg[1].detach().reshape(-1))**2)))\n",
    "ybar_threshed2 = np.where(power_threshed2>1600,torch.tensor(camimg[1].detach().reshape(-1)),0)\n",
    "ybar_threshed2 = torch.tensor(ybar_threshed2.reshape(16,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87265186-805f-4e40-8f66-2eae04474759",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3) \n",
    "# \n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax1)\n",
    "ax1.set_title(\"Input image\")\n",
    "# \n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax2)\n",
    "ax2.imshow((ybar_threshed).to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"CAT PART\")\n",
    "#\n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax3)\n",
    "ax3.imshow((ybar_threshed2).to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax3.set_title(\"DOG PART\")\n",
    "#\n",
    "fig.set_figwidth(12)            \n",
    "fig.set_figheight(12)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae66934-e3c4-4f55-94ab-6a436d087c00",
   "metadata": {},
   "source": [
    "- 판단 근거가 강할 수록 파란색 -> 보라색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc2f8d-c649-4505-879c-a172e2d5e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = net_r(x).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e4e62-86c0-433d-87c8-8492ae95c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(a)/ (np.exp(a)+np.exp(b)) ,  np.exp(b)/ (np.exp(a)+np.exp(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323c98bc-7eda-4393-aa63-6158f7424620",
   "metadata": {},
   "source": [
    "## mode 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c5d83-d095-4af4-a81d-74b3aceb61a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test=camimg_o[0]-torch.min(camimg_o[0])\n",
    "T = camimg[0]\n",
    "A1=torch.exp(-0.05*(ybar_threshed))\n",
    "T1 = torch.exp(-0.05*T)\n",
    "A2 = 1 - A1\n",
    "T2 = 1 - T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0460cdd1-27cc-438a-ad98-eac1cf6c365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2) \n",
    "# \n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax1)\n",
    "ax1.imshow(A2.data.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"MODE1 WEIGHTT\")\n",
    "#\n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax2)\n",
    "ax2.imshow(A1.data.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"MODE1 RES WEIGHT\")\n",
    "#\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d0e32c-c2e6-424b-9545-26efa0aa3c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode 1 res\n",
    "X1=np.array(A1.to(\"cpu\").detach(),dtype=np.float32)\n",
    "Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "x1=x.squeeze().to('cpu')*Y1-torch.min(x.squeeze().to('cpu'))*Y1\n",
    "\n",
    "# mode 1\n",
    "X12=np.array(A2.to(\"cpu\").detach(),dtype=np.float32)\n",
    "Y12=torch.Tensor(cv2.resize(X12,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "x12=x.squeeze().to('cpu')*Y12-torch.min(x.squeeze().to('cpu'))*Y12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8bb7b6-eda7-450d-b214-5716dcf3cf6b",
   "metadata": {},
   "source": [
    "`-` 1st CAM 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdeb3fc-d77b-4d09-9044-c971ce1c9075",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1,1) \n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax1)\n",
    "ax1.set_title(\"ORIGINAL\")\n",
    "fig.set_figwidth(4)            \n",
    "fig.set_figheight(4)\n",
    "fig.tight_layout()\n",
    "#\n",
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "(x12*0.3).squeeze().show(ax=ax1)  #MODE1\n",
    "(x1*0.2).squeeze().show(ax=ax2)  #MODE1_res\n",
    "ax1.set_title(\"MODE1\")\n",
    "ax2.set_title(\"MODE1 RES\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e2eea-0ee4-49e3-b2a2-d0176217436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x1.reshape(1,3,512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45370b1d-e604-409e-ab94-47f9f29eb6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_1.to('cpu')\n",
    "net_2.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04da5b6-9da7-4b0a-96f1-c542e315de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "camimg1 = torch.einsum('ij,jkl -> ikl', net_2[2].weight, net_1(x1).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cedeac-3a6a-4c33-afd6-f679b42cd98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1,b1 = net_r(x1).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea9fdd5-03f1-4757-9277-e8bd95c778f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(a1)/ (np.exp(a1)+np.exp(b1)) ,  np.exp(b1)/ (np.exp(a1)+np.exp(b1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f20d4dc-8c46-4acb-b123-e926ede192ec",
   "metadata": {},
   "source": [
    "`-` mode1 res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605b937-482f-4bf6-89b0-3c8b75c2846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2) \n",
    "# \n",
    "(x1*0.25).squeeze().show(ax=ax1)\n",
    "ax1.imshow(camimg1[0].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"CAT PART\")\n",
    "#\n",
    "(x1*0.25).squeeze().show(ax=ax2)\n",
    "ax2.imshow(camimg1[1].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"DOG PART\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e116a8-bcfa-44b5-a3df-b133ebd6d357",
   "metadata": {},
   "source": [
    "`-` 첫번째 CAM 결과와 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a4249c-84b8-4264-a521-2d238a484918",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2) \n",
    "# \n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax1)\n",
    "ax1.imshow(camimg[0].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"1ST CAM\")\n",
    "#\n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax2)\n",
    "ax2.imshow(camimg1[0].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"2ND CAM\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ac0c26-4cfc-4272-95f2-4357ee767937",
   "metadata": {},
   "source": [
    "`-` 2nd CAM 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d143b0-e848-4377-b445-43303b204630",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=camimg1[0]-torch.min(camimg1[0])\n",
    "A3 = torch.exp(-0.03*(test1))\n",
    "A4 = 1 - A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78686a8d-2385-4ded-993d-54d623dcbb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2) \n",
    "# \n",
    "x1.squeeze().show(ax=ax2)\n",
    "dls_r.train.decode((x1,))[0].squeeze().show(ax=ax1)\n",
    "ax1.imshow(A3.data.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"MODE2 RES WEIGHT\")\n",
    "#\n",
    "x1.squeeze().show(ax=ax2)\n",
    "dls_r.train.decode((x1,))[0].squeeze().show(ax=ax2)\n",
    "ax2.imshow(A4.data.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"MODE2 WEIGHT\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dcaf7f-4fad-41f5-9b60-c85657c16ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2=np.array(A3.to(\"cpu\").detach(),dtype=np.float32)\n",
    "\n",
    "Y2=torch.Tensor(cv2.resize(X2,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "\n",
    "x2=(x1*0.2)*Y2-torch.min((x1*0.2)*Y2)\n",
    "\n",
    "X22=np.array(A4.to(\"cpu\").detach(),dtype=np.float32)\n",
    "\n",
    "Y22=torch.Tensor(cv2.resize(X22,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "\n",
    "x22=(x1*0.2)*Y22-torch.min((x1*0.2)*Y22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be19eaa3-72c6-41e6-9633-6dbc9e665862",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1,1) \n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax1)\n",
    "ax1.set_title(\"ORIGINAL\")\n",
    "fig.set_figwidth(4)            \n",
    "fig.set_figheight(4)\n",
    "fig.tight_layout()\n",
    "#\n",
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "(x12*0.3).squeeze().show(ax=ax1)  #MODE1\n",
    "(x1*0.2).squeeze().show(ax=ax2)  #MODE1_res\n",
    "ax1.set_title(\"MODE1\")\n",
    "ax2.set_title(\"MODE1 RES\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()\n",
    "#\n",
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "(x22*4).squeeze().show(ax=ax1)  #MODE2\n",
    "(x2).squeeze().show(ax=ax2)  #MODE2_res\n",
    "ax1.set_title(\"MODE2\")\n",
    "ax2.set_title(\"MODE2 RES\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac93db-d272-40c9-820c-0786fcab7cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = x2.reshape(1,3,512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7c065a-8bd5-461e-ab2b-d84c7c3bd5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_1.to('cpu')\n",
    "net_2.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8144fa5c-78ca-436d-be33-a7509979ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "camimg2 = torch.einsum('ij,jkl -> ikl', net_2[2].weight, net_1(x2).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6079153d-38bd-4132-b69b-3c43adf2c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2,b2 = net_r(x2).tolist()[0]\n",
    "np.exp(a2)/(np.exp(a2)+np.exp(b2)), np.exp(b2)/(np.exp(a2)+np.exp(b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2654ae-26fc-445f-9988-abd41e354572",
   "metadata": {},
   "source": [
    "`-` mode2 res 에 CAM 결과 올리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98910f6-f7de-41fc-93a1-5b54b10a84cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "#\n",
    "x2.squeeze().show(ax=ax1)\n",
    "ax1.imshow(camimg2[0].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"CAT PART\")\n",
    "#\n",
    "x2.squeeze().show(ax=ax2)\n",
    "ax2.imshow(camimg2[1].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"DOG PART\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431017d0-2d3e-41a2-87de-32c2072969e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3) \n",
    "# \n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax1)\n",
    "ax1.imshow(camimg[0].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"1ST CAM\")\n",
    "#\n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax2)\n",
    "ax2.imshow(camimg1[0].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"2ND CAM\")\n",
    "#\n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax3)\n",
    "ax3.imshow(camimg2[0].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax3.set_title(\"3RD CAM\")\n",
    "fig.set_figwidth(12)            \n",
    "fig.set_figheight(12)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad96a9c9-d7a1-4c2d-aa38-bdc4351c75b1",
   "metadata": {},
   "source": [
    "## mode 3 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620eb894-6b33-430e-8a20-5241ff090cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2=camimg2[0]-torch.min(camimg2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469a5ebf-485c-4f47-ae76-9e63d4cb0ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5 = torch.exp(-0.05*(test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d72815-be75-4f80-a128-fc0f1843d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A6 = 1 - A5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66353d-f8c6-42cd-b149-e3cde3ed45c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "#\n",
    "x2.squeeze().show(ax=ax1)\n",
    "ax1.imshow(camimg2[0].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"CAT PART\")\n",
    "#\n",
    "x2.squeeze().show(ax=ax2)\n",
    "ax2.imshow(camimg2[1].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"DOG PART\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e404a68-5b24-4cf3-ad81-773e2245265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mode 3 res\n",
    "X3=np.array(A5.to(\"cpu\").detach(),dtype=np.float32)\n",
    "Y3=torch.Tensor(cv2.resize(X3,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "x3=x2*Y3-torch.min(x2*Y3)\n",
    "# mode 3\n",
    "X32=np.array(A6.to(\"cpu\").detach(),dtype=np.float32)\n",
    "Y32=torch.Tensor(cv2.resize(X32,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "x32=x2*Y32-torch.min(x2*Y32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0981d414-6808-4794-aa97-c80df5840f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1,1) \n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax1)\n",
    "ax1.set_title(\"ORIGINAL\")\n",
    "fig.set_figwidth(4)            \n",
    "fig.set_figheight(4)\n",
    "fig.tight_layout()\n",
    "#\n",
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "(x12*0.3).squeeze().show(ax=ax1)  #MODE1\n",
    "(x1*0.2).squeeze().show(ax=ax2)  #MODE1_res\n",
    "ax1.set_title(\"MODE1\")\n",
    "ax2.set_title(\"MODE1 RES\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()\n",
    "#\n",
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "(x22*4).squeeze().show(ax=ax1)  #MODE2\n",
    "(x2).squeeze().show(ax=ax2)  #MODE2_res\n",
    "ax1.set_title(\"MODE2\")\n",
    "ax2.set_title(\"MODE2 RES\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()\n",
    "#\n",
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "(x32*8).squeeze().show(ax=ax1)  #MODE3\n",
    "(x3).squeeze().show(ax=ax2)  #MODE3_res\n",
    "ax1.set_title(\"MODE3\")\n",
    "ax2.set_title(\"MODE3 RES\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd0f624-14a0-4721-9a94-313be2896945",
   "metadata": {},
   "source": [
    "# DOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769df0be-a850-4810-9fe5-e4819f2221be",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, = first(dls_r.test_dl([PILImage.create(get_image_files(path_r)[12])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3c1e3d-95c2-4959-8a3c-fe2a3de3ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "camimg = torch.einsum('ij,jkl -> ikl', net_2[2].weight, net_1(x).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b34f3-55d0-4234-b273-fdf3409e706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebayesthresh = importr('EbayesThresh').ebayesthresh\n",
    "\n",
    "power_threshed=np.array(ebayesthresh(FloatVector(torch.tensor(camimg[0].detach().reshape(-1))**2)))\n",
    "ybar_threshed = np.where(power_threshed>2000,torch.tensor(camimg[0].detach().reshape(-1)),0)\n",
    "ybar_threshed = torch.tensor(ybar_threshed.reshape(16,16))\n",
    "\n",
    "power_threshed2=np.array(ebayesthresh(FloatVector(torch.tensor(camimg[1].detach().reshape(-1))**2)))\n",
    "ybar_threshed2 = np.where(power_threshed2>1500,torch.tensor(camimg[1].detach().reshape(-1)),0)\n",
    "ybar_threshed2 = torch.tensor(ybar_threshed2.reshape(16,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf5046-0aa7-4c22-9e7f-53f00063c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3) \n",
    "# \n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax1)\n",
    "ax1.set_title(\"Input image\")\n",
    "# \n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax2)\n",
    "ax2.imshow((ybar_threshed).to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"CAT PART\")\n",
    "#\n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax3)\n",
    "ax3.imshow((ybar_threshed2).to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax3.set_title(\"DOG PART\")\n",
    "#\n",
    "fig.set_figwidth(12)            \n",
    "fig.set_figheight(12)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1390c8-c65f-4d3a-b1ab-e3b034738862",
   "metadata": {},
   "source": [
    "- 판단 근거가 강할 수록 파란색 -> 보라색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83560901-9303-4802-892f-3ec89ad9337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = net_r(x).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a172450-de26-4067-a243-940c53643964",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(a)/ (np.exp(a)+np.exp(b)) ,  np.exp(b)/ (np.exp(a)+np.exp(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0738de49-6436-40e6-b915-ff9617d80b60",
   "metadata": {},
   "source": [
    "## mode 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1cab18-fa6d-441d-b34c-d81e42fab308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test=camimg_o[0]-torch.min(camimg_o[0])\n",
    "T = camimg[1]\n",
    "A1=torch.exp(-0.05*(ybar_threshed2))\n",
    "T1 = torch.exp(-0.05*T)\n",
    "A2 = 1 - A1\n",
    "T2 = 1 - T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32dfe03-2466-4fd9-8ca0-a663da7b807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2) \n",
    "# \n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax1)\n",
    "ax1.imshow(A2.data.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"MODE1 WEIGHTT\")\n",
    "#\n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax2)\n",
    "ax2.imshow(A1.data.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"MODE1 RES WEIGHT\")\n",
    "#\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd191c1-3ef3-47ec-b2d6-45318635bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode 1 res\n",
    "X1=np.array(A1.to(\"cpu\").detach(),dtype=np.float32)\n",
    "Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "x1=x.squeeze().to('cpu')*Y1-torch.min(x.squeeze().to('cpu'))*Y1\n",
    "\n",
    "# mode 1\n",
    "X12=np.array(A2.to(\"cpu\").detach(),dtype=np.float32)\n",
    "Y12=torch.Tensor(cv2.resize(X12,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "x12=x.squeeze().to('cpu')*Y12-torch.min(x.squeeze().to('cpu'))*Y12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e8158e-64aa-493d-acaa-720ae542cd39",
   "metadata": {},
   "source": [
    "`-` 1st CAM 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76406615-071e-49a6-a203-15f1f07a159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1,1) \n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax1)\n",
    "ax1.set_title(\"ORIGINAL\")\n",
    "fig.set_figwidth(4)            \n",
    "fig.set_figheight(4)\n",
    "fig.tight_layout()\n",
    "#\n",
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "(x12*0.3).squeeze().show(ax=ax1)  #MODE1\n",
    "(x1*0.3).squeeze().show(ax=ax2)  #MODE1_res\n",
    "ax1.set_title(\"MODE1\")\n",
    "ax2.set_title(\"MODE1 RES\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ecc44-dbfd-4055-924f-6c4ecf6023af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x1.reshape(1,3,512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7581e-3ec6-42d1-8a54-4b244e7bde41",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_1.to('cpu')\n",
    "net_2.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539bd616-f2cd-42a0-b0a9-c0040c1958de",
   "metadata": {},
   "outputs": [],
   "source": [
    "camimg1 = torch.einsum('ij,jkl -> ikl', net_2[2].weight, net_1(x1).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac30c3-3b71-49c6-b86f-e6686dbe0386",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1,b1 = net_r(x1).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec206ece-7640-44f8-a744-3fc96be28244",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(a1)/ (np.exp(a1)+np.exp(b1)) ,  np.exp(b1)/ (np.exp(a1)+np.exp(b1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bb80c0-11b7-4a03-829d-0b9f843ec662",
   "metadata": {},
   "source": [
    "`-` mode1 res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578cdae0-96e1-474c-8e79-809b9d385287",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2) \n",
    "# \n",
    "(x1*0.25).squeeze().show(ax=ax1)\n",
    "ax1.imshow(camimg1[0].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"CAT PART\")\n",
    "#\n",
    "(x1*0.25).squeeze().show(ax=ax2)\n",
    "ax2.imshow(camimg1[1].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"DOG PART\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ab352-4c0e-431c-9263-3e5597fb72ff",
   "metadata": {},
   "source": [
    "`-` 첫번째 CAM 결과와 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86697a0e-ba2a-4985-84a5-6857f47e115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2) \n",
    "# \n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax1)\n",
    "ax1.imshow(camimg[0].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"1ST CAM\")\n",
    "#\n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax2)\n",
    "ax2.imshow(camimg1[0].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"2ND CAM\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e920dd81-f2d3-4a72-ba61-3a38993f8260",
   "metadata": {},
   "source": [
    "`-` 2nd CAM 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5eee38-5bc5-4857-8061-2a1ba2d6c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=camimg1[1]-torch.min(camimg1[1])\n",
    "A3 = torch.exp(-0.05*(test1))\n",
    "A4 = 1 - A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5461e715-92a4-4d23-a271-d1137f5812eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2) \n",
    "# \n",
    "x1.squeeze().show(ax=ax2)\n",
    "dls_r.train.decode((x1,))[0].squeeze().show(ax=ax1)\n",
    "ax1.imshow(A3.data.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"MODE2 RES WEIGHT\")\n",
    "#\n",
    "x1.squeeze().show(ax=ax2)\n",
    "dls_r.train.decode((x1,))[0].squeeze().show(ax=ax2)\n",
    "ax2.imshow(A4.data.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"MODE2 WEIGHT\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ed114-aa41-4eda-b8e4-e3cf1f04adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res \n",
    "X2=np.array(A3.to(\"cpu\").detach(),dtype=np.float32)\n",
    "Y2=torch.Tensor(cv2.resize(X2,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "x2=(x1*0.2)*Y2-torch.min((x1*0.2)*Y2)\n",
    "#\n",
    "X22=np.array(A4.to(\"cpu\").detach(),dtype=np.float32)\n",
    "Y22=torch.Tensor(cv2.resize(X22,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "x22=(x1*0.2)*Y22-torch.min((x1*0.2)*Y22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de261fcc-7b9e-4789-9d43-cd6ed332aafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1,1) \n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax1)\n",
    "ax1.set_title(\"ORIGINAL\")\n",
    "fig.set_figwidth(4)            \n",
    "fig.set_figheight(4)\n",
    "fig.tight_layout()\n",
    "#\n",
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "(x12*0.3).squeeze().show(ax=ax1)  #MODE1\n",
    "(x1*0.2).squeeze().show(ax=ax2)  #MODE1_res\n",
    "ax1.set_title(\"MODE1\")\n",
    "ax2.set_title(\"MODE1 RES\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()\n",
    "#\n",
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "(x22*3).squeeze().show(ax=ax1)  #MODE2\n",
    "(x2).squeeze().show(ax=ax2)  #MODE2_res\n",
    "ax1.set_title(\"MODE2\")\n",
    "ax2.set_title(\"MODE2 RES\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39944e2-cde5-4b13-8d16-63e68beb8027",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = x2.reshape(1,3,512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9fae3-165c-42e4-9b98-bf0ef7250a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_1.to('cpu')\n",
    "net_2.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36971ca2-e66d-4524-86c2-afd2a1d65db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "camimg2 = torch.einsum('ij,jkl -> ikl', net_2[2].weight, net_1(x2).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0c852-d2a2-4079-b1e9-5b12e5c09f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2,b2 = net_r(x2).tolist()[0]\n",
    "np.exp(a2)/(np.exp(a2)+np.exp(b2)), np.exp(b2)/(np.exp(a2)+np.exp(b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc5fb7-cacc-4a21-99d8-e8dc49517930",
   "metadata": {},
   "source": [
    "`-` mode2 res 에 CAM 결과 올리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45604924-6f4c-4d52-a138-6a5fbf111862",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "#\n",
    "x2.squeeze().show(ax=ax1)\n",
    "ax1.imshow(camimg2[0].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"CAT PART\")\n",
    "#\n",
    "x2.squeeze().show(ax=ax2)\n",
    "ax2.imshow(camimg2[1].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"DOG PART\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec619e4b-fdfc-4fc5-bc99-ebb34110f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3) \n",
    "# \n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax1)\n",
    "ax1.imshow(camimg[0].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"1ST CAM\")\n",
    "#\n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax2)\n",
    "ax2.imshow(camimg1[0].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"2ND CAM\")\n",
    "#\n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax3)\n",
    "ax3.imshow(camimg2[0].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax3.set_title(\"3RD CAM\")\n",
    "fig.set_figwidth(12)            \n",
    "fig.set_figheight(12)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f5b139-135e-4367-ad77-e7b2be5ced84",
   "metadata": {},
   "source": [
    "## mode 3 만들기 더이상 분리되지 않는 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea403f-ce95-4c37-9103-531426b06bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2=camimg2[1]-torch.min(camimg2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3f4814-54e1-4fc1-89b7-f65e3e3076f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5 = torch.exp(-0.05*(test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63760bc0-634b-44d3-9796-0d2f92127054",
   "metadata": {},
   "outputs": [],
   "source": [
    "A6 = 1 - A5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9535c1-cd26-4643-9940-62696c2779a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "#\n",
    "x2.squeeze().show(ax=ax1)\n",
    "ax1.imshow(camimg2[0].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax1.set_title(\"CAT PART\")\n",
    "#\n",
    "x2.squeeze().show(ax=ax2)\n",
    "ax2.imshow(camimg2[1].to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='cool')\n",
    "ax2.set_title(\"DOG PART\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864bc11-01ea-4a1c-ad14-2f0da57129e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mode 3 res\n",
    "X3=np.array(A5.to(\"cpu\").detach(),dtype=np.float32)\n",
    "Y3=torch.Tensor(cv2.resize(X3,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "x3=x2*Y3-torch.min(x2*Y3)\n",
    "# mode 3\n",
    "X32=np.array(A6.to(\"cpu\").detach(),dtype=np.float32)\n",
    "Y32=torch.Tensor(cv2.resize(X32,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "x32=x2*Y32-torch.min(x2*Y32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e0605-3b23-4e9c-a26e-df29bcda041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1,1) \n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax1)\n",
    "ax1.set_title(\"ORIGINAL\")\n",
    "fig.set_figwidth(4)            \n",
    "fig.set_figheight(4)\n",
    "fig.tight_layout()\n",
    "#\n",
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "(x12*0.3).squeeze().show(ax=ax1)  #MODE1\n",
    "(x1*0.2).squeeze().show(ax=ax2)  #MODE1_res\n",
    "ax1.set_title(\"MODE1\")\n",
    "ax2.set_title(\"MODE1 RES\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()\n",
    "#\n",
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "(x22*4).squeeze().show(ax=ax1)  #MODE2\n",
    "(x2).squeeze().show(ax=ax2)  #MODE2_res\n",
    "ax1.set_title(\"MODE2\")\n",
    "ax2.set_title(\"MODE2 RES\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()\n",
    "#\n",
    "fig, (ax1, ax2) = plt.subplots(1,2) \n",
    "(x32*8).squeeze().show(ax=ax1)  #MODE3\n",
    "(x3).squeeze().show(ax=ax2)  #MODE3_res\n",
    "ax1.set_title(\"MODE3\")\n",
    "ax2.set_title(\"MODE3 RES\")\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d80ce01-7874-4652-bb61-f27eca7dab06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637483a6-39e5-41aa-b473-cc2ed60f0677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880fd85e-28d1-4726-aa4d-f5d05c727acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29905bbc-0360-4e88-9426-d28742105a49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
