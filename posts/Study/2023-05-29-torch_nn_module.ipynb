{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f245db1f-9442-4763-92f1-3add2e31d913",
   "metadata": {
    "id": "cac470df-29e7-4148-9bbd-d8b9a32fa570",
    "tags": []
   },
   "source": [
    "---\n",
    "title: \"All about torch.nn.Module\"\n",
    "author: \"SEOYEON CHOI\"\n",
    "date: \"2023-05-29\"\n",
    "categories:\n",
    "  - Pytorch\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4cafde-c30b-4c7d-8800-e2c1e22dd412",
   "metadata": {
    "tags": []
   },
   "source": [
    "> Simulation Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58040af4-74eb-4b3c-ac19-4280f16980a9",
   "metadata": {},
   "source": [
    "Ref: [Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b0b83-c917-4fb7-8f30-4b59b4866c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "::: {.callout-note}\n",
    "Note that there are five types of callouts, including:\n",
    "`note`, `warning`, `important`, `tip`, and `caution`.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3da7b5-acb5-4486-8f31-2bca1690facc",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed6a6651-1dc6-408e-98de-0c958514e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric_temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5717fbf9-86cc-4151-9e55-4df945a54d88",
   "metadata": {},
   "source": [
    "# torch.nn.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa2bb47-7ff4-4635-b0ef-dc46e8db0b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torch/nn/modules/module.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     Identity, Linear, Bilinear, _ConvNd, Threshold, ReLU, RReLU, Hardtanh, Sigmoid, Hardsigmoid, ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.nn.Module?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7205a5b2-05d7-45ef-90c5-4c5a0b0238be",
   "metadata": {},
   "source": [
    "`-`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a2e876-e3a5-4b21-8355-6a465ab45242",
   "metadata": {},
   "source": [
    "Base class for all neural network modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dfdc81-21a4-4844-91a9-dd147e800f37",
   "metadata": {},
   "source": [
    "```python\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(20, 20, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.F.relu(self.conv1(x))\n",
    "        return torch.nn.functional.F.relu(self.conv2(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf33fee-cbe8-4544-b9ac-a28e6d9f6631",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "\n",
    "As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc1f14b7-3aa0-4fd4-9c68-714ba03d49ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader1 = torch_geometric_temporal.dataset.ChickenpoxDatasetLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cad37b-adf5-48d6-b80f-19ddd7618c7b",
   "metadata": {},
   "source": [
    "## add_module(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0ac71bf-bd91-43da-ba5a-b2c71afe375d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Module'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Adds a child module to the current module.\n",
       "\n",
       "The module can be accessed as an attribute using the given name.\n",
       "\n",
       "Args:\n",
       "    name (str): name of the child module. The child module can be\n",
       "        accessed from this module using the given name\n",
       "    module (Module): child module to be added to the module.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torch/nn/modules/module.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.nn.Module.add_module?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb121119-3d45-4108-8d51-a75bf1b303de",
   "metadata": {},
   "source": [
    "부모 클래스 상속받아 자식 클래스에서 모듈 추가하는 법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea64b7f5-b876-4ce4-a5f8-bfc7b745f1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (fc2): Linear(in_features=5, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(10, 5)  # 기존에 정의된 모듈\n",
    "        self.add_module(\"fc2\", torch.nn.Linear(5, 2))  # add_module을 사용하여 새로운 모듈 추가\n",
    "\n",
    "model = MyModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f4e2ee-d4d3-4c47-acd6-0f5ddd3e8b51",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "\n",
    "torch.nn.Module에 추가할 때\n",
    "\n",
    "- Identity, Linear, Bilinear, _ConvNd, Threshold, ReLU, RReLU, Hardtanh, Sigmoid, Hardsigmoid, 등\n",
    "\n",
    "다양하게 추가 가능\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a03091-3498-49b8-bcee-355c4714b3f4",
   "metadata": {},
   "source": [
    "## apply(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c2fc664-884f-4600-95a5-8a57ffe41704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Module'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
       "as well as self. Typical use includes initializing the parameters of a model\n",
       "(see also :ref:`nn-init-doc`).\n",
       "\n",
       "Args:\n",
       "    fn (:class:`Module` -> None): function to be applied to each submodule\n",
       "\n",
       "Returns:\n",
       "    Module: self\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> @torch.no_grad()\n",
       "    >>> def init_weights(m):\n",
       "    >>>     print(m)\n",
       "    >>>     if type(m) == nn.Linear:\n",
       "    >>>         m.weight.fill_(1.0)\n",
       "    >>>         print(m.weight)\n",
       "    >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
       "    >>> net.apply(init_weights)\n",
       "    Linear(in_features=2, out_features=2, bias=True)\n",
       "    Parameter containing:\n",
       "    tensor([[1., 1.],\n",
       "            [1., 1.]], requires_grad=True)\n",
       "    Linear(in_features=2, out_features=2, bias=True)\n",
       "    Parameter containing:\n",
       "    tensor([[1., 1.],\n",
       "            [1., 1.]], requires_grad=True)\n",
       "    Sequential(\n",
       "      (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "      (1): Linear(in_features=2, out_features=2, bias=True)\n",
       "    )\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torch/nn/modules/module.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.nn.Module.apply?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15863dc7-3c08-40be-824e-8eed4b0d0097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def init_weights(m):\n",
    "    print(m)\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        m.weight.fill_(1.0)\n",
    "        print(m.weight)\n",
    "net = torch.nn.Sequential(torch.nn.Linear(2, 2), torch.nn.Linear(2, 2))\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3678f995-621e-4576-a061-a9ade3fb3a14",
   "metadata": {},
   "source": [
    "::: {.callout-important}\n",
    "\n",
    "`@torch.no_grad()`는 데코레이터(decorator)\n",
    "\n",
    "- 해당 함수 또는 메서드를 실행할 때 **그래디언트 계산을 비활성화**하는 역할\n",
    "- PyTorch의 자동 미분(autograd) 기능을 사용하는 경우 유용한 기능\n",
    "\n",
    "\n",
    "`@torch.~`는 데코레이터로서 함수나 클래스를 수정 혹은 래핑하는 역할\n",
    "\n",
    "- 데코레이는 토치 공식 홈페이지에 섹션은 따로 없고 해당 함수 들어가면 쓰는 법만 나와 있음\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16865092-826f-4453-813e-b0b37f933dc9",
   "metadata": {},
   "source": [
    "## bfloat16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1b63833-38a4-4a0b-b3ef-807ec2952949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
       "\n",
       ".. note::\n",
       "    This method modifies the module in-place.\n",
       "\n",
       "Returns:\n",
       "    Module: self\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torch/nn/modules/module.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.nn.Module.bfloat16?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5dfc28-5926-492e-93bc-ccea38ced95f",
   "metadata": {},
   "source": [
    "데이터 타입 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3650c333-6f07-4fdd-b6c1-8106dccb0f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1415)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(3.1415);x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9ce81eb-a125-4430-b980-713a56883ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1406, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfloat_x = x.bfloat16();bfloat_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ed5014d-6145-4ff2-a0fd-4f67488c0e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1406, dtype=torch.bfloat16)\n",
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(bfloat_x)\n",
    "print(bfloat_x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeb625f-e595-40c6-a200-3be3d1ff502a",
   "metadata": {},
   "source": [
    "## buffers(recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cccef86b-23cf-4bb5-89eb-ce34cd87cc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Returns an iterator over module buffers.\n",
       "\n",
       "Args:\n",
       "    recurse (bool): if True, then yields buffers of this module\n",
       "        and all submodules. Otherwise, yields only buffers that\n",
       "        are direct members of this module.\n",
       "\n",
       "Yields:\n",
       "    torch.Tensor: module buffer\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> # xdoctest: +SKIP(\"undefined vars\")\n",
       "    >>> for buf in model.buffers():\n",
       "    >>>     print(type(buf), buf.size())\n",
       "    <class 'torch.Tensor'> (20L,)\n",
       "    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torch/nn/modules/module.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.nn.Module.buffers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaccf62e-ab1c-45c7-b89e-dcb1403498f3",
   "metadata": {},
   "source": [
    "모듈 내 일부 상태 저장, 학습 가능하지 않고 상수(평균 등) 저장하는데 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de58ed54-5b88-4b47-babe-3ebe740ef2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0242,  0.6775, -0.4657],\n",
      "        [ 1.4915,  2.1821,  1.5568],\n",
      "        [-1.0824,  0.0023, -0.0825]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.register_buffer('buffer1', torch.randn(3, 3))\n",
    "        self.submodule = torch.nn.Linear(10, 5)\n",
    "        self.register_buffer('buffer2', torch.zeros(2, 2))\n",
    "\n",
    "module = MyModule()\n",
    "\n",
    "# Iterate over buffers\n",
    "for buffer in module.buffers(recurse=True):\n",
    "    print(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a4e596-ae89-4940-bf0b-2003130cfb79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
