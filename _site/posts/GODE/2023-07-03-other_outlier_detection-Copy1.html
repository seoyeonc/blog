<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="SEOYEON CHOI">
<meta name="dcterms.date" content="2023-07-03">

<title>Seoyeon’s Blog for study - Other Outlier Detection</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar docked nav-fixed slimcontent">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Seoyeon’s Blog for study</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../about.html" aria-current="page">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/seoyeonc/blog/"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Other Outlier Detection</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title d-none d-lg-block">Other Outlier Detection</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">GODE</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>SEOYEON CHOI </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 3, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../about.html" class="sidebar-item-text sidebar-link">About</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Posts</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">FRAUD</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/FRAUD/2023-07-10-fraud_data.html" class="sidebar-item-text sidebar-link">Fraud data</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../posts/GCN/index.html" class="sidebar-item-text sidebar-link">GCN</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2022-12-29-STGCN-tutorial.html" class="sidebar-item-text sidebar-link"><strong>[IT-STGCN]</strong> STGCN 튜토리얼</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html" class="sidebar-item-text sidebar-link">1st ITSTGCN</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-01-20-Algorithm_traintest.html" class="sidebar-item-text sidebar-link">1st ST-GCN Example dividing train and test</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html" class="sidebar-item-text sidebar-link">2nd ITSTGCN</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-01-18-Algorithm_traintest_2.html" class="sidebar-item-text sidebar-link">2nd ST-GCN Example dividing train and test</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-05-27-AddingtheRecurrentGCNmodels.html" class="sidebar-item-text sidebar-link">Adding the RecurrentGCN models</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-01-26-guebin.html" class="sidebar-item-text sidebar-link">Class of Method</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-01-21-Class.html" class="sidebar-item-text sidebar-link">Class of Method</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html" class="sidebar-item-text sidebar-link">Class of Method(GNAR) lag 1</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html" class="sidebar-item-text sidebar-link">Class of Method(GNAR) lag 1 80% Missing repeat</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html" class="sidebar-item-text sidebar-link">Class of Method(GNAR) lag 2</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html" class="sidebar-item-text sidebar-link">Class of Method(WikiMath) lag 1</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html" class="sidebar-item-text sidebar-link">Class of Method(WikiMath) lag 4</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-03-20-data load, data save as pickle.html" class="sidebar-item-text sidebar-link">data load, data save as pickle</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-07-05-ITSTGCN_data_management.html" class="sidebar-item-text sidebar-link">Data management for ITSTGCN</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-06-13-DCRNN_simulation_table_reshape.html" class="sidebar-item-text sidebar-link">DCRNN_Simulation Tables_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-06-13-DCRNN_Simulation_boxplot_reshape.html" class="sidebar-item-text sidebar-link">DCRNN_Simulation_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-06-28-DYGRENCODER_simulation_table_reshape.html" class="sidebar-item-text sidebar-link">DYGRENCODER_Simulation Tables_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-06-28-DYGRENCODER_Simulation_boxplot_reshape.html" class="sidebar-item-text sidebar-link">DYGRENCODER_Simulation_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-05-20-EbayesThresh toy ex.html" class="sidebar-item-text sidebar-link">EbayesThresh Toy ex</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-07-01-EvolveGCNH_simulation_table_reshape.html" class="sidebar-item-text sidebar-link">EvolveGCNH_Simulation Tables_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-07-01-EvolveGCNH_Simulation_boxplot_reshape.html" class="sidebar-item-text sidebar-link">EvolveGCNH_Simulation_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-06-25-EvolveGCNO_simulation_table_reshape.html" class="sidebar-item-text sidebar-link">EvolveGCNO_Simulation Tables_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-06-25-EvolveGCNO_Simulation_boxplot_reshape.html" class="sidebar-item-text sidebar-link">EvolveGCNO_Simulation_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-06-06-GCLSTM_simulation_table_reshape.html" class="sidebar-item-text sidebar-link">GCLSTM_Simulation Tables_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-06-06-GCLSTM_Simulation_boxplot_reshape.html" class="sidebar-item-text sidebar-link">GCLSTM_Simulation_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-01-11-Algorithm_EX_1.html" class="sidebar-item-text sidebar-link">GCN Algorithm Example 1</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-05-25-GCONVGRU_simulation_table_reshape.html" class="sidebar-item-text sidebar-link">GConvGRU and GNAR_Simulation Tables_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-05-25-GCONVGRU_snd_Simulation_boxplot_reshape.html" class="sidebar-item-text sidebar-link">GConvGRU_Simulation Boxplot_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-05-17-xx_GCONVGRU_simulation_table_reshape.html" class="sidebar-item-text sidebar-link">GConvGRU_Simulation Tables_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-05-30-GConvLSTM_simulation_table_reshape.html" class="sidebar-item-text sidebar-link">GConvLSTM_Simulation Tables_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-05-30-GConvLSTM_Simulation_boxplot_reshape.html" class="sidebar-item-text sidebar-link">GConvLSTM_Simulation_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-01-05-GNAR.html" class="sidebar-item-text sidebar-link">GNAR data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2099-05-31-Other Method.html" class="sidebar-item-text sidebar-link">ITSTGCN add Model</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-05-06-article_refer.html" class="sidebar-item-text sidebar-link">ITSTGCN Article Refernece</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-03-17-ITSTGCN-Tutorial.html" class="sidebar-item-text sidebar-link">ITSTGCN-Tutorial</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-06-19-LRGCN_simulation_table_reshape.html" class="sidebar-item-text sidebar-link">LRGCN_Simulation Tables_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-06-19-LRGCN_Simulation_boxplot_reshape.html" class="sidebar-item-text sidebar-link">LRGCN_Simulation_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-04-06-METRLADatasetLoader.html" class="sidebar-item-text sidebar-link">METRLADatasetLoader-Tutorial</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-04-25-note_matrix.html" class="sidebar-item-text sidebar-link">Note_weight amatrix</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-04-29-pedalme_GSO_st.html" class="sidebar-item-text sidebar-link">Padalme GSO_st</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-05-11-CPUvsGPU.html" class="sidebar-item-text sidebar-link">PyG Geometric Temporal CPU vs GPU</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-05-11-PyGGeometricTemporalEx.html" class="sidebar-item-text sidebar-link">PyG Geometric Temporal Examples</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2022-12-21-ST-GCN_Dataset.html" class="sidebar-item-text sidebar-link">PyTorch ST-GCN Dataset</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-05-04-questions of pytorch geometric temporal.html" class="sidebar-item-text sidebar-link">Questions of PyTorch Geometric Temporal</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-05-18-Self Consistency toy ex.html" class="sidebar-item-text sidebar-link">Self Consistency Toy ex</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2099-03-22-SimulationPlanner-Tutorial_test_test.html" class="sidebar-item-text sidebar-link">SimualtionPlanner-Tutorial</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2099-03-18-SimulationPlanner-Tutorial.html" class="sidebar-item-text sidebar-link">SimualtionPlanner-Tutorial</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-04-05-Simulation_boxplot.html" class="sidebar-item-text sidebar-link">Simulation</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-04-05-Simulation.html" class="sidebar-item-text sidebar-link">Simulation</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2022-12-28-gcn_simulation.html" class="sidebar-item-text sidebar-link">Simulation of geometric-temporal</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-04-27-simulation_table.html" class="sidebar-item-text sidebar-link">Simulation Tables</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-05-17-itstgcntry)fail_version__Simulation_boxplot_reshape.html" class="sidebar-item-text sidebar-link">Simulation_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-06-04-Sparse_matrix.html" class="sidebar-item-text sidebar-link">Sparse matrix</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html" class="sidebar-item-text sidebar-link">SY 1st ITSTGCN</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-06-20-TGCN_simulation_table_reshape.html" class="sidebar-item-text sidebar-link">TGCN_Simulation Tables_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-06-20-TGCN_Simulation_boxplot_reshape.html" class="sidebar-item-text sidebar-link">TGCN_Simulation_reshape</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2022-12-07-torchgcn.html" class="sidebar-item-text sidebar-link">TORCH_GEOMETRIC.NN</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-07-04-toy_example_figure.html" class="sidebar-item-text sidebar-link">Toy Example Figure(Intro)</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-04-27-toy_example_notes.html" class="sidebar-item-text sidebar-link">Toy Example Note</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GCN/2023-07-08-toy_example_using_gnar.html" class="sidebar-item-text sidebar-link">Toy example using GNAR</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../posts/GODE/index.html" class="sidebar-item-text sidebar-link">GODE</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GODE/2022-11-19-class_code_for_paper.html" class="sidebar-item-text sidebar-link">Class code for Comparison Study</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GODE/2023-06-22-comparison_earthquake.html" class="sidebar-item-text sidebar-link">Comparison Results on Real Data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GODE/2022-12-27-DFT_study.html" class="sidebar-item-text sidebar-link">Discrete Fourier Transform</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GODE/2022-10-02-Earthquake_real.html" class="sidebar-item-text sidebar-link">Earthquake</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GODE/2022-12-01-graph_code_guebin.html" class="sidebar-item-text sidebar-link">Graph code</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GODE/2023-06-27-Linear_graph_code_for_paper.html" class="sidebar-item-text sidebar-link">Linear Graph code for Paper</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GODE/2023-07-03-other_outlier_detection.html" class="sidebar-item-text sidebar-link">Other Outlier Detection</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GODE/2023-07-03-other_outlier_detection-Copy1.html" class="sidebar-item-text sidebar-link active">Other Outlier Detection</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GODE/2022-09-02-paper_simulation.html" class="sidebar-item-text sidebar-link">Simulation</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/GODE/Untitled.html" class="sidebar-item-text sidebar-link">Untitled</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../posts/STOCK/index.html" class="sidebar-item-text sidebar-link">STOCK</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/STOCK/2023-07-07-Stock_Crawling.html" class="sidebar-item-text sidebar-link">Stock Crawling</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/STOCK/2023-07-08-stock_on_graph.html" class="sidebar-item-text sidebar-link">Stock on Graph</a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#import" id="toc-import" class="nav-link active" data-scroll-target="#import">Import</a>
  <ul class="collapse">
  <li><a href="#class-code" id="toc-class-code" class="nav-link" data-scroll-target="#class-code">Class Code</a></li>
  <li><a href="#linear-ebayesthresh" id="toc-linear-ebayesthresh" class="nav-link" data-scroll-target="#linear-ebayesthresh">Linear EbayesThresh</a></li>
  <li><a href="#linear" id="toc-linear" class="nav-link" data-scroll-target="#linear">Linear</a>
  <ul class="collapse">
  <li><a href="#gode" id="toc-gode" class="nav-link" data-scroll-target="#gode">GODE</a></li>
  <li><a href="#lofbreunig2000lofstar" id="toc-lofbreunig2000lofstar" class="nav-link" data-scroll-target="#lofbreunig2000lofstar">LOF<span class="citation" data-cites="breunig2000lof">(Breunig et al. 2000)</span><span class="math inline">\(\star\)</span></a></li>
  <li><a href="#knn" id="toc-knn" class="nav-link" data-scroll-target="#knn">KNN</a></li>
  <li><a href="#cblof오류" id="toc-cblof오류" class="nav-link" data-scroll-target="#cblof오류">CBLOF(오류)</a></li>
  <li><a href="#ocsvm" id="toc-ocsvm" class="nav-link" data-scroll-target="#ocsvm">OCSVM</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#ocsvm-iteration" id="toc-ocsvm-iteration" class="nav-link" data-scroll-target="#ocsvm-iteration">OCSVM iteration</a>
  <ul class="collapse">
  <li><a href="#mcdstar" id="toc-mcdstar" class="nav-link" data-scroll-target="#mcdstar">MCD<span class="math inline">\(\star\)</span></a></li>
  <li><a href="#feature-baggingstar" id="toc-feature-baggingstar" class="nav-link" data-scroll-target="#feature-baggingstar">Feature Bagging<span class="math inline">\(\star\)</span></a></li>
  <li><a href="#abodstar" id="toc-abodstar" class="nav-link" data-scroll-target="#abodstar">ABOD<span class="math inline">\(\star\)</span></a></li>
  <li><a href="#iforeststar" id="toc-iforeststar" class="nav-link" data-scroll-target="#iforeststar">IForest<span class="math inline">\(\star\)</span></a></li>
  <li><a href="#hbosstar" id="toc-hbosstar" class="nav-link" data-scroll-target="#hbosstar">HBOS<span class="math inline">\(\star\)</span></a></li>
  <li><a href="#sosstar" id="toc-sosstar" class="nav-link" data-scroll-target="#sosstar">SOS<span class="math inline">\(\star\)</span></a></li>
  <li><a href="#so_gaal" id="toc-so_gaal" class="nav-link" data-scroll-target="#so_gaal">SO_GAAL</a></li>
  <li><a href="#mo_gaalstar" id="toc-mo_gaalstar" class="nav-link" data-scroll-target="#mo_gaalstar">MO_GAAL<span class="math inline">\(\star\)</span></a></li>
  <li><a href="#lscpstar" id="toc-lscpstar" class="nav-link" data-scroll-target="#lscpstar">LSCP<span class="math inline">\(\star\)</span></a></li>
  <li><a href="#linear-result" id="toc-linear-result" class="nav-link" data-scroll-target="#linear-result">Linear Result</a></li>
  <li><a href="#orbit-ebayesthresh" id="toc-orbit-ebayesthresh" class="nav-link" data-scroll-target="#orbit-ebayesthresh">Orbit EbayesThresh</a></li>
  <li><a href="#orbit" id="toc-orbit" class="nav-link" data-scroll-target="#orbit">Orbit</a>
  <ul class="collapse">
  <li><a href="#gode-1" id="toc-gode-1" class="nav-link" data-scroll-target="#gode-1">GODE</a></li>
  <li><a href="#lofstar" id="toc-lofstar" class="nav-link" data-scroll-target="#lofstar">LOF<span class="math inline">\(\star\)</span></a></li>
  <li><a href="#knn-1" id="toc-knn-1" class="nav-link" data-scroll-target="#knn-1">KNN</a></li>
  <li><a href="#cblof" id="toc-cblof" class="nav-link" data-scroll-target="#cblof">CBLOF</a></li>
  <li><a href="#ocsvm-1" id="toc-ocsvm-1" class="nav-link" data-scroll-target="#ocsvm-1">OCSVM</a></li>
  <li><a href="#mcdstar-1" id="toc-mcdstar-1" class="nav-link" data-scroll-target="#mcdstar-1">MCD<span class="math inline">\(\star\)</span></a></li>
  <li><a href="#feature-baggingstar-1" id="toc-feature-baggingstar-1" class="nav-link" data-scroll-target="#feature-baggingstar-1">Feature Bagging<span class="math inline">\(\star\)</span></a></li>
  <li><a href="#abodstar-1" id="toc-abodstar-1" class="nav-link" data-scroll-target="#abodstar-1">ABOD<span class="math inline">\(\star\)</span></a></li>
  <li><a href="#iforeststar-1" id="toc-iforeststar-1" class="nav-link" data-scroll-target="#iforeststar-1">IForest<span class="math inline">\(\star\)</span></a></li>
  <li><a href="#hbosstar-1" id="toc-hbosstar-1" class="nav-link" data-scroll-target="#hbosstar-1">HBOS<span class="math inline">\(\star\)</span></a></li>
  <li><a href="#sosstar-1" id="toc-sosstar-1" class="nav-link" data-scroll-target="#sosstar-1">SOS<span class="math inline">\(\star\)</span></a></li>
  <li><a href="#so_gaalstar" id="toc-so_gaalstar" class="nav-link" data-scroll-target="#so_gaalstar">SO_GAAL<span class="math inline">\(\star\)</span></a></li>
  <li><a href="#mo_gaalstar-1" id="toc-mo_gaalstar-1" class="nav-link" data-scroll-target="#mo_gaalstar-1">MO_GAAL<span class="math inline">\(\star\)</span></a></li>
  <li><a href="#lscpstar-1" id="toc-lscpstar-1" class="nav-link" data-scroll-target="#lscpstar-1">LSCP<span class="math inline">\(\star\)</span></a></li>
  </ul></li>
  <li><a href="#orbit-result" id="toc-orbit-result" class="nav-link" data-scroll-target="#orbit-result">Orbit Result</a></li>
  <li><a href="#bunny" id="toc-bunny" class="nav-link" data-scroll-target="#bunny">Bunny</a>
  <ul class="collapse">
  <li><a href="#bunny-저장용" id="toc-bunny-저장용" class="nav-link" data-scroll-target="#bunny-저장용">bunny 저장용</a></li>
  <li><a href="#gode-2" id="toc-gode-2" class="nav-link" data-scroll-target="#gode-2">GODE</a></li>
  <li><a href="#lof" id="toc-lof" class="nav-link" data-scroll-target="#lof">LOF</a></li>
  <li><a href="#knn-2" id="toc-knn-2" class="nav-link" data-scroll-target="#knn-2">KNN</a></li>
  <li><a href="#cblof-1" id="toc-cblof-1" class="nav-link" data-scroll-target="#cblof-1">CBLOF</a></li>
  <li><a href="#ocsvm-2" id="toc-ocsvm-2" class="nav-link" data-scroll-target="#ocsvm-2">OCSVM</a></li>
  <li><a href="#mcd" id="toc-mcd" class="nav-link" data-scroll-target="#mcd">MCD</a></li>
  <li><a href="#feature-bagging" id="toc-feature-bagging" class="nav-link" data-scroll-target="#feature-bagging">Feature Bagging</a></li>
  <li><a href="#abod" id="toc-abod" class="nav-link" data-scroll-target="#abod">ABOD</a></li>
  <li><a href="#iforest" id="toc-iforest" class="nav-link" data-scroll-target="#iforest">IForest</a></li>
  <li><a href="#hbos" id="toc-hbos" class="nav-link" data-scroll-target="#hbos">HBOS</a></li>
  <li><a href="#sos" id="toc-sos" class="nav-link" data-scroll-target="#sos">SOS</a></li>
  <li><a href="#so_gaal-1" id="toc-so_gaal-1" class="nav-link" data-scroll-target="#so_gaal-1">SO_GAAL</a></li>
  <li><a href="#mo_gaal" id="toc-mo_gaal" class="nav-link" data-scroll-target="#mo_gaal">MO_GAAL</a></li>
  <li><a href="#lscp" id="toc-lscp" class="nav-link" data-scroll-target="#lscp">LSCP</a></li>
  </ul></li>
  <li><a href="#bunny-result" id="toc-bunny-result" class="nav-link" data-scroll-target="#bunny-result">Bunny Result</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">




<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>knn, cblof, ocsvm 을 제외한 이상치 탐지 기법들에 데이터 집합에서 이상치 비율을 지정할 수 있는 옵션이 존재하였음.</p>
<p>default값은 10%인데, ABOD 방법에서는 5로 지정해주었고, 다른 방법들은 default인 10%가 들어갔다.</p>
<p>일단 우리 방법이랑 비교해서 좋은지 보기</p>
</div>
</div>
<ul>
<li>Simple Linear</li>
</ul>
<p><span class="math inline">\(U^\star\)</span>, which is a mixture of uniform distributions <span class="math inline">\(U(5,7)\)</span> and <span class="math inline">\(U(-7,-5)\)</span>.</p>
<table class="table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Simple Linear 논문</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">GODE</td>
<td style="text-align: center;"><strong>0.998</strong></td>
<td style="text-align: center;">0.998</td>
<td style="text-align: center;"><strong>1.000</strong></td>
<td style="text-align: center;"><strong>0.994</strong></td>
</tr>
<tr class="even">
<td style="text-align: center;">LOF (Breunig et al., 2000)</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">0.962</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.930</td>
</tr>
<tr class="odd">
<td style="text-align: center;">kNN (Ramaswamy et al., 2000)</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;"><strong>1.000</strong></td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.973</td>
</tr>
<tr class="even">
<td style="text-align: center;">CBLOF (He et al., 2003)</td>
<td style="text-align: center;">0.972</td>
<td style="text-align: center;">0.985</td>
<td style="text-align: center;">0.985</td>
<td style="text-align: center;">0.985</td>
</tr>
<tr class="odd">
<td style="text-align: center;">OCSVM (Sch ̈olkopf et al., 2001)</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">0.994</td>
<td style="text-align: center;">0.942</td>
<td style="text-align: center;">0.968</td>
</tr>
<tr class="even">
<td style="text-align: center;">MCD (Hardin and Rocke, 2004)</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;"><strong>1.000</strong></td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.973</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Feature Bagging (Lazarevic and Kumar, 2005)</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;"><strong>1.000</strong></td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.973</td>
</tr>
<tr class="even">
<td style="text-align: center;">ABOD (Kriegel et al., 2008)</td>
<td style="text-align: center;"><strong>0.988</strong></td>
<td style="text-align: center;">0.994</td>
<td style="text-align: center;">0.994</td>
<td style="text-align: center;"><strong>0.994</strong></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Isolation Forest (Liu et al., 2008)</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;"><strong>1.000</strong></td>
<td style="text-align: center;">0.883</td>
<td style="text-align: center;">0.938</td>
</tr>
<tr class="even">
<td style="text-align: center;">HBOS (Goldstein and Dengel, 2012)</td>
<td style="text-align: center;">0.960</td>
<td style="text-align: center;">0.978</td>
<td style="text-align: center;">0.980</td>
<td style="text-align: center;">0.979</td>
</tr>
<tr class="odd">
<td style="text-align: center;">SOS (Janssens et al., 2012)</td>
<td style="text-align: center;">0.960</td>
<td style="text-align: center;">0.978</td>
<td style="text-align: center;">0.980</td>
<td style="text-align: center;">0.979</td>
</tr>
<tr class="even">
<td style="text-align: center;">SO-GAAL (Liu et al., 2019)</td>
<td style="text-align: center;">0.895</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.943</td>
</tr>
<tr class="odd">
<td style="text-align: center;">MO-GAAL (Liu et al., 2019)</td>
<td style="text-align: center;">0.896</td>
<td style="text-align: center;">0.970</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.944</td>
</tr>
<tr class="even">
<td style="text-align: center;">LSCP (Zhao et al., 2019)</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;"><strong>1.000</strong></td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.973</td>
</tr>
</tbody>
</table>
<div class="cell" data-execution_count="1628">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>fourteen.<span class="bu">round</span>(<span class="dv">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1628">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Accuracy</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>GODE</th>
      <td>0.998</td>
      <td>0.999</td>
      <td>0.999</td>
      <td>0.999</td>
    </tr>
    <tr>
      <th>LOF (Breunig et al., 2000)</th>
      <td>0.926</td>
      <td>0.961</td>
      <td>0.961</td>
      <td>0.961</td>
    </tr>
    <tr>
      <th>kNN (Ramaswamy et al., 2000)</th>
      <td>0.950</td>
      <td>1.000</td>
      <td>0.947</td>
      <td>0.973</td>
    </tr>
    <tr>
      <th>OCSVM (Sch ̈olkopf et al., 2001)</th>
      <td>0.935</td>
      <td>0.991</td>
      <td>0.940</td>
      <td>0.965</td>
    </tr>
    <tr>
      <th>MCD (Hardin and Rocke, 2004)</th>
      <td>0.998</td>
      <td>0.999</td>
      <td>0.999</td>
      <td>0.999</td>
    </tr>
    <tr>
      <th>Feature Bagging (Lazarevic and Kumar, 2005)</th>
      <td>0.986</td>
      <td>0.993</td>
      <td>0.993</td>
      <td>0.993</td>
    </tr>
    <tr>
      <th>ABOD (Kriegel et al., 2008)</th>
      <td>0.988</td>
      <td>0.994</td>
      <td>0.994</td>
      <td>0.994</td>
    </tr>
    <tr>
      <th>Isolation Forest (Liu et al., 2008)</th>
      <td>0.868</td>
      <td>0.999</td>
      <td>0.862</td>
      <td>0.925</td>
    </tr>
    <tr>
      <th>HBOS (Goldstein and Dengel, 2012)</th>
      <td>0.960</td>
      <td>0.978</td>
      <td>0.980</td>
      <td>0.979</td>
    </tr>
    <tr>
      <th>SOS (Janssens et al., 2012)</th>
      <td>0.916</td>
      <td>0.956</td>
      <td>0.956</td>
      <td>0.956</td>
    </tr>
    <tr>
      <th>SO-GAAL (Liu et al., 2019)</th>
      <td>0.936</td>
      <td>0.966</td>
      <td>0.966</td>
      <td>0.966</td>
    </tr>
    <tr>
      <th>MO-GAAL (Liu et al., 2019)</th>
      <td>0.940</td>
      <td>0.965</td>
      <td>0.972</td>
      <td>0.969</td>
    </tr>
    <tr>
      <th>LSCP (Zhao et al., 2019)</th>
      <td>0.988</td>
      <td>0.994</td>
      <td>0.994</td>
      <td>0.994</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<ul>
<li>Orbit</li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">Orbit 논문</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">GODE</td>
<td style="text-align: center;"><strong>0.997</strong></td>
<td style="text-align: center;">0.997</td>
<td style="text-align: center;"><strong>1.000</strong></td>
<td style="text-align: center;"><strong>0.998</strong></td>
</tr>
<tr class="even">
<td style="text-align: center;">LOF (Breunig et al., 2000)</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.987</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.937</td>
</tr>
<tr class="odd">
<td style="text-align: center;">kNN (Ramaswamy et al., 2000)</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;"><strong>0.999</strong></td>
<td style="text-align: center;">0.946</td>
<td style="text-align: center;">0.972</td>
</tr>
<tr class="even">
<td style="text-align: center;">CBLOF (He et al., 2003)</td>
<td style="text-align: center;">0.918</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.957</td>
</tr>
<tr class="odd">
<td style="text-align: center;">OCSVM (Sch ̈olkopf et al., 2001)</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.988</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.958</td>
</tr>
<tr class="even">
<td style="text-align: center;">MCD (Hardin and Rocke, 2004)</td>
<td style="text-align: center;">0.866</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">0.928</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Feature Bagging (Lazarevic and Kumar, 2005)</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.979</td>
<td style="text-align: center;">0.927</td>
<td style="text-align: center;">0.952</td>
</tr>
<tr class="even">
<td style="text-align: center;">ABOD (Kriegel et al., 2008)</td>
<td style="text-align: center;">0.988</td>
<td style="text-align: center;">0.994</td>
<td style="text-align: center;">0.994</td>
<td style="text-align: center;">0.994</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Isolation Forest (Liu et al., 2008)</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.997</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.514</td>
</tr>
<tr class="even">
<td style="text-align: center;">HBOS (Goldstein and Dengel, 2012)</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.936</td>
</tr>
<tr class="odd">
<td style="text-align: center;">SOS (Janssens et al., 2012)</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.936</td>
</tr>
<tr class="even">
<td style="text-align: center;">SO-GAAL (Liu et al., 2019)</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.908</td>
<td style="text-align: center;">0.933</td>
</tr>
<tr class="odd">
<td style="text-align: center;">MO-GAAL (Liu et al., 2019)</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;"><strong>1.000</strong></td>
<td style="text-align: center;">0.974</td>
</tr>
<tr class="even">
<td style="text-align: center;">LSCP (Zhao et al., 2019)</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;"><strong>0.999</strong></td>
<td style="text-align: center;">0.946</td>
<td style="text-align: center;">0.972</td>
</tr>
</tbody>
</table>
<div class="cell" data-execution_count="1739">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>fourteen.<span class="bu">round</span>(<span class="dv">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1739">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Accuracy</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>GODE</th>
      <td>0.998</td>
      <td>0.999</td>
      <td>0.999</td>
      <td>0.999</td>
    </tr>
    <tr>
      <th>LOF (Breunig et al., 2000)</th>
      <td>0.954</td>
      <td>0.976</td>
      <td>0.976</td>
      <td>0.976</td>
    </tr>
    <tr>
      <th>kNN (Ramaswamy et al., 2000)</th>
      <td>0.948</td>
      <td>0.999</td>
      <td>0.946</td>
      <td>0.972</td>
    </tr>
    <tr>
      <th>OCSVM (Sch ̈olkopf et al., 2001)</th>
      <td>0.908</td>
      <td>0.977</td>
      <td>0.925</td>
      <td>0.950</td>
    </tr>
    <tr>
      <th>MCD (Hardin and Rocke, 2004)</th>
      <td>0.916</td>
      <td>0.956</td>
      <td>0.956</td>
      <td>0.956</td>
    </tr>
    <tr>
      <th>Feature Bagging (Lazarevic and Kumar, 2005)</th>
      <td>0.942</td>
      <td>0.969</td>
      <td>0.969</td>
      <td>0.969</td>
    </tr>
    <tr>
      <th>ABOD (Kriegel et al., 2008)</th>
      <td>0.988</td>
      <td>0.994</td>
      <td>0.994</td>
      <td>0.994</td>
    </tr>
    <tr>
      <th>Isolation Forest (Liu et al., 2008)</th>
      <td>0.443</td>
      <td>0.992</td>
      <td>0.417</td>
      <td>0.587</td>
    </tr>
    <tr>
      <th>HBOS (Goldstein and Dengel, 2012)</th>
      <td>0.935</td>
      <td>0.960</td>
      <td>0.973</td>
      <td>0.966</td>
    </tr>
    <tr>
      <th>SOS (Janssens et al., 2012)</th>
      <td>0.950</td>
      <td>0.974</td>
      <td>0.974</td>
      <td>0.974</td>
    </tr>
    <tr>
      <th>SO-GAAL (Liu et al., 2019)</th>
      <td>0.950</td>
      <td>0.950</td>
      <td>1.000</td>
      <td>0.974</td>
    </tr>
    <tr>
      <th>MO-GAAL (Liu et al., 2019)</th>
      <td>0.950</td>
      <td>0.950</td>
      <td>1.000</td>
      <td>0.974</td>
    </tr>
    <tr>
      <th>LSCP (Zhao et al., 2019)</th>
      <td>0.988</td>
      <td>0.994</td>
      <td>0.994</td>
      <td>0.994</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<ul>
<li>Stanford Bunny</li>
</ul>
<p><span class="math inline">\(U^\star\)</span>, which is a mixture of uniform distributions <span class="math inline">\(U(3,7)\)</span> and <span class="math inline">\(U(-7,-3)\)</span>.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">Stanford Bunny 논문</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">GODE</td>
<td style="text-align: center;"><strong>0.995</strong></td>
<td style="text-align: center;">0.995</td>
<td style="text-align: center;">0.999</td>
<td style="text-align: center;"><strong>0.997</strong></td>
</tr>
<tr class="even">
<td style="text-align: center;">LOF (Breunig et al., 2000)</td>
<td style="text-align: center;">0.928</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.869</td>
<td style="text-align: center;">0.963</td>
</tr>
<tr class="odd">
<td style="text-align: center;">kNN (Ramaswamy et al., 2000)</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;"><strong>0.996</strong></td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">0.968</td>
</tr>
<tr class="even">
<td style="text-align: center;">CBLOF (He et al., 2003)</td>
<td style="text-align: center;">0.978</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.987</td>
<td style="text-align: center;">0.988</td>
</tr>
<tr class="odd">
<td style="text-align: center;">OCSVM (Sch ̈olkopf et al., 2001)</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.937</td>
<td style="text-align: center;">0.963</td>
</tr>
<tr class="even">
<td style="text-align: center;">MCD (Hardin and Rocke, 2004)</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.993</td>
<td style="text-align: center;">0.938</td>
<td style="text-align: center;">0.965</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Feature Bagging (Lazarevic and Kumar, 2005)</td>
<td style="text-align: center;">0.915</td>
<td style="text-align: center;">0.982</td>
<td style="text-align: center;">0.928</td>
<td style="text-align: center;">0.965</td>
</tr>
<tr class="even">
<td style="text-align: center;">ABOD (Kriegel et al., 2008)</td>
<td style="text-align: center;">0.977</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.987</td>
<td style="text-align: center;">0.988</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Isolation Forest (Liu et al., 2008)</td>
<td style="text-align: center;">0.794</td>
<td style="text-align: center;">0.995</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.879</td>
</tr>
<tr class="even">
<td style="text-align: center;">HBOS (Goldstein and Dengel, 2012)</td>
<td style="text-align: center;">0.895</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.944</td>
</tr>
<tr class="odd">
<td style="text-align: center;">SOS (Janssens et al., 2012)</td>
<td style="text-align: center;">0.895</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.944</td>
</tr>
<tr class="even">
<td style="text-align: center;">SO-GAAL (Liu et al., 2019)</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;"><strong>1.000</strong></td>
<td style="text-align: center;">0.975</td>
</tr>
<tr class="odd">
<td style="text-align: center;">MO-GAAL (Liu et al., 2019)</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;"><strong>1.000</strong></td>
<td style="text-align: center;">0.975</td>
</tr>
<tr class="even">
<td style="text-align: center;">LSCP (Zhao et al., 2019)</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;"><strong>0.996</strong></td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">0.967</td>
</tr>
</tbody>
</table>
<div class="cell" data-execution_count="1852">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>fourteen.<span class="bu">round</span>(<span class="dv">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1852">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Accuracy</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>GODE</th>
      <td>0.988</td>
      <td>0.995</td>
      <td>0.993</td>
      <td>0.994</td>
    </tr>
    <tr>
      <th>LOF (Breunig et al., 2000)</th>
      <td>0.913</td>
      <td>0.955</td>
      <td>0.953</td>
      <td>0.954</td>
    </tr>
    <tr>
      <th>kNN (Ramaswamy et al., 2000)</th>
      <td>0.942</td>
      <td>0.997</td>
      <td>0.942</td>
      <td>0.969</td>
    </tr>
    <tr>
      <th>OCSVM (Sch ̈olkopf et al., 2001)</th>
      <td>0.935</td>
      <td>0.992</td>
      <td>0.939</td>
      <td>0.965</td>
    </tr>
    <tr>
      <th>MCD (Hardin and Rocke, 2004)</th>
      <td>0.982</td>
      <td>0.992</td>
      <td>0.989</td>
      <td>0.990</td>
    </tr>
    <tr>
      <th>Feature Bagging (Lazarevic and Kumar, 2005)</th>
      <td>0.954</td>
      <td>0.977</td>
      <td>0.974</td>
      <td>0.976</td>
    </tr>
    <tr>
      <th>ABOD (Kriegel et al., 2008)</th>
      <td>0.979</td>
      <td>0.990</td>
      <td>0.988</td>
      <td>0.989</td>
    </tr>
    <tr>
      <th>Isolation Forest (Liu et al., 2008)</th>
      <td>0.827</td>
      <td>0.995</td>
      <td>0.822</td>
      <td>0.900</td>
    </tr>
    <tr>
      <th>HBOS (Goldstein and Dengel, 2012)</th>
      <td>0.919</td>
      <td>0.958</td>
      <td>0.956</td>
      <td>0.957</td>
    </tr>
    <tr>
      <th>SOS (Janssens et al., 2012)</th>
      <td>0.912</td>
      <td>0.955</td>
      <td>0.953</td>
      <td>0.954</td>
    </tr>
    <tr>
      <th>SO-GAAL (Liu et al., 2019)</th>
      <td>0.952</td>
      <td>0.952</td>
      <td>1.000</td>
      <td>0.975</td>
    </tr>
    <tr>
      <th>MO-GAAL (Liu et al., 2019)</th>
      <td>0.952</td>
      <td>0.952</td>
      <td>1.000</td>
      <td>0.975</td>
    </tr>
    <tr>
      <th>LSCP (Zhao et al., 2019)</th>
      <td>0.978</td>
      <td>0.990</td>
      <td>0.987</td>
      <td>0.989</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<section id="import" class="level1 page-columns page-full">
<h1>Import</h1>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> OneClassSVM</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> SGDOneClassSVM</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.kernel_approximation <span class="im">import</span> Nystroem</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> LocalOutlierFactor</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> rpy2</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> rpy2.robjects <span class="im">as</span> ro </span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> rpy2.robjects.vectors <span class="im">import</span> FloatVector </span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> rpy2.robjects.packages <span class="im">import</span> importr</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_kddcup99, fetch_covtype, fetch_openml</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelBinarizer</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tqdm</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pygsp <span class="im">import</span> graphs, filters, plotting, utils</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_score, recall_score, f1_score, accuracy_score</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.covariance <span class="im">import</span> EmpiricalCovariance, MinCovDet</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> alibi_detect.od <span class="im">import</span> IForest</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="co"># from pyod.models.iforest import IForest</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyod.models.abod <span class="im">import</span> ABOD</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyod.models.cblof <span class="im">import</span> CBLOF</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PyNomaly <span class="im">import</span> loop</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> svm</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyod.models.lscp <span class="im">import</span> LSCP</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyod.models.hbos <span class="im">import</span> HBOS</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyod.models.so_gaal <span class="im">import</span> SO_GAAL</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyod.models.mcd <span class="im">import</span> MCD</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyod.models.mo_gaal <span class="im">import</span> MO_GAAL</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyod.models.knn <span class="im">import</span> KNN</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyod.models.lof <span class="im">import</span> LOF</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyod.models.ocsvm <span class="im">import</span> OCSVM</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyod.models.feature_bagging <span class="im">import</span> FeatureBagging</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyod.models.sos <span class="im">import</span> SOS</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm</code></pre>
</div>
</div>
<section id="class-code" class="level2">
<h2 class="anchored" data-anchor-id="class-code">Class Code</h2>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>tab_linear <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">"Accuracy"</span>,<span class="st">"Precision"</span>,<span class="st">"Recall"</span>,<span class="st">"F1"</span>])</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>tab_orbit <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">"Accuracy"</span>,<span class="st">"Precision"</span>,<span class="st">"Recall"</span>,<span class="st">"F1"</span>])</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>tab_bunny <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">"Accuracy"</span>,<span class="st">"Precision"</span>,<span class="st">"Recall"</span>,<span class="st">"F1"</span>])</span></code></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Conf_matrx:</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,original,compare,tab):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.original <span class="op">=</span> original</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.compare <span class="op">=</span> compare</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tab <span class="op">=</span> tab</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> conf(<span class="va">self</span>,name):</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conf_matrix <span class="op">=</span> confusion_matrix(<span class="va">self</span>.original, <span class="va">self</span>.compare)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        ax.matshow(<span class="va">self</span>.conf_matrix, cmap<span class="op">=</span>plt.cm.Oranges, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.conf_matrix.shape[<span class="dv">0</span>]):</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.conf_matrix.shape[<span class="dv">1</span>]):</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>                ax.text(x<span class="op">=</span>j, y<span class="op">=</span>i,s<span class="op">=</span><span class="va">self</span>.conf_matrix[i, j], va<span class="op">=</span><span class="st">'center'</span>, ha<span class="op">=</span><span class="st">'center'</span>, size<span class="op">=</span><span class="st">'xx-large'</span>)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        plt.xlabel(<span class="st">'Predictions'</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        plt.ylabel(<span class="st">'Actuals'</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">'Confusion Matrix'</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.acc <span class="op">=</span> accuracy_score(<span class="va">self</span>.original, <span class="va">self</span>.compare)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pre <span class="op">=</span> precision_score(<span class="va">self</span>.original, <span class="va">self</span>.compare)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rec <span class="op">=</span> recall_score(<span class="va">self</span>.original, <span class="va">self</span>.compare)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.f1 <span class="op">=</span> f1_score(<span class="va">self</span>.original, <span class="va">self</span>.compare)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> <span class="va">self</span>.acc)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Precision: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> <span class="va">self</span>.pre)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Recall: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> <span class="va">self</span>.rec)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'F1 Score: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> <span class="va">self</span>.f1)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tab <span class="op">=</span> <span class="va">self</span>.tab.append(pd.DataFrame({<span class="st">"Accuracy"</span>:[<span class="va">self</span>.acc],<span class="st">"Precision"</span>:[<span class="va">self</span>.pre],<span class="st">"Recall"</span>:[<span class="va">self</span>.rec],<span class="st">"F1"</span>:[<span class="va">self</span>.f1]},index <span class="op">=</span> [name]))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Linear:</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,df):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.df <span class="op">=</span> df</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.y <span class="op">=</span> df.y.to_numpy()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        <span class="co">#self.y1 = df.y1.to_numpy()</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.x <span class="op">=</span> df.x.to_numpy()</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.y)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W <span class="op">=</span> w</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _eigen(<span class="va">self</span>):</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        d<span class="op">=</span> <span class="va">self</span>.W.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        D<span class="op">=</span> np.diag(d)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.L <span class="op">=</span> np.diag(<span class="dv">1</span><span class="op">/</span>np.sqrt(d)) <span class="op">@</span> (D<span class="op">-</span><span class="va">self</span>.W) <span class="op">@</span> np.diag(<span class="dv">1</span><span class="op">/</span>np.sqrt(d))</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lamb, <span class="va">self</span>.Psi <span class="op">=</span> np.linalg.eigh(<span class="va">self</span>.L)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Lamb <span class="op">=</span> np.diag(<span class="va">self</span>.lamb)      </span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>,sd<span class="op">=</span><span class="dv">20</span>): <span class="co"># fit with ebayesthresh</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._eigen()</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ybar <span class="op">=</span> <span class="va">self</span>.Psi.T <span class="op">@</span> <span class="va">self</span>.y <span class="co"># fbar := graph fourier transform of f</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.power <span class="op">=</span> <span class="va">self</span>.ybar<span class="op">**</span><span class="dv">2</span> </span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        ebayesthresh <span class="op">=</span> importr(<span class="st">'EbayesThresh'</span>).ebayesthresh</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.power_threshed<span class="op">=</span>np.array(ebayesthresh(FloatVector(<span class="va">self</span>.ybar<span class="op">**</span><span class="dv">2</span>),sd<span class="op">=</span>sd))</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ybar_threshed <span class="op">=</span> np.where(<span class="va">self</span>.power_threshed<span class="op">&gt;</span><span class="dv">0</span>,<span class="va">self</span>.ybar,<span class="dv">0</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.yhat <span class="op">=</span> <span class="va">self</span>.Psi<span class="op">@</span>self.ybar_threshed</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.df <span class="op">=</span> <span class="va">self</span>.df.assign(yHat <span class="op">=</span> <span class="va">self</span>.yhat)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.df <span class="op">=</span> <span class="va">self</span>.df.assign(Residual <span class="op">=</span> <span class="va">self</span>.df.y<span class="op">-</span> <span class="va">self</span>.df.yHat)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Orbit:</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,df):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.df <span class="op">=</span> df </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.f <span class="op">=</span> df.f.to_numpy()</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.x <span class="op">=</span> df.x.to_numpy()</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.y <span class="op">=</span> df.y.to_numpy()</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.f)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.theta<span class="op">=</span> <span class="va">None</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_distance(<span class="va">self</span>):</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.D <span class="op">=</span> np.zeros([<span class="va">self</span>.n,<span class="va">self</span>.n])</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        locations <span class="op">=</span> np.stack([<span class="va">self</span>.x, <span class="va">self</span>.y],axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> tqdm.tqdm(<span class="bu">range</span>(<span class="va">self</span>.n)):</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i,<span class="va">self</span>.n):</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.D[i,j]<span class="op">=</span>np.linalg.norm(locations[i]<span class="op">-</span>locations[j])</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.D <span class="op">=</span> <span class="va">self</span>.D <span class="op">+</span> <span class="va">self</span>.D.T</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_weightmatrix(<span class="va">self</span>,theta<span class="op">=</span><span class="dv">1</span>,beta<span class="op">=</span><span class="fl">0.5</span>,kappa<span class="op">=</span><span class="dv">4000</span>):</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.theta <span class="op">=</span> theta</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        dist <span class="op">=</span> np.where(<span class="va">self</span>.D <span class="op">&lt;</span> kappa,<span class="va">self</span>.D,<span class="dv">0</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W <span class="op">=</span> np.exp(<span class="op">-</span>(dist<span class="op">/</span><span class="va">self</span>.theta)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _eigen(<span class="va">self</span>):</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        d<span class="op">=</span> <span class="va">self</span>.W.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        D<span class="op">=</span> np.diag(d)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.L <span class="op">=</span> np.diag(<span class="dv">1</span><span class="op">/</span>np.sqrt(d)) <span class="op">@</span> (D<span class="op">-</span><span class="va">self</span>.W) <span class="op">@</span> np.diag(<span class="dv">1</span><span class="op">/</span>np.sqrt(d))</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lamb, <span class="va">self</span>.Psi <span class="op">=</span> np.linalg.eigh(<span class="va">self</span>.L)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Lamb <span class="op">=</span> np.diag(<span class="va">self</span>.lamb)       </span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>,sd<span class="op">=</span><span class="dv">5</span>,ref<span class="op">=</span><span class="dv">20</span>): <span class="co"># fit with ebayesthresh</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._eigen()</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fbar <span class="op">=</span> <span class="va">self</span>.Psi.T <span class="op">@</span> <span class="va">self</span>.f <span class="co"># fbar := graph fourier transform of f</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.power <span class="op">=</span> <span class="va">self</span>.fbar<span class="op">**</span><span class="dv">2</span> </span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>        ebayesthresh <span class="op">=</span> importr(<span class="st">'EbayesThresh'</span>).ebayesthresh</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.power_threshed<span class="op">=</span>np.array(ebayesthresh(FloatVector(<span class="va">self</span>.fbar<span class="op">**</span><span class="dv">2</span>),sd<span class="op">=</span>sd))</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fbar_threshed <span class="op">=</span> np.where(<span class="va">self</span>.power_threshed<span class="op">&gt;</span><span class="dv">0</span>,<span class="va">self</span>.fbar,<span class="dv">0</span>)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fhat <span class="op">=</span> <span class="va">self</span>.Psi<span class="op">@</span>self.fbar_threshed</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.df <span class="op">=</span> <span class="va">self</span>.df.assign(fHat <span class="op">=</span> <span class="va">self</span>.fhat)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.df <span class="op">=</span> <span class="va">self</span>.df.assign(Residual <span class="op">=</span> <span class="va">self</span>.df.f<span class="op">-</span> <span class="va">self</span>.df.fHat)</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottom <span class="op">=</span> np.zeros_like(<span class="va">self</span>.f)</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.width<span class="op">=</span><span class="fl">0.05</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.depth<span class="op">=</span><span class="fl">0.05</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BUNNY:</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,df):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.df <span class="op">=</span> df </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.f <span class="op">=</span> df.f.to_numpy()</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.z <span class="op">=</span> df.z.to_numpy()</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.x <span class="op">=</span> df.x.to_numpy()</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.y <span class="op">=</span> df.y.to_numpy()</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.noise <span class="op">=</span> df.noise.to_numpy()</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fnoise <span class="op">=</span> <span class="va">self</span>.f <span class="op">+</span> <span class="va">self</span>.noise</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W <span class="op">=</span> _W</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.f)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.theta<span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _eigen(<span class="va">self</span>):</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        d<span class="op">=</span> <span class="va">self</span>.W.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>        D<span class="op">=</span> np.diag(d)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.L <span class="op">=</span> np.diag(<span class="dv">1</span><span class="op">/</span>np.sqrt(d)) <span class="op">@</span> (D<span class="op">-</span><span class="va">self</span>.W) <span class="op">@</span> np.diag(<span class="dv">1</span><span class="op">/</span>np.sqrt(d))</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lamb, <span class="va">self</span>.Psi <span class="op">=</span> np.linalg.eigh(<span class="va">self</span>.L)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Lamb <span class="op">=</span> np.diag(<span class="va">self</span>.lamb)       </span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>,sd<span class="op">=</span><span class="dv">5</span>,ref<span class="op">=</span><span class="dv">6</span>): <span class="co"># fit with ebayesthresh</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._eigen()</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fbar <span class="op">=</span> <span class="va">self</span>.Psi.T <span class="op">@</span> <span class="va">self</span>.fnoise <span class="co"># fbar := graph fourier transform of f</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.power <span class="op">=</span> <span class="va">self</span>.fbar<span class="op">**</span><span class="dv">2</span> </span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        ebayesthresh <span class="op">=</span> importr(<span class="st">'EbayesThresh'</span>).ebayesthresh</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.power_threshed<span class="op">=</span>np.array(ebayesthresh(FloatVector(<span class="va">self</span>.fbar<span class="op">**</span><span class="dv">2</span>),sd<span class="op">=</span>sd))</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fbar_threshed <span class="op">=</span> np.where(<span class="va">self</span>.power_threshed<span class="op">&gt;</span><span class="dv">0</span>,<span class="va">self</span>.fbar,<span class="dv">0</span>)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fhat <span class="op">=</span> <span class="va">self</span>.Psi<span class="op">@</span>self.fbar_threshed</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.df <span class="op">=</span> <span class="va">self</span>.df.assign(fnoise <span class="op">=</span> <span class="va">self</span>.fnoise)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.df <span class="op">=</span> <span class="va">self</span>.df.assign(fHat <span class="op">=</span> <span class="va">self</span>.fhat)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.df <span class="op">=</span> <span class="va">self</span>.df.assign(Residual <span class="op">=</span> <span class="va">self</span>.df.f <span class="op">+</span> <span class="va">self</span>.df.noise <span class="op">-</span> <span class="va">self</span>.df.fHat)</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottom <span class="op">=</span> np.zeros_like(<span class="va">self</span>.f)</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.width<span class="op">=</span><span class="fl">0.05</span></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.depth<span class="op">=</span><span class="fl">0.05</span></span></code></pre></div>
</div>
</section>
<section id="linear-ebayesthresh" class="level2">
<h2 class="anchored" data-anchor-id="linear-ebayesthresh">Linear EbayesThresh</h2>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext rpy2.ipython</span></code></pre></div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>R</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>library(EbayesThresh)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span>.seed(<span class="dv">1</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> rnorm(<span class="dv">1000</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># signal_1 = sample(c(runif(25,-2,-1.5), runif(25,1.5,2), rep(0,950)))</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>signal_1 <span class="op">=</span> sample(c(runif(<span class="dv">25</span>,<span class="op">-</span><span class="dv">7</span>,<span class="op">-</span><span class="dv">5</span>), runif(<span class="dv">25</span>,<span class="dv">5</span>,<span class="dv">7</span>), rep(<span class="dv">0</span>,<span class="dv">950</span>)))</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>index_of_trueoutlier_1 <span class="op">=</span> which(signal_1<span class="op">!=</span><span class="dv">0</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>index_of_trueoutlier_1</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>x_1<span class="op">=</span>signal_1<span class="op">+</span>epsilon</span></code></pre></div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>R <span class="op">-</span>o x_1</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>R <span class="op">-</span>o index_of_trueoutlier_1</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>R <span class="op">-</span>o signal_1</span></code></pre></div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>ebayesthresh <span class="op">=</span> importr(<span class="st">'EbayesThresh'</span>).ebayesthresh</span></code></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>outlier_true_index_1 <span class="op">=</span> index_of_trueoutlier_1</span></code></pre></div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>outlier_true_value_1 <span class="op">=</span> x_1[index_of_trueoutlier_1]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>outlier_true_one_1 <span class="op">=</span> signal_1.copy()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>outlier_true_one_1 <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="op">-</span><span class="dv">1</span> <span class="cf">if</span> x<span class="op">!=</span><span class="dv">0</span> <span class="cf">else</span> <span class="dv">1</span>,outlier_true_one_1))</span></code></pre></div>
</div>
</section>
<section id="linear" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="linear">Linear</h2>
<div class="cell" data-tags="[]" data-execution_count="15">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>_x_1 <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1000</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>_y1_1 <span class="op">=</span> <span class="dv">5</span><span class="op">*</span>_x_1</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>_y_1 <span class="op">=</span> _y1_1 <span class="op">+</span> x_1 <span class="co"># x is epsilon</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>_df<span class="op">=</span>pd.DataFrame({<span class="st">'x'</span>:_x_1, <span class="st">'y'</span>:_y_1})</span></code></pre></div>
</div>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array(_df)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># _df.to_csv('simple_linear_df.csv')</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pd.DataFrame(outlier_true_one_1).to_csv('simple_linear_outlier.csv')</span></span></code></pre></div>
</div>
<section id="gode" class="level3">
<h3 class="anchored" data-anchor-id="gode">GODE</h3>
<div class="cell" data-execution_count="1490">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>w<span class="op">=</span>np.zeros((<span class="dv">1000</span>,<span class="dv">1000</span>))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1491">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i<span class="op">==</span>j :</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>            w[i,j] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> np.<span class="bu">abs</span>(i<span class="op">-</span>j) <span class="op">&lt;=</span> <span class="dv">1</span> : </span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>            w[i,j] <span class="op">=</span> <span class="dv">1</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="1492">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>_Linear <span class="op">=</span> Linear(_df)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1499">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>_Linear.fit(sd<span class="op">=</span><span class="dv">20</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1543">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>outlier_simul_one <span class="op">=</span> (_Linear.df[<span class="st">'Residual'</span>]<span class="op">**</span><span class="dv">2</span>).tolist()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1544">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>outlier_simul_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="op">-</span><span class="dv">1</span> <span class="cf">if</span> x <span class="op">&gt;</span> <span class="fl">9.8</span> <span class="cf">else</span> <span class="dv">1</span>,outlier_simul_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1545">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_1,outlier_simul_one,tab_linear)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1546">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>outlier_simul_one.count(<span class="dv">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1546">
<pre><code>950</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1547">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>outlier_simul_one.count(<span class="op">-</span><span class="dv">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1547">
<pre><code>50</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1548">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"GODE"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-33-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.998
Precision: 0.999
Recall: 0.999
F1 Score: 0.999</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1549">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>one <span class="op">=</span> _conf.tab</span></code></pre></div>
</div>
</section>
<section id="lofbreunig2000lofstar" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="lofbreunig2000lofstar">LOF<span class="citation" data-cites="breunig2000lof">(<a href="#ref-breunig2000lof" role="doc-biblioref">Breunig et al. 2000</a>)</span><span class="math inline">\(\star\)</span></h3>
<div class="no-row-height column-margin column-container"><div id="ref-breunig2000lof" class="csl-entry" role="doc-biblioentry">
Breunig, Markus M, Hans-Peter Kriegel, Raymond T Ng, and Jörg Sander. 2000. <span>“LOF: Identifying Density-Based Local Outliers.”</span> In <em>Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data</em>, 93–104.
</div></div><div class="cell" data-execution_count="1550">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LocalOutlierFactor(n_neighbors<span class="op">=</span><span class="dv">2</span>,contamination<span class="op">=</span><span class="fl">0.05</span>)</span></code></pre></div>
</div>
<p>Lof 논문 원문에 따라 LOF를 계산하고, min-max 범위를 넘으면 이상치</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>LocalOutlierFactor?</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Init signature:</span>
LocalOutlierFactor<span class="ansi-blue-fg">(</span>
    n_neighbors<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">20</span><span class="ansi-blue-fg">,</span>
    <span class="ansi-blue-fg">*</span><span class="ansi-blue-fg">,</span>
    algorithm<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">'auto'</span><span class="ansi-blue-fg">,</span>
    leaf_size<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">30</span><span class="ansi-blue-fg">,</span>
    metric<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">'minkowski'</span><span class="ansi-blue-fg">,</span>
    p<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">,</span>
    metric_params<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    contamination<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">'auto'</span><span class="ansi-blue-fg">,</span>
    novelty<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    n_jobs<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
<span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Docstring:</span>     
Unsupervised Outlier Detection using the Local Outlier Factor (LOF).
The anomaly score of each sample is called the Local Outlier Factor.
It measures the local deviation of the density of a given sample with respect
to its neighbors.
It is local in that the anomaly score depends on how isolated the object
is with respect to the surrounding neighborhood.
More precisely, locality is given by k-nearest neighbors, whose distance
is used to estimate the local density.
By comparing the local density of a sample to the local densities of its
neighbors, one can identify samples that have a substantially lower density
than their neighbors. These are considered outliers.
.. versionadded:: 0.19
Parameters
----------
n_neighbors : int, default=20
    Number of neighbors to use by default for :meth:`kneighbors` queries.
    If n_neighbors is larger than the number of samples provided,
    all samples will be used.
algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'
    Algorithm used to compute the nearest neighbors:
    - 'ball_tree' will use :class:`BallTree`
    - 'kd_tree' will use :class:`KDTree`
    - 'brute' will use a brute-force search.
    - 'auto' will attempt to decide the most appropriate algorithm
      based on the values passed to :meth:`fit` method.
    Note: fitting on sparse input will override the setting of
    this parameter, using brute force.
leaf_size : int, default=30
    Leaf is size passed to :class:`BallTree` or :class:`KDTree`. This can
    affect the speed of the construction and query, as well as the memory
    required to store the tree. The optimal value depends on the
    nature of the problem.
metric : str or callable, default='minkowski'
    Metric to use for distance computation. Default is "minkowski", which
    results in the standard Euclidean distance when p = 2. See the
    documentation of `scipy.spatial.distance
    &lt;https://docs.scipy.org/doc/scipy/reference/spatial.distance.html&gt;`_ and
    the metrics listed in
    :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric
    values.
    If metric is "precomputed", X is assumed to be a distance matrix and
    must be square during fit. X may be a :term:`sparse graph`, in which
    case only "nonzero" elements may be considered neighbors.
    If metric is a callable function, it takes two arrays representing 1D
    vectors as inputs and must return one value indicating the distance
    between those vectors. This works for Scipy's metrics, but is less
    efficient than passing the metric name as a string.
p : int, default=2
    Parameter for the Minkowski metric from
    :func:`sklearn.metrics.pairwise.pairwise_distances`. When p = 1, this
    is equivalent to using manhattan_distance (l1), and euclidean_distance
    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
metric_params : dict, default=None
    Additional keyword arguments for the metric function.
contamination : 'auto' or float, default='auto'
    The amount of contamination of the data set, i.e. the proportion
    of outliers in the data set. When fitting this is used to define the
    threshold on the scores of the samples.
    - if 'auto', the threshold is determined as in the
      original paper,
    - if a float, the contamination should be in the range (0, 0.5].
    .. versionchanged:: 0.22
       The default value of ``contamination`` changed from 0.1
       to ``'auto'``.
novelty : bool, default=False
    By default, LocalOutlierFactor is only meant to be used for outlier
    detection (novelty=False). Set novelty to True if you want to use
    LocalOutlierFactor for novelty detection. In this case be aware that
    you should only use predict, decision_function and score_samples
    on new unseen data and not on the training set; and note that the
    results obtained this way may differ from the standard LOF results.
    .. versionadded:: 0.20
n_jobs : int, default=None
    The number of parallel jobs to run for neighbors search.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`
    for more details.
Attributes
----------
negative_outlier_factor_ : ndarray of shape (n_samples,)
    The opposite LOF of the training samples. The higher, the more normal.
    Inliers tend to have a LOF score close to 1
    (``negative_outlier_factor_`` close to -1), while outliers tend to have
    a larger LOF score.
    The local outlier factor (LOF) of a sample captures its
    supposed 'degree of abnormality'.
    It is the average of the ratio of the local reachability density of
    a sample and those of its k-nearest neighbors.
n_neighbors_ : int
    The actual number of neighbors used for :meth:`kneighbors` queries.
offset_ : float
    Offset used to obtain binary labels from the raw scores.
    Observations having a negative_outlier_factor smaller than `offset_`
    are detected as abnormal.
    The offset is set to -1.5 (inliers score around -1), except when a
    contamination parameter different than "auto" is provided. In that
    case, the offset is defined in such a way we obtain the expected
    number of outliers in training.
    .. versionadded:: 0.20
effective_metric_ : str
    The effective metric used for the distance computation.
effective_metric_params_ : dict
    The effective additional keyword arguments for the metric function.
n_features_in_ : int
    Number of features seen during :term:`fit`.
    .. versionadded:: 0.24
feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.
    .. versionadded:: 1.0
n_samples_fit_ : int
    It is the number of samples in the fitted data.
See Also
--------
sklearn.svm.OneClassSVM: Unsupervised Outlier Detection using
    Support Vector Machine.
References
----------
.. [1] Breunig, M. M., Kriegel, H. P., Ng, R. T., &amp; Sander, J. (2000, May).
       LOF: identifying density-based local outliers. In ACM sigmod record.
Examples
--------
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.neighbors import LocalOutlierFactor
&gt;&gt;&gt; X = [[-1.1], [0.2], [101.1], [0.3]]
&gt;&gt;&gt; clf = LocalOutlierFactor(n_neighbors=2)
&gt;&gt;&gt; clf.fit_predict(X)
array([ 1,  1, -1,  1])
&gt;&gt;&gt; clf.negative_outlier_factor_
array([ -0.9821...,  -1.0370..., -73.3697...,  -0.9821...])
<span class="ansi-red-fg">File:</span>           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/sklearn/neighbors/_lof.py
<span class="ansi-red-fg">Type:</span>           ABCMeta
<span class="ansi-red-fg">Subclasses:</span>     
</pre>
</div>
</div>
</div>
<div class="cell" data-execution_count="1551">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_1,clf.fit_predict(X),tab_linear)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1552">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"LOF (Breunig et al., 2000)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-38-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.926
Precision: 0.961
Recall: 0.961
F1 Score: 0.961</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1553">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>two <span class="op">=</span> one.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  two = one.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="knn" class="level3">
<h3 class="anchored" data-anchor-id="knn">KNN</h3>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyod.models.knn <span class="im">import</span> KNN</span></code></pre></div>
</div>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>KNN?</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">

<pre><span class="ansi-red-fg">Init signature:</span>
KNN<span class="ansi-blue-fg">(</span>
    contamination<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.1</span><span class="ansi-blue-fg">,</span>
    n_neighbors<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">5</span><span class="ansi-blue-fg">,</span>
    method<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">'largest'</span><span class="ansi-blue-fg">,</span>
    radius<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1.0</span><span class="ansi-blue-fg">,</span>
    algorithm<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">'auto'</span><span class="ansi-blue-fg">,</span>
    leaf_size<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">30</span><span class="ansi-blue-fg">,</span>
    metric<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">'minkowski'</span><span class="ansi-blue-fg">,</span>
    p<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">,</span>
    metric_params<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    n_jobs<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span>
    <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">,</span>
<span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Docstring:</span>     
kNN class for outlier detection.
For an observation, its distance to its kth nearest neighbor could be
viewed as the outlying score. It could be viewed as a way to measure
the density. See :cite:`ramaswamy2000efficient,angiulli2002fast` for
details.
Three kNN detectors are supported:
largest: use the distance to the kth neighbor as the outlier score
mean: use the average of all k neighbors as the outlier score
median: use the median of the distance to k neighbors as the outlier score
Parameters
----------
contamination : float in (0., 0.5), optional (default=0.1)
    The amount of contamination of the data set,
    i.e. the proportion of outliers in the data set. Used when fitting to
    define the threshold on the decision function.
n_neighbors : int, optional (default = 5)
    Number of neighbors to use by default for k neighbors queries.
method : str, optional (default='largest')
    {'largest', 'mean', 'median'}
    - 'largest': use the distance to the kth neighbor as the outlier score
    - 'mean': use the average of all k neighbors as the outlier score
    - 'median': use the median of the distance to k neighbors as the
      outlier score
radius : float, optional (default = 1.0)
    Range of parameter space to use by default for `radius_neighbors`
    queries.
algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
    Algorithm used to compute the nearest neighbors:
    - 'ball_tree' will use BallTree
    - 'kd_tree' will use KDTree
    - 'brute' will use a brute-force search.
    - 'auto' will attempt to decide the most appropriate algorithm
      based on the values passed to :meth:`fit` method.
    Note: fitting on sparse input will override the setting of
    this parameter, using brute force.
    .. deprecated:: 0.74
       ``algorithm`` is deprecated in PyOD 0.7.4 and will not be
       possible in 0.7.6. It has to use BallTree for consistency.
leaf_size : int, optional (default = 30)
    Leaf size passed to BallTree. This can affect the
    speed of the construction and query, as well as the memory
    required to store the tree.  The optimal value depends on the
    nature of the problem.
metric : string or callable, default 'minkowski'
    metric to use for distance computation. Any metric from scikit-learn
    or scipy.spatial.distance can be used.
    If metric is a callable function, it is called on each
    pair of instances (rows) and the resulting value recorded. The callable
    should take two arrays as input and return one value indicating the
    distance between them. This works for Scipy's metrics, but is less
    efficient than passing the metric name as a string.
    Distance matrices are not supported.
    Valid values for metric are:
    - from scikit-learn: ['cityblock', 'euclidean', 'l1', 'l2',
      'manhattan']
    - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',
      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',
      'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto',
      'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath',
      'sqeuclidean', 'yule']
    See the documentation for scipy.spatial.distance for details on these
    metrics.
p : integer, optional (default = 2)
    Parameter for the Minkowski metric from
    sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is
    equivalent to using manhattan_distance (l1), and euclidean_distance
    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
    See http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances
metric_params : dict, optional (default = None)
    Additional keyword arguments for the metric function.
n_jobs : int, optional (default = 1)
    The number of parallel jobs to run for neighbors search.
    If ``-1``, then the number of jobs is set to the number of CPU cores.
    Affects only kneighbors and kneighbors_graph methods.
Attributes
----------
decision_scores_ : numpy array of shape (n_samples,)
    The outlier scores of the training data.
    The higher, the more abnormal. Outliers tend to have higher
    scores. This value is available once the detector is
    fitted.
threshold_ : float
    The threshold is based on ``contamination``. It is the
    ``n_samples * contamination`` most abnormal samples in
    ``decision_scores_``. The threshold is calculated for generating
    binary outlier labels.
labels_ : int, either 0 or 1
    The binary labels of the training data. 0 stands for inliers
    and 1 for outliers/anomalies. It is generated by applying
    ``threshold_`` on ``decision_scores_``.
<span class="ansi-red-fg">File:</span>           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/pyod/models/knn.py
<span class="ansi-red-fg">Type:</span>           ABCMeta
<span class="ansi-red-fg">Subclasses:</span>     
</pre>
</div>
</div>
</div>
<div class="cell" data-tags="[]" data-execution_count="1555">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> KNN()</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>]])</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'knn_Clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
</div>
<p>k번째 이상은 outlier로 본다.</p>
<p><strong>이상치 비율 정하지 않음</strong></p>
<p>Three kNN detectors are supported:</p>
<ul>
<li>largest: use the distance to the kth neighbor as the outlier score</li>
<li>mean: use the average of all k neighbors as the outlier score</li>
<li>median: use the median of the distance to k neighbors as the outlier score</li>
</ul>
<div class="cell" data-execution_count="1556">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>outlier_KNN_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1557">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>outlier_KNN_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_KNN_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1558">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_1,outlier_KNN_one,tab_linear)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1559">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"kNN (Ramaswamy et al., 2000)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-46-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.950
Precision: 1.000
Recall: 0.947
F1 Score: 0.973</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1560">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>three <span class="op">=</span> two.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  three = two.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="cblof오류" class="level3">
<h3 class="anchored" data-anchor-id="cblof오류">CBLOF(오류)</h3>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>CBLOF?</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">

<pre><span class="ansi-red-fg">Init signature:</span>
CBLOF<span class="ansi-blue-fg">(</span>
    n_clusters<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">8</span><span class="ansi-blue-fg">,</span>
    contamination<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.1</span><span class="ansi-blue-fg">,</span>
    clustering_estimator<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    alpha<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.9</span><span class="ansi-blue-fg">,</span>
    beta<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">5</span><span class="ansi-blue-fg">,</span>
    use_weights<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    check_estimator<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    random_state<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    n_jobs<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span>
<span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Docstring:</span>     
The CBLOF operator calculates the outlier score based on cluster-based
local outlier factor.
CBLOF takes as an input the data set and the cluster model that was
generated by a clustering algorithm. It classifies the clusters into small
clusters and large clusters using the parameters alpha and beta.
The anomaly score is then calculated based on the size of the cluster the
point belongs to as well as the distance to the nearest large cluster.
Use weighting for outlier factor based on the sizes of the clusters as
proposed in the original publication. Since this might lead to unexpected
behavior (outliers close to small clusters are not found), it is disabled
by default.Outliers scores are solely computed based on their distance to
the closest large cluster center.
By default, kMeans is used for clustering algorithm instead of
Squeezer algorithm mentioned in the original paper for multiple reasons.
See :cite:`he2003discovering` for details.
Parameters
----------
n_clusters : int, optional (default=8)
    The number of clusters to form as well as the number of
    centroids to generate.
contamination : float in (0., 0.5), optional (default=0.1)
    The amount of contamination of the data set,
    i.e. the proportion of outliers in the data set. Used when fitting to
    define the threshold on the decision function.
clustering_estimator : Estimator, optional (default=None)
    The base clustering algorithm for performing data clustering.
    A valid clustering algorithm should be passed in. The estimator should
    have standard sklearn APIs, fit() and predict(). The estimator should
    have attributes ``labels_`` and ``cluster_centers_``.
    If ``cluster_centers_`` is not in the attributes once the model is fit,
    it is calculated as the mean of the samples in a cluster.
    If not set, CBLOF uses KMeans for scalability. See
    https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
alpha : float in (0.5, 1), optional (default=0.9)
    Coefficient for deciding small and large clusters. The ratio
    of the number of samples in large clusters to the number of samples in
    small clusters.
beta : int or float in (1,), optional (default=5).
    Coefficient for deciding small and large clusters. For a list
    sorted clusters by size `|C1|, \|C2|, ..., |Cn|, beta = |Ck|/|Ck-1|`
use_weights : bool, optional (default=False)
    If set to True, the size of clusters are used as weights in
    outlier score calculation.
check_estimator : bool, optional (default=False)
    If set to True, check whether the base estimator is consistent with
    sklearn standard.
    .. warning::
        check_estimator may throw errors with scikit-learn 0.20 above.
random_state : int, RandomState or None, optional (default=None)
    If int, random_state is the seed used by the random
    number generator; If RandomState instance, random_state is the random
    number generator; If None, the random number generator is the
    RandomState instance used by `np.random`.
Attributes
----------
clustering_estimator_ : Estimator, sklearn instance
    Base estimator for clustering.
cluster_labels_ : list of shape (n_samples,)
    Cluster assignment for the training samples.
n_clusters_ : int
    Actual number of clusters (possibly different from n_clusters).
cluster_sizes_ : list of shape (n_clusters_,)
    The size of each cluster once fitted with the training data.
decision_scores_ : numpy array of shape (n_samples,)
    The outlier scores of the training data.
    The higher, the more abnormal. Outliers tend to have higher scores.
    This value is available once the detector is fitted.
cluster_centers_ : numpy array of shape (n_clusters_, n_features)
    The center of each cluster.
small_cluster_labels_ : list of clusters numbers
    The cluster assignments belonging to small clusters.
large_cluster_labels_ : list of clusters numbers
    The cluster assignments belonging to large clusters.
threshold_ : float
    The threshold is based on ``contamination``. It is the
    ``n_samples * contamination`` most abnormal samples in
    ``decision_scores_``. The threshold is calculated for generating
    binary outlier labels.
labels_ : int, either 0 or 1
    The binary labels of the training data. 0 stands for inliers
    and 1 for outliers/anomalies. It is generated by applying
    ``threshold_`` on ``decision_scores_``.
<span class="ansi-red-fg">File:</span>           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/pyod/models/cblof.py
<span class="ansi-red-fg">Type:</span>           ABCMeta
<span class="ansi-red-fg">Subclasses:</span>     
</pre>
</div>
</div>
</div>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code></pre></div>
</div>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> CBLOF(contamination<span class="op">=</span><span class="fl">0.05</span>,check_estimator<span class="op">=</span><span class="va">False</span>, random_state<span class="op">=</span><span class="dv">77</span>)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>]])</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'CBLOF_Clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/csy/anaconda3/envs/pygsp/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>_df <span class="op">=</span>  pd.read_csv(<span class="st">'simple_linear_df.csv'</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>outlier_true_one_1 <span class="op">=</span> pd.read_csv(<span class="st">'simple_linear_outlier.csv'</span>).iloc[:,<span class="dv">1</span>].tolist()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> CBLOF(contamination<span class="op">=</span><span class="fl">0.05</span>,check_estimator<span class="op">=</span><span class="va">False</span>, random_state<span class="op">=</span><span class="dv">77</span>)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>]])</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'CBLOF_Clf'</span>] <span class="op">=</span> clf.labels_</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>outlier_CBLOF_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>outlier_CBLOF_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_CBLOF_one))</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_1,outlier_CBLOF_one,tab_linear)</span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/csy/anaconda3/envs/pygsp/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"CBLOF (He et al., 2003)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-54-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.972
Precision: 0.985
Recall: 0.985
F1 Score: 0.985</code></pre>
</div>
<div class="cell-output cell-output-error">
<pre><code>AttributeError: 'DataFrame' object has no attribute 'append'</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># four = three.append(_conf.tab)</span></span></code></pre></div>
</div>
<ul>
<li>Accuracy: 0.972</li>
<li>Precision: 0.985</li>
<li>Recall: 0.985</li>
<li>F1 Score: 0.985</li>
</ul>
</section>
<section id="ocsvm" class="level3">
<h3 class="anchored" data-anchor-id="ocsvm">OCSVM</h3>
</section>
</section>
</section>
<section id="ocsvm-iteration" class="level1">
<h1>OCSVM iteration</h1>
<ul>
<li><p>max_iter : int, default=-1</p></li>
<li><p>Hard limit on iterations within solver, or -1 for no limit</p></li>
<li><p>default 가 -1이라 연구에는 nolinit 결과 들어감</p></li>
</ul>
<p>default=10%</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>svm.OneClassSVM?</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Init signature:</span>
svm<span class="ansi-blue-fg">.</span>OneClassSVM<span class="ansi-blue-fg">(</span>
    <span class="ansi-blue-fg">*</span><span class="ansi-blue-fg">,</span>
    kernel<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">'rbf'</span><span class="ansi-blue-fg">,</span>
    degree<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">3</span><span class="ansi-blue-fg">,</span>
    gamma<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">'scale'</span><span class="ansi-blue-fg">,</span>
    coef0<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.0</span><span class="ansi-blue-fg">,</span>
    tol<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.001</span><span class="ansi-blue-fg">,</span>
    nu<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.5</span><span class="ansi-blue-fg">,</span>
    shrinking<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">,</span>
    cache_size<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">200</span><span class="ansi-blue-fg">,</span>
    verbose<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    max_iter<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span>
<span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Docstring:</span>     
Unsupervised Outlier Detection.
Estimate the support of a high-dimensional distribution.
The implementation is based on libsvm.
Read more in the :ref:`User Guide &lt;outlier_detection&gt;`.
Parameters
----------
kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'
     Specifies the kernel type to be used in the algorithm.
     If none is given, 'rbf' will be used. If a callable is given it is
     used to precompute the kernel matrix.
degree : int, default=3
    Degree of the polynomial kernel function ('poly').
    Must be non-negative. Ignored by all other kernels.
gamma : {'scale', 'auto'} or float, default='scale'
    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.
    - if ``gamma='scale'`` (default) is passed then it uses
      1 / (n_features * X.var()) as value of gamma,
    - if 'auto', uses 1 / n_features
    - if float, must be non-negative.
    .. versionchanged:: 0.22
       The default value of ``gamma`` changed from 'auto' to 'scale'.
coef0 : float, default=0.0
    Independent term in kernel function.
    It is only significant in 'poly' and 'sigmoid'.
tol : float, default=1e-3
    Tolerance for stopping criterion.
nu : float, default=0.5
    An upper bound on the fraction of training
    errors and a lower bound of the fraction of support
    vectors. Should be in the interval (0, 1]. By default 0.5
    will be taken.
shrinking : bool, default=True
    Whether to use the shrinking heuristic.
    See the :ref:`User Guide &lt;shrinking_svm&gt;`.
cache_size : float, default=200
    Specify the size of the kernel cache (in MB).
verbose : bool, default=False
    Enable verbose output. Note that this setting takes advantage of a
    per-process runtime setting in libsvm that, if enabled, may not work
    properly in a multithreaded context.
max_iter : int, default=-1
    Hard limit on iterations within solver, or -1 for no limit.
Attributes
----------
class_weight_ : ndarray of shape (n_classes,)
    Multipliers of parameter C for each class.
    Computed based on the ``class_weight`` parameter.
    .. deprecated:: 1.2
        `class_weight_` was deprecated in version 1.2 and will be removed in 1.4.
coef_ : ndarray of shape (1, n_features)
    Weights assigned to the features (coefficients in the primal
    problem). This is only available in the case of a linear kernel.
    `coef_` is readonly property derived from `dual_coef_` and
    `support_vectors_`.
dual_coef_ : ndarray of shape (1, n_SV)
    Coefficients of the support vectors in the decision function.
fit_status_ : int
    0 if correctly fitted, 1 otherwise (will raise warning)
intercept_ : ndarray of shape (1,)
    Constant in the decision function.
n_features_in_ : int
    Number of features seen during :term:`fit`.
    .. versionadded:: 0.24
feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during :term:`fit`. Defined only when `X`
    has feature names that are all strings.
    .. versionadded:: 1.0
n_iter_ : int
    Number of iterations run by the optimization routine to fit the model.
    .. versionadded:: 1.1
n_support_ : ndarray of shape (n_classes,), dtype=int32
    Number of support vectors for each class.
offset_ : float
    Offset used to define the decision function from the raw scores.
    We have the relation: decision_function = score_samples - `offset_`.
    The offset is the opposite of `intercept_` and is provided for
    consistency with other outlier detection algorithms.
    .. versionadded:: 0.20
shape_fit_ : tuple of int of shape (n_dimensions_of_X,)
    Array dimensions of training vector ``X``.
support_ : ndarray of shape (n_SV,)
    Indices of support vectors.
support_vectors_ : ndarray of shape (n_SV, n_features)
    Support vectors.
See Also
--------
sklearn.linear_model.SGDOneClassSVM : Solves linear One-Class SVM using
    Stochastic Gradient Descent.
sklearn.neighbors.LocalOutlierFactor : Unsupervised Outlier Detection using
    Local Outlier Factor (LOF).
sklearn.ensemble.IsolationForest : Isolation Forest Algorithm.
Examples
--------
&gt;&gt;&gt; from sklearn.svm import OneClassSVM
&gt;&gt;&gt; X = [[0], [0.44], [0.45], [0.46], [1]]
&gt;&gt;&gt; clf = OneClassSVM(gamma='auto').fit(X)
&gt;&gt;&gt; clf.predict(X)
array([-1,  1,  1,  1, -1])
&gt;&gt;&gt; clf.score_samples(X)
array([1.7798..., 2.0547..., 2.0556..., 2.0561..., 1.7332...])
<span class="ansi-red-fg">File:</span>           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/sklearn/svm/_classes.py
<span class="ansi-red-fg">Type:</span>           ABCMeta
<span class="ansi-red-fg">Subclasses:</span>     
</pre>
</div>
</div>
</div>
<div class="cell" data-execution_count="1562">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> svm.OneClassSVM(nu<span class="op">=</span><span class="fl">0.1</span>, kernel<span class="op">=</span><span class="st">"rbf"</span>, gamma<span class="op">=</span><span class="fl">0.05</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1563">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>clf.fit(X)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1563">
<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-7" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>OneClassSVM(gamma=0.05, nu=0.1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-7" type="checkbox" checked=""><label for="sk-estimator-id-7" class="sk-toggleable__label sk-toggleable__label-arrow">OneClassSVM</label><div class="sk-toggleable__content"><pre>OneClassSVM(gamma=0.05, nu=0.1)</pre></div></div></div></div></div>
</div>
</div>
<div class="cell" data-execution_count="1564">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>outlier_OSVM_one <span class="op">=</span> <span class="bu">list</span>(clf.predict(X))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1565">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_1,outlier_OSVM_one,tab_linear)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1566">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"OCSVM (Sch ̈olkopf et al., 2001)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-61-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.935
Precision: 0.991
Recall: 0.940
F1 Score: 0.965</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1567">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>five <span class="op">=</span> three.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  five = three.append(_conf.tab)</code></pre>
</div>
</div>
<section id="mcdstar" class="level3">
<h3 class="anchored" data-anchor-id="mcdstar">MCD<span class="math inline">\(\star\)</span></h3>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>MCD?</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Init signature:</span>
MCD<span class="ansi-blue-fg">(</span>
    contamination<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.1</span><span class="ansi-blue-fg">,</span>
    store_precision<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">,</span>
    assume_centered<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    support_fraction<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    random_state<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
<span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Docstring:</span>     
Detecting outliers in a Gaussian distributed dataset using
Minimum Covariance Determinant (MCD): robust estimator of covariance.
The Minimum Covariance Determinant covariance estimator is to be applied
on Gaussian-distributed data, but could still be relevant on data
drawn from a unimodal, symmetric distribution. It is not meant to be used
with multi-modal data (the algorithm used to fit a MinCovDet object is
likely to fail in such a case).
One should consider projection pursuit methods to deal with multi-modal
datasets.
First fit a minimum covariance determinant model and then compute the
Mahalanobis distance as the outlier degree of the data
See :cite:`rousseeuw1999fast,hardin2004outlier` for details.
Parameters
----------
contamination : float in (0., 0.5), optional (default=0.1)
    The amount of contamination of the data set,
    i.e. the proportion of outliers in the data set. Used when fitting to
    define the threshold on the decision function.
store_precision : bool
    Specify if the estimated precision is stored.
assume_centered : bool
    If True, the support of the robust location and the covariance
    estimates is computed, and a covariance estimate is recomputed from
    it, without centering the data.
    Useful to work with data whose mean is significantly equal to
    zero but is not exactly zero.
    If False, the robust location and covariance are directly computed
    with the FastMCD algorithm without additional treatment.
support_fraction : float, 0 &lt; support_fraction &lt; 1
    The proportion of points to be included in the support of the raw
    MCD estimate. Default is None, which implies that the minimum
    value of support_fraction will be used within the algorithm:
    [n_sample + n_features + 1] / 2
random_state : int, RandomState instance or None, optional (default=None)
    If int, random_state is the seed used by the random number generator;
    If RandomState instance, random_state is the random number generator;
    If None, the random number generator is the RandomState instance used
    by `np.random`.
Attributes
----------
raw_location_ : array-like, shape (n_features,)
    The raw robust estimated location before correction and re-weighting.
raw_covariance_ : array-like, shape (n_features, n_features)
    The raw robust estimated covariance before correction and re-weighting.
raw_support_ : array-like, shape (n_samples,)
    A mask of the observations that have been used to compute
    the raw robust estimates of location and shape, before correction
    and re-weighting.
location_ : array-like, shape (n_features,)
    Estimated robust location
covariance_ : array-like, shape (n_features, n_features)
    Estimated robust covariance matrix
precision_ : array-like, shape (n_features, n_features)
    Estimated pseudo inverse matrix.
    (stored only if store_precision is True)
support_ : array-like, shape (n_samples,)
    A mask of the observations that have been used to compute
    the robust estimates of location and shape.
decision_scores_ : numpy array of shape (n_samples,)
    The outlier scores of the training data.
    The higher, the more abnormal. Outliers tend to have higher
    scores. This value is available once the detector is
    fitted. Mahalanobis distances of the training set (on which
    `:meth:`fit` is called) observations.
threshold_ : float
    The threshold is based on ``contamination``. It is the
    ``n_samples * contamination`` most abnormal samples in
    ``decision_scores_``. The threshold is calculated for generating
    binary outlier labels.
labels_ : int, either 0 or 1
    The binary labels of the training data. 0 stands for inliers
    and 1 for outliers/anomalies. It is generated by applying
    ``threshold_`` on ``decision_scores_``.
<span class="ansi-red-fg">File:</span>           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/pyod/models/mcd.py
<span class="ansi-red-fg">Type:</span>           ABCMeta
<span class="ansi-red-fg">Subclasses:</span>     
</pre>
</div>
</div>
</div>
<div class="cell" data-scrolled="true" data-tags="[]" data-execution_count="1568">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> MCD(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>]])</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'MCD_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1569">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>outlier_MCD_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1570">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>outlier_MCD_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_MCD_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1571">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_1,outlier_MCD_one,tab_linear)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1572">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"MCD (Hardin and Rocke, 2004)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-68-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.998
Precision: 0.999
Recall: 0.999
F1 Score: 0.999</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1573">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>six <span class="op">=</span> five.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  six = five.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="feature-baggingstar" class="level3">
<h3 class="anchored" data-anchor-id="feature-baggingstar">Feature Bagging<span class="math inline">\(\star\)</span></h3>
<p>default값은 10%로 설정되어 있었고, 5%로 지정한 결과, 평가지표값이 전반적으로 1%이상 낮아졌다.</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>FeatureBagging?</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Init signature:</span>
FeatureBagging<span class="ansi-blue-fg">(</span>
    base_estimator<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    n_estimators<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">10</span><span class="ansi-blue-fg">,</span>
    contamination<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.1</span><span class="ansi-blue-fg">,</span>
    max_features<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1.0</span><span class="ansi-blue-fg">,</span>
    bootstrap_features<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    check_detector<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">,</span>
    check_estimator<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    n_jobs<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span>
    random_state<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    combination<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">'average'</span><span class="ansi-blue-fg">,</span>
    verbose<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span>
    estimator_params<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
<span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Docstring:</span>     
A feature bagging detector is a meta estimator that fits a number of
base detectors on various sub-samples of the dataset and use averaging
or other combination methods to improve the predictive accuracy and
control over-fitting.
The sub-sample size is always the same as the original input sample size
but the features are randomly sampled from half of the features to all
features.
By default, LOF is used as the base estimator. However, any estimator
could be used as the base estimator, such as kNN and ABOD.
Feature bagging first construct n subsamples by random selecting a subset
of features, which induces the diversity of base estimators.
Finally, the prediction score is generated by averaging/taking the maximum
of all base detectors. See :cite:`lazarevic2005feature` for details.
Parameters
----------
base_estimator : object or None, optional (default=None)
    The base estimator to fit on random subsets of the dataset.
    If None, then the base estimator is a LOF detector.
n_estimators : int, optional (default=10)
    The number of base estimators in the ensemble.
contamination : float in (0., 0.5), optional (default=0.1)
    The amount of contamination of the data set,
    i.e. the proportion of outliers in the data set. Used when fitting to
    define the threshold on the decision function.
max_features : int or float, optional (default=1.0)
    The number of features to draw from X to train each base estimator.
    - If int, then draw `max_features` features.
    - If float, then draw `max_features * X.shape[1]` features.
bootstrap_features : bool, optional (default=False)
    Whether features are drawn with replacement.
check_detector : bool, optional (default=True)
    If set to True, check whether the base estimator is consistent with
    pyod standard.
check_estimator : bool, optional (default=False)
    If set to True, check whether the base estimator is consistent with
    sklearn standard.
    .. deprecated:: 0.6.9
      `check_estimator` will be removed in pyod 0.8.0.; it will be
      replaced by `check_detector`.
n_jobs : optional (default=1)
    The number of jobs to run in parallel for both `fit` and
    `predict`. If -1, then the number of jobs is set to the
    number of cores.
random_state : int, RandomState or None, optional (default=None)
    If int, random_state is the seed used by the random
    number generator; If RandomState instance, random_state is the random
    number generator; If None, the random number generator is the
    RandomState instance used by `np.random`.
combination : str, optional (default='average')
    The method of combination:
    - if 'average': take the average of all detectors
    - if 'max': take the maximum scores of all detectors
verbose : int, optional (default=0)
    Controls the verbosity of the building process.
estimator_params : dict, optional (default=None)
    The list of attributes to use as parameters
    when instantiating a new base estimator. If none are given,
    default parameters are used.
Attributes
----------
decision_scores_ : numpy array of shape (n_samples,)
    The outlier scores of the training data.
    The higher, the more abnormal. Outliers tend to have higher
    scores. This value is available once the detector is
    fitted.
threshold_ : float
    The threshold is based on ``contamination``. It is the
    ``n_samples * contamination`` most abnormal samples in
    ``decision_scores_``. The threshold is calculated for generating
    binary outlier labels.
labels_ : int, either 0 or 1
    The binary labels of the training data. 0 stands for inliers
    and 1 for outliers/anomalies. It is generated by applying
    ``threshold_`` on ``decision_scores_``.
<span class="ansi-red-fg">File:</span>           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/pyod/models/feature_bagging.py
<span class="ansi-red-fg">Type:</span>           ABCMeta
<span class="ansi-red-fg">Subclasses:</span>     
</pre>
</div>
</div>
</div>
<div class="cell" data-scrolled="true" data-tags="[]" data-execution_count="1574">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> FeatureBagging(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>]])</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'FeatureBagging_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1575">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>outlier_FeatureBagging_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1576">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>outlier_FeatureBagging_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_FeatureBagging_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1577">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_1,outlier_FeatureBagging_one,tab_linear)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1578">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"Feature Bagging (Lazarevic and Kumar, 2005)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-75-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.986
Precision: 0.993
Recall: 0.993
F1 Score: 0.993</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1579">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>seven <span class="op">=</span> six.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  seven = six.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="abodstar" class="level3">
<h3 class="anchored" data-anchor-id="abodstar">ABOD<span class="math inline">\(\star\)</span></h3>
<p>default 값이 5%이며, 이미 지정된 채려 시뮬레이션 돌림</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>ABOD?</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Init signature:</span> ABOD<span class="ansi-blue-fg">(</span>contamination<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.1</span><span class="ansi-blue-fg">,</span> n_neighbors<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">5</span><span class="ansi-blue-fg">,</span> method<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">'fast'</span><span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Docstring:</span>     
ABOD class for Angle-base Outlier Detection.
For an observation, the variance of its weighted cosine scores to all
neighbors could be viewed as the outlying score.
See :cite:`kriegel2008angle` for details.
Two version of ABOD are supported:
- Fast ABOD: use k nearest neighbors to approximate.
- Original ABOD: consider all training points with high time complexity at
  O(n^3).
Parameters
----------
contamination : float in (0., 0.5), optional (default=0.1)
    The amount of contamination of the data set, i.e.
    the proportion of outliers in the data set. Used when fitting to
    define the threshold on the decision function.
n_neighbors : int, optional (default=10)
    Number of neighbors to use by default for k neighbors queries.
method: str, optional (default='fast')
    Valid values for metric are:
    - 'fast': fast ABOD. Only consider n_neighbors of training points
    - 'default': original ABOD with all training points, which could be
      slow
Attributes
----------
decision_scores_ : numpy array of shape (n_samples,)
    The outlier scores of the training data.
    The higher, the more abnormal. Outliers tend to have higher
    scores. This value is available once the detector is
    fitted.
threshold_ : float
    The threshold is based on ``contamination``. It is the
    ``n_samples * contamination`` most abnormal samples in
    ``decision_scores_``. The threshold is calculated for generating
    binary outlier labels.
labels_ : int, either 0 or 1
    The binary labels of the training data. 0 stands for inliers
    and 1 for outliers/anomalies. It is generated by applying
    ``threshold_`` on ``decision_scores_``.
<span class="ansi-red-fg">File:</span>           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/pyod/models/abod.py
<span class="ansi-red-fg">Type:</span>           ABCMeta
<span class="ansi-red-fg">Subclasses:</span>     
</pre>
</div>
</div>
</div>
<div class="cell" data-execution_count="1580">
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> ABOD(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>]])</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'ABOD_Clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
</div>
<p><strong>contamination</strong> : float in (0., 0.5), optional (default=0.1)</p>
<ul>
<li>The amount of contamination of the data set, i.e.</li>
<li>the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function.</li>
</ul>
<div class="cell" data-execution_count="1581">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>outlier_ABOD_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1582">
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>outlier_ABOD_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_ABOD_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1583">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_1,outlier_ABOD_one,tab_linear)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1584">
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"ABOD (Kriegel et al., 2008)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-82-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.988
Precision: 0.994
Recall: 0.994
F1 Score: 0.994</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1585">
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>eight <span class="op">=</span> seven.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  eight = seven.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="iforeststar" class="level3">
<h3 class="anchored" data-anchor-id="iforeststar">IForest<span class="math inline">\(\star\)</span></h3>
<p>n_estimators Number of base estimators in the ensemble.</p>
<ul>
<li>n이 총 1000개니까 5%인 50 지정해줄 수 있음</li>
</ul>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>IForest?</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Init signature:</span>
IForest<span class="ansi-blue-fg">(</span>
    threshold<span class="ansi-blue-fg">:</span> float <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    n_estimators<span class="ansi-blue-fg">:</span> int <span class="ansi-blue-fg">=</span> <span class="ansi-cyan-fg">100</span><span class="ansi-blue-fg">,</span>
    max_samples<span class="ansi-blue-fg">:</span> Union<span class="ansi-blue-fg">[</span>str<span class="ansi-blue-fg">,</span> int<span class="ansi-blue-fg">,</span> float<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">'auto'</span><span class="ansi-blue-fg">,</span>
    max_features<span class="ansi-blue-fg">:</span> Union<span class="ansi-blue-fg">[</span>int<span class="ansi-blue-fg">,</span> float<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-cyan-fg">1.0</span><span class="ansi-blue-fg">,</span>
    bootstrap<span class="ansi-blue-fg">:</span> bool <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    n_jobs<span class="ansi-blue-fg">:</span> int <span class="ansi-blue-fg">=</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">,</span>
    data_type<span class="ansi-blue-fg">:</span> str <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">'tabular'</span><span class="ansi-blue-fg">,</span>
<span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">-&gt;</span> <span class="ansi-green-fg">None</span>
<span class="ansi-red-fg">Docstring:</span>      Base class for outlier, adversarial and drift detection algorithms.
<span class="ansi-red-fg">Init docstring:</span>
Outlier detector for tabular data using isolation forests.
Parameters
----------
threshold
    Threshold used for outlier score to determine outliers.
n_estimators
    Number of base estimators in the ensemble.
max_samples
    Number of samples to draw from the training data to train each base estimator.
    If int, draw 'max_samples' samples.
    If float, draw 'max_samples * number of features' samples.
    If 'auto', max_samples = min(256, number of samples)
max_features
    Number of features to draw from the training data to train each base estimator.
    If int, draw 'max_features' features.
    If float, draw 'max_features * number of features' features.
bootstrap
    Whether to fit individual trees on random subsets of the training data, sampled with replacement.
n_jobs
    Number of jobs to run in parallel for 'fit' and 'predict'.
data_type
    Optionally specify the data type (tabular, image or time-series). Added to metadata.
<span class="ansi-red-fg">File:</span>           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/alibi_detect/od/isolationforest.py
<span class="ansi-red-fg">Type:</span>           ABCMeta
<span class="ansi-red-fg">Subclasses:</span>     
</pre>
</div>
</div>
</div>
<div class="cell" data-execution_count="1586">
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>od <span class="op">=</span> IForest(</span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>    threshold<span class="op">=</span><span class="fl">0.</span>,</span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">50</span></span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1587">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>od.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>]])</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1588">
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> od.predict(</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a>    _df[[<span class="st">'x'</span>, <span class="st">'y'</span>]],</span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a>    return_instance_score<span class="op">=</span><span class="va">True</span></span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1589">
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'IF_alibi'</span>] <span class="op">=</span> preds[<span class="st">'data'</span>][<span class="st">'is_outlier'</span>]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1590">
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>outlier_alibi_one <span class="op">=</span> _df[<span class="st">'IF_alibi'</span>]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1591">
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>outlier_alibi_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_alibi_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1592">
<div class="sourceCode cell-code" id="cb117"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_1,outlier_alibi_one,tab_linear)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1593">
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"Isolation Forest (Liu et al., 2008)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-92-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.868
Precision: 0.999
Recall: 0.862
F1 Score: 0.925</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1594">
<div class="sourceCode cell-code" id="cb121"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>nine <span class="op">=</span> eight.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  nine = eight.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="hbosstar" class="level3">
<h3 class="anchored" data-anchor-id="hbosstar">HBOS<span class="math inline">\(\star\)</span></h3>
<p>default값은 이상치값을 10%로 지정하였으며, 5%로 지정한 결과 값 다 작아짐</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb123"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a>HBOS?</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Init signature:</span> HBOS<span class="ansi-blue-fg">(</span>n_bins<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">10</span><span class="ansi-blue-fg">,</span> alpha<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.1</span><span class="ansi-blue-fg">,</span> tol<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.5</span><span class="ansi-blue-fg">,</span> contamination<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.1</span><span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Docstring:</span>     
Histogram- based outlier detection (HBOS) is an efficient unsupervised
method. It assumes the feature independence and calculates the degree
of outlyingness by building histograms. See :cite:`goldstein2012histogram`
for details.    
Two versions of HBOS are supported:        
- Static number of bins: uses a static number of bins for all features.
- Automatic number of bins: every feature uses a number of bins deemed to 
  be optimal according to the Birge-Rozenblac method
  (:cite:`birge2006many`).
  
Parameters
----------
n_bins : int or string, optional (default=10)
    The number of bins. "auto" uses the birge-rozenblac method for
    automatic selection of the optimal number of bins for each feature.
alpha : float in (0, 1), optional (default=0.1)
    The regularizer for preventing overflow.
tol : float in (0, 1), optional (default=0.5)
    The parameter to decide the flexibility while dealing
    the samples falling outside the bins.
contamination : float in (0., 0.5), optional (default=0.1)
    The amount of contamination of the data set,
    i.e. the proportion of outliers in the data set. Used when fitting to
    define the threshold on the decision function.
Attributes
----------
bin_edges_ : numpy array of shape (n_bins + 1, n_features )
    The edges of the bins.
hist_ : numpy array of shape (n_bins, n_features)
    The density of each histogram.
decision_scores_ : numpy array of shape (n_samples,)
    The outlier scores of the training data.
    The higher, the more abnormal. Outliers tend to have higher
    scores. This value is available once the detector is fitted.
threshold_ : float
    The threshold is based on ``contamination``. It is the
    ``n_samples * contamination`` most abnormal samples in
    ``decision_scores_``. The threshold is calculated for generating
    binary outlier labels.
labels_ : int, either 0 or 1
    The binary labels of the training data. 0 stands for inliers
    and 1 for outliers/anomalies. It is generated by applying
    ``threshold_`` on ``decision_scores_``.
<span class="ansi-red-fg">File:</span>           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/pyod/models/hbos.py
<span class="ansi-red-fg">Type:</span>           ABCMeta
<span class="ansi-red-fg">Subclasses:</span>     
</pre>
</div>
</div>
</div>
<div class="cell" data-execution_count="1595">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> HBOS(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>]])</span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'HBOS_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1596">
<div class="sourceCode cell-code" id="cb125"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>outlier_HBOS_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1597">
<div class="sourceCode cell-code" id="cb126"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a>outlier_HBOS_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_HBOS_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1598">
<div class="sourceCode cell-code" id="cb127"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_1,outlier_HBOS_one,tab_linear)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1599">
<div class="sourceCode cell-code" id="cb128"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"HBOS (Goldstein and Dengel, 2012)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-99-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.960
Precision: 0.978
Recall: 0.980
F1 Score: 0.979</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1600">
<div class="sourceCode cell-code" id="cb131"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>ten <span class="op">=</span> nine.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  ten = nine.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="sosstar" class="level3">
<h3 class="anchored" data-anchor-id="sosstar">SOS<span class="math inline">\(\star\)</span></h3>
<p>default 는 10%</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb133"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a>SOS?</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Init signature:</span> SOS<span class="ansi-blue-fg">(</span>contamination<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.1</span><span class="ansi-blue-fg">,</span> perplexity<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">4.5</span><span class="ansi-blue-fg">,</span> metric<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">'euclidean'</span><span class="ansi-blue-fg">,</span> eps<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1e-05</span><span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Docstring:</span>     
Stochastic Outlier Selection.
SOS employs the concept of affinity to quantify
the relationship from one data point to another data point. Affinity is 
proportional to the similarity between two data points. So, a data point 
has little affinity with a dissimilar data point. A data point is 
selected as an outlier when all the other data points have insufficient
affinity with it.
Read more in the :cite:`janssens2012stochastic`.
Parameters
----------
contamination : float in (0., 0.5), optional (default=0.1) 
    The amount of contamination of the data set, i.e.
    the proportion of outliers in the data set. Used when fitting to
    define the threshold on the decision function.
perplexity : float, optional (default=4.5)
    A smooth measure of the effective number of neighbours. The perplexity
    parameter is similar to the parameter `k` in kNN algorithm (the number
    of nearest neighbors). The range of perplexity can be any real number
    between 1 and n-1, where `n` is the number of samples.
metric: str, default 'euclidean'
    Metric used for the distance computation. Any metric from
    scipy.spatial.distance can be used.
    Valid values for metric are:
    - 'euclidean'
    - from scipy.spatial.distance: ['braycurtis', 'canberra',
      'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard',
      'kulsinski', 'mahalanobis', 'matching', 'minkowski',
      'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener',
      'sokalsneath', 'sqeuclidean', 'yule']
    See the documentation for scipy.spatial.distance for details on these
    metrics:
    http://docs.scipy.org/doc/scipy/reference/spatial.distance.html
eps : float, optional (default = 1e-5)
    Tolerance threshold for floating point errors.
    
Attributes
----------
decision_scores_ : numpy array of shape (n_samples,)
    The outlier scores of the training data.
    The higher, the more abnormal. Outliers tend to have higher
    scores. This value is available once the detector is fitted.
threshold_ : float
    The threshold is based on ``contamination``. It is the
    ``n_samples * contamination`` most abnormal samples in
    ``decision_scores_``. The threshold is calculated for generating
    binary outlier labels.
labels_ : int, either 0 or 1
    The binary labels of the training data. 0 stands for inliers
    and 1 for outliers/anomalies. It is generated by applying
    ``threshold_`` on ``decision_scores_``.
    
    
Examples
--------
&gt;&gt;&gt; from pyod.models.sos import SOS
&gt;&gt;&gt; from pyod.utils.data import generate_data
&gt;&gt;&gt; n_train = 50
&gt;&gt;&gt; n_test = 50
&gt;&gt;&gt; contamination = 0.1
&gt;&gt;&gt; X_train, y_train, X_test, y_test = generate_data(
...     n_train=n_train, n_test=n_test,
...     contamination=contamination, random_state=42)
&gt;&gt;&gt;
&gt;&gt;&gt; clf = SOS()
&gt;&gt;&gt; clf.fit(X_train)
SOS(contamination=0.1, eps=1e-05, metric='euclidean', perplexity=4.5)
<span class="ansi-red-fg">File:</span>           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/pyod/models/sos.py
<span class="ansi-red-fg">Type:</span>           ABCMeta
<span class="ansi-red-fg">Subclasses:</span>     
</pre>
</div>
</div>
</div>
<div class="cell" data-execution_count="1601">
<div class="sourceCode cell-code" id="cb134"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> SOS(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>]])</span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'SOS_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1602">
<div class="sourceCode cell-code" id="cb135"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a>outlier_SOS_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1603">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>outlier_SOS_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_SOS_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1604">
<div class="sourceCode cell-code" id="cb137"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_1,outlier_SOS_one,tab_linear)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1605">
<div class="sourceCode cell-code" id="cb138"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"SOS (Janssens et al., 2012)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-106-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.916
Precision: 0.956
Recall: 0.956
F1 Score: 0.956</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1606">
<div class="sourceCode cell-code" id="cb141"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a>eleven <span class="op">=</span> ten.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  eleven = ten.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="so_gaal" class="level3">
<h3 class="anchored" data-anchor-id="so_gaal">SO_GAAL</h3>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb143"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a>SO_GAAL?</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Init signature:</span>
SO_GAAL<span class="ansi-blue-fg">(</span>
    stop_epochs<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">20</span><span class="ansi-blue-fg">,</span>
    lr_d<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.01</span><span class="ansi-blue-fg">,</span>
    lr_g<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.0001</span><span class="ansi-blue-fg">,</span>
    momentum<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.9</span><span class="ansi-blue-fg">,</span>
    contamination<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.1</span><span class="ansi-blue-fg">,</span>
<span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Docstring:</span>     
Single-Objective Generative Adversarial Active Learning.
SO-GAAL directly generates informative potential outliers to assist the
classifier in describing a boundary that can separate outliers from normal
data effectively. Moreover, to prevent the generator from falling into the
mode collapsing problem, the network structure of SO-GAAL is expanded from
a single generator (SO-GAAL) to multiple generators with different
objectives (MO-GAAL) to generate a reasonable reference distribution for
the whole dataset.
Read more in the :cite:`liu2019generative`.
Parameters
----------
contamination : float in (0., 0.5), optional (default=0.1)
    The amount of contamination of the data set, i.e.
    the proportion of outliers in the data set. Used when fitting to
    define the threshold on the decision function.
stop_epochs : int, optional (default=20)
    The number of epochs of training. The number of total epochs equals to three times of stop_epochs.
lr_d : float, optional (default=0.01)
    The learn rate of the discriminator.
lr_g : float, optional (default=0.0001)
    The learn rate of the generator.
momentum : float, optional (default=0.9)
    The momentum parameter for SGD.
Attributes
----------
decision_scores_ : numpy array of shape (n_samples,)
    The outlier scores of the training data.
    The higher, the more abnormal. Outliers tend to have higher
    scores. This value is available once the detector is fitted.
threshold_ : float
    The threshold is based on ``contamination``. It is the
    ``n_samples * contamination`` most abnormal samples in
    ``decision_scores_``. The threshold is calculated for generating
    binary outlier labels.
labels_ : int, either 0 or 1
    The binary labels of the training data. 0 stands for inliers
    and 1 for outliers/anomalies. It is generated by applying
    ``threshold_`` on ``decision_scores_``.
<span class="ansi-red-fg">File:</span>           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/pyod/models/so_gaal.py
<span class="ansi-red-fg">Type:</span>           ABCMeta
<span class="ansi-red-fg">Subclasses:</span>     
</pre>
</div>
</div>
</div>
<div class="cell" data-scrolled="true" data-tags="[]" data-execution_count="1607">
<div class="sourceCode cell-code" id="cb144"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> SO_GAAL(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>]])</span>
<span id="cb144-3"><a href="#cb144-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'SO_GAAL_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1 of 60

Testing for epoch 1 index 1:</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super().__init__(name, **kwargs)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Testing for epoch 1 index 2:
Epoch 2 of 60

Testing for epoch 2 index 1:

Testing for epoch 2 index 2:
Epoch 3 of 60

Testing for epoch 3 index 1:

Testing for epoch 3 index 2:
Epoch 4 of 60

Testing for epoch 4 index 1:

Testing for epoch 4 index 2:
Epoch 5 of 60

Testing for epoch 5 index 1:

Testing for epoch 5 index 2:
Epoch 6 of 60

Testing for epoch 6 index 1:

Testing for epoch 6 index 2:
Epoch 7 of 60

Testing for epoch 7 index 1:

Testing for epoch 7 index 2:
Epoch 8 of 60

Testing for epoch 8 index 1:

Testing for epoch 8 index 2:
Epoch 9 of 60

Testing for epoch 9 index 1:

Testing for epoch 9 index 2:
Epoch 10 of 60

Testing for epoch 10 index 1:

Testing for epoch 10 index 2:
Epoch 11 of 60

Testing for epoch 11 index 1:

Testing for epoch 11 index 2:
Epoch 12 of 60

Testing for epoch 12 index 1:

Testing for epoch 12 index 2:
Epoch 13 of 60

Testing for epoch 13 index 1:

Testing for epoch 13 index 2:
Epoch 14 of 60

Testing for epoch 14 index 1:

Testing for epoch 14 index 2:
Epoch 15 of 60

Testing for epoch 15 index 1:

Testing for epoch 15 index 2:
Epoch 16 of 60

Testing for epoch 16 index 1:

Testing for epoch 16 index 2:
Epoch 17 of 60

Testing for epoch 17 index 1:

Testing for epoch 17 index 2:
Epoch 18 of 60

Testing for epoch 18 index 1:

Testing for epoch 18 index 2:
Epoch 19 of 60

Testing for epoch 19 index 1:

Testing for epoch 19 index 2:
Epoch 20 of 60

Testing for epoch 20 index 1:

Testing for epoch 20 index 2:
Epoch 21 of 60

Testing for epoch 21 index 1:

Testing for epoch 21 index 2:
Epoch 22 of 60

Testing for epoch 22 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.3130

Testing for epoch 22 index 2:
16/16 [==============================] - 0s 3ms/step - loss: 1.3524
Epoch 23 of 60

Testing for epoch 23 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.3562

Testing for epoch 23 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.3857
Epoch 24 of 60

Testing for epoch 24 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.3845

Testing for epoch 24 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.3516
Epoch 25 of 60

Testing for epoch 25 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.3861

Testing for epoch 25 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.4008
Epoch 26 of 60

Testing for epoch 26 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.3870

Testing for epoch 26 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.4348
Epoch 27 of 60

Testing for epoch 27 index 1:
16/16 [==============================] - 0s 3ms/step - loss: 1.3913

Testing for epoch 27 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.4431
Epoch 28 of 60

Testing for epoch 28 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.4510

Testing for epoch 28 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.4427
Epoch 29 of 60

Testing for epoch 29 index 1:
16/16 [==============================] - 0s 5ms/step - loss: 1.4704

Testing for epoch 29 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.4752
Epoch 30 of 60

Testing for epoch 30 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.4794

Testing for epoch 30 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.4972
Epoch 31 of 60

Testing for epoch 31 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.4998

Testing for epoch 31 index 2:
16/16 [==============================] - 0s 3ms/step - loss: 1.5168
Epoch 32 of 60

Testing for epoch 32 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.5228

Testing for epoch 32 index 2:
16/16 [==============================] - 0s 4ms/step - loss: 1.5560
Epoch 33 of 60

Testing for epoch 33 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.5677

Testing for epoch 33 index 2:
16/16 [==============================] - 0s 3ms/step - loss: 1.4929
Epoch 34 of 60

Testing for epoch 34 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.5675

Testing for epoch 34 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.5508
Epoch 35 of 60

Testing for epoch 35 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.5679

Testing for epoch 35 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.5563
Epoch 36 of 60

Testing for epoch 36 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.5806

Testing for epoch 36 index 2:
16/16 [==============================] - 0s 4ms/step - loss: 1.5637
Epoch 37 of 60

Testing for epoch 37 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.5749

Testing for epoch 37 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 1.6370
Epoch 38 of 60

Testing for epoch 38 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.6088

Testing for epoch 38 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.6408
Epoch 39 of 60

Testing for epoch 39 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.6699

Testing for epoch 39 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.5958
Epoch 40 of 60

Testing for epoch 40 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.5661

Testing for epoch 40 index 2:
16/16 [==============================] - 0s 4ms/step - loss: 1.6471
Epoch 41 of 60

Testing for epoch 41 index 1:
16/16 [==============================] - 0s 3ms/step - loss: 1.6815

Testing for epoch 41 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.6419
Epoch 42 of 60

Testing for epoch 42 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 1.6967

Testing for epoch 42 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.7016
Epoch 43 of 60

Testing for epoch 43 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.6348

Testing for epoch 43 index 2:
16/16 [==============================] - 0s 5ms/step - loss: 1.6519
Epoch 44 of 60

Testing for epoch 44 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.6470

Testing for epoch 44 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.6582
Epoch 45 of 60

Testing for epoch 45 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.6890

Testing for epoch 45 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.7197
Epoch 46 of 60

Testing for epoch 46 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.7613

Testing for epoch 46 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 1.7085
Epoch 47 of 60

Testing for epoch 47 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.6933

Testing for epoch 47 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 1.7013
Epoch 48 of 60

Testing for epoch 48 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.7330

Testing for epoch 48 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.7275
Epoch 49 of 60

Testing for epoch 49 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 1.7635

Testing for epoch 49 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 1.7682
Epoch 50 of 60

Testing for epoch 50 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 1.8321

Testing for epoch 50 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.7557
Epoch 51 of 60

Testing for epoch 51 index 1:
16/16 [==============================] - 0s 4ms/step - loss: 1.7231

Testing for epoch 51 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 1.7787
Epoch 52 of 60

Testing for epoch 52 index 1:
16/16 [==============================] - 0s 5ms/step - loss: 1.7553

Testing for epoch 52 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.7782
Epoch 53 of 60

Testing for epoch 53 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.7678

Testing for epoch 53 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.8069
Epoch 54 of 60

Testing for epoch 54 index 1:
16/16 [==============================] - 0s 3ms/step - loss: 1.7798

Testing for epoch 54 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.8038
Epoch 55 of 60

Testing for epoch 55 index 1:
16/16 [==============================] - 0s 5ms/step - loss: 1.8120

Testing for epoch 55 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.7591
Epoch 56 of 60

Testing for epoch 56 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 1.8204

Testing for epoch 56 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.8033
Epoch 57 of 60

Testing for epoch 57 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.8414

Testing for epoch 57 index 2:
16/16 [==============================] - 0s 3ms/step - loss: 1.7215
Epoch 58 of 60

Testing for epoch 58 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.8414

Testing for epoch 58 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.8143
Epoch 59 of 60

Testing for epoch 59 index 1:
16/16 [==============================] - 0s 3ms/step - loss: 1.8406

Testing for epoch 59 index 2:
16/16 [==============================] - 0s 3ms/step - loss: 1.8562
Epoch 60 of 60

Testing for epoch 60 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.8167

Testing for epoch 60 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.8597
32/32 [==============================] - 0s 2ms/step</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1608">
<div class="sourceCode cell-code" id="cb148"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a>outlier_SO_GAAL_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1609">
<div class="sourceCode cell-code" id="cb149"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a>outlier_SO_GAAL_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_SO_GAAL_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1610">
<div class="sourceCode cell-code" id="cb150"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_1,outlier_SO_GAAL_one,tab_linear)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1611">
<div class="sourceCode cell-code" id="cb151"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"SO-GAAL (Liu et al., 2019)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-113-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.936
Precision: 0.966
Recall: 0.966
F1 Score: 0.966</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1612">
<div class="sourceCode cell-code" id="cb154"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a>twelve <span class="op">=</span> eleven.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  twelve = eleven.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="mo_gaalstar" class="level3">
<h3 class="anchored" data-anchor-id="mo_gaalstar">MO_GAAL<span class="math inline">\(\star\)</span></h3>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb156"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a>MO_GAAL?</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Init signature:</span>
MO_GAAL<span class="ansi-blue-fg">(</span>
    k<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">10</span><span class="ansi-blue-fg">,</span>
    stop_epochs<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">20</span><span class="ansi-blue-fg">,</span>
    lr_d<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.01</span><span class="ansi-blue-fg">,</span>
    lr_g<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.0001</span><span class="ansi-blue-fg">,</span>
    momentum<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.9</span><span class="ansi-blue-fg">,</span>
    contamination<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.1</span><span class="ansi-blue-fg">,</span>
<span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Docstring:</span>     
Multi-Objective Generative Adversarial Active Learning.
MO_GAAL directly generates informative potential outliers to assist the
classifier in describing a boundary that can separate outliers from normal
data effectively. Moreover, to prevent the generator from falling into the
mode collapsing problem, the network structure of SO-GAAL is expanded from
a single generator (SO-GAAL) to multiple generators with different
objectives (MO-GAAL) to generate a reasonable reference distribution for
the whole dataset.
Read more in the :cite:`liu2019generative`.
Parameters
----------
contamination : float in (0., 0.5), optional (default=0.1)
    The amount of contamination of the data set, i.e.
    the proportion of outliers in the data set. Used when fitting to
    define the threshold on the decision function.
k : int, optional (default=10)
    The number of sub generators.
stop_epochs : int, optional (default=20)
    The number of epochs of training. The number of total epochs equals to three times of stop_epochs.
lr_d : float, optional (default=0.01)
    The learn rate of the discriminator.
lr_g : float, optional (default=0.0001)
    The learn rate of the generator.
momentum : float, optional (default=0.9)
    The momentum parameter for SGD.
Attributes
----------
decision_scores_ : numpy array of shape (n_samples,)
    The outlier scores of the training data.
    The higher, the more abnormal. Outliers tend to have higher
    scores. This value is available once the detector is fitted.
threshold_ : float
    The threshold is based on ``contamination``. It is the
    ``n_samples * contamination`` most abnormal samples in
    ``decision_scores_``. The threshold is calculated for generating
    binary outlier labels.
labels_ : int, either 0 or 1
    The binary labels of the training data. 0 stands for inliers
    and 1 for outliers/anomalies. It is generated by applying
    ``threshold_`` on ``decision_scores_``.
<span class="ansi-red-fg">File:</span>           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/pyod/models/mo_gaal.py
<span class="ansi-red-fg">Type:</span>           ABCMeta
<span class="ansi-red-fg">Subclasses:</span>     
</pre>
</div>
</div>
</div>
<div class="cell" data-scrolled="true" data-tags="[]" data-execution_count="1613">
<div class="sourceCode cell-code" id="cb157"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> MO_GAAL(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb157-2"><a href="#cb157-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>]])</span>
<span id="cb157-3"><a href="#cb157-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'MO_GAAL_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super().__init__(name, **kwargs)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1 of 60

Testing for epoch 1 index 1:
32/32 [==============================] - 0s 658us/step

Testing for epoch 1 index 2:
32/32 [==============================] - 0s 865us/step
Epoch 2 of 60

Testing for epoch 2 index 1:
32/32 [==============================] - 0s 1ms/step

Testing for epoch 2 index 2:
32/32 [==============================] - 0s 1ms/step
Epoch 3 of 60

Testing for epoch 3 index 1:
32/32 [==============================] - 0s 1ms/step

Testing for epoch 3 index 2:
32/32 [==============================] - 0s 597us/step
Epoch 4 of 60

Testing for epoch 4 index 1:
32/32 [==============================] - 0s 623us/step

Testing for epoch 4 index 2:
32/32 [==============================] - 0s 633us/step
Epoch 5 of 60

Testing for epoch 5 index 1:
32/32 [==============================] - 0s 597us/step

Testing for epoch 5 index 2:
32/32 [==============================] - 0s 605us/step
Epoch 6 of 60

Testing for epoch 6 index 1:
32/32 [==============================] - 0s 810us/step

Testing for epoch 6 index 2:
32/32 [==============================] - 0s 613us/step
Epoch 7 of 60

Testing for epoch 7 index 1:
32/32 [==============================] - 0s 610us/step

Testing for epoch 7 index 2:
32/32 [==============================] - 0s 624us/step
Epoch 8 of 60

Testing for epoch 8 index 1:
32/32 [==============================] - 0s 597us/step

Testing for epoch 8 index 2:
32/32 [==============================] - 0s 833us/step
Epoch 9 of 60

Testing for epoch 9 index 1:
32/32 [==============================] - 0s 615us/step

Testing for epoch 9 index 2:
32/32 [==============================] - 0s 600us/step
Epoch 10 of 60

Testing for epoch 10 index 1:
32/32 [==============================] - 0s 614us/step

Testing for epoch 10 index 2:
32/32 [==============================] - 0s 635us/step
Epoch 11 of 60

Testing for epoch 11 index 1:
32/32 [==============================] - 0s 619us/step

Testing for epoch 11 index 2:
32/32 [==============================] - 0s 610us/step
Epoch 12 of 60

Testing for epoch 12 index 1:
32/32 [==============================] - 0s 610us/step

Testing for epoch 12 index 2:
32/32 [==============================] - 0s 613us/step
Epoch 13 of 60

Testing for epoch 13 index 1:
32/32 [==============================] - 0s 616us/step

Testing for epoch 13 index 2:
32/32 [==============================] - 0s 627us/step
Epoch 14 of 60

Testing for epoch 14 index 1:
32/32 [==============================] - 0s 621us/step

Testing for epoch 14 index 2:
32/32 [==============================] - 0s 614us/step
Epoch 15 of 60

Testing for epoch 15 index 1:
32/32 [==============================] - 0s 846us/step

Testing for epoch 15 index 2:
32/32 [==============================] - 0s 623us/step
Epoch 16 of 60

Testing for epoch 16 index 1:
32/32 [==============================] - 0s 915us/step

Testing for epoch 16 index 2:
32/32 [==============================] - 0s 836us/step
Epoch 17 of 60

Testing for epoch 17 index 1:
32/32 [==============================] - 0s 2ms/step

Testing for epoch 17 index 2:
32/32 [==============================] - 0s 829us/step
Epoch 18 of 60

Testing for epoch 18 index 1:
32/32 [==============================] - 0s 860us/step

Testing for epoch 18 index 2:
32/32 [==============================] - 0s 1ms/step
Epoch 19 of 60

Testing for epoch 19 index 1:
32/32 [==============================] - 0s 1ms/step

Testing for epoch 19 index 2:
32/32 [==============================] - 0s 2ms/step
Epoch 20 of 60

Testing for epoch 20 index 1:
32/32 [==============================] - 0s 832us/step

Testing for epoch 20 index 2:
32/32 [==============================] - 0s 2ms/step
Epoch 21 of 60

Testing for epoch 21 index 1:
32/32 [==============================] - 0s 870us/step

Testing for epoch 21 index 2:
32/32 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.3802
16/16 [==============================] - 0s 992us/step - loss: 0.7194
16/16 [==============================] - 0s 1ms/step - loss: 0.9382
16/16 [==============================] - 0s 2ms/step - loss: 1.1679
16/16 [==============================] - 0s 1ms/step - loss: 1.2953
16/16 [==============================] - 0s 982us/step - loss: 1.3707
16/16 [==============================] - 0s 1ms/step - loss: 1.4090
16/16 [==============================] - 0s 1ms/step - loss: 1.4370
16/16 [==============================] - 0s 1ms/step - loss: 1.4481
16/16 [==============================] - 0s 1ms/step - loss: 1.4524
Epoch 22 of 60

Testing for epoch 22 index 1:
32/32 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.3784
16/16 [==============================] - 0s 3ms/step - loss: 0.7280
16/16 [==============================] - 0s 1ms/step - loss: 0.9713
16/16 [==============================] - 0s 2ms/step - loss: 1.2091
16/16 [==============================] - 0s 1ms/step - loss: 1.3341
16/16 [==============================] - 0s 2ms/step - loss: 1.4019
16/16 [==============================] - 0s 2ms/step - loss: 1.4333
16/16 [==============================] - 0s 1ms/step - loss: 1.4551
16/16 [==============================] - 0s 1ms/step - loss: 1.4629
16/16 [==============================] - 0s 1ms/step - loss: 1.4656

Testing for epoch 22 index 2:
32/32 [==============================] - 0s 913us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.3866
16/16 [==============================] - 0s 1ms/step - loss: 0.7327
16/16 [==============================] - 0s 2ms/step - loss: 0.9839
16/16 [==============================] - 0s 5ms/step - loss: 1.2335
16/16 [==============================] - 0s 1ms/step - loss: 1.3481
16/16 [==============================] - 0s 3ms/step - loss: 1.4093
16/16 [==============================] - 0s 1ms/step - loss: 1.4348
16/16 [==============================] - 0s 2ms/step - loss: 1.4506
16/16 [==============================] - 0s 1ms/step - loss: 1.4559
16/16 [==============================] - 0s 2ms/step - loss: 1.4576
Epoch 23 of 60

Testing for epoch 23 index 1:
32/32 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 3ms/step - loss: 0.3956
16/16 [==============================] - 0s 1ms/step - loss: 0.7406
16/16 [==============================] - 0s 1ms/step - loss: 0.9936
16/16 [==============================] - 0s 2ms/step - loss: 1.2356
16/16 [==============================] - 0s 2ms/step - loss: 1.3418
16/16 [==============================] - 0s 1ms/step - loss: 1.3928
16/16 [==============================] - 0s 2ms/step - loss: 1.4131
16/16 [==============================] - 0s 1ms/step - loss: 1.4239
16/16 [==============================] - 0s 1ms/step - loss: 1.4275
16/16 [==============================] - 0s 1ms/step - loss: 1.4283

Testing for epoch 23 index 2:
32/32 [==============================] - 0s 5ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.3925
16/16 [==============================] - 0s 1ms/step - loss: 0.7523
16/16 [==============================] - 0s 1ms/step - loss: 1.0367
16/16 [==============================] - 0s 1ms/step - loss: 1.2950
16/16 [==============================] - 0s 2ms/step - loss: 1.3968
16/16 [==============================] - 0s 2ms/step - loss: 1.4439
16/16 [==============================] - 0s 2ms/step - loss: 1.4623
16/16 [==============================] - 0s 1ms/step - loss: 1.4710
16/16 [==============================] - 0s 2ms/step - loss: 1.4735
16/16 [==============================] - 0s 2ms/step - loss: 1.4740
Epoch 24 of 60

Testing for epoch 24 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.4051
16/16 [==============================] - 0s 2ms/step - loss: 0.7528
16/16 [==============================] - 0s 2ms/step - loss: 1.0473
16/16 [==============================] - 0s 1ms/step - loss: 1.2922
16/16 [==============================] - 0s 1ms/step - loss: 1.3798
16/16 [==============================] - 0s 2ms/step - loss: 1.4177
16/16 [==============================] - 0s 2ms/step - loss: 1.4316
16/16 [==============================] - 0s 2ms/step - loss: 1.4376
16/16 [==============================] - 0s 1ms/step - loss: 1.4391
16/16 [==============================] - 0s 2ms/step - loss: 1.4393

Testing for epoch 24 index 2:
32/32 [==============================] - 0s 897us/step
16/16 [==============================] - 0s 3ms/step - loss: 0.4123
16/16 [==============================] - 0s 2ms/step - loss: 0.7576
16/16 [==============================] - 0s 2ms/step - loss: 1.0566
16/16 [==============================] - 0s 2ms/step - loss: 1.2987
16/16 [==============================] - 0s 2ms/step - loss: 1.3765
16/16 [==============================] - 0s 4ms/step - loss: 1.4095
16/16 [==============================] - 0s 2ms/step - loss: 1.4206
16/16 [==============================] - 0s 2ms/step - loss: 1.4250
16/16 [==============================] - 0s 2ms/step - loss: 1.4259
16/16 [==============================] - 0s 1ms/step - loss: 1.4259
Epoch 25 of 60

Testing for epoch 25 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.4149
16/16 [==============================] - 0s 2ms/step - loss: 0.7675
16/16 [==============================] - 0s 1ms/step - loss: 1.0765
16/16 [==============================] - 0s 2ms/step - loss: 1.3167
16/16 [==============================] - 0s 1ms/step - loss: 1.3880
16/16 [==============================] - 0s 1ms/step - loss: 1.4164
16/16 [==============================] - 0s 2ms/step - loss: 1.4257
16/16 [==============================] - 0s 1ms/step - loss: 1.4288
16/16 [==============================] - 0s 1ms/step - loss: 1.4294
16/16 [==============================] - 0s 1ms/step - loss: 1.4293

Testing for epoch 25 index 2:
32/32 [==============================] - 0s 899us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.4172
16/16 [==============================] - 0s 1ms/step - loss: 0.7677
16/16 [==============================] - 0s 1ms/step - loss: 1.0818
16/16 [==============================] - 0s 1ms/step - loss: 1.3143
16/16 [==============================] - 0s 1ms/step - loss: 1.3797
16/16 [==============================] - 0s 1ms/step - loss: 1.4048
16/16 [==============================] - 0s 1ms/step - loss: 1.4123
16/16 [==============================] - 0s 1ms/step - loss: 1.4147
16/16 [==============================] - 0s 2ms/step - loss: 1.4150
16/16 [==============================] - 0s 2ms/step - loss: 1.4148
Epoch 26 of 60

Testing for epoch 26 index 1:
32/32 [==============================] - 0s 754us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.4148
16/16 [==============================] - 0s 2ms/step - loss: 0.7766
16/16 [==============================] - 0s 4ms/step - loss: 1.1064
16/16 [==============================] - 0s 2ms/step - loss: 1.3376
16/16 [==============================] - 0s 1ms/step - loss: 1.4002
16/16 [==============================] - 0s 1ms/step - loss: 1.4228
16/16 [==============================] - 0s 1ms/step - loss: 1.4290
16/16 [==============================] - 0s 1ms/step - loss: 1.4308
16/16 [==============================] - 0s 1ms/step - loss: 1.4309
16/16 [==============================] - 0s 1ms/step - loss: 1.4307

Testing for epoch 26 index 2:
32/32 [==============================] - 0s 842us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.4190
16/16 [==============================] - 0s 1ms/step - loss: 0.7761
16/16 [==============================] - 0s 2ms/step - loss: 1.1055
16/16 [==============================] - 0s 1ms/step - loss: 1.3282
16/16 [==============================] - 0s 2ms/step - loss: 1.3846
16/16 [==============================] - 0s 1ms/step - loss: 1.4044
16/16 [==============================] - 0s 2ms/step - loss: 1.4095
16/16 [==============================] - 0s 1ms/step - loss: 1.4108
16/16 [==============================] - 0s 2ms/step - loss: 1.4108
16/16 [==============================] - 0s 2ms/step - loss: 1.4105
Epoch 27 of 60

Testing for epoch 27 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.4225
16/16 [==============================] - 0s 1ms/step - loss: 0.7746
16/16 [==============================] - 0s 1ms/step - loss: 1.1026
16/16 [==============================] - 0s 1ms/step - loss: 1.3159
16/16 [==============================] - 0s 1ms/step - loss: 1.3665
16/16 [==============================] - 0s 2ms/step - loss: 1.3838
16/16 [==============================] - 0s 972us/step - loss: 1.3880
16/16 [==============================] - 0s 1ms/step - loss: 1.3888
16/16 [==============================] - 0s 1ms/step - loss: 1.3887
16/16 [==============================] - 0s 2ms/step - loss: 1.3884

Testing for epoch 27 index 2:
32/32 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.4196
16/16 [==============================] - 0s 1ms/step - loss: 0.7825
16/16 [==============================] - 0s 2ms/step - loss: 1.1245
16/16 [==============================] - 0s 1ms/step - loss: 1.3411
16/16 [==============================] - 0s 2ms/step - loss: 1.3905
16/16 [==============================] - 0s 1ms/step - loss: 1.4071
16/16 [==============================] - 0s 2ms/step - loss: 1.4109
16/16 [==============================] - 0s 2ms/step - loss: 1.4116
16/16 [==============================] - 0s 1ms/step - loss: 1.4115
16/16 [==============================] - 0s 3ms/step - loss: 1.4112
Epoch 28 of 60

Testing for epoch 28 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.4124
16/16 [==============================] - 0s 2ms/step - loss: 0.7896
16/16 [==============================] - 0s 2ms/step - loss: 1.1481
16/16 [==============================] - 0s 2ms/step - loss: 1.3684
16/16 [==============================] - 0s 2ms/step - loss: 1.4180
16/16 [==============================] - 0s 2ms/step - loss: 1.4340
16/16 [==============================] - 0s 2ms/step - loss: 1.4374
16/16 [==============================] - 0s 1ms/step - loss: 1.4380
16/16 [==============================] - 0s 1ms/step - loss: 1.4378
16/16 [==============================] - 0s 1ms/step - loss: 1.4375

Testing for epoch 28 index 2:
32/32 [==============================] - 0s 3ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.4173
16/16 [==============================] - 0s 2ms/step - loss: 0.7865
16/16 [==============================] - 0s 2ms/step - loss: 1.1353
16/16 [==============================] - 0s 1ms/step - loss: 1.3465
16/16 [==============================] - 0s 1ms/step - loss: 1.3927
16/16 [==============================] - 0s 1ms/step - loss: 1.4072
16/16 [==============================] - 0s 2ms/step - loss: 1.4101
16/16 [==============================] - 0s 1ms/step - loss: 1.4105
16/16 [==============================] - 0s 1ms/step - loss: 1.4102
16/16 [==============================] - 0s 1ms/step - loss: 1.4099
Epoch 29 of 60

Testing for epoch 29 index 1:
32/32 [==============================] - 0s 636us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.4145
16/16 [==============================] - 0s 1ms/step - loss: 0.7933
16/16 [==============================] - 0s 1ms/step - loss: 1.1517
16/16 [==============================] - 0s 1ms/step - loss: 1.3656
16/16 [==============================] - 0s 5ms/step - loss: 1.4111
16/16 [==============================] - 0s 4ms/step - loss: 1.4251
16/16 [==============================] - 0s 4ms/step - loss: 1.4278
16/16 [==============================] - 0s 2ms/step - loss: 1.4281
16/16 [==============================] - 0s 2ms/step - loss: 1.4278
16/16 [==============================] - 0s 2ms/step - loss: 1.4275

Testing for epoch 29 index 2:
32/32 [==============================] - 0s 3ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.4111
16/16 [==============================] - 0s 1ms/step - loss: 0.7909
16/16 [==============================] - 0s 1ms/step - loss: 1.1516
16/16 [==============================] - 0s 2ms/step - loss: 1.3647
16/16 [==============================] - 0s 1ms/step - loss: 1.4090
16/16 [==============================] - 0s 1ms/step - loss: 1.4224
16/16 [==============================] - 0s 2ms/step - loss: 1.4249
16/16 [==============================] - 0s 2ms/step - loss: 1.4250
16/16 [==============================] - 0s 2ms/step - loss: 1.4247
16/16 [==============================] - 0s 2ms/step - loss: 1.4244
Epoch 30 of 60

Testing for epoch 30 index 1:
32/32 [==============================] - 0s 869us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.4107
16/16 [==============================] - 0s 1ms/step - loss: 0.7938
16/16 [==============================] - 0s 1ms/step - loss: 1.1611
16/16 [==============================] - 0s 2ms/step - loss: 1.3750
16/16 [==============================] - 0s 994us/step - loss: 1.4188
16/16 [==============================] - 0s 903us/step - loss: 1.4319
16/16 [==============================] - 0s 916us/step - loss: 1.4343
16/16 [==============================] - 0s 997us/step - loss: 1.4344
16/16 [==============================] - 0s 4ms/step - loss: 1.4341
16/16 [==============================] - 0s 1ms/step - loss: 1.4338

Testing for epoch 30 index 2:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.4054
16/16 [==============================] - 0s 4ms/step - loss: 0.7927
16/16 [==============================] - 0s 3ms/step - loss: 1.1646
16/16 [==============================] - 0s 2ms/step - loss: 1.3805
16/16 [==============================] - 0s 4ms/step - loss: 1.4241
16/16 [==============================] - 0s 2ms/step - loss: 1.4369
16/16 [==============================] - 0s 2ms/step - loss: 1.4391
16/16 [==============================] - 0s 1ms/step - loss: 1.4391
16/16 [==============================] - 0s 1ms/step - loss: 1.4388
16/16 [==============================] - 0s 1ms/step - loss: 1.4384
Epoch 31 of 60

Testing for epoch 31 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.3962
16/16 [==============================] - 0s 2ms/step - loss: 0.7984
16/16 [==============================] - 0s 1ms/step - loss: 1.1888
16/16 [==============================] - 0s 5ms/step - loss: 1.4144
16/16 [==============================] - 0s 3ms/step - loss: 1.4594
16/16 [==============================] - 0s 2ms/step - loss: 1.4726
16/16 [==============================] - 0s 4ms/step - loss: 1.4748
16/16 [==============================] - 0s 2ms/step - loss: 1.4749
16/16 [==============================] - 0s 1ms/step - loss: 1.4745
16/16 [==============================] - 0s 1ms/step - loss: 1.4742

Testing for epoch 31 index 2:
32/32 [==============================] - 0s 898us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.3984
16/16 [==============================] - 0s 1ms/step - loss: 0.7936
16/16 [==============================] - 0s 1ms/step - loss: 1.1789
16/16 [==============================] - 0s 3ms/step - loss: 1.4023
16/16 [==============================] - 0s 2ms/step - loss: 1.4465
16/16 [==============================] - 0s 1ms/step - loss: 1.4593
16/16 [==============================] - 0s 954us/step - loss: 1.4615
16/16 [==============================] - 0s 1ms/step - loss: 1.4616
16/16 [==============================] - 0s 998us/step - loss: 1.4612
16/16 [==============================] - 0s 2ms/step - loss: 1.4608
Epoch 32 of 60

Testing for epoch 32 index 1:
32/32 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.3891
16/16 [==============================] - 0s 1ms/step - loss: 0.7938
16/16 [==============================] - 0s 1ms/step - loss: 1.1911
16/16 [==============================] - 0s 1ms/step - loss: 1.4219
16/16 [==============================] - 0s 3ms/step - loss: 1.4666
16/16 [==============================] - 0s 2ms/step - loss: 1.4796
16/16 [==============================] - 0s 2ms/step - loss: 1.4818
16/16 [==============================] - 0s 1ms/step - loss: 1.4818
16/16 [==============================] - 0s 1ms/step - loss: 1.4815
16/16 [==============================] - 0s 1ms/step - loss: 1.4811

Testing for epoch 32 index 2:
32/32 [==============================] - 0s 913us/step
16/16 [==============================] - 0s 3ms/step - loss: 0.3883
16/16 [==============================] - 0s 3ms/step - loss: 0.7919
16/16 [==============================] - 0s 2ms/step - loss: 1.1930
16/16 [==============================] - 0s 1ms/step - loss: 1.4285
16/16 [==============================] - 0s 2ms/step - loss: 1.4721
16/16 [==============================] - 0s 1ms/step - loss: 1.4852
16/16 [==============================] - 0s 2ms/step - loss: 1.4875
16/16 [==============================] - 0s 1ms/step - loss: 1.4875
16/16 [==============================] - 0s 1ms/step - loss: 1.4871
16/16 [==============================] - 0s 3ms/step - loss: 1.4867
Epoch 33 of 60

Testing for epoch 33 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.3772
16/16 [==============================] - 0s 1ms/step - loss: 0.7929
16/16 [==============================] - 0s 2ms/step - loss: 1.2122
16/16 [==============================] - 0s 1ms/step - loss: 1.4584
16/16 [==============================] - 0s 3ms/step - loss: 1.5041
16/16 [==============================] - 0s 3ms/step - loss: 1.5178
16/16 [==============================] - 0s 1ms/step - loss: 1.5201
16/16 [==============================] - 0s 1ms/step - loss: 1.5202
16/16 [==============================] - 0s 1ms/step - loss: 1.5198
16/16 [==============================] - 0s 973us/step - loss: 1.5194

Testing for epoch 33 index 2:
32/32 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.3685
16/16 [==============================] - 0s 2ms/step - loss: 0.7940
16/16 [==============================] - 0s 1ms/step - loss: 1.2262
16/16 [==============================] - 0s 2ms/step - loss: 1.4829
16/16 [==============================] - 0s 2ms/step - loss: 1.5311
16/16 [==============================] - 0s 1ms/step - loss: 1.5455
16/16 [==============================] - 0s 2ms/step - loss: 1.5481
16/16 [==============================] - 0s 4ms/step - loss: 1.5482
16/16 [==============================] - 0s 2ms/step - loss: 1.5478
16/16 [==============================] - 0s 3ms/step - loss: 1.5474
Epoch 34 of 60

Testing for epoch 34 index 1:
32/32 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 4ms/step - loss: 0.3693
16/16 [==============================] - 0s 1ms/step - loss: 0.7909
16/16 [==============================] - 0s 1ms/step - loss: 1.2196
16/16 [==============================] - 0s 1ms/step - loss: 1.4752
16/16 [==============================] - 0s 2ms/step - loss: 1.5233
16/16 [==============================] - 0s 2ms/step - loss: 1.5375
16/16 [==============================] - 0s 1ms/step - loss: 1.5400
16/16 [==============================] - 0s 1ms/step - loss: 1.5400
16/16 [==============================] - 0s 2ms/step - loss: 1.5396
16/16 [==============================] - 0s 3ms/step - loss: 1.5392

Testing for epoch 34 index 2:
32/32 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.3592
16/16 [==============================] - 0s 1ms/step - loss: 0.7936
16/16 [==============================] - 0s 2ms/step - loss: 1.2413
16/16 [==============================] - 0s 2ms/step - loss: 1.5117
16/16 [==============================] - 0s 2ms/step - loss: 1.5633
16/16 [==============================] - 0s 2ms/step - loss: 1.5786
16/16 [==============================] - 0s 4ms/step - loss: 1.5814
16/16 [==============================] - 0s 2ms/step - loss: 1.5816
16/16 [==============================] - 0s 2ms/step - loss: 1.5812
16/16 [==============================] - 0s 1ms/step - loss: 1.5808
Epoch 35 of 60

Testing for epoch 35 index 1:
32/32 [==============================] - 0s 833us/step
16/16 [==============================] - 0s 923us/step - loss: 0.3582
16/16 [==============================] - 0s 1ms/step - loss: 0.7895
16/16 [==============================] - 0s 1ms/step - loss: 1.2360
16/16 [==============================] - 0s 1ms/step - loss: 1.5072
16/16 [==============================] - 0s 952us/step - loss: 1.5584
16/16 [==============================] - 0s 2ms/step - loss: 1.5735
16/16 [==============================] - 0s 2ms/step - loss: 1.5763
16/16 [==============================] - 0s 1ms/step - loss: 1.5764
16/16 [==============================] - 0s 2ms/step - loss: 1.5760
16/16 [==============================] - 0s 1ms/step - loss: 1.5756

Testing for epoch 35 index 2:
32/32 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.3488
16/16 [==============================] - 0s 2ms/step - loss: 0.7942
16/16 [==============================] - 0s 1ms/step - loss: 1.2601
16/16 [==============================] - 0s 1ms/step - loss: 1.5469
16/16 [==============================] - 0s 1ms/step - loss: 1.6015
16/16 [==============================] - 0s 1ms/step - loss: 1.6177
16/16 [==============================] - 0s 1ms/step - loss: 1.6207
16/16 [==============================] - 0s 2ms/step - loss: 1.6210
16/16 [==============================] - 0s 2ms/step - loss: 1.6206
16/16 [==============================] - 0s 1ms/step - loss: 1.6202
Epoch 36 of 60

Testing for epoch 36 index 1:
32/32 [==============================] - 0s 829us/step
16/16 [==============================] - 0s 980us/step - loss: 0.3512
16/16 [==============================] - 0s 943us/step - loss: 0.7854
16/16 [==============================] - 0s 842us/step - loss: 1.2413
16/16 [==============================] - 0s 816us/step - loss: 1.5228
16/16 [==============================] - 0s 1ms/step - loss: 1.5761
16/16 [==============================] - 0s 829us/step - loss: 1.5918
16/16 [==============================] - 0s 819us/step - loss: 1.5947
16/16 [==============================] - 0s 801us/step - loss: 1.5949
16/16 [==============================] - 0s 852us/step - loss: 1.5945
16/16 [==============================] - 0s 874us/step - loss: 1.5940

Testing for epoch 36 index 2:
32/32 [==============================] - 0s 637us/step
16/16 [==============================] - 0s 798us/step - loss: 0.3437
16/16 [==============================] - 0s 798us/step - loss: 0.7857
16/16 [==============================] - 0s 793us/step - loss: 1.2551
16/16 [==============================] - 0s 784us/step - loss: 1.5475
16/16 [==============================] - 0s 778us/step - loss: 1.6036
16/16 [==============================] - 0s 789us/step - loss: 1.6201
16/16 [==============================] - 0s 800us/step - loss: 1.6232
16/16 [==============================] - 0s 786us/step - loss: 1.6234
16/16 [==============================] - 0s 804us/step - loss: 1.6230
16/16 [==============================] - 0s 771us/step - loss: 1.6226
Epoch 37 of 60

Testing for epoch 37 index 1:
32/32 [==============================] - 0s 837us/step
16/16 [==============================] - 0s 906us/step - loss: 0.3392
16/16 [==============================] - 0s 772us/step - loss: 0.7786
16/16 [==============================] - 0s 1ms/step - loss: 1.2518
16/16 [==============================] - 0s 1ms/step - loss: 1.5415
16/16 [==============================] - 0s 1ms/step - loss: 1.5975
16/16 [==============================] - 0s 1ms/step - loss: 1.6140
16/16 [==============================] - 0s 836us/step - loss: 1.6171
16/16 [==============================] - 0s 781us/step - loss: 1.6173
16/16 [==============================] - 0s 805us/step - loss: 1.6169
16/16 [==============================] - 0s 802us/step - loss: 1.6165

Testing for epoch 37 index 2:
32/32 [==============================] - 0s 616us/step
16/16 [==============================] - 0s 831us/step - loss: 0.3362
16/16 [==============================] - 0s 795us/step - loss: 0.7794
16/16 [==============================] - 0s 806us/step - loss: 1.2570
16/16 [==============================] - 0s 780us/step - loss: 1.5521
16/16 [==============================] - 0s 821us/step - loss: 1.6094
16/16 [==============================] - 0s 778us/step - loss: 1.6265
16/16 [==============================] - 0s 835us/step - loss: 1.6298
16/16 [==============================] - 0s 774us/step - loss: 1.6300
16/16 [==============================] - 0s 773us/step - loss: 1.6296
16/16 [==============================] - 0s 782us/step - loss: 1.6291
Epoch 38 of 60

Testing for epoch 38 index 1:
32/32 [==============================] - 0s 607us/step
16/16 [==============================] - 0s 821us/step - loss: 0.3350
16/16 [==============================] - 0s 780us/step - loss: 0.7829
16/16 [==============================] - 0s 807us/step - loss: 1.2628
16/16 [==============================] - 0s 798us/step - loss: 1.5617
16/16 [==============================] - 0s 790us/step - loss: 1.6196
16/16 [==============================] - 0s 810us/step - loss: 1.6369
16/16 [==============================] - 0s 814us/step - loss: 1.6402
16/16 [==============================] - 0s 784us/step - loss: 1.6404
16/16 [==============================] - 0s 812us/step - loss: 1.6400
16/16 [==============================] - 0s 808us/step - loss: 1.6395

Testing for epoch 38 index 2:
32/32 [==============================] - 0s 612us/step
16/16 [==============================] - 0s 854us/step - loss: 0.3207
16/16 [==============================] - 0s 771us/step - loss: 0.7883
16/16 [==============================] - 0s 791us/step - loss: 1.2870
16/16 [==============================] - 0s 762us/step - loss: 1.6028
16/16 [==============================] - 0s 770us/step - loss: 1.6645
16/16 [==============================] - 0s 770us/step - loss: 1.6830
16/16 [==============================] - 0s 792us/step - loss: 1.6866
16/16 [==============================] - 0s 788us/step - loss: 1.6869
16/16 [==============================] - 0s 827us/step - loss: 1.6865
16/16 [==============================] - 0s 758us/step - loss: 1.6861
Epoch 39 of 60

Testing for epoch 39 index 1:
32/32 [==============================] - 0s 608us/step
16/16 [==============================] - 0s 807us/step - loss: 0.3234
16/16 [==============================] - 0s 771us/step - loss: 0.7890
16/16 [==============================] - 0s 777us/step - loss: 1.2822
16/16 [==============================] - 0s 780us/step - loss: 1.5961
16/16 [==============================] - 0s 807us/step - loss: 1.6572
16/16 [==============================] - 0s 795us/step - loss: 1.6753
16/16 [==============================] - 0s 803us/step - loss: 1.6788
16/16 [==============================] - 0s 1ms/step - loss: 1.6791
16/16 [==============================] - 0s 682us/step - loss: 1.6786
16/16 [==============================] - 0s 750us/step - loss: 1.6782

Testing for epoch 39 index 2:
32/32 [==============================] - 0s 593us/step
16/16 [==============================] - 0s 660us/step - loss: 0.3169
16/16 [==============================] - 0s 1ms/step - loss: 0.7846
16/16 [==============================] - 0s 664us/step - loss: 1.2831
16/16 [==============================] - 0s 655us/step - loss: 1.6025
16/16 [==============================] - 0s 771us/step - loss: 1.6649
16/16 [==============================] - 0s 675us/step - loss: 1.6836
16/16 [==============================] - 0s 677us/step - loss: 1.6870
16/16 [==============================] - 0s 815us/step - loss: 1.6873
16/16 [==============================] - 0s 791us/step - loss: 1.6868
16/16 [==============================] - 0s 785us/step - loss: 1.6863
Epoch 40 of 60

Testing for epoch 40 index 1:
32/32 [==============================] - 0s 612us/step
16/16 [==============================] - 0s 798us/step - loss: 0.3223
16/16 [==============================] - 0s 769us/step - loss: 0.7759
16/16 [==============================] - 0s 774us/step - loss: 1.2607
16/16 [==============================] - 0s 769us/step - loss: 1.5701
16/16 [==============================] - 0s 806us/step - loss: 1.6300
16/16 [==============================] - 0s 781us/step - loss: 1.6478
16/16 [==============================] - 0s 791us/step - loss: 1.6509
16/16 [==============================] - 0s 779us/step - loss: 1.6511
16/16 [==============================] - 0s 1ms/step - loss: 1.6506
16/16 [==============================] - 0s 779us/step - loss: 1.6501

Testing for epoch 40 index 2:
32/32 [==============================] - 0s 592us/step
16/16 [==============================] - 0s 779us/step - loss: 0.3189
16/16 [==============================] - 0s 824us/step - loss: 0.7810
16/16 [==============================] - 0s 808us/step - loss: 1.2802
16/16 [==============================] - 0s 809us/step - loss: 1.6001
16/16 [==============================] - 0s 777us/step - loss: 1.6622
16/16 [==============================] - 0s 785us/step - loss: 1.6807
16/16 [==============================] - 0s 780us/step - loss: 1.6840
16/16 [==============================] - 0s 805us/step - loss: 1.6842
16/16 [==============================] - 0s 783us/step - loss: 1.6838
16/16 [==============================] - 0s 818us/step - loss: 1.6833
Epoch 41 of 60

Testing for epoch 41 index 1:
32/32 [==============================] - 0s 606us/step
16/16 [==============================] - 0s 797us/step - loss: 0.3072
16/16 [==============================] - 0s 828us/step - loss: 0.7912
16/16 [==============================] - 0s 783us/step - loss: 1.3176
16/16 [==============================] - 0s 819us/step - loss: 1.6547
16/16 [==============================] - 0s 783us/step - loss: 1.7199
16/16 [==============================] - 0s 810us/step - loss: 1.7392
16/16 [==============================] - 0s 818us/step - loss: 1.7426
16/16 [==============================] - 0s 773us/step - loss: 1.7428
16/16 [==============================] - 0s 769us/step - loss: 1.7424
16/16 [==============================] - 0s 772us/step - loss: 1.7419

Testing for epoch 41 index 2:
32/32 [==============================] - 0s 613us/step
16/16 [==============================] - 0s 783us/step - loss: 0.3088
16/16 [==============================] - 0s 818us/step - loss: 0.7812
16/16 [==============================] - 0s 804us/step - loss: 1.2987
16/16 [==============================] - 0s 774us/step - loss: 1.6308
16/16 [==============================] - 0s 770us/step - loss: 1.6949
16/16 [==============================] - 0s 772us/step - loss: 1.7138
16/16 [==============================] - 0s 803us/step - loss: 1.7172
16/16 [==============================] - 0s 773us/step - loss: 1.7174
16/16 [==============================] - 0s 789us/step - loss: 1.7170
16/16 [==============================] - 0s 771us/step - loss: 1.7165
Epoch 42 of 60

Testing for epoch 42 index 1:
32/32 [==============================] - 0s 592us/step
16/16 [==============================] - 0s 788us/step - loss: 0.3021
16/16 [==============================] - 0s 792us/step - loss: 0.7873
16/16 [==============================] - 0s 673us/step - loss: 1.3203
16/16 [==============================] - 0s 786us/step - loss: 1.6612
16/16 [==============================] - 0s 2ms/step - loss: 1.7264
16/16 [==============================] - 0s 2ms/step - loss: 1.7454
16/16 [==============================] - 0s 2ms/step - loss: 1.7487
16/16 [==============================] - 0s 767us/step - loss: 1.7489
16/16 [==============================] - 0s 2ms/step - loss: 1.7484
16/16 [==============================] - 0s 844us/step - loss: 1.7479

Testing for epoch 42 index 2:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.2994
16/16 [==============================] - 0s 655us/step - loss: 0.7933
16/16 [==============================] - 0s 1ms/step - loss: 1.3330
16/16 [==============================] - 0s 818us/step - loss: 1.6811
16/16 [==============================] - 0s 1ms/step - loss: 1.7477
16/16 [==============================] - 0s 806us/step - loss: 1.7671
16/16 [==============================] - 0s 825us/step - loss: 1.7705
16/16 [==============================] - 0s 2ms/step - loss: 1.7707
16/16 [==============================] - 0s 1ms/step - loss: 1.7702
16/16 [==============================] - 0s 2ms/step - loss: 1.7697
Epoch 43 of 60

Testing for epoch 43 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.3054
16/16 [==============================] - 0s 2ms/step - loss: 0.7894
16/16 [==============================] - 0s 2ms/step - loss: 1.3140
16/16 [==============================] - 0s 799us/step - loss: 1.6513
16/16 [==============================] - 0s 1ms/step - loss: 1.7147
16/16 [==============================] - 0s 894us/step - loss: 1.7329
16/16 [==============================] - 0s 2ms/step - loss: 1.7359
16/16 [==============================] - 0s 896us/step - loss: 1.7360
16/16 [==============================] - 0s 1ms/step - loss: 1.7355
16/16 [==============================] - 0s 2ms/step - loss: 1.7350

Testing for epoch 43 index 2:
32/32 [==============================] - 0s 551us/step
16/16 [==============================] - 0s 2ms/step - loss: 0.2985
16/16 [==============================] - 0s 2ms/step - loss: 0.7894
16/16 [==============================] - 0s 2ms/step - loss: 1.3189
16/16 [==============================] - 0s 2ms/step - loss: 1.6614
16/16 [==============================] - 0s 803us/step - loss: 1.7256
16/16 [==============================] - 0s 2ms/step - loss: 1.7439
16/16 [==============================] - 0s 2ms/step - loss: 1.7470
16/16 [==============================] - 0s 2ms/step - loss: 1.7471
16/16 [==============================] - 0s 2ms/step - loss: 1.7465
16/16 [==============================] - 0s 2ms/step - loss: 1.7460
Epoch 44 of 60

Testing for epoch 44 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.2929
16/16 [==============================] - 0s 773us/step - loss: 0.7969
16/16 [==============================] - 0s 1ms/step - loss: 1.3407
16/16 [==============================] - 0s 2ms/step - loss: 1.6916
16/16 [==============================] - 0s 2ms/step - loss: 1.7567
16/16 [==============================] - 0s 819us/step - loss: 1.7749
16/16 [==============================] - 0s 2ms/step - loss: 1.7779
16/16 [==============================] - 0s 2ms/step - loss: 1.7780
16/16 [==============================] - 0s 2ms/step - loss: 1.7775
16/16 [==============================] - 0s 2ms/step - loss: 1.7769

Testing for epoch 44 index 2:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 796us/step - loss: 0.2909
16/16 [==============================] - 0s 772us/step - loss: 0.7933
16/16 [==============================] - 0s 823us/step - loss: 1.3388
16/16 [==============================] - 0s 2ms/step - loss: 1.6890
16/16 [==============================] - 0s 951us/step - loss: 1.7538
16/16 [==============================] - 0s 831us/step - loss: 1.7717
16/16 [==============================] - 0s 1ms/step - loss: 1.7746
16/16 [==============================] - 0s 2ms/step - loss: 1.7747
16/16 [==============================] - 0s 796us/step - loss: 1.7741
16/16 [==============================] - 0s 2ms/step - loss: 1.7736
Epoch 45 of 60

Testing for epoch 45 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 791us/step - loss: 0.2946
16/16 [==============================] - 0s 786us/step - loss: 0.7951
16/16 [==============================] - 0s 823us/step - loss: 1.3410
16/16 [==============================] - 0s 1ms/step - loss: 1.6875
16/16 [==============================] - 0s 1ms/step - loss: 1.7506
16/16 [==============================] - 0s 812us/step - loss: 1.7678
16/16 [==============================] - 0s 1ms/step - loss: 1.7705
16/16 [==============================] - 0s 2ms/step - loss: 1.7705
16/16 [==============================] - 0s 1ms/step - loss: 1.7700
16/16 [==============================] - 0s 2ms/step - loss: 1.7694

Testing for epoch 45 index 2:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.2911
16/16 [==============================] - 0s 789us/step - loss: 0.7943
16/16 [==============================] - 0s 817us/step - loss: 1.3485
16/16 [==============================] - 0s 2ms/step - loss: 1.6968
16/16 [==============================] - 0s 934us/step - loss: 1.7600
16/16 [==============================] - 0s 2ms/step - loss: 1.7771
16/16 [==============================] - 0s 816us/step - loss: 1.7797
16/16 [==============================] - 0s 807us/step - loss: 1.7796
16/16 [==============================] - 0s 820us/step - loss: 1.7791
16/16 [==============================] - 0s 932us/step - loss: 1.7785
Epoch 46 of 60

Testing for epoch 46 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 804us/step - loss: 0.2932
16/16 [==============================] - 0s 2ms/step - loss: 0.7938
16/16 [==============================] - 0s 2ms/step - loss: 1.3487
16/16 [==============================] - 0s 942us/step - loss: 1.6919
16/16 [==============================] - 0s 984us/step - loss: 1.7534
16/16 [==============================] - 0s 828us/step - loss: 1.7699
16/16 [==============================] - 0s 2ms/step - loss: 1.7723
16/16 [==============================] - 0s 839us/step - loss: 1.7722
16/16 [==============================] - 0s 801us/step - loss: 1.7717
16/16 [==============================] - 0s 796us/step - loss: 1.7711

Testing for epoch 46 index 2:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 805us/step - loss: 0.2842
16/16 [==============================] - 0s 2ms/step - loss: 0.8012
16/16 [==============================] - 0s 2ms/step - loss: 1.3785
16/16 [==============================] - 0s 809us/step - loss: 1.7329
16/16 [==============================] - 0s 853us/step - loss: 1.7961
16/16 [==============================] - 0s 2ms/step - loss: 1.8129
16/16 [==============================] - 0s 812us/step - loss: 1.8153
16/16 [==============================] - 0s 1ms/step - loss: 1.8152
16/16 [==============================] - 0s 760us/step - loss: 1.8146
16/16 [==============================] - 0s 794us/step - loss: 1.8141
Epoch 47 of 60

Testing for epoch 47 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 831us/step - loss: 0.2767
16/16 [==============================] - 0s 805us/step - loss: 0.8072
16/16 [==============================] - 0s 1ms/step - loss: 1.4037
16/16 [==============================] - 0s 869us/step - loss: 1.7654
16/16 [==============================] - 0s 843us/step - loss: 1.8289
16/16 [==============================] - 0s 814us/step - loss: 1.8455
16/16 [==============================] - 0s 837us/step - loss: 1.8479
16/16 [==============================] - 0s 827us/step - loss: 1.8477
16/16 [==============================] - 0s 2ms/step - loss: 1.8471
16/16 [==============================] - 0s 2ms/step - loss: 1.8466

Testing for epoch 47 index 2:
32/32 [==============================] - 0s 582us/step
16/16 [==============================] - 0s 2ms/step - loss: 0.2830
16/16 [==============================] - 0s 2ms/step - loss: 0.8001
16/16 [==============================] - 0s 2ms/step - loss: 1.3796
16/16 [==============================] - 0s 799us/step - loss: 1.7297
16/16 [==============================] - 0s 822us/step - loss: 1.7904
16/16 [==============================] - 0s 2ms/step - loss: 1.8061
16/16 [==============================] - 0s 830us/step - loss: 1.8082
16/16 [==============================] - 0s 2ms/step - loss: 1.8080
16/16 [==============================] - 0s 805us/step - loss: 1.8074
16/16 [==============================] - 0s 842us/step - loss: 1.8069
Epoch 48 of 60

Testing for epoch 48 index 1:
32/32 [==============================] - 0s 671us/step
16/16 [==============================] - 0s 763us/step - loss: 0.2810
16/16 [==============================] - 0s 786us/step - loss: 0.8040
16/16 [==============================] - 0s 770us/step - loss: 1.3884
16/16 [==============================] - 0s 764us/step - loss: 1.7378
16/16 [==============================] - 0s 757us/step - loss: 1.7973
16/16 [==============================] - 0s 769us/step - loss: 1.8124
16/16 [==============================] - 0s 773us/step - loss: 1.8144
16/16 [==============================] - 0s 1ms/step - loss: 1.8141
16/16 [==============================] - 0s 1ms/step - loss: 1.8134
16/16 [==============================] - 0s 1ms/step - loss: 1.8129

Testing for epoch 48 index 2:
32/32 [==============================] - 0s 602us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.2789
16/16 [==============================] - 0s 783us/step - loss: 0.8059
16/16 [==============================] - 0s 779us/step - loss: 1.3913
16/16 [==============================] - 0s 762us/step - loss: 1.7401
16/16 [==============================] - 0s 762us/step - loss: 1.7989
16/16 [==============================] - 0s 846us/step - loss: 1.8136
16/16 [==============================] - 0s 857us/step - loss: 1.8155
16/16 [==============================] - 0s 839us/step - loss: 1.8151
16/16 [==============================] - 0s 860us/step - loss: 1.8145
16/16 [==============================] - 0s 877us/step - loss: 1.8139
Epoch 49 of 60

Testing for epoch 49 index 1:
32/32 [==============================] - 0s 602us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.2754
16/16 [==============================] - 0s 800us/step - loss: 0.8119
16/16 [==============================] - 0s 791us/step - loss: 1.4077
16/16 [==============================] - 0s 786us/step - loss: 1.7589
16/16 [==============================] - 0s 1ms/step - loss: 1.8171
16/16 [==============================] - 0s 801us/step - loss: 1.8315
16/16 [==============================] - 0s 886us/step - loss: 1.8332
16/16 [==============================] - 0s 1ms/step - loss: 1.8328
16/16 [==============================] - 0s 875us/step - loss: 1.8322
16/16 [==============================] - 0s 861us/step - loss: 1.8316

Testing for epoch 49 index 2:
32/32 [==============================] - 0s 617us/step
16/16 [==============================] - 0s 789us/step - loss: 0.2707
16/16 [==============================] - 0s 784us/step - loss: 0.8127
16/16 [==============================] - 0s 817us/step - loss: 1.4127
16/16 [==============================] - 0s 815us/step - loss: 1.7644
16/16 [==============================] - 0s 792us/step - loss: 1.8221
16/16 [==============================] - 0s 817us/step - loss: 1.8361
16/16 [==============================] - 0s 800us/step - loss: 1.8377
16/16 [==============================] - 0s 800us/step - loss: 1.8373
16/16 [==============================] - 0s 812us/step - loss: 1.8366
16/16 [==============================] - 0s 856us/step - loss: 1.8360
Epoch 50 of 60

Testing for epoch 50 index 1:
32/32 [==============================] - 0s 610us/step
16/16 [==============================] - 0s 809us/step - loss: 0.2747
16/16 [==============================] - 0s 783us/step - loss: 0.8236
16/16 [==============================] - 0s 793us/step - loss: 1.4341
16/16 [==============================] - 0s 1ms/step - loss: 1.7868
16/16 [==============================] - 0s 904us/step - loss: 1.8438
16/16 [==============================] - 0s 877us/step - loss: 1.8574
16/16 [==============================] - 0s 811us/step - loss: 1.8589
16/16 [==============================] - 0s 807us/step - loss: 1.8584
16/16 [==============================] - 0s 791us/step - loss: 1.8578
16/16 [==============================] - 0s 820us/step - loss: 1.8572

Testing for epoch 50 index 2:
32/32 [==============================] - 0s 613us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.2854
16/16 [==============================] - 0s 818us/step - loss: 0.8126
16/16 [==============================] - 0s 790us/step - loss: 1.3992
16/16 [==============================] - 0s 837us/step - loss: 1.7342
16/16 [==============================] - 0s 789us/step - loss: 1.7876
16/16 [==============================] - 0s 781us/step - loss: 1.8000
16/16 [==============================] - 0s 785us/step - loss: 1.8013
16/16 [==============================] - 0s 779us/step - loss: 1.8007
16/16 [==============================] - 0s 781us/step - loss: 1.8000
16/16 [==============================] - 0s 806us/step - loss: 1.7995
Epoch 51 of 60

Testing for epoch 51 index 1:
32/32 [==============================] - 0s 601us/step
16/16 [==============================] - 0s 788us/step - loss: 0.2657
16/16 [==============================] - 0s 813us/step - loss: 0.8298
16/16 [==============================] - 0s 787us/step - loss: 1.4637
16/16 [==============================] - 0s 791us/step - loss: 1.8214
16/16 [==============================] - 0s 790us/step - loss: 1.8778
16/16 [==============================] - 0s 780us/step - loss: 1.8907
16/16 [==============================] - 0s 803us/step - loss: 1.8920
16/16 [==============================] - 0s 817us/step - loss: 1.8915
16/16 [==============================] - 0s 779us/step - loss: 1.8908
16/16 [==============================] - 0s 777us/step - loss: 1.8902

Testing for epoch 51 index 2:
32/32 [==============================] - 0s 594us/step
16/16 [==============================] - 0s 818us/step - loss: 0.2715
16/16 [==============================] - 0s 1ms/step - loss: 0.8213
16/16 [==============================] - 0s 1ms/step - loss: 1.4421
16/16 [==============================] - 0s 1ms/step - loss: 1.7886
16/16 [==============================] - 0s 1ms/step - loss: 1.8427
16/16 [==============================] - 0s 787us/step - loss: 1.8549
16/16 [==============================] - 0s 1ms/step - loss: 1.8561
16/16 [==============================] - 0s 1ms/step - loss: 1.8555
16/16 [==============================] - 0s 1ms/step - loss: 1.8549
16/16 [==============================] - 0s 796us/step - loss: 1.8543
Epoch 52 of 60

Testing for epoch 52 index 1:
32/32 [==============================] - 0s 612us/step
16/16 [==============================] - 0s 821us/step - loss: 0.2673
16/16 [==============================] - 0s 799us/step - loss: 0.8241
16/16 [==============================] - 0s 780us/step - loss: 1.4541
16/16 [==============================] - 0s 796us/step - loss: 1.8011
16/16 [==============================] - 0s 811us/step - loss: 1.8543
16/16 [==============================] - 0s 779us/step - loss: 1.8660
16/16 [==============================] - 0s 791us/step - loss: 1.8671
16/16 [==============================] - 0s 771us/step - loss: 1.8665
16/16 [==============================] - 0s 823us/step - loss: 1.8658
16/16 [==============================] - 0s 774us/step - loss: 1.8652

Testing for epoch 52 index 2:
32/32 [==============================] - 0s 635us/step
16/16 [==============================] - 0s 801us/step - loss: 0.2792
16/16 [==============================] - 0s 780us/step - loss: 0.8209
16/16 [==============================] - 0s 825us/step - loss: 1.4352
16/16 [==============================] - 0s 812us/step - loss: 1.7676
16/16 [==============================] - 0s 821us/step - loss: 1.8181
16/16 [==============================] - 0s 814us/step - loss: 1.8289
16/16 [==============================] - 0s 1ms/step - loss: 1.8298
16/16 [==============================] - 0s 1ms/step - loss: 1.8291
16/16 [==============================] - 0s 802us/step - loss: 1.8284
16/16 [==============================] - 0s 797us/step - loss: 1.8278
Epoch 53 of 60

Testing for epoch 53 index 1:
32/32 [==============================] - 0s 640us/step
16/16 [==============================] - 0s 859us/step - loss: 0.2657
16/16 [==============================] - 0s 817us/step - loss: 0.8274
16/16 [==============================] - 0s 799us/step - loss: 1.4711
16/16 [==============================] - 0s 898us/step - loss: 1.8159
16/16 [==============================] - 0s 791us/step - loss: 1.8675
16/16 [==============================] - 0s 821us/step - loss: 1.8786
16/16 [==============================] - 0s 802us/step - loss: 1.8795
16/16 [==============================] - 0s 823us/step - loss: 1.8789
16/16 [==============================] - 0s 779us/step - loss: 1.8782
16/16 [==============================] - 0s 797us/step - loss: 1.8776

Testing for epoch 53 index 2:
32/32 [==============================] - 0s 612us/step
16/16 [==============================] - 0s 792us/step - loss: 0.2711
16/16 [==============================] - 0s 779us/step - loss: 0.8247
16/16 [==============================] - 0s 798us/step - loss: 1.4613
16/16 [==============================] - 0s 772us/step - loss: 1.7968
16/16 [==============================] - 0s 780us/step - loss: 1.8466
16/16 [==============================] - 0s 797us/step - loss: 1.8571
16/16 [==============================] - 0s 811us/step - loss: 1.8579
16/16 [==============================] - 0s 801us/step - loss: 1.8572
16/16 [==============================] - 0s 776us/step - loss: 1.8565
16/16 [==============================] - 0s 774us/step - loss: 1.8559
Epoch 54 of 60

Testing for epoch 54 index 1:
32/32 [==============================] - 0s 840us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.2669
16/16 [==============================] - 0s 830us/step - loss: 0.8337
16/16 [==============================] - 0s 782us/step - loss: 1.4889
16/16 [==============================] - 0s 901us/step - loss: 1.8301
16/16 [==============================] - 0s 812us/step - loss: 1.8797
16/16 [==============================] - 0s 816us/step - loss: 1.8901
16/16 [==============================] - 0s 775us/step - loss: 1.8908
16/16 [==============================] - 0s 778us/step - loss: 1.8901
16/16 [==============================] - 0s 797us/step - loss: 1.8894
16/16 [==============================] - 0s 784us/step - loss: 1.8888

Testing for epoch 54 index 2:
32/32 [==============================] - 0s 605us/step
16/16 [==============================] - 0s 811us/step - loss: 0.2687
16/16 [==============================] - 0s 800us/step - loss: 0.8254
16/16 [==============================] - 0s 794us/step - loss: 1.4729
16/16 [==============================] - 0s 777us/step - loss: 1.8040
16/16 [==============================] - 0s 775us/step - loss: 1.8518
16/16 [==============================] - 0s 790us/step - loss: 1.8616
16/16 [==============================] - 0s 806us/step - loss: 1.8622
16/16 [==============================] - 0s 781us/step - loss: 1.8615
16/16 [==============================] - 0s 804us/step - loss: 1.8608
16/16 [==============================] - 0s 782us/step - loss: 1.8602
Epoch 55 of 60

Testing for epoch 55 index 1:
32/32 [==============================] - 0s 848us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.2630
16/16 [==============================] - 0s 1ms/step - loss: 0.8345
16/16 [==============================] - 0s 1ms/step - loss: 1.5023
16/16 [==============================] - 0s 1ms/step - loss: 1.8396
16/16 [==============================] - 0s 1ms/step - loss: 1.8872
16/16 [==============================] - 0s 769us/step - loss: 1.8970
16/16 [==============================] - 0s 822us/step - loss: 1.8976
16/16 [==============================] - 0s 778us/step - loss: 1.8968
16/16 [==============================] - 0s 778us/step - loss: 1.8961
16/16 [==============================] - 0s 804us/step - loss: 1.8955

Testing for epoch 55 index 2:
32/32 [==============================] - 0s 613us/step
16/16 [==============================] - 0s 787us/step - loss: 0.2649
16/16 [==============================] - 0s 825us/step - loss: 0.8307
16/16 [==============================] - 0s 785us/step - loss: 1.4969
16/16 [==============================] - 0s 799us/step - loss: 1.8271
16/16 [==============================] - 0s 800us/step - loss: 1.8735
16/16 [==============================] - 0s 1ms/step - loss: 1.8829
16/16 [==============================] - 0s 1ms/step - loss: 1.8834
16/16 [==============================] - 0s 1ms/step - loss: 1.8826
16/16 [==============================] - 0s 1ms/step - loss: 1.8818
16/16 [==============================] - 0s 1ms/step - loss: 1.8813
Epoch 56 of 60

Testing for epoch 56 index 1:
32/32 [==============================] - 0s 845us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.2646
16/16 [==============================] - 0s 1ms/step - loss: 0.8302
16/16 [==============================] - 0s 1ms/step - loss: 1.4973
16/16 [==============================] - 0s 780us/step - loss: 1.8229
16/16 [==============================] - 0s 773us/step - loss: 1.8679
16/16 [==============================] - 0s 794us/step - loss: 1.8768
16/16 [==============================] - 0s 1ms/step - loss: 1.8772
16/16 [==============================] - 0s 1ms/step - loss: 1.8763
16/16 [==============================] - 0s 804us/step - loss: 1.8756
16/16 [==============================] - 0s 806us/step - loss: 1.8750

Testing for epoch 56 index 2:
32/32 [==============================] - 0s 843us/step
16/16 [==============================] - 0s 844us/step - loss: 0.2654
16/16 [==============================] - 0s 777us/step - loss: 0.8271
16/16 [==============================] - 0s 797us/step - loss: 1.4953
16/16 [==============================] - 0s 781us/step - loss: 1.8151
16/16 [==============================] - 0s 797us/step - loss: 1.8594
16/16 [==============================] - 0s 774us/step - loss: 1.8680
16/16 [==============================] - 0s 811us/step - loss: 1.8683
16/16 [==============================] - 0s 808us/step - loss: 1.8674
16/16 [==============================] - 0s 782us/step - loss: 1.8667
16/16 [==============================] - 0s 785us/step - loss: 1.8661
Epoch 57 of 60

Testing for epoch 57 index 1:
32/32 [==============================] - 0s 626us/step
16/16 [==============================] - 0s 793us/step - loss: 0.2603
16/16 [==============================] - 0s 797us/step - loss: 0.8334
16/16 [==============================] - 0s 781us/step - loss: 1.5190
16/16 [==============================] - 0s 775us/step - loss: 1.8422
16/16 [==============================] - 0s 772us/step - loss: 1.8866
16/16 [==============================] - 0s 819us/step - loss: 1.8950
16/16 [==============================] - 0s 814us/step - loss: 1.8952
16/16 [==============================] - 0s 790us/step - loss: 1.8943
16/16 [==============================] - 0s 798us/step - loss: 1.8935
16/16 [==============================] - 0s 802us/step - loss: 1.8930

Testing for epoch 57 index 2:
32/32 [==============================] - 0s 635us/step
16/16 [==============================] - 0s 785us/step - loss: 0.2586
16/16 [==============================] - 0s 775us/step - loss: 0.8385
16/16 [==============================] - 0s 784us/step - loss: 1.5405
16/16 [==============================] - 0s 786us/step - loss: 1.8654
16/16 [==============================] - 0s 784us/step - loss: 1.9105
16/16 [==============================] - 0s 787us/step - loss: 1.9188
16/16 [==============================] - 0s 787us/step - loss: 1.9191
16/16 [==============================] - 0s 825us/step - loss: 1.9182
16/16 [==============================] - 0s 796us/step - loss: 1.9175
16/16 [==============================] - 0s 790us/step - loss: 1.9169
Epoch 58 of 60

Testing for epoch 58 index 1:
32/32 [==============================] - 0s 896us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.2664
16/16 [==============================] - 0s 887us/step - loss: 0.8317
16/16 [==============================] - 0s 789us/step - loss: 1.5162
16/16 [==============================] - 0s 1ms/step - loss: 1.8275
16/16 [==============================] - 0s 807us/step - loss: 1.8701
16/16 [==============================] - 0s 780us/step - loss: 1.8777
16/16 [==============================] - 0s 782us/step - loss: 1.8779
16/16 [==============================] - 0s 782us/step - loss: 1.8769
16/16 [==============================] - 0s 778us/step - loss: 1.8762
16/16 [==============================] - 0s 801us/step - loss: 1.8756

Testing for epoch 58 index 2:
32/32 [==============================] - 0s 604us/step
16/16 [==============================] - 0s 793us/step - loss: 0.2565
16/16 [==============================] - 0s 790us/step - loss: 0.8364
16/16 [==============================] - 0s 832us/step - loss: 1.5469
16/16 [==============================] - 0s 782us/step - loss: 1.8653
16/16 [==============================] - 0s 796us/step - loss: 1.9092
16/16 [==============================] - 0s 783us/step - loss: 1.9169
16/16 [==============================] - 0s 790us/step - loss: 1.9171
16/16 [==============================] - 0s 785us/step - loss: 1.9162
16/16 [==============================] - 0s 799us/step - loss: 1.9154
16/16 [==============================] - 0s 822us/step - loss: 1.9149
Epoch 59 of 60

Testing for epoch 59 index 1:
32/32 [==============================] - 0s 636us/step
16/16 [==============================] - 0s 843us/step - loss: 0.2551
16/16 [==============================] - 0s 823us/step - loss: 0.8408
16/16 [==============================] - 0s 1ms/step - loss: 1.5610
16/16 [==============================] - 0s 804us/step - loss: 1.8795
16/16 [==============================] - 0s 1ms/step - loss: 1.9230
16/16 [==============================] - 0s 832us/step - loss: 1.9304
16/16 [==============================] - 0s 1ms/step - loss: 1.9306
16/16 [==============================] - 0s 1ms/step - loss: 1.9296
16/16 [==============================] - 0s 875us/step - loss: 1.9289
16/16 [==============================] - 0s 825us/step - loss: 1.9283

Testing for epoch 59 index 2:
32/32 [==============================] - 0s 794us/step
16/16 [==============================] - 0s 827us/step - loss: 0.2557
16/16 [==============================] - 0s 812us/step - loss: 0.8404
16/16 [==============================] - 0s 1ms/step - loss: 1.5661
16/16 [==============================] - 0s 1ms/step - loss: 1.8818
16/16 [==============================] - 0s 1ms/step - loss: 1.9253
16/16 [==============================] - 0s 1ms/step - loss: 1.9326
16/16 [==============================] - 0s 820us/step - loss: 1.9327
16/16 [==============================] - 0s 1ms/step - loss: 1.9318
16/16 [==============================] - 0s 1ms/step - loss: 1.9311
16/16 [==============================] - 0s 1ms/step - loss: 1.9305
Epoch 60 of 60

Testing for epoch 60 index 1:
32/32 [==============================] - 0s 605us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.2626
16/16 [==============================] - 0s 1ms/step - loss: 0.8359
16/16 [==============================] - 0s 1ms/step - loss: 1.5479
16/16 [==============================] - 0s 1ms/step - loss: 1.8518
16/16 [==============================] - 0s 1ms/step - loss: 1.8930
16/16 [==============================] - 0s 1ms/step - loss: 1.8996
16/16 [==============================] - 0s 853us/step - loss: 1.8996
16/16 [==============================] - 0s 825us/step - loss: 1.8986
16/16 [==============================] - 0s 819us/step - loss: 1.8979
16/16 [==============================] - 0s 857us/step - loss: 1.8973

Testing for epoch 60 index 2:
32/32 [==============================] - 0s 645us/step
16/16 [==============================] - 0s 813us/step - loss: 0.2643
16/16 [==============================] - 0s 836us/step - loss: 0.8245
16/16 [==============================] - 0s 851us/step - loss: 1.5218
16/16 [==============================] - 0s 854us/step - loss: 1.8173
16/16 [==============================] - 0s 807us/step - loss: 1.8571
16/16 [==============================] - 0s 829us/step - loss: 1.8634
16/16 [==============================] - 0s 803us/step - loss: 1.8633
16/16 [==============================] - 0s 814us/step - loss: 1.8623
16/16 [==============================] - 0s 811us/step - loss: 1.8615
16/16 [==============================] - 0s 828us/step - loss: 1.8609
32/32 [==============================] - 0s 634us/step</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1614">
<div class="sourceCode cell-code" id="cb160"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb160-1"><a href="#cb160-1" aria-hidden="true" tabindex="-1"></a>outlier_MO_GAAL_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1615">
<div class="sourceCode cell-code" id="cb161"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a>outlier_MO_GAAL_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_MO_GAAL_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1616">
<div class="sourceCode cell-code" id="cb162"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb162-1"><a href="#cb162-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_1,outlier_MO_GAAL_one,tab_linear)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1617">
<div class="sourceCode cell-code" id="cb163"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb163-1"><a href="#cb163-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"MO-GAAL (Liu et al., 2019)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-120-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.940
Precision: 0.965
Recall: 0.972
F1 Score: 0.969</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1618">
<div class="sourceCode cell-code" id="cb166"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb166-1"><a href="#cb166-1" aria-hidden="true" tabindex="-1"></a>therteen <span class="op">=</span> twelve.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  therteen = twelve.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="lscpstar" class="level3">
<h3 class="anchored" data-anchor-id="lscpstar">LSCP<span class="math inline">\(\star\)</span></h3>
<p>default=10%</p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb168"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb168-1"><a href="#cb168-1" aria-hidden="true" tabindex="-1"></a>LSCP?</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Init signature:</span>
LSCP<span class="ansi-blue-fg">(</span>
    detector_list<span class="ansi-blue-fg">,</span>
    local_region_size<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">30</span><span class="ansi-blue-fg">,</span>
    local_max_features<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1.0</span><span class="ansi-blue-fg">,</span>
    n_bins<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">10</span><span class="ansi-blue-fg">,</span>
    random_state<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    contamination<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.1</span><span class="ansi-blue-fg">,</span>
<span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Docstring:</span>     
Locally Selection Combination in Parallel Outlier Ensembles
LSCP is an unsupervised parallel outlier detection ensemble which selects
competent detectors in the local region of a test instance. This
implementation uses an Average of Maximum strategy. First, a heterogeneous
list of base detectors is fit to the training data and then generates a
pseudo ground truth for each train instance is generated by
taking the maximum outlier score.
For each test instance:
1) The local region is defined to be the set of nearest training points in
randomly sampled feature subspaces which occur more frequently than
a defined threshold over multiple iterations.
2) Using the local region, a local pseudo ground truth is defined and the
pearson correlation is calculated between each base detector's training
outlier scores and the pseudo ground truth.
3) A histogram is built out of pearson correlation scores; detectors in
the largest bin are selected as competent base detectors for the given
test instance.
4) The average outlier score of the selected competent detectors is taken
to be the final score.
See :cite:`zhao2019lscp` for details.
Parameters
----------
detector_list : List, length must be greater than 1
    Base unsupervised outlier detectors from PyOD. (Note: requires fit and
    decision_function methods)
local_region_size : int, optional (default=30)
    Number of training points to consider in each iteration of the local
    region generation process (30 by default).
local_max_features : float in (0.5, 1.), optional (default=1.0)
    Maximum proportion of number of features to consider when defining the
    local region (1.0 by default).
n_bins : int, optional (default=10)
    Number of bins to use when selecting the local region
random_state : RandomState, optional (default=None)
    A random number generator instance to define the state of the random
    permutations generator.
contamination : float in (0., 0.5), optional (default=0.1)
    The amount of contamination of the data set, i.e.
    the proportion of outliers in the data set. Used when fitting to
    define the threshold on the decision function (0.1 by default).
Attributes
----------
decision_scores_ : numpy array of shape (n_samples,)
    The outlier scores of the training data.
    The higher, the more abnormal. Outliers tend to have higher
    scores. This value is available once the detector is fitted.
threshold_ : float
    The threshold is based on ``contamination``. It is the
    ``n_samples * contamination`` most abnormal samples in
    ``decision_scores_``. The threshold is calculated for generating
    binary outlier labels.
labels_ : int, either 0 or 1
    The binary labels of the training data. 0 stands for inliers
    and 1 for outliers/anomalies. It is generated by applying
    ``threshold_`` on ``decision_scores_``.
Examples
--------
&gt;&gt;&gt; from pyod.utils.data import generate_data
&gt;&gt;&gt; from pyod.utils.utility import standardizer
&gt;&gt;&gt; from pyod.models.lscp import LSCP
&gt;&gt;&gt; from pyod.models.lof import LOF
&gt;&gt;&gt; X_train, y_train, X_test, y_test = generate_data(
...     n_train=50, n_test=50,
...     contamination=0.1, random_state=42)
&gt;&gt;&gt; X_train, X_test = standardizer(X_train, X_test)
&gt;&gt;&gt; detector_list = [LOF(), LOF()]
&gt;&gt;&gt; clf = LSCP(detector_list)
&gt;&gt;&gt; clf.fit(X_train)
LSCP(...)
<span class="ansi-red-fg">File:</span>           ~/anaconda3/envs/temp_csy/lib/python3.8/site-packages/pyod/models/lscp.py
<span class="ansi-red-fg">Type:</span>           ABCMeta
<span class="ansi-red-fg">Subclasses:</span>     
</pre>
</div>
</div>
</div>
<div class="cell" data-execution_count="1619">
<div class="sourceCode cell-code" id="cb169"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb169-1"><a href="#cb169-1" aria-hidden="true" tabindex="-1"></a>detectors <span class="op">=</span> [KNN(), LOF(), OCSVM()]</span>
<span id="cb169-2"><a href="#cb169-2" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LSCP(detectors,contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb169-3"><a href="#cb169-3" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>]])</span>
<span id="cb169-4"><a href="#cb169-4" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'LSCP_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/pyod/models/lscp.py:382: UserWarning: The number of histogram bins is greater than the number of classifiers, reducing n_bins to n_clf.
  warnings.warn(</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1620">
<div class="sourceCode cell-code" id="cb171"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb171-1"><a href="#cb171-1" aria-hidden="true" tabindex="-1"></a>outlier_LSCP_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1621">
<div class="sourceCode cell-code" id="cb172"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb172-1"><a href="#cb172-1" aria-hidden="true" tabindex="-1"></a>outlier_LSCP_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_LSCP_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1622">
<div class="sourceCode cell-code" id="cb173"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb173-1"><a href="#cb173-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_1,outlier_LSCP_one,tab_linear)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1623">
<div class="sourceCode cell-code" id="cb174"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb174-1"><a href="#cb174-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"LSCP (Zhao et al., 2019)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-127-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.988
Precision: 0.994
Recall: 0.994
F1 Score: 0.994</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1624">
<div class="sourceCode cell-code" id="cb177"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb177-1"><a href="#cb177-1" aria-hidden="true" tabindex="-1"></a>fourteen <span class="op">=</span> therteen.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  fourteen = therteen.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="linear-result" class="level2">
<h2 class="anchored" data-anchor-id="linear-result">Linear Result</h2>
<p><span class="math inline">\(U^\star\)</span>, which is a mixture of uniform distributions <span class="math inline">\(U(5,7)\)</span> and <span class="math inline">\(U(-7,-5)\)</span>.</p>
<div class="cell" data-execution_count="1625">
<div class="sourceCode cell-code" id="cb179"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb179-1"><a href="#cb179-1" aria-hidden="true" tabindex="-1"></a>fourteen</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1625">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Accuracy</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>GODE</th>
      <td>0.998</td>
      <td>0.998947</td>
      <td>0.998947</td>
      <td>0.998947</td>
    </tr>
    <tr>
      <th>LOF (Breunig et al., 2000)</th>
      <td>0.926</td>
      <td>0.961053</td>
      <td>0.961053</td>
      <td>0.961053</td>
    </tr>
    <tr>
      <th>kNN (Ramaswamy et al., 2000)</th>
      <td>0.950</td>
      <td>1.000000</td>
      <td>0.947368</td>
      <td>0.972973</td>
    </tr>
    <tr>
      <th>OCSVM (Sch ̈olkopf et al., 2001)</th>
      <td>0.935</td>
      <td>0.991121</td>
      <td>0.940000</td>
      <td>0.964884</td>
    </tr>
    <tr>
      <th>MCD (Hardin and Rocke, 2004)</th>
      <td>0.998</td>
      <td>0.998947</td>
      <td>0.998947</td>
      <td>0.998947</td>
    </tr>
    <tr>
      <th>Feature Bagging (Lazarevic and Kumar, 2005)</th>
      <td>0.986</td>
      <td>0.992632</td>
      <td>0.992632</td>
      <td>0.992632</td>
    </tr>
    <tr>
      <th>ABOD (Kriegel et al., 2008)</th>
      <td>0.988</td>
      <td>0.993684</td>
      <td>0.993684</td>
      <td>0.993684</td>
    </tr>
    <tr>
      <th>Isolation Forest (Liu et al., 2008)</th>
      <td>0.868</td>
      <td>0.998780</td>
      <td>0.862105</td>
      <td>0.925424</td>
    </tr>
    <tr>
      <th>HBOS (Goldstein and Dengel, 2012)</th>
      <td>0.960</td>
      <td>0.977941</td>
      <td>0.980000</td>
      <td>0.978970</td>
    </tr>
    <tr>
      <th>SOS (Janssens et al., 2012)</th>
      <td>0.916</td>
      <td>0.955789</td>
      <td>0.955789</td>
      <td>0.955789</td>
    </tr>
    <tr>
      <th>SO-GAAL (Liu et al., 2019)</th>
      <td>0.936</td>
      <td>0.966316</td>
      <td>0.966316</td>
      <td>0.966316</td>
    </tr>
    <tr>
      <th>MO-GAAL (Liu et al., 2019)</th>
      <td>0.940</td>
      <td>0.965481</td>
      <td>0.971579</td>
      <td>0.968520</td>
    </tr>
    <tr>
      <th>LSCP (Zhao et al., 2019)</th>
      <td>0.988</td>
      <td>0.993684</td>
      <td>0.993684</td>
      <td>0.993684</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</section>
<section id="orbit-ebayesthresh" class="level2">
<h2 class="anchored" data-anchor-id="orbit-ebayesthresh">Orbit EbayesThresh</h2>
<div class="cell" data-execution_count="1899">
<div class="sourceCode cell-code" id="cb180"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb180-1"><a href="#cb180-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext rpy2.ipython</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The rpy2.ipython extension is already loaded. To reload it, use:
  %reload_ext rpy2.ipython</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1900">
<div class="sourceCode cell-code" id="cb182"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb182-1"><a href="#cb182-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>R</span>
<span id="cb182-2"><a href="#cb182-2" aria-hidden="true" tabindex="-1"></a>library(EbayesThresh)</span>
<span id="cb182-3"><a href="#cb182-3" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span>.seed(<span class="dv">1</span>)</span>
<span id="cb182-4"><a href="#cb182-4" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> rnorm(<span class="dv">1000</span>)</span>
<span id="cb182-5"><a href="#cb182-5" aria-hidden="true" tabindex="-1"></a>signal <span class="op">=</span> sample(c(runif(<span class="dv">25</span>,<span class="op">-</span><span class="dv">7</span>,<span class="op">-</span><span class="dv">5</span>), runif(<span class="dv">25</span>,<span class="dv">5</span>,<span class="dv">7</span>), rep(<span class="dv">0</span>,<span class="dv">950</span>)))</span>
<span id="cb182-6"><a href="#cb182-6" aria-hidden="true" tabindex="-1"></a>index_of_trueoutlier <span class="op">=</span> which(signal<span class="op">!=</span><span class="dv">0</span>)</span>
<span id="cb182-7"><a href="#cb182-7" aria-hidden="true" tabindex="-1"></a>index_of_trueoutlier</span>
<span id="cb182-8"><a href="#cb182-8" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>signal<span class="op">+</span>epsilon</span>
<span id="cb182-9"><a href="#cb182-9" aria-hidden="true" tabindex="-1"></a>plot(<span class="dv">1</span>:<span class="dv">1000</span>,x)</span>
<span id="cb182-10"><a href="#cb182-10" aria-hidden="true" tabindex="-1"></a>points(index_of_trueoutlier,x[index_of_trueoutlier],col<span class="op">=</span><span class="dv">2</span>,cex<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb182-11"><a href="#cb182-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb182-12"><a href="#cb182-12" aria-hidden="true" tabindex="-1"></a><span class="co">#plot(x,type='l')</span></span>
<span id="cb182-13"><a href="#cb182-13" aria-hidden="true" tabindex="-1"></a><span class="co">#mu &lt;- EbayesThresh::ebayesthresh(x,sdev=2)</span></span>
<span id="cb182-14"><a href="#cb182-14" aria-hidden="true" tabindex="-1"></a><span class="co">#lines(mu,col=2,lty=2,lwd=2)</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-131-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="1901">
<div class="sourceCode cell-code" id="cb183"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb183-1"><a href="#cb183-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>R <span class="op">-</span>o x</span>
<span id="cb183-2"><a href="#cb183-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>R <span class="op">-</span>o index_of_trueoutlier</span>
<span id="cb183-3"><a href="#cb183-3" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>R <span class="op">-</span>o signal</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1902">
<div class="sourceCode cell-code" id="cb184"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb184-1"><a href="#cb184-1" aria-hidden="true" tabindex="-1"></a>ebayesthresh <span class="op">=</span> importr(<span class="st">'EbayesThresh'</span>).ebayesthresh</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1903">
<div class="sourceCode cell-code" id="cb185"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb185-1"><a href="#cb185-1" aria-hidden="true" tabindex="-1"></a>xhat <span class="op">=</span> np.array(ebayesthresh(FloatVector(x)))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1904">
<div class="sourceCode cell-code" id="cb186"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb186-1"><a href="#cb186-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.plot(x)</span></span>
<span id="cb186-2"><a href="#cb186-2" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.plot(xhat)</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="1905">
<div class="sourceCode cell-code" id="cb187"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb187-1"><a href="#cb187-1" aria-hidden="true" tabindex="-1"></a>outlier_true_index <span class="op">=</span> index_of_trueoutlier</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1906">
<div class="sourceCode cell-code" id="cb188"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb188-1"><a href="#cb188-1" aria-hidden="true" tabindex="-1"></a>outlier_true_value <span class="op">=</span> x[index_of_trueoutlier]</span></code></pre></div>
</div>
<p>package와 비교를 위해 outlier는 -1, inlier는 1로 표시</p>
<div class="cell" data-execution_count="1907">
<div class="sourceCode cell-code" id="cb189"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb189-1"><a href="#cb189-1" aria-hidden="true" tabindex="-1"></a>outlier_true_one <span class="op">=</span> signal.copy()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1908">
<div class="sourceCode cell-code" id="cb190"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb190-1"><a href="#cb190-1" aria-hidden="true" tabindex="-1"></a>outlier_true_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="op">-</span><span class="dv">1</span> <span class="cf">if</span> x<span class="op">!=</span><span class="dv">0</span> <span class="cf">else</span> <span class="dv">1</span>,outlier_true_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1909">
<div class="sourceCode cell-code" id="cb191"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb191-1"><a href="#cb191-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pd.DataFrame(outlier_true_one).to_csv('orbit_outlier.csv')</span></span></code></pre></div>
</div>
</section>
<section id="orbit" class="level2">
<h2 class="anchored" data-anchor-id="orbit">Orbit</h2>
<div class="cell" data-execution_count="1877">
<div class="sourceCode cell-code" id="cb192"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb192-1"><a href="#cb192-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">777</span>)</span>
<span id="cb192-2"><a href="#cb192-2" aria-hidden="true" tabindex="-1"></a>pi<span class="op">=</span>np.pi</span>
<span id="cb192-3"><a href="#cb192-3" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">1000</span></span>
<span id="cb192-4"><a href="#cb192-4" aria-hidden="true" tabindex="-1"></a>ang<span class="op">=</span>np.linspace(<span class="op">-</span>pi,pi<span class="op">-</span><span class="dv">2</span><span class="op">*</span>pi<span class="op">/</span>n,n)</span>
<span id="cb192-5"><a href="#cb192-5" aria-hidden="true" tabindex="-1"></a>r<span class="op">=</span><span class="dv">5</span><span class="op">+</span>np.cos(np.linspace(<span class="dv">0</span>,<span class="dv">12</span><span class="op">*</span>pi,n))</span>
<span id="cb192-6"><a href="#cb192-6" aria-hidden="true" tabindex="-1"></a>vx<span class="op">=</span>r<span class="op">*</span>np.cos(ang)</span>
<span id="cb192-7"><a href="#cb192-7" aria-hidden="true" tabindex="-1"></a>vy<span class="op">=</span>r<span class="op">*</span>np.sin(ang)</span>
<span id="cb192-8"><a href="#cb192-8" aria-hidden="true" tabindex="-1"></a>f1<span class="op">=</span><span class="dv">10</span><span class="op">*</span>np.sin(np.linspace(<span class="dv">0</span>,<span class="dv">6</span><span class="op">*</span>pi,n))</span>
<span id="cb192-9"><a href="#cb192-9" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> f1 <span class="op">+</span> x</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1878">
<div class="sourceCode cell-code" id="cb193"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb193-1"><a href="#cb193-1" aria-hidden="true" tabindex="-1"></a>_df <span class="op">=</span> pd.DataFrame({<span class="st">'x'</span> : vx, <span class="st">'y'</span> : vy, <span class="st">'f'</span> : f})</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1879">
<div class="sourceCode cell-code" id="cb194"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb194-1"><a href="#cb194-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array(_df)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1642">
<div class="sourceCode cell-code" id="cb195"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb195-1"><a href="#cb195-1" aria-hidden="true" tabindex="-1"></a><span class="co"># save_data(_df,'Orbit.pkl')</span></span></code></pre></div>
</div>
<section id="gode-1" class="level3">
<h3 class="anchored" data-anchor-id="gode-1">GODE</h3>
<div class="cell" data-execution_count="1643">
<div class="sourceCode cell-code" id="cb196"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb196-1"><a href="#cb196-1" aria-hidden="true" tabindex="-1"></a>_Orbit <span class="op">=</span> Orbit(_df)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1644">
<div class="sourceCode cell-code" id="cb197"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb197-1"><a href="#cb197-1" aria-hidden="true" tabindex="-1"></a>_Orbit.get_distance()</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 1000/1000 [00:02&lt;00:00, 340.03it/s]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1645">
<div class="sourceCode cell-code" id="cb199"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb199-1"><a href="#cb199-1" aria-hidden="true" tabindex="-1"></a>_Orbit.get_weightmatrix(theta<span class="op">=</span>(_Orbit.D[_Orbit.D<span class="op">&gt;</span><span class="dv">0</span>].mean()),kappa<span class="op">=</span><span class="dv">2500</span>) </span></code></pre></div>
</div>
<div class="cell" data-execution_count="1646">
<div class="sourceCode cell-code" id="cb200"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb200-1"><a href="#cb200-1" aria-hidden="true" tabindex="-1"></a>_Orbit.fit(sd<span class="op">=</span><span class="dv">15</span>,ref<span class="op">=</span><span class="dv">20</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1647">
<div class="sourceCode cell-code" id="cb201"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb201-1"><a href="#cb201-1" aria-hidden="true" tabindex="-1"></a>outlier_simul_one <span class="op">=</span> (_Orbit.df[<span class="st">'Residual'</span>]<span class="op">**</span><span class="dv">2</span>).tolist()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1648">
<div class="sourceCode cell-code" id="cb202"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb202-1"><a href="#cb202-1" aria-hidden="true" tabindex="-1"></a>outlier_simul_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="op">-</span><span class="dv">1</span> <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">13</span> <span class="cf">else</span> <span class="dv">1</span>,outlier_simul_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1649">
<div class="sourceCode cell-code" id="cb203"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb203-1"><a href="#cb203-1" aria-hidden="true" tabindex="-1"></a>outlier_simul_one.count(<span class="dv">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1649">
<pre><code>950</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1650">
<div class="sourceCode cell-code" id="cb205"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb205-1"><a href="#cb205-1" aria-hidden="true" tabindex="-1"></a>outlier_simul_one.count(<span class="op">-</span><span class="dv">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1650">
<pre><code>50</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1651">
<div class="sourceCode cell-code" id="cb207"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb207-1"><a href="#cb207-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one,outlier_simul_one,tab_orbit)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1652">
<div class="sourceCode cell-code" id="cb208"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb208-1"><a href="#cb208-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"GODE"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-154-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.998
Precision: 0.999
Recall: 0.999
F1 Score: 0.999</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1653">
<div class="sourceCode cell-code" id="cb211"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb211-1"><a href="#cb211-1" aria-hidden="true" tabindex="-1"></a>one <span class="op">=</span> _conf.tab</span></code></pre></div>
</div>
</section>
<section id="lofstar" class="level3">
<h3 class="anchored" data-anchor-id="lofstar">LOF<span class="math inline">\(\star\)</span></h3>
<div class="cell" data-execution_count="1654">
<div class="sourceCode cell-code" id="cb212"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb212-1"><a href="#cb212-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LocalOutlierFactor(n_neighbors<span class="op">=</span><span class="dv">2</span>,contamination<span class="op">=</span><span class="fl">0.05</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1655">
<div class="sourceCode cell-code" id="cb213"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb213-1"><a href="#cb213-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one,clf.fit_predict(X),tab_orbit)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1656">
<div class="sourceCode cell-code" id="cb214"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb214-1"><a href="#cb214-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"LOF (Breunig et al., 2000)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-158-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.954
Precision: 0.976
Recall: 0.976
F1 Score: 0.976</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1658">
<div class="sourceCode cell-code" id="cb217"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb217-1"><a href="#cb217-1" aria-hidden="true" tabindex="-1"></a>two <span class="op">=</span> one.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  two = one.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="knn-1" class="level3">
<h3 class="anchored" data-anchor-id="knn-1">KNN</h3>
<div class="cell" data-tags="[]" data-execution_count="1659">
<div class="sourceCode cell-code" id="cb219"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb219-1"><a href="#cb219-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> KNN()</span>
<span id="cb219-2"><a href="#cb219-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'f'</span>]])</span>
<span id="cb219-3"><a href="#cb219-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'knn_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1660">
<div class="sourceCode cell-code" id="cb220"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb220-1"><a href="#cb220-1" aria-hidden="true" tabindex="-1"></a>outlier_KNN_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1661">
<div class="sourceCode cell-code" id="cb221"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb221-1"><a href="#cb221-1" aria-hidden="true" tabindex="-1"></a>outlier_KNN_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_KNN_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1662">
<div class="sourceCode cell-code" id="cb222"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb222-1"><a href="#cb222-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one,outlier_KNN_one,tab_orbit)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1663">
<div class="sourceCode cell-code" id="cb223"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb223-1"><a href="#cb223-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"kNN (Ramaswamy et al., 2000)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-164-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.948
Precision: 0.999
Recall: 0.946
F1 Score: 0.972</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1664">
<div class="sourceCode cell-code" id="cb226"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb226-1"><a href="#cb226-1" aria-hidden="true" tabindex="-1"></a>three <span class="op">=</span> two.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  three = two.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="cblof" class="level3">
<h3 class="anchored" data-anchor-id="cblof">CBLOF</h3>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb228"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb228-1"><a href="#cb228-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span></code></pre></div>
</div>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb229"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb229-1"><a href="#cb229-1" aria-hidden="true" tabindex="-1"></a>_df <span class="op">=</span> load_data(<span class="st">'Orbit.pkl'</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb230"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb230-1"><a href="#cb230-1" aria-hidden="true" tabindex="-1"></a>outlier_true_one <span class="op">=</span> pd.read_csv(<span class="st">'orbit_outlier.csv'</span>).iloc[:,<span class="dv">1</span>].tolist()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb231"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb231-1"><a href="#cb231-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> CBLOF(contamination<span class="op">=</span><span class="fl">0.05</span>,check_estimator<span class="op">=</span><span class="va">False</span>, random_state<span class="op">=</span><span class="dv">77</span>)</span>
<span id="cb231-2"><a href="#cb231-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'f'</span>]])</span>
<span id="cb231-3"><a href="#cb231-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'CBLOF_Clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/csy/anaconda3/envs/pygsp/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb233"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb233-1"><a href="#cb233-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> CBLOF(contamination<span class="op">=</span><span class="fl">0.05</span>,check_estimator<span class="op">=</span><span class="va">False</span>, random_state<span class="op">=</span><span class="dv">77</span>)</span>
<span id="cb233-2"><a href="#cb233-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'f'</span>]])</span>
<span id="cb233-3"><a href="#cb233-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'CBLOF_Clf'</span>] <span class="op">=</span> clf.labels_</span>
<span id="cb233-4"><a href="#cb233-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb233-5"><a href="#cb233-5" aria-hidden="true" tabindex="-1"></a>outlier_CBLOF_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span>
<span id="cb233-6"><a href="#cb233-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb233-7"><a href="#cb233-7" aria-hidden="true" tabindex="-1"></a>outlier_CBLOF_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_CBLOF_one))</span>
<span id="cb233-8"><a href="#cb233-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb233-9"><a href="#cb233-9" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one,outlier_CBLOF_one,tab_orbit)</span>
<span id="cb233-10"><a href="#cb233-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb233-11"><a href="#cb233-11" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"CBLOF (He et al., 2003)"</span>)</span>
<span id="cb233-12"><a href="#cb233-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb233-13"><a href="#cb233-13" aria-hidden="true" tabindex="-1"></a><span class="co"># four = three.append(_conf.tab)</span></span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/csy/anaconda3/envs/pygsp/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-170-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.916
Precision: 0.956
Recall: 0.956
F1 Score: 0.956</code></pre>
</div>
<div class="cell-output cell-output-error">
<pre><code>AttributeError: 'DataFrame' object has no attribute 'append'</code></pre>
</div>
</div>
<ul>
<li>Accuracy: 0.916</li>
<li>Precision: 0.956</li>
<li>Recall: 0.956</li>
<li>F1 Score: 0.956</li>
</ul>
</section>
<section id="ocsvm-1" class="level3">
<h3 class="anchored" data-anchor-id="ocsvm-1">OCSVM</h3>
<div class="cell" data-execution_count="1668">
<div class="sourceCode cell-code" id="cb237"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb237-1"><a href="#cb237-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> svm.OneClassSVM(nu<span class="op">=</span><span class="fl">0.1</span>, kernel<span class="op">=</span><span class="st">"rbf"</span>, gamma<span class="op">=</span><span class="fl">0.05</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1669">
<div class="sourceCode cell-code" id="cb238"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb238-1"><a href="#cb238-1" aria-hidden="true" tabindex="-1"></a>clf.fit(X)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1669">
<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-8" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>OneClassSVM(gamma=0.05, nu=0.1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-8" type="checkbox" checked=""><label for="sk-estimator-id-8" class="sk-toggleable__label sk-toggleable__label-arrow">OneClassSVM</label><div class="sk-toggleable__content"><pre>OneClassSVM(gamma=0.05, nu=0.1)</pre></div></div></div></div></div>
</div>
</div>
<div class="cell" data-execution_count="1670">
<div class="sourceCode cell-code" id="cb239"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb239-1"><a href="#cb239-1" aria-hidden="true" tabindex="-1"></a>outlier_OSVM_one <span class="op">=</span> <span class="bu">list</span>(clf.predict(X))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1671">
<div class="sourceCode cell-code" id="cb240"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb240-1"><a href="#cb240-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one,outlier_OSVM_one,tab_orbit)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1672">
<div class="sourceCode cell-code" id="cb241"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb241-1"><a href="#cb241-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"OCSVM (Sch ̈olkopf et al., 2001)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-175-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.908
Precision: 0.977
Recall: 0.925
F1 Score: 0.950</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1674">
<div class="sourceCode cell-code" id="cb244"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb244-1"><a href="#cb244-1" aria-hidden="true" tabindex="-1"></a>five <span class="op">=</span> three.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  five = three.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="mcdstar-1" class="level3">
<h3 class="anchored" data-anchor-id="mcdstar-1">MCD<span class="math inline">\(\star\)</span></h3>
<div class="cell" data-scrolled="true" data-tags="[]" data-execution_count="1675">
<div class="sourceCode cell-code" id="cb246"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb246-1"><a href="#cb246-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> MCD(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb246-2"><a href="#cb246-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'f'</span>]])</span>
<span id="cb246-3"><a href="#cb246-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'MCD_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1676">
<div class="sourceCode cell-code" id="cb247"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb247-1"><a href="#cb247-1" aria-hidden="true" tabindex="-1"></a>outlier_MCD_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1677">
<div class="sourceCode cell-code" id="cb248"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb248-1"><a href="#cb248-1" aria-hidden="true" tabindex="-1"></a>outlier_MCD_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_MCD_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1678">
<div class="sourceCode cell-code" id="cb249"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb249-1"><a href="#cb249-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one,outlier_MCD_one,tab_orbit)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1679">
<div class="sourceCode cell-code" id="cb250"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb250-1"><a href="#cb250-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"MCD (Hardin and Rocke, 2004)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-181-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.916
Precision: 0.956
Recall: 0.956
F1 Score: 0.956</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1682">
<div class="sourceCode cell-code" id="cb253"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb253-1"><a href="#cb253-1" aria-hidden="true" tabindex="-1"></a>six <span class="op">=</span> five.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  six = five.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="feature-baggingstar-1" class="level3">
<h3 class="anchored" data-anchor-id="feature-baggingstar-1">Feature Bagging<span class="math inline">\(\star\)</span></h3>
<div class="cell" data-scrolled="true" data-tags="[]" data-execution_count="1683">
<div class="sourceCode cell-code" id="cb255"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb255-1"><a href="#cb255-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> FeatureBagging(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb255-2"><a href="#cb255-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'f'</span>]])</span>
<span id="cb255-3"><a href="#cb255-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'FeatureBagging_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1684">
<div class="sourceCode cell-code" id="cb256"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb256-1"><a href="#cb256-1" aria-hidden="true" tabindex="-1"></a>outlier_FeatureBagging_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1685">
<div class="sourceCode cell-code" id="cb257"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb257-1"><a href="#cb257-1" aria-hidden="true" tabindex="-1"></a>outlier_FeatureBagging_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_FeatureBagging_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1686">
<div class="sourceCode cell-code" id="cb258"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb258-1"><a href="#cb258-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one,outlier_FeatureBagging_one,tab_orbit)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1687">
<div class="sourceCode cell-code" id="cb259"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb259-1"><a href="#cb259-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"Feature Bagging (Lazarevic and Kumar, 2005)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-187-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.942
Precision: 0.969
Recall: 0.969
F1 Score: 0.969</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1688">
<div class="sourceCode cell-code" id="cb262"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb262-1"><a href="#cb262-1" aria-hidden="true" tabindex="-1"></a>seven <span class="op">=</span> six.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  seven = six.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="abodstar-1" class="level3">
<h3 class="anchored" data-anchor-id="abodstar-1">ABOD<span class="math inline">\(\star\)</span></h3>
<div class="cell" data-execution_count="1689">
<div class="sourceCode cell-code" id="cb264"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb264-1"><a href="#cb264-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> ABOD(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb264-2"><a href="#cb264-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'f'</span>]])</span>
<span id="cb264-3"><a href="#cb264-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'ABOD_Clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1690">
<div class="sourceCode cell-code" id="cb265"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb265-1"><a href="#cb265-1" aria-hidden="true" tabindex="-1"></a>outlier_ABOD_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1691">
<div class="sourceCode cell-code" id="cb266"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb266-1"><a href="#cb266-1" aria-hidden="true" tabindex="-1"></a>outlier_ABOD_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_ABOD_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1692">
<div class="sourceCode cell-code" id="cb267"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb267-1"><a href="#cb267-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one,outlier_ABOD_one,tab_orbit)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1693">
<div class="sourceCode cell-code" id="cb268"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb268-1"><a href="#cb268-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"ABOD (Kriegel et al., 2008)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-193-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.988
Precision: 0.994
Recall: 0.994
F1 Score: 0.994</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1695">
<div class="sourceCode cell-code" id="cb271"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb271-1"><a href="#cb271-1" aria-hidden="true" tabindex="-1"></a>eight <span class="op">=</span> seven.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  eight = seven.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="iforeststar-1" class="level3">
<h3 class="anchored" data-anchor-id="iforeststar-1">IForest<span class="math inline">\(\star\)</span></h3>
<div class="cell" data-execution_count="1696">
<div class="sourceCode cell-code" id="cb273"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb273-1"><a href="#cb273-1" aria-hidden="true" tabindex="-1"></a>od <span class="op">=</span> IForest(</span>
<span id="cb273-2"><a href="#cb273-2" aria-hidden="true" tabindex="-1"></a>    threshold<span class="op">=</span><span class="fl">0.</span>,</span>
<span id="cb273-3"><a href="#cb273-3" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">50</span></span>
<span id="cb273-4"><a href="#cb273-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1697">
<div class="sourceCode cell-code" id="cb274"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb274-1"><a href="#cb274-1" aria-hidden="true" tabindex="-1"></a>od.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'f'</span>]])</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1698">
<div class="sourceCode cell-code" id="cb275"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb275-1"><a href="#cb275-1" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> od.predict(</span>
<span id="cb275-2"><a href="#cb275-2" aria-hidden="true" tabindex="-1"></a>    _df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'f'</span>]],</span>
<span id="cb275-3"><a href="#cb275-3" aria-hidden="true" tabindex="-1"></a>    return_instance_score<span class="op">=</span><span class="va">True</span></span>
<span id="cb275-4"><a href="#cb275-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1699">
<div class="sourceCode cell-code" id="cb276"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb276-1"><a href="#cb276-1" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'IF_alibi'</span>] <span class="op">=</span> preds[<span class="st">'data'</span>][<span class="st">'is_outlier'</span>]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1700">
<div class="sourceCode cell-code" id="cb277"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb277-1"><a href="#cb277-1" aria-hidden="true" tabindex="-1"></a>outlier_alibi_one <span class="op">=</span> _df[<span class="st">'IF_alibi'</span>]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1701">
<div class="sourceCode cell-code" id="cb278"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb278-1"><a href="#cb278-1" aria-hidden="true" tabindex="-1"></a>outlier_alibi_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_alibi_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1702">
<div class="sourceCode cell-code" id="cb279"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb279-1"><a href="#cb279-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one,outlier_alibi_one,tab_orbit)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1703">
<div class="sourceCode cell-code" id="cb280"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb280-1"><a href="#cb280-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"Isolation Forest (Liu et al., 2008)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-202-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.443
Precision: 0.992
Recall: 0.417
F1 Score: 0.587</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1704">
<div class="sourceCode cell-code" id="cb283"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb283-1"><a href="#cb283-1" aria-hidden="true" tabindex="-1"></a>nine <span class="op">=</span> eight.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  nine = eight.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="hbosstar-1" class="level3">
<h3 class="anchored" data-anchor-id="hbosstar-1">HBOS<span class="math inline">\(\star\)</span></h3>
<div class="cell" data-execution_count="1705">
<div class="sourceCode cell-code" id="cb285"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb285-1"><a href="#cb285-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> HBOS(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb285-2"><a href="#cb285-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'f'</span>]])</span>
<span id="cb285-3"><a href="#cb285-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'HBOS_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1706">
<div class="sourceCode cell-code" id="cb286"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb286-1"><a href="#cb286-1" aria-hidden="true" tabindex="-1"></a>outlier_HBOS_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1707">
<div class="sourceCode cell-code" id="cb287"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb287-1"><a href="#cb287-1" aria-hidden="true" tabindex="-1"></a>outlier_HBOS_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_HBOS_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1708">
<div class="sourceCode cell-code" id="cb288"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb288-1"><a href="#cb288-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one,outlier_HBOS_one,tab_orbit)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1709">
<div class="sourceCode cell-code" id="cb289"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb289-1"><a href="#cb289-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"HBOS (Goldstein and Dengel, 2012)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-208-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.935
Precision: 0.960
Recall: 0.973
F1 Score: 0.966</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1711">
<div class="sourceCode cell-code" id="cb292"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb292-1"><a href="#cb292-1" aria-hidden="true" tabindex="-1"></a>ten <span class="op">=</span> nine.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  ten = nine.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="sosstar-1" class="level3">
<h3 class="anchored" data-anchor-id="sosstar-1">SOS<span class="math inline">\(\star\)</span></h3>
<div class="cell" data-execution_count="1712">
<div class="sourceCode cell-code" id="cb294"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb294-1"><a href="#cb294-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> SOS(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb294-2"><a href="#cb294-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'f'</span>]])</span>
<span id="cb294-3"><a href="#cb294-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'SOS_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1713">
<div class="sourceCode cell-code" id="cb295"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb295-1"><a href="#cb295-1" aria-hidden="true" tabindex="-1"></a>outlier_SOS_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1714">
<div class="sourceCode cell-code" id="cb296"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb296-1"><a href="#cb296-1" aria-hidden="true" tabindex="-1"></a>outlier_SOS_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_SOS_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1715">
<div class="sourceCode cell-code" id="cb297"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb297-1"><a href="#cb297-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one,outlier_SOS_one,tab_orbit)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1716">
<div class="sourceCode cell-code" id="cb298"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb298-1"><a href="#cb298-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"SOS (Janssens et al., 2012)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-214-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.950
Precision: 0.974
Recall: 0.974
F1 Score: 0.974</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1718">
<div class="sourceCode cell-code" id="cb301"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb301-1"><a href="#cb301-1" aria-hidden="true" tabindex="-1"></a>eleven <span class="op">=</span> ten.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  eleven = ten.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="so_gaalstar" class="level3">
<h3 class="anchored" data-anchor-id="so_gaalstar">SO_GAAL<span class="math inline">\(\star\)</span></h3>
<div class="cell" data-scrolled="true" data-tags="[]" data-execution_count="1719">
<div class="sourceCode cell-code" id="cb303"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb303-1"><a href="#cb303-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> SO_GAAL(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb303-2"><a href="#cb303-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'f'</span>]])</span>
<span id="cb303-3"><a href="#cb303-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'SO_GAAL_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super().__init__(name, **kwargs)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1 of 60

Testing for epoch 1 index 1:

Testing for epoch 1 index 2:
Epoch 2 of 60

Testing for epoch 2 index 1:

Testing for epoch 2 index 2:
Epoch 3 of 60

Testing for epoch 3 index 1:

Testing for epoch 3 index 2:
Epoch 4 of 60

Testing for epoch 4 index 1:

Testing for epoch 4 index 2:
Epoch 5 of 60

Testing for epoch 5 index 1:

Testing for epoch 5 index 2:
Epoch 6 of 60

Testing for epoch 6 index 1:

Testing for epoch 6 index 2:
Epoch 7 of 60

Testing for epoch 7 index 1:

Testing for epoch 7 index 2:
Epoch 8 of 60

Testing for epoch 8 index 1:

Testing for epoch 8 index 2:
Epoch 9 of 60

Testing for epoch 9 index 1:

Testing for epoch 9 index 2:
Epoch 10 of 60

Testing for epoch 10 index 1:

Testing for epoch 10 index 2:
Epoch 11 of 60

Testing for epoch 11 index 1:

Testing for epoch 11 index 2:
Epoch 12 of 60

Testing for epoch 12 index 1:

Testing for epoch 12 index 2:
Epoch 13 of 60

Testing for epoch 13 index 1:

Testing for epoch 13 index 2:
Epoch 14 of 60

Testing for epoch 14 index 1:

Testing for epoch 14 index 2:
Epoch 15 of 60

Testing for epoch 15 index 1:

Testing for epoch 15 index 2:
Epoch 16 of 60

Testing for epoch 16 index 1:

Testing for epoch 16 index 2:
Epoch 17 of 60

Testing for epoch 17 index 1:

Testing for epoch 17 index 2:
Epoch 18 of 60

Testing for epoch 18 index 1:

Testing for epoch 18 index 2:
Epoch 19 of 60

Testing for epoch 19 index 1:

Testing for epoch 19 index 2:
Epoch 20 of 60

Testing for epoch 20 index 1:

Testing for epoch 20 index 2:
Epoch 21 of 60

Testing for epoch 21 index 1:

Testing for epoch 21 index 2:
Epoch 22 of 60

Testing for epoch 22 index 1:
16/16 [==============================] - 0s 969us/step - loss: 1.3463

Testing for epoch 22 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.3506
Epoch 23 of 60

Testing for epoch 23 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.3586

Testing for epoch 23 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 1.3721
Epoch 24 of 60

Testing for epoch 24 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.3866

Testing for epoch 24 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 1.3800
Epoch 25 of 60

Testing for epoch 25 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.4006

Testing for epoch 25 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 1.4023
Epoch 26 of 60

Testing for epoch 26 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.4122

Testing for epoch 26 index 2:
16/16 [==============================] - 0s 4ms/step - loss: 1.4314
Epoch 27 of 60

Testing for epoch 27 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.4473

Testing for epoch 27 index 2:
16/16 [==============================] - 0s 3ms/step - loss: 1.4588
Epoch 28 of 60

Testing for epoch 28 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.4734

Testing for epoch 28 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.4913
Epoch 29 of 60

Testing for epoch 29 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.5128

Testing for epoch 29 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.5221
Epoch 30 of 60

Testing for epoch 30 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 1.5512

Testing for epoch 30 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.5569
Epoch 31 of 60

Testing for epoch 31 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.5717

Testing for epoch 31 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.5833
Epoch 32 of 60

Testing for epoch 32 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 1.5991

Testing for epoch 32 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.6237
Epoch 33 of 60

Testing for epoch 33 index 1:
16/16 [==============================] - 0s 4ms/step - loss: 1.6486

Testing for epoch 33 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.6528
Epoch 34 of 60

Testing for epoch 34 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.6775

Testing for epoch 34 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.6728
Epoch 35 of 60

Testing for epoch 35 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 1.6961

Testing for epoch 35 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.7114
Epoch 36 of 60

Testing for epoch 36 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 1.7382

Testing for epoch 36 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 1.7361
Epoch 37 of 60

Testing for epoch 37 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 1.7442

Testing for epoch 37 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.7632
Epoch 38 of 60

Testing for epoch 38 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 1.7813

Testing for epoch 38 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.7997
Epoch 39 of 60

Testing for epoch 39 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.8135

Testing for epoch 39 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.8074
Epoch 40 of 60

Testing for epoch 40 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.8185

Testing for epoch 40 index 2:
16/16 [==============================] - 0s 3ms/step - loss: 1.8404
Epoch 41 of 60

Testing for epoch 41 index 1:
16/16 [==============================] - 0s 4ms/step - loss: 1.8432

Testing for epoch 41 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.8590
Epoch 42 of 60

Testing for epoch 42 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.8644

Testing for epoch 42 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 1.8872
Epoch 43 of 60

Testing for epoch 43 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.9012

Testing for epoch 43 index 2:
16/16 [==============================] - 0s 4ms/step - loss: 1.9049
Epoch 44 of 60

Testing for epoch 44 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.9139

Testing for epoch 44 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.9184
Epoch 45 of 60

Testing for epoch 45 index 1:
16/16 [==============================] - 0s 3ms/step - loss: 1.9365

Testing for epoch 45 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.9581
Epoch 46 of 60

Testing for epoch 46 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 1.9824

Testing for epoch 46 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 1.9639
Epoch 47 of 60

Testing for epoch 47 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 2.0122

Testing for epoch 47 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.0024
Epoch 48 of 60

Testing for epoch 48 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 2.0080

Testing for epoch 48 index 2:
16/16 [==============================] - 0s 3ms/step - loss: 2.0230
Epoch 49 of 60

Testing for epoch 49 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.0424

Testing for epoch 49 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.0419
Epoch 50 of 60

Testing for epoch 50 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 2.0648

Testing for epoch 50 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.0705
Epoch 51 of 60

Testing for epoch 51 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.0983

Testing for epoch 51 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.1023
Epoch 52 of 60

Testing for epoch 52 index 1:
16/16 [==============================] - 0s 3ms/step - loss: 2.1145

Testing for epoch 52 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 2.1403
Epoch 53 of 60

Testing for epoch 53 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.1403

Testing for epoch 53 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.1572
Epoch 54 of 60

Testing for epoch 54 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.1621

Testing for epoch 54 index 2:
16/16 [==============================] - 0s 3ms/step - loss: 2.1594
Epoch 55 of 60

Testing for epoch 55 index 1:
16/16 [==============================] - 0s 3ms/step - loss: 2.1776

Testing for epoch 55 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.1913
Epoch 56 of 60

Testing for epoch 56 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 2.2041

Testing for epoch 56 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.2355
Epoch 57 of 60

Testing for epoch 57 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.2292

Testing for epoch 57 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.2431
Epoch 58 of 60

Testing for epoch 58 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.2475

Testing for epoch 58 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 2.2408
Epoch 59 of 60

Testing for epoch 59 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.2696

Testing for epoch 59 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 2.2748
Epoch 60 of 60

Testing for epoch 60 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.3000

Testing for epoch 60 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 2.3168
32/32 [==============================] - 0s 2ms/step</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1720">
<div class="sourceCode cell-code" id="cb306"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb306-1"><a href="#cb306-1" aria-hidden="true" tabindex="-1"></a>outlier_SO_GAAL_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1721">
<div class="sourceCode cell-code" id="cb307"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb307-1"><a href="#cb307-1" aria-hidden="true" tabindex="-1"></a>outlier_SO_GAAL_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_SO_GAAL_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1722">
<div class="sourceCode cell-code" id="cb308"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb308-1"><a href="#cb308-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one,outlier_SO_GAAL_one,tab_orbit)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1723">
<div class="sourceCode cell-code" id="cb309"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb309-1"><a href="#cb309-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"SO-GAAL (Liu et al., 2019)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-220-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.950
Precision: 0.950
Recall: 1.000
F1 Score: 0.974</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1724">
<div class="sourceCode cell-code" id="cb312"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb312-1"><a href="#cb312-1" aria-hidden="true" tabindex="-1"></a>twelve <span class="op">=</span> eleven.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  twelve = eleven.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="mo_gaalstar-1" class="level3">
<h3 class="anchored" data-anchor-id="mo_gaalstar-1">MO_GAAL<span class="math inline">\(\star\)</span></h3>
<div class="cell" data-scrolled="true" data-tags="[]" data-execution_count="1725">
<div class="sourceCode cell-code" id="cb314"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb314-1"><a href="#cb314-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> MO_GAAL(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb314-2"><a href="#cb314-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'f'</span>]])</span>
<span id="cb314-3"><a href="#cb314-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'MO_GAAL_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super().__init__(name, **kwargs)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1 of 60

Testing for epoch 1 index 1:
32/32 [==============================] - 0s 1ms/step

Testing for epoch 1 index 2:
32/32 [==============================] - 0s 2ms/step
Epoch 2 of 60

Testing for epoch 2 index 1:
32/32 [==============================] - 0s 1ms/step

Testing for epoch 2 index 2:
32/32 [==============================] - 0s 2ms/step
Epoch 3 of 60

Testing for epoch 3 index 1:
32/32 [==============================] - 0s 1ms/step

Testing for epoch 3 index 2:
32/32 [==============================] - 0s 2ms/step
Epoch 4 of 60

Testing for epoch 4 index 1:
32/32 [==============================] - 0s 1ms/step

Testing for epoch 4 index 2:
32/32 [==============================] - 0s 1ms/step
Epoch 5 of 60

Testing for epoch 5 index 1:
32/32 [==============================] - 0s 2ms/step

Testing for epoch 5 index 2:
32/32 [==============================] - 0s 3ms/step
Epoch 6 of 60

Testing for epoch 6 index 1:
32/32 [==============================] - 0s 1ms/step

Testing for epoch 6 index 2:
32/32 [==============================] - 0s 1ms/step
Epoch 7 of 60

Testing for epoch 7 index 1:
32/32 [==============================] - 0s 1ms/step

Testing for epoch 7 index 2:
32/32 [==============================] - 0s 2ms/step
Epoch 8 of 60

Testing for epoch 8 index 1:
32/32 [==============================] - 0s 1ms/step

Testing for epoch 8 index 2:
32/32 [==============================] - 0s 1ms/step
Epoch 9 of 60

Testing for epoch 9 index 1:
32/32 [==============================] - 0s 1ms/step

Testing for epoch 9 index 2:
32/32 [==============================] - 0s 2ms/step
Epoch 10 of 60

Testing for epoch 10 index 1:
32/32 [==============================] - 0s 2ms/step

Testing for epoch 10 index 2:
32/32 [==============================] - 0s 1ms/step
Epoch 11 of 60

Testing for epoch 11 index 1:
32/32 [==============================] - 0s 1ms/step

Testing for epoch 11 index 2:
32/32 [==============================] - 0s 1ms/step
Epoch 12 of 60

Testing for epoch 12 index 1:
32/32 [==============================] - 0s 3ms/step

Testing for epoch 12 index 2:
32/32 [==============================] - 0s 2ms/step
Epoch 13 of 60

Testing for epoch 13 index 1:
32/32 [==============================] - 0s 2ms/step

Testing for epoch 13 index 2:
32/32 [==============================] - 0s 1ms/step
Epoch 14 of 60

Testing for epoch 14 index 1:
32/32 [==============================] - 0s 2ms/step

Testing for epoch 14 index 2:
32/32 [==============================] - 0s 1ms/step
Epoch 15 of 60

Testing for epoch 15 index 1:
32/32 [==============================] - 0s 1ms/step

Testing for epoch 15 index 2:
32/32 [==============================] - 0s 2ms/step
Epoch 16 of 60

Testing for epoch 16 index 1:
32/32 [==============================] - 0s 1ms/step

Testing for epoch 16 index 2:
32/32 [==============================] - 0s 992us/step
Epoch 17 of 60

Testing for epoch 17 index 1:
32/32 [==============================] - 0s 1ms/step

Testing for epoch 17 index 2:
32/32 [==============================] - 0s 782us/step
Epoch 18 of 60

Testing for epoch 18 index 1:
32/32 [==============================] - 0s 680us/step

Testing for epoch 18 index 2:
32/32 [==============================] - 0s 801us/step
Epoch 19 of 60

Testing for epoch 19 index 1:
32/32 [==============================] - 0s 642us/step

Testing for epoch 19 index 2:
32/32 [==============================] - 0s 617us/step
Epoch 20 of 60

Testing for epoch 20 index 1:
32/32 [==============================] - 0s 801us/step

Testing for epoch 20 index 2:
32/32 [==============================] - 0s 2ms/step
Epoch 21 of 60

Testing for epoch 21 index 1:
32/32 [==============================] - 0s 894us/step

Testing for epoch 21 index 2:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.4662
16/16 [==============================] - 0s 1ms/step - loss: 1.1250
16/16 [==============================] - 0s 1ms/step - loss: 1.1652
16/16 [==============================] - 0s 1ms/step - loss: 1.1668
16/16 [==============================] - 0s 997us/step - loss: 1.1673
16/16 [==============================] - 0s 965us/step - loss: 1.1677
16/16 [==============================] - 0s 1ms/step - loss: 1.1681
16/16 [==============================] - 0s 1ms/step - loss: 1.1688
16/16 [==============================] - 0s 971us/step - loss: 1.1690
16/16 [==============================] - 0s 2ms/step - loss: 1.1690
Epoch 22 of 60

Testing for epoch 22 index 1:
32/32 [==============================] - 0s 858us/step
16/16 [==============================] - 0s 2ms/step - loss: 0.4861
16/16 [==============================] - 0s 2ms/step - loss: 1.1078
16/16 [==============================] - 0s 1ms/step - loss: 1.1395
16/16 [==============================] - 0s 1ms/step - loss: 1.1408
16/16 [==============================] - 0s 1ms/step - loss: 1.1412
16/16 [==============================] - 0s 1ms/step - loss: 1.1417
16/16 [==============================] - 0s 2ms/step - loss: 1.1423
16/16 [==============================] - 0s 2ms/step - loss: 1.1430
16/16 [==============================] - 0s 1ms/step - loss: 1.1433
16/16 [==============================] - 0s 1ms/step - loss: 1.1433

Testing for epoch 22 index 2:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.4978
16/16 [==============================] - 0s 2ms/step - loss: 1.1226
16/16 [==============================] - 0s 1ms/step - loss: 1.1496
16/16 [==============================] - 0s 2ms/step - loss: 1.1507
16/16 [==============================] - 0s 1ms/step - loss: 1.1512
16/16 [==============================] - 0s 4ms/step - loss: 1.1516
16/16 [==============================] - 0s 3ms/step - loss: 1.1520
16/16 [==============================] - 0s 2ms/step - loss: 1.1527
16/16 [==============================] - 0s 2ms/step - loss: 1.1529
16/16 [==============================] - 0s 1ms/step - loss: 1.1529
Epoch 23 of 60

Testing for epoch 23 index 1:
32/32 [==============================] - 0s 900us/step
16/16 [==============================] - 0s 2ms/step - loss: 0.5048
16/16 [==============================] - 0s 1ms/step - loss: 1.1336
16/16 [==============================] - 0s 2ms/step - loss: 1.1607
16/16 [==============================] - 0s 1ms/step - loss: 1.1618
16/16 [==============================] - 0s 2ms/step - loss: 1.1622
16/16 [==============================] - 0s 4ms/step - loss: 1.1625
16/16 [==============================] - 0s 2ms/step - loss: 1.1630
16/16 [==============================] - 0s 1ms/step - loss: 1.1636
16/16 [==============================] - 0s 1ms/step - loss: 1.1638
16/16 [==============================] - 0s 2ms/step - loss: 1.1638

Testing for epoch 23 index 2:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.5183
16/16 [==============================] - 0s 2ms/step - loss: 1.1345
16/16 [==============================] - 0s 2ms/step - loss: 1.1583
16/16 [==============================] - 0s 2ms/step - loss: 1.1593
16/16 [==============================] - 0s 2ms/step - loss: 1.1597
16/16 [==============================] - 0s 2ms/step - loss: 1.1601
16/16 [==============================] - 0s 1ms/step - loss: 1.1606
16/16 [==============================] - 0s 1ms/step - loss: 1.1612
16/16 [==============================] - 0s 2ms/step - loss: 1.1614
16/16 [==============================] - 0s 2ms/step - loss: 1.1614
Epoch 24 of 60

Testing for epoch 24 index 1:
32/32 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.5228
16/16 [==============================] - 0s 1ms/step - loss: 1.1392
16/16 [==============================] - 0s 1ms/step - loss: 1.1615
16/16 [==============================] - 0s 977us/step - loss: 1.1623
16/16 [==============================] - 0s 1ms/step - loss: 1.1628
16/16 [==============================] - 0s 1ms/step - loss: 1.1633
16/16 [==============================] - 0s 1ms/step - loss: 1.1639
16/16 [==============================] - 0s 2ms/step - loss: 1.1646
16/16 [==============================] - 0s 1ms/step - loss: 1.1649
16/16 [==============================] - 0s 1ms/step - loss: 1.1649

Testing for epoch 24 index 2:
32/32 [==============================] - 0s 848us/step
16/16 [==============================] - 0s 2ms/step - loss: 0.5259
16/16 [==============================] - 0s 1ms/step - loss: 1.1609
16/16 [==============================] - 0s 1ms/step - loss: 1.1832
16/16 [==============================] - 0s 1ms/step - loss: 1.1841
16/16 [==============================] - 0s 2ms/step - loss: 1.1845
16/16 [==============================] - 0s 2ms/step - loss: 1.1848
16/16 [==============================] - 0s 2ms/step - loss: 1.1853
16/16 [==============================] - 0s 2ms/step - loss: 1.1859
16/16 [==============================] - 0s 2ms/step - loss: 1.1861
16/16 [==============================] - 0s 2ms/step - loss: 1.1861
Epoch 25 of 60

Testing for epoch 25 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.5249
16/16 [==============================] - 0s 2ms/step - loss: 1.1680
16/16 [==============================] - 0s 2ms/step - loss: 1.1921
16/16 [==============================] - 0s 1ms/step - loss: 1.1930
16/16 [==============================] - 0s 1ms/step - loss: 1.1933
16/16 [==============================] - 0s 1ms/step - loss: 1.1937
16/16 [==============================] - 0s 959us/step - loss: 1.1941
16/16 [==============================] - 0s 3ms/step - loss: 1.1947
16/16 [==============================] - 0s 1ms/step - loss: 1.1949
16/16 [==============================] - 0s 1ms/step - loss: 1.1949

Testing for epoch 25 index 2:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.5256
16/16 [==============================] - 0s 4ms/step - loss: 1.1816
16/16 [==============================] - 0s 1ms/step - loss: 1.2066
16/16 [==============================] - 0s 1ms/step - loss: 1.2075
16/16 [==============================] - 0s 1ms/step - loss: 1.2079
16/16 [==============================] - 0s 2ms/step - loss: 1.2082
16/16 [==============================] - 0s 1ms/step - loss: 1.2087
16/16 [==============================] - 0s 1ms/step - loss: 1.2092
16/16 [==============================] - 0s 1ms/step - loss: 1.2095
16/16 [==============================] - 0s 2ms/step - loss: 1.2095
Epoch 26 of 60

Testing for epoch 26 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.5199
16/16 [==============================] - 0s 1ms/step - loss: 1.1842
16/16 [==============================] - 0s 1ms/step - loss: 1.2109
16/16 [==============================] - 0s 2ms/step - loss: 1.2118
16/16 [==============================] - 0s 2ms/step - loss: 1.2122
16/16 [==============================] - 0s 1ms/step - loss: 1.2126
16/16 [==============================] - 0s 1ms/step - loss: 1.2131
16/16 [==============================] - 0s 4ms/step - loss: 1.2137
16/16 [==============================] - 0s 2ms/step - loss: 1.2140
16/16 [==============================] - 0s 878us/step - loss: 1.2140

Testing for epoch 26 index 2:
32/32 [==============================] - 0s 656us/step
16/16 [==============================] - 0s 831us/step - loss: 0.5174
16/16 [==============================] - 0s 864us/step - loss: 1.1946
16/16 [==============================] - 0s 732us/step - loss: 1.2231
16/16 [==============================] - 0s 767us/step - loss: 1.2241
16/16 [==============================] - 0s 810us/step - loss: 1.2245
16/16 [==============================] - 0s 822us/step - loss: 1.2248
16/16 [==============================] - 0s 808us/step - loss: 1.2252
16/16 [==============================] - 0s 788us/step - loss: 1.2257
16/16 [==============================] - 0s 821us/step - loss: 1.2259
16/16 [==============================] - 0s 810us/step - loss: 1.2260
Epoch 27 of 60

Testing for epoch 27 index 1:
32/32 [==============================] - 0s 615us/step
16/16 [==============================] - 0s 796us/step - loss: 0.5052
16/16 [==============================] - 0s 812us/step - loss: 1.2202
16/16 [==============================] - 0s 814us/step - loss: 1.2513
16/16 [==============================] - 0s 778us/step - loss: 1.2525
16/16 [==============================] - 0s 782us/step - loss: 1.2529
16/16 [==============================] - 0s 804us/step - loss: 1.2532
16/16 [==============================] - 0s 777us/step - loss: 1.2536
16/16 [==============================] - 0s 785us/step - loss: 1.2541
16/16 [==============================] - 0s 795us/step - loss: 1.2544
16/16 [==============================] - 0s 797us/step - loss: 1.2544

Testing for epoch 27 index 2:
32/32 [==============================] - 0s 631us/step
16/16 [==============================] - 0s 837us/step - loss: 0.5002
16/16 [==============================] - 0s 831us/step - loss: 1.2353
16/16 [==============================] - 0s 827us/step - loss: 1.2665
16/16 [==============================] - 0s 780us/step - loss: 1.2678
16/16 [==============================] - 0s 798us/step - loss: 1.2682
16/16 [==============================] - 0s 771us/step - loss: 1.2685
16/16 [==============================] - 0s 778us/step - loss: 1.2690
16/16 [==============================] - 0s 809us/step - loss: 1.2696
16/16 [==============================] - 0s 818us/step - loss: 1.2698
16/16 [==============================] - 0s 784us/step - loss: 1.2698
Epoch 28 of 60

Testing for epoch 28 index 1:
32/32 [==============================] - 0s 603us/step
16/16 [==============================] - 0s 792us/step - loss: 0.4877
16/16 [==============================] - 0s 796us/step - loss: 1.2490
16/16 [==============================] - 0s 816us/step - loss: 1.2845
16/16 [==============================] - 0s 825us/step - loss: 1.2858
16/16 [==============================] - 0s 782us/step - loss: 1.2862
16/16 [==============================] - 0s 824us/step - loss: 1.2866
16/16 [==============================] - 0s 812us/step - loss: 1.2870
16/16 [==============================] - 0s 799us/step - loss: 1.2875
16/16 [==============================] - 0s 792us/step - loss: 1.2877
16/16 [==============================] - 0s 811us/step - loss: 1.2877

Testing for epoch 28 index 2:
32/32 [==============================] - 0s 596us/step
16/16 [==============================] - 0s 782us/step - loss: 0.4795
16/16 [==============================] - 0s 819us/step - loss: 1.2762
16/16 [==============================] - 0s 807us/step - loss: 1.3130
16/16 [==============================] - 0s 810us/step - loss: 1.3144
16/16 [==============================] - 0s 812us/step - loss: 1.3148
16/16 [==============================] - 0s 816us/step - loss: 1.3151
16/16 [==============================] - 0s 774us/step - loss: 1.3156
16/16 [==============================] - 0s 780us/step - loss: 1.3161
16/16 [==============================] - 0s 805us/step - loss: 1.3163
16/16 [==============================] - 0s 775us/step - loss: 1.3163
Epoch 29 of 60

Testing for epoch 29 index 1:
32/32 [==============================] - 0s 610us/step
16/16 [==============================] - 0s 791us/step - loss: 0.4700
16/16 [==============================] - 0s 815us/step - loss: 1.2786
16/16 [==============================] - 0s 795us/step - loss: 1.3164
16/16 [==============================] - 0s 800us/step - loss: 1.3177
16/16 [==============================] - 0s 781us/step - loss: 1.3182
16/16 [==============================] - 0s 768us/step - loss: 1.3186
16/16 [==============================] - 0s 775us/step - loss: 1.3191
16/16 [==============================] - 0s 795us/step - loss: 1.3197
16/16 [==============================] - 0s 799us/step - loss: 1.3199
16/16 [==============================] - 0s 775us/step - loss: 1.3199

Testing for epoch 29 index 2:
32/32 [==============================] - 0s 611us/step
16/16 [==============================] - 0s 790us/step - loss: 0.4639
16/16 [==============================] - 0s 816us/step - loss: 1.2951
16/16 [==============================] - 0s 784us/step - loss: 1.3344
16/16 [==============================] - 0s 821us/step - loss: 1.3358
16/16 [==============================] - 0s 769us/step - loss: 1.3362
16/16 [==============================] - 0s 798us/step - loss: 1.3366
16/16 [==============================] - 0s 773us/step - loss: 1.3371
16/16 [==============================] - 0s 781us/step - loss: 1.3376
16/16 [==============================] - 0s 803us/step - loss: 1.3379
16/16 [==============================] - 0s 795us/step - loss: 1.3379
Epoch 30 of 60

Testing for epoch 30 index 1:
32/32 [==============================] - 0s 629us/step
16/16 [==============================] - 0s 792us/step - loss: 0.4496
16/16 [==============================] - 0s 777us/step - loss: 1.3229
16/16 [==============================] - 0s 782us/step - loss: 1.3686
16/16 [==============================] - 0s 793us/step - loss: 1.3703
16/16 [==============================] - 0s 798us/step - loss: 1.3707
16/16 [==============================] - 0s 792us/step - loss: 1.3710
16/16 [==============================] - 0s 784us/step - loss: 1.3714
16/16 [==============================] - 0s 780us/step - loss: 1.3719
16/16 [==============================] - 0s 803us/step - loss: 1.3720
16/16 [==============================] - 0s 773us/step - loss: 1.3721

Testing for epoch 30 index 2:
32/32 [==============================] - 0s 598us/step
16/16 [==============================] - 0s 780us/step - loss: 0.4461
16/16 [==============================] - 0s 772us/step - loss: 1.3343
16/16 [==============================] - 0s 771us/step - loss: 1.3836
16/16 [==============================] - 0s 777us/step - loss: 1.3853
16/16 [==============================] - 0s 823us/step - loss: 1.3857
16/16 [==============================] - 0s 797us/step - loss: 1.3861
16/16 [==============================] - 0s 783us/step - loss: 1.3865
16/16 [==============================] - 0s 770us/step - loss: 1.3870
16/16 [==============================] - 0s 769us/step - loss: 1.3871
16/16 [==============================] - 0s 791us/step - loss: 1.3872
Epoch 31 of 60

Testing for epoch 31 index 1:
32/32 [==============================] - 0s 591us/step
16/16 [==============================] - 0s 793us/step - loss: 0.4388
16/16 [==============================] - 0s 803us/step - loss: 1.3348
16/16 [==============================] - 0s 785us/step - loss: 1.3881
16/16 [==============================] - 0s 795us/step - loss: 1.3898
16/16 [==============================] - 0s 784us/step - loss: 1.3903
16/16 [==============================] - 0s 814us/step - loss: 1.3906
16/16 [==============================] - 0s 764us/step - loss: 1.3910
16/16 [==============================] - 0s 769us/step - loss: 1.3915
16/16 [==============================] - 0s 766us/step - loss: 1.3917
16/16 [==============================] - 0s 770us/step - loss: 1.3917

Testing for epoch 31 index 2:
32/32 [==============================] - 0s 601us/step
16/16 [==============================] - 0s 785us/step - loss: 0.4335
16/16 [==============================] - 0s 778us/step - loss: 1.3689
16/16 [==============================] - 0s 776us/step - loss: 1.4249
16/16 [==============================] - 0s 794us/step - loss: 1.4268
16/16 [==============================] - 0s 773us/step - loss: 1.4272
16/16 [==============================] - 0s 770us/step - loss: 1.4275
16/16 [==============================] - 0s 770us/step - loss: 1.4279
16/16 [==============================] - 0s 788us/step - loss: 1.4283
16/16 [==============================] - 0s 792us/step - loss: 1.4285
16/16 [==============================] - 0s 802us/step - loss: 1.4285
Epoch 32 of 60

Testing for epoch 32 index 1:
32/32 [==============================] - 0s 602us/step
16/16 [==============================] - 0s 797us/step - loss: 0.4265
16/16 [==============================] - 0s 805us/step - loss: 1.3817
16/16 [==============================] - 0s 815us/step - loss: 1.4402
16/16 [==============================] - 0s 784us/step - loss: 1.4422
16/16 [==============================] - 0s 785us/step - loss: 1.4426
16/16 [==============================] - 0s 791us/step - loss: 1.4429
16/16 [==============================] - 0s 811us/step - loss: 1.4432
16/16 [==============================] - 0s 791us/step - loss: 1.4435
16/16 [==============================] - 0s 809us/step - loss: 1.4437
16/16 [==============================] - 0s 777us/step - loss: 1.4437

Testing for epoch 32 index 2:
32/32 [==============================] - 0s 622us/step
16/16 [==============================] - 0s 819us/step - loss: 0.4303
16/16 [==============================] - 0s 794us/step - loss: 1.3806
16/16 [==============================] - 0s 809us/step - loss: 1.4368
16/16 [==============================] - 0s 791us/step - loss: 1.4388
16/16 [==============================] - 0s 781us/step - loss: 1.4392
16/16 [==============================] - 0s 797us/step - loss: 1.4396
16/16 [==============================] - 0s 784us/step - loss: 1.4399
16/16 [==============================] - 0s 786us/step - loss: 1.4404
16/16 [==============================] - 0s 808us/step - loss: 1.4406
16/16 [==============================] - 0s 809us/step - loss: 1.4406
Epoch 33 of 60

Testing for epoch 33 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 4ms/step - loss: 0.4255
16/16 [==============================] - 0s 2ms/step - loss: 1.3957
16/16 [==============================] - 0s 2ms/step - loss: 1.4540
16/16 [==============================] - 0s 2ms/step - loss: 1.4562
16/16 [==============================] - 0s 1ms/step - loss: 1.4566
16/16 [==============================] - 0s 2ms/step - loss: 1.4569
16/16 [==============================] - 0s 1ms/step - loss: 1.4573
16/16 [==============================] - 0s 2ms/step - loss: 1.4577
16/16 [==============================] - 0s 2ms/step - loss: 1.4579
16/16 [==============================] - 0s 4ms/step - loss: 1.4579

Testing for epoch 33 index 2:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.4288
16/16 [==============================] - 0s 1ms/step - loss: 1.4084
16/16 [==============================] - 0s 1ms/step - loss: 1.4660
16/16 [==============================] - 0s 1ms/step - loss: 1.4681
16/16 [==============================] - 0s 2ms/step - loss: 1.4686
16/16 [==============================] - 0s 2ms/step - loss: 1.4689
16/16 [==============================] - 0s 1ms/step - loss: 1.4692
16/16 [==============================] - 0s 1ms/step - loss: 1.4696
16/16 [==============================] - 0s 1ms/step - loss: 1.4698
16/16 [==============================] - 0s 1ms/step - loss: 1.4698
Epoch 34 of 60

Testing for epoch 34 index 1:
32/32 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.4279
16/16 [==============================] - 0s 1ms/step - loss: 1.4181
16/16 [==============================] - 0s 1ms/step - loss: 1.4764
16/16 [==============================] - 0s 5ms/step - loss: 1.4786
16/16 [==============================] - 0s 4ms/step - loss: 1.4790
16/16 [==============================] - 0s 2ms/step - loss: 1.4793
16/16 [==============================] - 0s 2ms/step - loss: 1.4796
16/16 [==============================] - 0s 1ms/step - loss: 1.4800
16/16 [==============================] - 0s 1ms/step - loss: 1.4802
16/16 [==============================] - 0s 1ms/step - loss: 1.4802

Testing for epoch 34 index 2:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 4ms/step - loss: 0.4335
16/16 [==============================] - 0s 1ms/step - loss: 1.4360
16/16 [==============================] - 0s 2ms/step - loss: 1.4928
16/16 [==============================] - 0s 3ms/step - loss: 1.4949
16/16 [==============================] - 0s 1ms/step - loss: 1.4954
16/16 [==============================] - 0s 1ms/step - loss: 1.4957
16/16 [==============================] - 0s 938us/step - loss: 1.4960
16/16 [==============================] - 0s 3ms/step - loss: 1.4964
16/16 [==============================] - 0s 4ms/step - loss: 1.4966
16/16 [==============================] - 0s 2ms/step - loss: 1.4966
Epoch 35 of 60

Testing for epoch 35 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.4357
16/16 [==============================] - 0s 1ms/step - loss: 1.4423
16/16 [==============================] - 0s 1ms/step - loss: 1.4991
16/16 [==============================] - 0s 1ms/step - loss: 1.5013
16/16 [==============================] - 0s 987us/step - loss: 1.5017
16/16 [==============================] - 0s 2ms/step - loss: 1.5019
16/16 [==============================] - 0s 2ms/step - loss: 1.5022
16/16 [==============================] - 0s 1ms/step - loss: 1.5026
16/16 [==============================] - 0s 1ms/step - loss: 1.5028
16/16 [==============================] - 0s 2ms/step - loss: 1.5028

Testing for epoch 35 index 2:
32/32 [==============================] - 0s 4ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.4458
16/16 [==============================] - 0s 3ms/step - loss: 1.4532
16/16 [==============================] - 0s 1ms/step - loss: 1.5074
16/16 [==============================] - 0s 1ms/step - loss: 1.5094
16/16 [==============================] - 0s 1ms/step - loss: 1.5098
16/16 [==============================] - 0s 1ms/step - loss: 1.5100
16/16 [==============================] - 0s 1ms/step - loss: 1.5103
16/16 [==============================] - 0s 1ms/step - loss: 1.5106
16/16 [==============================] - 0s 1ms/step - loss: 1.5107
16/16 [==============================] - 0s 2ms/step - loss: 1.5107
Epoch 36 of 60

Testing for epoch 36 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.4508
16/16 [==============================] - 0s 2ms/step - loss: 1.4504
16/16 [==============================] - 0s 2ms/step - loss: 1.5026
16/16 [==============================] - 0s 1ms/step - loss: 1.5046
16/16 [==============================] - 0s 1ms/step - loss: 1.5050
16/16 [==============================] - 0s 1ms/step - loss: 1.5053
16/16 [==============================] - 0s 1ms/step - loss: 1.5056
16/16 [==============================] - 0s 1ms/step - loss: 1.5060
16/16 [==============================] - 0s 1ms/step - loss: 1.5062
16/16 [==============================] - 0s 1ms/step - loss: 1.5062

Testing for epoch 36 index 2:
32/32 [==============================] - 0s 988us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.4614
16/16 [==============================] - 0s 1ms/step - loss: 1.4690
16/16 [==============================] - 0s 1ms/step - loss: 1.5199
16/16 [==============================] - 0s 1ms/step - loss: 1.5217
16/16 [==============================] - 0s 1ms/step - loss: 1.5222
16/16 [==============================] - 0s 1ms/step - loss: 1.5225
16/16 [==============================] - 0s 2ms/step - loss: 1.5228
16/16 [==============================] - 0s 891us/step - loss: 1.5233
16/16 [==============================] - 0s 1ms/step - loss: 1.5235
16/16 [==============================] - 0s 1ms/step - loss: 1.5235
Epoch 37 of 60

Testing for epoch 37 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.4674
16/16 [==============================] - 0s 1ms/step - loss: 1.4703
16/16 [==============================] - 0s 3ms/step - loss: 1.5202
16/16 [==============================] - 0s 1ms/step - loss: 1.5220
16/16 [==============================] - 0s 1ms/step - loss: 1.5224
16/16 [==============================] - 0s 1ms/step - loss: 1.5226
16/16 [==============================] - 0s 1ms/step - loss: 1.5229
16/16 [==============================] - 0s 1ms/step - loss: 1.5232
16/16 [==============================] - 0s 1ms/step - loss: 1.5233
16/16 [==============================] - 0s 1ms/step - loss: 1.5233

Testing for epoch 37 index 2:
32/32 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.4817
16/16 [==============================] - 0s 2ms/step - loss: 1.4794
16/16 [==============================] - 0s 2ms/step - loss: 1.5276
16/16 [==============================] - 0s 2ms/step - loss: 1.5293
16/16 [==============================] - 0s 1ms/step - loss: 1.5296
16/16 [==============================] - 0s 1ms/step - loss: 1.5299
16/16 [==============================] - 0s 2ms/step - loss: 1.5301
16/16 [==============================] - 0s 2ms/step - loss: 1.5304
16/16 [==============================] - 0s 2ms/step - loss: 1.5305
16/16 [==============================] - 0s 2ms/step - loss: 1.5305
Epoch 38 of 60

Testing for epoch 38 index 1:
32/32 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.4878
16/16 [==============================] - 0s 1ms/step - loss: 1.4939
16/16 [==============================] - 0s 1ms/step - loss: 1.5415
16/16 [==============================] - 0s 2ms/step - loss: 1.5431
16/16 [==============================] - 0s 2ms/step - loss: 1.5435
16/16 [==============================] - 0s 1ms/step - loss: 1.5437
16/16 [==============================] - 0s 2ms/step - loss: 1.5439
16/16 [==============================] - 0s 2ms/step - loss: 1.5442
16/16 [==============================] - 0s 2ms/step - loss: 1.5443
16/16 [==============================] - 0s 2ms/step - loss: 1.5443

Testing for epoch 38 index 2:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.5035
16/16 [==============================] - 0s 1ms/step - loss: 1.4905
16/16 [==============================] - 0s 1ms/step - loss: 1.5351
16/16 [==============================] - 0s 1ms/step - loss: 1.5365
16/16 [==============================] - 0s 1ms/step - loss: 1.5370
16/16 [==============================] - 0s 1ms/step - loss: 1.5373
16/16 [==============================] - 0s 988us/step - loss: 1.5376
16/16 [==============================] - 0s 2ms/step - loss: 1.5381
16/16 [==============================] - 0s 2ms/step - loss: 1.5382
16/16 [==============================] - 0s 2ms/step - loss: 1.5383
Epoch 39 of 60

Testing for epoch 39 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.5103
16/16 [==============================] - 0s 1ms/step - loss: 1.5127
16/16 [==============================] - 0s 1ms/step - loss: 1.5577
16/16 [==============================] - 0s 1ms/step - loss: 1.5592
16/16 [==============================] - 0s 4ms/step - loss: 1.5596
16/16 [==============================] - 0s 4ms/step - loss: 1.5598
16/16 [==============================] - 0s 1ms/step - loss: 1.5601
16/16 [==============================] - 0s 3ms/step - loss: 1.5604
16/16 [==============================] - 0s 1ms/step - loss: 1.5605
16/16 [==============================] - 0s 1ms/step - loss: 1.5605

Testing for epoch 39 index 2:
32/32 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.5264
16/16 [==============================] - 0s 1ms/step - loss: 1.5167
16/16 [==============================] - 0s 3ms/step - loss: 1.5597
16/16 [==============================] - 0s 2ms/step - loss: 1.5612
16/16 [==============================] - 0s 1ms/step - loss: 1.5615
16/16 [==============================] - 0s 1ms/step - loss: 1.5617
16/16 [==============================] - 0s 1ms/step - loss: 1.5620
16/16 [==============================] - 0s 1ms/step - loss: 1.5622
16/16 [==============================] - 0s 1ms/step - loss: 1.5623
16/16 [==============================] - 0s 1ms/step - loss: 1.5623
Epoch 40 of 60

Testing for epoch 40 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.5342
16/16 [==============================] - 0s 2ms/step - loss: 1.5283
16/16 [==============================] - 0s 2ms/step - loss: 1.5709
16/16 [==============================] - 0s 1ms/step - loss: 1.5724
16/16 [==============================] - 0s 1ms/step - loss: 1.5727
16/16 [==============================] - 0s 1ms/step - loss: 1.5729
16/16 [==============================] - 0s 2ms/step - loss: 1.5732
16/16 [==============================] - 0s 1ms/step - loss: 1.5734
16/16 [==============================] - 0s 1ms/step - loss: 1.5735
16/16 [==============================] - 0s 1ms/step - loss: 1.5735

Testing for epoch 40 index 2:
32/32 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.5499
16/16 [==============================] - 0s 2ms/step - loss: 1.5318
16/16 [==============================] - 0s 2ms/step - loss: 1.5719
16/16 [==============================] - 0s 1ms/step - loss: 1.5733
16/16 [==============================] - 0s 1ms/step - loss: 1.5736
16/16 [==============================] - 0s 2ms/step - loss: 1.5738
16/16 [==============================] - 0s 2ms/step - loss: 1.5740
16/16 [==============================] - 0s 2ms/step - loss: 1.5743
16/16 [==============================] - 0s 1ms/step - loss: 1.5744
16/16 [==============================] - 0s 1ms/step - loss: 1.5744
Epoch 41 of 60

Testing for epoch 41 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.5570
16/16 [==============================] - 0s 1ms/step - loss: 1.5475
16/16 [==============================] - 0s 1ms/step - loss: 1.5874
16/16 [==============================] - 0s 1ms/step - loss: 1.5888
16/16 [==============================] - 0s 2ms/step - loss: 1.5891
16/16 [==============================] - 0s 2ms/step - loss: 1.5892
16/16 [==============================] - 0s 2ms/step - loss: 1.5893
16/16 [==============================] - 0s 1ms/step - loss: 1.5895
16/16 [==============================] - 0s 1ms/step - loss: 1.5896
16/16 [==============================] - 0s 2ms/step - loss: 1.5896

Testing for epoch 41 index 2:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.5733
16/16 [==============================] - 0s 1ms/step - loss: 1.5531
16/16 [==============================] - 0s 1ms/step - loss: 1.5914
16/16 [==============================] - 0s 1ms/step - loss: 1.5927
16/16 [==============================] - 0s 2ms/step - loss: 1.5931
16/16 [==============================] - 0s 2ms/step - loss: 1.5932
16/16 [==============================] - 0s 2ms/step - loss: 1.5934
16/16 [==============================] - 0s 1ms/step - loss: 1.5937
16/16 [==============================] - 0s 1ms/step - loss: 1.5938
16/16 [==============================] - 0s 1ms/step - loss: 1.5938
Epoch 42 of 60

Testing for epoch 42 index 1:
32/32 [==============================] - 0s 830us/step
16/16 [==============================] - 0s 2ms/step - loss: 0.5804
16/16 [==============================] - 0s 2ms/step - loss: 1.5605
16/16 [==============================] - 0s 1ms/step - loss: 1.5989
16/16 [==============================] - 0s 2ms/step - loss: 1.6002
16/16 [==============================] - 0s 1ms/step - loss: 1.6005
16/16 [==============================] - 0s 1ms/step - loss: 1.6007
16/16 [==============================] - 0s 2ms/step - loss: 1.6010
16/16 [==============================] - 0s 2ms/step - loss: 1.6013
16/16 [==============================] - 0s 1ms/step - loss: 1.6014
16/16 [==============================] - 0s 2ms/step - loss: 1.6014

Testing for epoch 42 index 2:
32/32 [==============================] - 0s 770us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.5964
16/16 [==============================] - 0s 3ms/step - loss: 1.5755
16/16 [==============================] - 0s 2ms/step - loss: 1.6137
16/16 [==============================] - 0s 2ms/step - loss: 1.6149
16/16 [==============================] - 0s 1ms/step - loss: 1.6153
16/16 [==============================] - 0s 1ms/step - loss: 1.6155
16/16 [==============================] - 0s 1ms/step - loss: 1.6157
16/16 [==============================] - 0s 2ms/step - loss: 1.6160
16/16 [==============================] - 0s 2ms/step - loss: 1.6161
16/16 [==============================] - 0s 3ms/step - loss: 1.6161
Epoch 43 of 60

Testing for epoch 43 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.6020
16/16 [==============================] - 0s 2ms/step - loss: 1.5861
16/16 [==============================] - 0s 4ms/step - loss: 1.6247
16/16 [==============================] - 0s 2ms/step - loss: 1.6259
16/16 [==============================] - 0s 2ms/step - loss: 1.6263
16/16 [==============================] - 0s 2ms/step - loss: 1.6264
16/16 [==============================] - 0s 1ms/step - loss: 1.6266
16/16 [==============================] - 0s 2ms/step - loss: 1.6268
16/16 [==============================] - 0s 2ms/step - loss: 1.6269
16/16 [==============================] - 0s 2ms/step - loss: 1.6269

Testing for epoch 43 index 2:
32/32 [==============================] - 0s 896us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.6162
16/16 [==============================] - 0s 1ms/step - loss: 1.5924
16/16 [==============================] - 0s 988us/step - loss: 1.6302
16/16 [==============================] - 0s 2ms/step - loss: 1.6314
16/16 [==============================] - 0s 2ms/step - loss: 1.6318
16/16 [==============================] - 0s 1ms/step - loss: 1.6319
16/16 [==============================] - 0s 1ms/step - loss: 1.6321
16/16 [==============================] - 0s 1ms/step - loss: 1.6323
16/16 [==============================] - 0s 2ms/step - loss: 1.6324
16/16 [==============================] - 0s 2ms/step - loss: 1.6324
Epoch 44 of 60

Testing for epoch 44 index 1:
32/32 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 5ms/step - loss: 0.6208
16/16 [==============================] - 0s 3ms/step - loss: 1.6042
16/16 [==============================] - 0s 2ms/step - loss: 1.6422
16/16 [==============================] - 0s 1ms/step - loss: 1.6435
16/16 [==============================] - 0s 2ms/step - loss: 1.6438
16/16 [==============================] - 0s 2ms/step - loss: 1.6440
16/16 [==============================] - 0s 2ms/step - loss: 1.6442
16/16 [==============================] - 0s 1ms/step - loss: 1.6444
16/16 [==============================] - 0s 1ms/step - loss: 1.6445
16/16 [==============================] - 0s 1ms/step - loss: 1.6445

Testing for epoch 44 index 2:
32/32 [==============================] - 0s 855us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.6333
16/16 [==============================] - 0s 1ms/step - loss: 1.6115
16/16 [==============================] - 0s 2ms/step - loss: 1.6493
16/16 [==============================] - 0s 2ms/step - loss: 1.6505
16/16 [==============================] - 0s 1ms/step - loss: 1.6508
16/16 [==============================] - 0s 1ms/step - loss: 1.6510
16/16 [==============================] - 0s 2ms/step - loss: 1.6512
16/16 [==============================] - 0s 2ms/step - loss: 1.6515
16/16 [==============================] - 0s 1ms/step - loss: 1.6516
16/16 [==============================] - 0s 2ms/step - loss: 1.6516
Epoch 45 of 60

Testing for epoch 45 index 1:
32/32 [==============================] - 0s 836us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.6363
16/16 [==============================] - 0s 2ms/step - loss: 1.6203
16/16 [==============================] - 0s 1ms/step - loss: 1.6589
16/16 [==============================] - 0s 1ms/step - loss: 1.6601
16/16 [==============================] - 0s 1ms/step - loss: 1.6604
16/16 [==============================] - 0s 1ms/step - loss: 1.6606
16/16 [==============================] - 0s 771us/step - loss: 1.6608
16/16 [==============================] - 0s 1ms/step - loss: 1.6610
16/16 [==============================] - 0s 954us/step - loss: 1.6611
16/16 [==============================] - 0s 943us/step - loss: 1.6611

Testing for epoch 45 index 2:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.6489
16/16 [==============================] - 0s 1ms/step - loss: 1.6337
16/16 [==============================] - 0s 1ms/step - loss: 1.6729
16/16 [==============================] - 0s 1ms/step - loss: 1.6741
16/16 [==============================] - 0s 1ms/step - loss: 1.6744
16/16 [==============================] - 0s 1ms/step - loss: 1.6746
16/16 [==============================] - 0s 2ms/step - loss: 1.6747
16/16 [==============================] - 0s 2ms/step - loss: 1.6749
16/16 [==============================] - 0s 1ms/step - loss: 1.6749
16/16 [==============================] - 0s 2ms/step - loss: 1.6750
Epoch 46 of 60

Testing for epoch 46 index 1:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.6501
16/16 [==============================] - 0s 1ms/step - loss: 1.6367
16/16 [==============================] - 0s 1ms/step - loss: 1.6765
16/16 [==============================] - 0s 3ms/step - loss: 1.6777
16/16 [==============================] - 0s 2ms/step - loss: 1.6780
16/16 [==============================] - 0s 2ms/step - loss: 1.6781
16/16 [==============================] - 0s 2ms/step - loss: 1.6783
16/16 [==============================] - 0s 1ms/step - loss: 1.6784
16/16 [==============================] - 0s 2ms/step - loss: 1.6785
16/16 [==============================] - 0s 1ms/step - loss: 1.6785

Testing for epoch 46 index 2:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 819us/step - loss: 0.6612
16/16 [==============================] - 0s 804us/step - loss: 1.6441
16/16 [==============================] - 0s 1ms/step - loss: 1.6840
16/16 [==============================] - 0s 785us/step - loss: 1.6852
16/16 [==============================] - 0s 1ms/step - loss: 1.6855
16/16 [==============================] - 0s 1ms/step - loss: 1.6856
16/16 [==============================] - 0s 1ms/step - loss: 1.6857
16/16 [==============================] - 0s 1ms/step - loss: 1.6858
16/16 [==============================] - 0s 1ms/step - loss: 1.6858
16/16 [==============================] - 0s 1ms/step - loss: 1.6858
Epoch 47 of 60

Testing for epoch 47 index 1:
32/32 [==============================] - 0s 835us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.6626
16/16 [==============================] - 0s 1ms/step - loss: 1.6545
16/16 [==============================] - 0s 805us/step - loss: 1.6944
16/16 [==============================] - 0s 804us/step - loss: 1.6957
16/16 [==============================] - 0s 1ms/step - loss: 1.6960
16/16 [==============================] - 0s 1ms/step - loss: 1.6961
16/16 [==============================] - 0s 1ms/step - loss: 1.6961
16/16 [==============================] - 0s 1ms/step - loss: 1.6962
16/16 [==============================] - 0s 778us/step - loss: 1.6962
16/16 [==============================] - 0s 792us/step - loss: 1.6962

Testing for epoch 47 index 2:
32/32 [==============================] - 0s 600us/step
16/16 [==============================] - 0s 791us/step - loss: 0.6744
16/16 [==============================] - 0s 976us/step - loss: 1.6678
16/16 [==============================] - 0s 1ms/step - loss: 1.7065
16/16 [==============================] - 0s 804us/step - loss: 1.7078
16/16 [==============================] - 0s 1ms/step - loss: 1.7081
16/16 [==============================] - 0s 1ms/step - loss: 1.7083
16/16 [==============================] - 0s 1ms/step - loss: 1.7085
16/16 [==============================] - 0s 777us/step - loss: 1.7087
16/16 [==============================] - 0s 800us/step - loss: 1.7088
16/16 [==============================] - 0s 789us/step - loss: 1.7088
Epoch 48 of 60

Testing for epoch 48 index 1:
32/32 [==============================] - 0s 618us/step
16/16 [==============================] - 0s 829us/step - loss: 0.6775
16/16 [==============================] - 0s 800us/step - loss: 1.6875
16/16 [==============================] - 0s 1ms/step - loss: 1.7276
16/16 [==============================] - 0s 1ms/step - loss: 1.7289
16/16 [==============================] - 0s 808us/step - loss: 1.7292
16/16 [==============================] - 0s 776us/step - loss: 1.7293
16/16 [==============================] - 0s 792us/step - loss: 1.7294
16/16 [==============================] - 0s 1ms/step - loss: 1.7295
16/16 [==============================] - 0s 1ms/step - loss: 1.7296
16/16 [==============================] - 0s 1ms/step - loss: 1.7296

Testing for epoch 48 index 2:
32/32 [==============================] - 0s 831us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.6904
16/16 [==============================] - 0s 1ms/step - loss: 1.7060
16/16 [==============================] - 0s 1ms/step - loss: 1.7464
16/16 [==============================] - 0s 1ms/step - loss: 1.7477
16/16 [==============================] - 0s 1ms/step - loss: 1.7480
16/16 [==============================] - 0s 1ms/step - loss: 1.7481
16/16 [==============================] - 0s 1ms/step - loss: 1.7482
16/16 [==============================] - 0s 1ms/step - loss: 1.7484
16/16 [==============================] - 0s 813us/step - loss: 1.7485
16/16 [==============================] - 0s 1ms/step - loss: 1.7485
Epoch 49 of 60

Testing for epoch 49 index 1:
32/32 [==============================] - 0s 591us/step
16/16 [==============================] - 0s 779us/step - loss: 0.6923
16/16 [==============================] - 0s 771us/step - loss: 1.7161
16/16 [==============================] - 0s 799us/step - loss: 1.7569
16/16 [==============================] - 0s 819us/step - loss: 1.7583
16/16 [==============================] - 0s 1ms/step - loss: 1.7586
16/16 [==============================] - 0s 1ms/step - loss: 1.7587
16/16 [==============================] - 0s 1ms/step - loss: 1.7589
16/16 [==============================] - 0s 1ms/step - loss: 1.7591
16/16 [==============================] - 0s 1ms/step - loss: 1.7592
16/16 [==============================] - 0s 1ms/step - loss: 1.7592

Testing for epoch 49 index 2:
32/32 [==============================] - 0s 596us/step
16/16 [==============================] - 0s 779us/step - loss: 0.7016
16/16 [==============================] - 0s 782us/step - loss: 1.7163
16/16 [==============================] - 0s 798us/step - loss: 1.7574
16/16 [==============================] - 0s 790us/step - loss: 1.7587
16/16 [==============================] - 0s 796us/step - loss: 1.7589
16/16 [==============================] - 0s 778us/step - loss: 1.7590
16/16 [==============================] - 0s 770us/step - loss: 1.7591
16/16 [==============================] - 0s 799us/step - loss: 1.7591
16/16 [==============================] - 0s 810us/step - loss: 1.7591
16/16 [==============================] - 0s 779us/step - loss: 1.7591
Epoch 50 of 60

Testing for epoch 50 index 1:
32/32 [==============================] - 0s 605us/step
16/16 [==============================] - 0s 792us/step - loss: 0.7041
16/16 [==============================] - 0s 787us/step - loss: 1.7274
16/16 [==============================] - 0s 833us/step - loss: 1.7685
16/16 [==============================] - 0s 826us/step - loss: 1.7698
16/16 [==============================] - 0s 792us/step - loss: 1.7702
16/16 [==============================] - 0s 818us/step - loss: 1.7703
16/16 [==============================] - 0s 778us/step - loss: 1.7705
16/16 [==============================] - 0s 841us/step - loss: 1.7707
16/16 [==============================] - 0s 805us/step - loss: 1.7708
16/16 [==============================] - 0s 838us/step - loss: 1.7708

Testing for epoch 50 index 2:
32/32 [==============================] - 0s 624us/step
16/16 [==============================] - 0s 824us/step - loss: 0.7152
16/16 [==============================] - 0s 796us/step - loss: 1.7311
16/16 [==============================] - 0s 817us/step - loss: 1.7720
16/16 [==============================] - 0s 781us/step - loss: 1.7732
16/16 [==============================] - 0s 1ms/step - loss: 1.7736
16/16 [==============================] - 0s 1ms/step - loss: 1.7737
16/16 [==============================] - 0s 1ms/step - loss: 1.7739
16/16 [==============================] - 0s 895us/step - loss: 1.7741
16/16 [==============================] - 0s 776us/step - loss: 1.7742
16/16 [==============================] - 0s 804us/step - loss: 1.7742
Epoch 51 of 60

Testing for epoch 51 index 1:
32/32 [==============================] - 0s 608us/step
16/16 [==============================] - 0s 813us/step - loss: 0.7194
16/16 [==============================] - 0s 799us/step - loss: 1.7544
16/16 [==============================] - 0s 796us/step - loss: 1.7966
16/16 [==============================] - 0s 790us/step - loss: 1.7980
16/16 [==============================] - 0s 810us/step - loss: 1.7983
16/16 [==============================] - 0s 804us/step - loss: 1.7984
16/16 [==============================] - 0s 821us/step - loss: 1.7985
16/16 [==============================] - 0s 791us/step - loss: 1.7987
16/16 [==============================] - 0s 781us/step - loss: 1.7987
16/16 [==============================] - 0s 789us/step - loss: 1.7987

Testing for epoch 51 index 2:
32/32 [==============================] - 0s 617us/step
16/16 [==============================] - 0s 789us/step - loss: 0.7322
16/16 [==============================] - 0s 802us/step - loss: 1.7637
16/16 [==============================] - 0s 793us/step - loss: 1.8060
16/16 [==============================] - 0s 795us/step - loss: 1.8073
16/16 [==============================] - 0s 783us/step - loss: 1.8076
16/16 [==============================] - 0s 776us/step - loss: 1.8077
16/16 [==============================] - 0s 770us/step - loss: 1.8079
16/16 [==============================] - 0s 779us/step - loss: 1.8080
16/16 [==============================] - 0s 787us/step - loss: 1.8081
16/16 [==============================] - 0s 805us/step - loss: 1.8081
Epoch 52 of 60

Testing for epoch 52 index 1:
32/32 [==============================] - 0s 608us/step
16/16 [==============================] - 0s 809us/step - loss: 0.7334
16/16 [==============================] - 0s 807us/step - loss: 1.7648
16/16 [==============================] - 0s 785us/step - loss: 1.8070
16/16 [==============================] - 0s 789us/step - loss: 1.8083
16/16 [==============================] - 0s 788us/step - loss: 1.8086
16/16 [==============================] - 0s 825us/step - loss: 1.8088
16/16 [==============================] - 0s 794us/step - loss: 1.8089
16/16 [==============================] - 0s 839us/step - loss: 1.8091
16/16 [==============================] - 0s 827us/step - loss: 1.8092
16/16 [==============================] - 0s 832us/step - loss: 1.8092

Testing for epoch 52 index 2:
32/32 [==============================] - 0s 640us/step
16/16 [==============================] - 0s 953us/step - loss: 0.7473
16/16 [==============================] - 0s 1ms/step - loss: 1.7771
16/16 [==============================] - 0s 1ms/step - loss: 1.8202
16/16 [==============================] - 0s 782us/step - loss: 1.8215
16/16 [==============================] - 0s 806us/step - loss: 1.8217
16/16 [==============================] - 0s 789us/step - loss: 1.8217
16/16 [==============================] - 0s 821us/step - loss: 1.8217
16/16 [==============================] - 0s 810us/step - loss: 1.8217
16/16 [==============================] - 0s 706us/step - loss: 1.8217
16/16 [==============================] - 0s 675us/step - loss: 1.8217
Epoch 53 of 60

Testing for epoch 53 index 1:
32/32 [==============================] - 0s 543us/step
16/16 [==============================] - 0s 815us/step - loss: 0.7534
16/16 [==============================] - 0s 798us/step - loss: 1.7954
16/16 [==============================] - 0s 682us/step - loss: 1.8386
16/16 [==============================] - 0s 821us/step - loss: 1.8399
16/16 [==============================] - 0s 786us/step - loss: 1.8402
16/16 [==============================] - 0s 788us/step - loss: 1.8403
16/16 [==============================] - 0s 801us/step - loss: 1.8404
16/16 [==============================] - 0s 864us/step - loss: 1.8405
16/16 [==============================] - 0s 866us/step - loss: 1.8405
16/16 [==============================] - 0s 861us/step - loss: 1.8405

Testing for epoch 53 index 2:
32/32 [==============================] - 0s 821us/step
16/16 [==============================] - 0s 901us/step - loss: 0.7658
16/16 [==============================] - 0s 876us/step - loss: 1.7944
16/16 [==============================] - 0s 904us/step - loss: 1.8367
16/16 [==============================] - 0s 923us/step - loss: 1.8380
16/16 [==============================] - 0s 863us/step - loss: 1.8382
16/16 [==============================] - 0s 888us/step - loss: 1.8383
16/16 [==============================] - 0s 892us/step - loss: 1.8384
16/16 [==============================] - 0s 872us/step - loss: 1.8385
16/16 [==============================] - 0s 874us/step - loss: 1.8385
16/16 [==============================] - 0s 1ms/step - loss: 1.8385
Epoch 54 of 60

Testing for epoch 54 index 1:
32/32 [==============================] - 0s 675us/step
16/16 [==============================] - 0s 837us/step - loss: 0.7743
16/16 [==============================] - 0s 878us/step - loss: 1.8193
16/16 [==============================] - 0s 800us/step - loss: 1.8622
16/16 [==============================] - 0s 791us/step - loss: 1.8635
16/16 [==============================] - 0s 786us/step - loss: 1.8638
16/16 [==============================] - 0s 794us/step - loss: 1.8639
16/16 [==============================] - 0s 798us/step - loss: 1.8640
16/16 [==============================] - 0s 828us/step - loss: 1.8641
16/16 [==============================] - 0s 797us/step - loss: 1.8642
16/16 [==============================] - 0s 812us/step - loss: 1.8642

Testing for epoch 54 index 2:
32/32 [==============================] - 0s 623us/step
16/16 [==============================] - 0s 818us/step - loss: 0.7864
16/16 [==============================] - 0s 822us/step - loss: 1.8117
16/16 [==============================] - 0s 809us/step - loss: 1.8535
16/16 [==============================] - 0s 794us/step - loss: 1.8547
16/16 [==============================] - 0s 811us/step - loss: 1.8550
16/16 [==============================] - 0s 806us/step - loss: 1.8551
16/16 [==============================] - 0s 844us/step - loss: 1.8552
16/16 [==============================] - 0s 794us/step - loss: 1.8553
16/16 [==============================] - 0s 784us/step - loss: 1.8554
16/16 [==============================] - 0s 802us/step - loss: 1.8554
Epoch 55 of 60

Testing for epoch 55 index 1:
32/32 [==============================] - 0s 621us/step
16/16 [==============================] - 0s 873us/step - loss: 0.7953
16/16 [==============================] - 0s 873us/step - loss: 1.8319
16/16 [==============================] - 0s 808us/step - loss: 1.8741
16/16 [==============================] - 0s 859us/step - loss: 1.8754
16/16 [==============================] - 0s 809us/step - loss: 1.8757
16/16 [==============================] - 0s 847us/step - loss: 1.8758
16/16 [==============================] - 0s 827us/step - loss: 1.8758
16/16 [==============================] - 0s 860us/step - loss: 1.8759
16/16 [==============================] - 0s 804us/step - loss: 1.8760
16/16 [==============================] - 0s 876us/step - loss: 1.8760

Testing for epoch 55 index 2:
32/32 [==============================] - 0s 643us/step
16/16 [==============================] - 0s 808us/step - loss: 0.8131
16/16 [==============================] - 0s 893us/step - loss: 1.8454
16/16 [==============================] - 0s 809us/step - loss: 1.8876
16/16 [==============================] - 0s 907us/step - loss: 1.8889
16/16 [==============================] - 0s 873us/step - loss: 1.8891
16/16 [==============================] - 0s 886us/step - loss: 1.8892
16/16 [==============================] - 0s 869us/step - loss: 1.8892
16/16 [==============================] - 0s 876us/step - loss: 1.8892
16/16 [==============================] - 0s 808us/step - loss: 1.8892
16/16 [==============================] - 0s 796us/step - loss: 1.8892
Epoch 56 of 60

Testing for epoch 56 index 1:
32/32 [==============================] - 0s 613us/step
16/16 [==============================] - 0s 694us/step - loss: 0.8154
16/16 [==============================] - 0s 812us/step - loss: 1.8371
16/16 [==============================] - 0s 815us/step - loss: 1.8781
16/16 [==============================] - 0s 796us/step - loss: 1.8793
16/16 [==============================] - 0s 867us/step - loss: 1.8796
16/16 [==============================] - 0s 798us/step - loss: 1.8797
16/16 [==============================] - 0s 791us/step - loss: 1.8798
16/16 [==============================] - 0s 878us/step - loss: 1.8799
16/16 [==============================] - 0s 872us/step - loss: 1.8800
16/16 [==============================] - 0s 799us/step - loss: 1.8800

Testing for epoch 56 index 2:
32/32 [==============================] - 0s 611us/step
16/16 [==============================] - 0s 830us/step - loss: 0.8363
16/16 [==============================] - 0s 883us/step - loss: 1.8623
16/16 [==============================] - 0s 873us/step - loss: 1.9037
16/16 [==============================] - 0s 817us/step - loss: 1.9049
16/16 [==============================] - 0s 837us/step - loss: 1.9052
16/16 [==============================] - 0s 811us/step - loss: 1.9052
16/16 [==============================] - 0s 862us/step - loss: 1.9053
16/16 [==============================] - 0s 859us/step - loss: 1.9054
16/16 [==============================] - 0s 839us/step - loss: 1.9054
16/16 [==============================] - 0s 809us/step - loss: 1.9054
Epoch 57 of 60

Testing for epoch 57 index 1:
32/32 [==============================] - 0s 860us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.8392
16/16 [==============================] - 0s 866us/step - loss: 1.8604
16/16 [==============================] - 0s 854us/step - loss: 1.9012
16/16 [==============================] - 0s 1ms/step - loss: 1.9024
16/16 [==============================] - 0s 1ms/step - loss: 1.9027
16/16 [==============================] - 0s 853us/step - loss: 1.9028
16/16 [==============================] - 0s 892us/step - loss: 1.9029
16/16 [==============================] - 0s 901us/step - loss: 1.9030
16/16 [==============================] - 0s 666us/step - loss: 1.9031
16/16 [==============================] - 0s 2ms/step - loss: 1.9031

Testing for epoch 57 index 2:
32/32 [==============================] - 0s 570us/step
16/16 [==============================] - 0s 875us/step - loss: 0.8581
16/16 [==============================] - 0s 839us/step - loss: 1.8764
16/16 [==============================] - 0s 1ms/step - loss: 1.9169
16/16 [==============================] - 0s 793us/step - loss: 1.9180
16/16 [==============================] - 0s 829us/step - loss: 1.9183
16/16 [==============================] - 0s 852us/step - loss: 1.9184
16/16 [==============================] - 0s 2ms/step - loss: 1.9185
16/16 [==============================] - 0s 786us/step - loss: 1.9186
16/16 [==============================] - 0s 890us/step - loss: 1.9187
16/16 [==============================] - 0s 2ms/step - loss: 1.9187
Epoch 58 of 60

Testing for epoch 58 index 1:
32/32 [==============================] - 0s 549us/step
16/16 [==============================] - 0s 2ms/step - loss: 0.8680
16/16 [==============================] - 0s 922us/step - loss: 1.8985
16/16 [==============================] - 0s 765us/step - loss: 1.9394
16/16 [==============================] - 0s 781us/step - loss: 1.9405
16/16 [==============================] - 0s 820us/step - loss: 1.9408
16/16 [==============================] - 0s 1ms/step - loss: 1.9409
16/16 [==============================] - 0s 781us/step - loss: 1.9410
16/16 [==============================] - 0s 755us/step - loss: 1.9411
16/16 [==============================] - 0s 811us/step - loss: 1.9412
16/16 [==============================] - 0s 2ms/step - loss: 1.9412

Testing for epoch 58 index 2:
32/32 [==============================] - 0s 613us/step
16/16 [==============================] - 0s 863us/step - loss: 0.8825
16/16 [==============================] - 0s 2ms/step - loss: 1.9008
16/16 [==============================] - 0s 799us/step - loss: 1.9412
16/16 [==============================] - 0s 2ms/step - loss: 1.9423
16/16 [==============================] - 0s 2ms/step - loss: 1.9425
16/16 [==============================] - 0s 2ms/step - loss: 1.9426
16/16 [==============================] - 0s 791us/step - loss: 1.9427
16/16 [==============================] - 0s 851us/step - loss: 1.9427
16/16 [==============================] - 0s 2ms/step - loss: 1.9427
16/16 [==============================] - 0s 2ms/step - loss: 1.9427
Epoch 59 of 60

Testing for epoch 59 index 1:
32/32 [==============================] - 0s 577us/step
16/16 [==============================] - 0s 818us/step - loss: 0.8879
16/16 [==============================] - 0s 803us/step - loss: 1.9056
16/16 [==============================] - 0s 2ms/step - loss: 1.9456
16/16 [==============================] - 0s 1ms/step - loss: 1.9468
16/16 [==============================] - 0s 962us/step - loss: 1.9470
16/16 [==============================] - 0s 794us/step - loss: 1.9470
16/16 [==============================] - 0s 800us/step - loss: 1.9471
16/16 [==============================] - 0s 2ms/step - loss: 1.9471
16/16 [==============================] - 0s 2ms/step - loss: 1.9471
16/16 [==============================] - 0s 1ms/step - loss: 1.9471

Testing for epoch 59 index 2:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 830us/step - loss: 0.9059
16/16 [==============================] - 0s 1ms/step - loss: 1.9160
16/16 [==============================] - 0s 838us/step - loss: 1.9552
16/16 [==============================] - 0s 788us/step - loss: 1.9563
16/16 [==============================] - 0s 2ms/step - loss: 1.9566
16/16 [==============================] - 0s 761us/step - loss: 1.9567
16/16 [==============================] - 0s 808us/step - loss: 1.9569
16/16 [==============================] - 0s 2ms/step - loss: 1.9571
16/16 [==============================] - 0s 2ms/step - loss: 1.9571
16/16 [==============================] - 0s 1ms/step - loss: 1.9571
Epoch 60 of 60

Testing for epoch 60 index 1:
32/32 [==============================] - 0s 561us/step
16/16 [==============================] - 0s 2ms/step - loss: 0.9083
16/16 [==============================] - 0s 2ms/step - loss: 1.9140
16/16 [==============================] - 0s 800us/step - loss: 1.9538
16/16 [==============================] - 0s 793us/step - loss: 1.9549
16/16 [==============================] - 0s 877us/step - loss: 1.9551
16/16 [==============================] - 0s 783us/step - loss: 1.9551
16/16 [==============================] - 0s 1ms/step - loss: 1.9551
16/16 [==============================] - 0s 1ms/step - loss: 1.9552
16/16 [==============================] - 0s 884us/step - loss: 1.9552
16/16 [==============================] - 0s 2ms/step - loss: 1.9552

Testing for epoch 60 index 2:
32/32 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.9274
16/16 [==============================] - 0s 2ms/step - loss: 1.9310
16/16 [==============================] - 0s 2ms/step - loss: 1.9710
16/16 [==============================] - 0s 775us/step - loss: 1.9721
16/16 [==============================] - 0s 960us/step - loss: 1.9723
16/16 [==============================] - 0s 2ms/step - loss: 1.9723
16/16 [==============================] - 0s 794us/step - loss: 1.9722
16/16 [==============================] - 0s 1ms/step - loss: 1.9721
16/16 [==============================] - 0s 2ms/step - loss: 1.9721
16/16 [==============================] - 0s 2ms/step - loss: 1.9721
32/32 [==============================] - 0s 1ms/step</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1726">
<div class="sourceCode cell-code" id="cb317"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb317-1"><a href="#cb317-1" aria-hidden="true" tabindex="-1"></a>outlier_MO_GAAL_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1727">
<div class="sourceCode cell-code" id="cb318"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb318-1"><a href="#cb318-1" aria-hidden="true" tabindex="-1"></a>outlier_MO_GAAL_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_MO_GAAL_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1728">
<div class="sourceCode cell-code" id="cb319"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb319-1"><a href="#cb319-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one,outlier_MO_GAAL_one,tab_orbit)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1729">
<div class="sourceCode cell-code" id="cb320"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb320-1"><a href="#cb320-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"MO-GAAL (Liu et al., 2019)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-226-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.950
Precision: 0.950
Recall: 1.000
F1 Score: 0.974</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1731">
<div class="sourceCode cell-code" id="cb323"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb323-1"><a href="#cb323-1" aria-hidden="true" tabindex="-1"></a>thirteen <span class="op">=</span> twelve.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  thirteen = twelve.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="lscpstar-1" class="level3">
<h3 class="anchored" data-anchor-id="lscpstar-1">LSCP<span class="math inline">\(\star\)</span></h3>
<div class="cell" data-execution_count="1732">
<div class="sourceCode cell-code" id="cb325"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb325-1"><a href="#cb325-1" aria-hidden="true" tabindex="-1"></a>detectors <span class="op">=</span> [KNN(), LOF(), OCSVM()]</span>
<span id="cb325-2"><a href="#cb325-2" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LSCP(detectors,contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb325-3"><a href="#cb325-3" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'f'</span>]])</span>
<span id="cb325-4"><a href="#cb325-4" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'LSCP_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/pyod/models/lscp.py:382: UserWarning: The number of histogram bins is greater than the number of classifiers, reducing n_bins to n_clf.
  warnings.warn(</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1733">
<div class="sourceCode cell-code" id="cb327"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb327-1"><a href="#cb327-1" aria-hidden="true" tabindex="-1"></a>outlier_LSCP_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1734">
<div class="sourceCode cell-code" id="cb328"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb328-1"><a href="#cb328-1" aria-hidden="true" tabindex="-1"></a>outlier_LSCP_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_LSCP_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1735">
<div class="sourceCode cell-code" id="cb329"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb329-1"><a href="#cb329-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one,outlier_LSCP_one,tab_orbit)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1736">
<div class="sourceCode cell-code" id="cb330"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb330-1"><a href="#cb330-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"LSCP (Zhao et al., 2019)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-232-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.988
Precision: 0.994
Recall: 0.994
F1 Score: 0.994</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1737">
<div class="sourceCode cell-code" id="cb333"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb333-1"><a href="#cb333-1" aria-hidden="true" tabindex="-1"></a>fourteen <span class="op">=</span> thirteen.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  fourteen = thirteen.append(_conf.tab)</code></pre>
</div>
</div>
</section>
</section>
<section id="orbit-result" class="level2">
<h2 class="anchored" data-anchor-id="orbit-result">Orbit Result</h2>
<div class="cell" data-execution_count="1738">
<div class="sourceCode cell-code" id="cb335"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb335-1"><a href="#cb335-1" aria-hidden="true" tabindex="-1"></a><span class="bu">round</span>(fourteen,<span class="dv">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1738">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Accuracy</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>GODE</th>
      <td>0.998</td>
      <td>0.999</td>
      <td>0.999</td>
      <td>0.999</td>
    </tr>
    <tr>
      <th>LOF (Breunig et al., 2000)</th>
      <td>0.954</td>
      <td>0.976</td>
      <td>0.976</td>
      <td>0.976</td>
    </tr>
    <tr>
      <th>kNN (Ramaswamy et al., 2000)</th>
      <td>0.948</td>
      <td>0.999</td>
      <td>0.946</td>
      <td>0.972</td>
    </tr>
    <tr>
      <th>OCSVM (Sch ̈olkopf et al., 2001)</th>
      <td>0.908</td>
      <td>0.977</td>
      <td>0.925</td>
      <td>0.950</td>
    </tr>
    <tr>
      <th>MCD (Hardin and Rocke, 2004)</th>
      <td>0.916</td>
      <td>0.956</td>
      <td>0.956</td>
      <td>0.956</td>
    </tr>
    <tr>
      <th>Feature Bagging (Lazarevic and Kumar, 2005)</th>
      <td>0.942</td>
      <td>0.969</td>
      <td>0.969</td>
      <td>0.969</td>
    </tr>
    <tr>
      <th>ABOD (Kriegel et al., 2008)</th>
      <td>0.988</td>
      <td>0.994</td>
      <td>0.994</td>
      <td>0.994</td>
    </tr>
    <tr>
      <th>Isolation Forest (Liu et al., 2008)</th>
      <td>0.443</td>
      <td>0.992</td>
      <td>0.417</td>
      <td>0.587</td>
    </tr>
    <tr>
      <th>HBOS (Goldstein and Dengel, 2012)</th>
      <td>0.935</td>
      <td>0.960</td>
      <td>0.973</td>
      <td>0.966</td>
    </tr>
    <tr>
      <th>SOS (Janssens et al., 2012)</th>
      <td>0.950</td>
      <td>0.974</td>
      <td>0.974</td>
      <td>0.974</td>
    </tr>
    <tr>
      <th>SO-GAAL (Liu et al., 2019)</th>
      <td>0.950</td>
      <td>0.950</td>
      <td>1.000</td>
      <td>0.974</td>
    </tr>
    <tr>
      <th>MO-GAAL (Liu et al., 2019)</th>
      <td>0.950</td>
      <td>0.950</td>
      <td>1.000</td>
      <td>0.974</td>
    </tr>
    <tr>
      <th>LSCP (Zhao et al., 2019)</th>
      <td>0.988</td>
      <td>0.994</td>
      <td>0.994</td>
      <td>0.994</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">Orbit</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">LOF (Breunig et al., 2000)</td>
<td style="text-align: center;">0.954</td>
<td style="text-align: center;">0.976</td>
<td style="text-align: center;">0.976</td>
<td style="text-align: center;">0.976</td>
</tr>
<tr class="even">
<td style="text-align: center;">KNN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">CBLOF</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">OCSVM (Sch ̈olkopf et al., 2001)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">MCD (Hardin and Rocke, 2004)</td>
<td style="text-align: center;">0.916</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.956</td>
</tr>
<tr class="even">
<td style="text-align: center;">Feature Bagging (Lazarevic and Kumar, 2005)</td>
<td style="text-align: center;">0.942</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">0.969</td>
</tr>
<tr class="odd">
<td style="text-align: center;">ABOD (Kriegel et al., 2008)</td>
<td style="text-align: center;">0.988</td>
<td style="text-align: center;">0.994</td>
<td style="text-align: center;">0.994</td>
<td style="text-align: center;">0.994</td>
</tr>
<tr class="even">
<td style="text-align: center;">Isolation Forest (Liu et al., 2008)</td>
<td style="text-align: center;">0.443</td>
<td style="text-align: center;">0.992</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.587</td>
</tr>
<tr class="odd">
<td style="text-align: center;">HBOS (Goldstein and Dengel, 2012)</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.960</td>
<td style="text-align: center;">0.973</td>
<td style="text-align: center;">0.966</td>
</tr>
<tr class="even">
<td style="text-align: center;">SOS (Janssens et al., 2012)</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.974</td>
</tr>
<tr class="odd">
<td style="text-align: center;">SO-GAAL (Liu et al., 2019)</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.974</td>
</tr>
<tr class="even">
<td style="text-align: center;">MO-GAAL (Liu et al., 2019)</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.974</td>
</tr>
<tr class="odd">
<td style="text-align: center;">LSCP (Zhao et al., 2019)</td>
<td style="text-align: center;">0.988</td>
<td style="text-align: center;">0.994</td>
<td style="text-align: center;">0.994</td>
<td style="text-align: center;">0.994</td>
</tr>
</tbody>
</table>
</section>
<section id="bunny" class="level2">
<h2 class="anchored" data-anchor-id="bunny">Bunny</h2>
<hr>
<section id="bunny-저장용" class="level3">
<h3 class="anchored" data-anchor-id="bunny-저장용">bunny 저장용</h3>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb336"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb336-1"><a href="#cb336-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pygsp <span class="im">import</span> graphs, filters, plotting, utils</span></code></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb337"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb337-1"><a href="#cb337-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> save_data(data_dict,fname):</span>
<span id="cb337-2"><a href="#cb337-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(fname,<span class="st">'wb'</span>) <span class="im">as</span> outfile:</span>
<span id="cb337-3"><a href="#cb337-3" aria-hidden="true" tabindex="-1"></a>        pickle.dump(data_dict,outfile)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb338"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb338-1"><a href="#cb338-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb339"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb339-1"><a href="#cb339-1" aria-hidden="true" tabindex="-1"></a>G <span class="op">=</span> graphs.Bunny()</span>
<span id="cb339-2"><a href="#cb339-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> G.N</span></code></pre></div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb340"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb340-1"><a href="#cb340-1" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> filters.Heat(G, tau<span class="op">=</span><span class="dv">75</span>) </span></code></pre></div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb341"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb341-1"><a href="#cb341-1" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">2503</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb342"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb342-1"><a href="#cb342-1" aria-hidden="true" tabindex="-1"></a>normal <span class="op">=</span> np.random.randn(n)</span>
<span id="cb342-2"><a href="#cb342-2" aria-hidden="true" tabindex="-1"></a>unif <span class="op">=</span> np.concatenate([np.random.uniform(low<span class="op">=</span><span class="dv">3</span>,high<span class="op">=</span><span class="dv">7</span>,size<span class="op">=</span><span class="dv">60</span>), np.random.uniform(low<span class="op">=-</span><span class="dv">7</span>,high<span class="op">=-</span><span class="dv">3</span>,size<span class="op">=</span><span class="dv">60</span>),np.zeros(n<span class="op">-</span><span class="dv">120</span>)])<span class="op">;</span> np.random.shuffle(unif)</span>
<span id="cb342-3"><a href="#cb342-3" aria-hidden="true" tabindex="-1"></a>noise <span class="op">=</span> normal <span class="op">+</span> unif</span>
<span id="cb342-4"><a href="#cb342-4" aria-hidden="true" tabindex="-1"></a>index_of_trueoutlier2 <span class="op">=</span> np.where(unif<span class="op">!=</span><span class="dv">0</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb343"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb343-1"><a href="#cb343-1" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> np.zeros(n)</span>
<span id="cb343-2"><a href="#cb343-2" aria-hidden="true" tabindex="-1"></a>f[<span class="dv">1000</span>] <span class="op">=</span> <span class="op">-</span><span class="dv">3234</span></span>
<span id="cb343-3"><a href="#cb343-3" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> g.<span class="bu">filter</span>(f, method<span class="op">=</span><span class="st">'chebyshev'</span>) </span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2023-07-04 17:37:32,017:[WARNING](pygsp.graphs.graph.lmax): The largest eigenvalue G.lmax is not available, we need to estimate it. Explicitly call G.estimate_lmax() or G.compute_fourier_basis() once beforehand to suppress the warning.</code></pre>
</div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb345"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb345-1"><a href="#cb345-1" aria-hidden="true" tabindex="-1"></a>G.coords.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>(2503, 3)</code></pre>
</div>
</div>
<div class="cell" data-tags="[]" data-execution_count="10">
<div class="sourceCode cell-code" id="cb347"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb347-1"><a href="#cb347-1" aria-hidden="true" tabindex="-1"></a>_W <span class="op">=</span> G.W.toarray()</span>
<span id="cb347-2"><a href="#cb347-2" aria-hidden="true" tabindex="-1"></a>_x <span class="op">=</span> G.coords[:,<span class="dv">0</span>]</span>
<span id="cb347-3"><a href="#cb347-3" aria-hidden="true" tabindex="-1"></a>_y <span class="op">=</span> G.coords[:,<span class="dv">1</span>]</span>
<span id="cb347-4"><a href="#cb347-4" aria-hidden="true" tabindex="-1"></a>_z <span class="op">=</span> <span class="op">-</span>G.coords[:,<span class="dv">2</span>]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb348"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb348-1"><a href="#cb348-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span></code></pre></div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb349"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb349-1"><a href="#cb349-1" aria-hidden="true" tabindex="-1"></a>_df <span class="op">=</span> pd.DataFrame({<span class="st">'x'</span>:_x,<span class="st">'y'</span>:_y,<span class="st">'z'</span>:_z})</span></code></pre></div>
</div>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb350"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb350-1"><a href="#cb350-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span></code></pre></div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb351"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb351-1"><a href="#cb351-1" aria-hidden="true" tabindex="-1"></a>_df <span class="op">=</span> {<span class="st">'W'</span>:_W,<span class="st">'x'</span>:_x,<span class="st">'y'</span>:_y,<span class="st">'z'</span>:_z, <span class="st">'fnoise'</span>:f<span class="op">+</span>noise,<span class="st">'f'</span> : f, <span class="st">'noise'</span>: noise,<span class="st">'unif'</span>:unif,<span class="st">'index_of_trueoutlier2'</span>:index_of_trueoutlier2}</span></code></pre></div>
</div>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb352"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb352-1"><a href="#cb352-1" aria-hidden="true" tabindex="-1"></a>save_data(_df,<span class="st">'Bunny.pkl'</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb353"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb353-1"><a href="#cb353-1" aria-hidden="true" tabindex="-1"></a>_df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>{'W': array([[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]]),
 'x': array([ 0.26815193, -0.58456893, -0.02730755, ...,  0.15397547,
        -0.45056488, -0.29405249]),
 'y': array([ 0.39314334,  0.63468595,  0.33280949, ...,  0.80205526,
         0.6207154 , -0.40187451]),
 'z': array([-0.13834514, -0.22438843,  0.08658215, ...,  0.33698514,
         0.58353051, -0.08647485]),
 'fnoise': array([-1.63569131,  0.49423926, -1.04026277, ..., -1.0694093 ,
        -0.24395499,  0.41729667]),
 'f': array([-1.54422488, -0.03596483, -0.93972715, ..., -0.01924028,
        -0.02470869, -0.26266752]),
 'noise': array([-0.09146643,  0.53020409, -0.10053563, ..., -1.05016902,
        -0.2192463 ,  0.67996419]),
 'unif': array([0., 0., 0., ..., 0., 0., 0.]),
 'index_of_trueoutlier2': (array([  15,   33,   34,   36,   45,   52,   61,  153,  227,  228,  235,
          240,  249,  267,  270,  273,  291,  313,  333,  353,  375,  389,
          397,  402,  439,  440,  447,  449,  456,  457,  472,  509,  564,
          569,  589,  638,  700,  713,  714,  732,  749,  814,  836,  851,
          858,  888,  910,  927,  934,  948,  953,  972,  986, 1002, 1041,
         1073, 1087, 1090, 1139, 1182, 1227, 1270, 1276, 1344, 1347, 1459,
         1461, 1467, 1499, 1500, 1512, 1515, 1544, 1562, 1610, 1637, 1640,
         1649, 1665, 1695, 1699, 1737, 1740, 1783, 1788, 1808, 1857, 1868,
         1882, 1928, 1941, 1954, 1973, 2014, 2017, 2020, 2065, 2108, 2115,
         2135, 2153, 2191, 2198, 2210, 2219, 2241, 2274, 2278, 2283, 2292,
         2314, 2328, 2340, 2341, 2357, 2387, 2399, 2477, 2485, 2487]),)}</code></pre>
</div>
</div>
<hr>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb355"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb355-1"><a href="#cb355-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_data(fname):</span>
<span id="cb355-2"><a href="#cb355-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(fname, <span class="st">'rb'</span>) <span class="im">as</span> outfile:</span>
<span id="cb355-3"><a href="#cb355-3" aria-hidden="true" tabindex="-1"></a>        data_dict <span class="op">=</span> pickle.load(outfile)</span>
<span id="cb355-4"><a href="#cb355-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> data_dict</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1911">
<div class="sourceCode cell-code" id="cb356"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb356-1"><a href="#cb356-1" aria-hidden="true" tabindex="-1"></a>_df1 <span class="op">=</span> load_data(<span class="st">'Bunny.pkl'</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1912">
<div class="sourceCode cell-code" id="cb357"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb357-1"><a href="#cb357-1" aria-hidden="true" tabindex="-1"></a>_df <span class="op">=</span> pd.DataFrame({<span class="st">'x'</span>: _df1[<span class="st">'x'</span>],<span class="st">'y'</span>:_df1[<span class="st">'y'</span>],<span class="st">'z'</span>:_df1[<span class="st">'z'</span>],<span class="st">'fnoise'</span>:_df1[<span class="st">'fnoise'</span>],<span class="st">'f'</span>:_df1[<span class="st">'f'</span>],<span class="st">'noise'</span>:_df1[<span class="st">'noise'</span>]})</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1913">
<div class="sourceCode cell-code" id="cb358"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb358-1"><a href="#cb358-1" aria-hidden="true" tabindex="-1"></a>unif <span class="op">=</span> _df1[<span class="st">'unif'</span>]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1914">
<div class="sourceCode cell-code" id="cb359"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb359-1"><a href="#cb359-1" aria-hidden="true" tabindex="-1"></a>_df1[<span class="st">'index_of_trueoutlier2'</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1914">
<pre><code>(array([  15,   33,   34,   36,   45,   52,   61,  153,  227,  228,  235,
         240,  249,  267,  270,  273,  291,  313,  333,  353,  375,  389,
         397,  402,  439,  440,  447,  449,  456,  457,  472,  509,  564,
         569,  589,  638,  700,  713,  714,  732,  749,  814,  836,  851,
         858,  888,  910,  927,  934,  948,  953,  972,  986, 1002, 1041,
        1073, 1087, 1090, 1139, 1182, 1227, 1270, 1276, 1344, 1347, 1459,
        1461, 1467, 1499, 1500, 1512, 1515, 1544, 1562, 1610, 1637, 1640,
        1649, 1665, 1695, 1699, 1737, 1740, 1783, 1788, 1808, 1857, 1868,
        1882, 1928, 1941, 1954, 1973, 2014, 2017, 2020, 2065, 2108, 2115,
        2135, 2153, 2191, 2198, 2210, 2219, 2241, 2274, 2278, 2283, 2292,
        2314, 2328, 2340, 2341, 2357, 2387, 2399, 2477, 2485, 2487]),)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1915">
<div class="sourceCode cell-code" id="cb361"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb361-1"><a href="#cb361-1" aria-hidden="true" tabindex="-1"></a><span class="co"># _df = pd.DataFrame({'x' : _x, 'y' : _y, 'z' : _z, 'fnoise':f+noise,'f' : f, 'noise': noise})</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="1916">
<div class="sourceCode cell-code" id="cb362"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb362-1"><a href="#cb362-1" aria-hidden="true" tabindex="-1"></a>outlier_true_one_2 <span class="op">=</span> unif.copy()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1917">
<div class="sourceCode cell-code" id="cb363"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb363-1"><a href="#cb363-1" aria-hidden="true" tabindex="-1"></a>outlier_true_one_2 <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="op">-</span><span class="dv">1</span> <span class="cf">if</span> x <span class="op">!=</span><span class="dv">0</span>  <span class="cf">else</span> <span class="dv">1</span>,outlier_true_one_2))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1920">
<div class="sourceCode cell-code" id="cb364"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb364-1"><a href="#cb364-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pd.DataFrame(outlier_true_one_2).to_csv('bunny_outlier.csv')</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="1748">
<div class="sourceCode cell-code" id="cb365"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb365-1"><a href="#cb365-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array(_df)[:,:<span class="dv">4</span>]</span></code></pre></div>
</div>
</section>
<section id="gode-2" class="level3">
<h3 class="anchored" data-anchor-id="gode-2">GODE</h3>
<div class="cell" data-execution_count="1749">
<div class="sourceCode cell-code" id="cb366"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb366-1"><a href="#cb366-1" aria-hidden="true" tabindex="-1"></a>_W <span class="op">=</span> _df1[<span class="st">'W'</span>]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1750">
<div class="sourceCode cell-code" id="cb367"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb367-1"><a href="#cb367-1" aria-hidden="true" tabindex="-1"></a>_BUNNY <span class="op">=</span> BUNNY(_df)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1751">
<div class="sourceCode cell-code" id="cb368"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb368-1"><a href="#cb368-1" aria-hidden="true" tabindex="-1"></a>_BUNNY.fit(sd<span class="op">=</span><span class="dv">20</span>,ref<span class="op">=</span><span class="dv">10</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1752">
<div class="sourceCode cell-code" id="cb369"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb369-1"><a href="#cb369-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(_BUNNY.f)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1752">
<pre><code>2503</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1753">
<div class="sourceCode cell-code" id="cb371"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb371-1"><a href="#cb371-1" aria-hidden="true" tabindex="-1"></a><span class="dv">2503</span><span class="op">*</span><span class="fl">0.05</span></span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1753">
<pre><code>125.15</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1754">
<div class="sourceCode cell-code" id="cb373"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb373-1"><a href="#cb373-1" aria-hidden="true" tabindex="-1"></a>outlier_simul_one <span class="op">=</span> (_BUNNY.df[<span class="st">'Residual'</span>]<span class="op">**</span><span class="dv">2</span>).tolist()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1755">
<div class="sourceCode cell-code" id="cb374"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb374-1"><a href="#cb374-1" aria-hidden="true" tabindex="-1"></a><span class="co"># outlier_simul_one = list(map(lambda x: -1 if x &gt; 8.7 else 1,outlier_simul_one))</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="1756">
<div class="sourceCode cell-code" id="cb375"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb375-1"><a href="#cb375-1" aria-hidden="true" tabindex="-1"></a>outlier_simul_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="op">-</span><span class="dv">1</span> <span class="cf">if</span> x <span class="op">&gt;</span> <span class="fl">8.05</span> <span class="cf">else</span> <span class="dv">1</span>,outlier_simul_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1757">
<div class="sourceCode cell-code" id="cb376"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb376-1"><a href="#cb376-1" aria-hidden="true" tabindex="-1"></a>outlier_simul_one.count(<span class="dv">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1757">
<pre><code>2378</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1758">
<div class="sourceCode cell-code" id="cb378"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb378-1"><a href="#cb378-1" aria-hidden="true" tabindex="-1"></a>outlier_simul_one.count(<span class="op">-</span><span class="dv">1</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1758">
<pre><code>125</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1759">
<div class="sourceCode cell-code" id="cb380"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb380-1"><a href="#cb380-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_2,outlier_simul_one,tab_bunny)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1760">
<div class="sourceCode cell-code" id="cb381"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb381-1"><a href="#cb381-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"GODE"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-272-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.988
Precision: 0.995
Recall: 0.993
F1 Score: 0.994</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1761">
<div class="sourceCode cell-code" id="cb384"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb384-1"><a href="#cb384-1" aria-hidden="true" tabindex="-1"></a>one <span class="op">=</span> _conf.tab</span></code></pre></div>
</div>
</section>
<section id="lof" class="level3">
<h3 class="anchored" data-anchor-id="lof">LOF</h3>
<div class="cell" data-execution_count="1762">
<div class="sourceCode cell-code" id="cb385"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb385-1"><a href="#cb385-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LocalOutlierFactor(n_neighbors<span class="op">=</span><span class="dv">2</span>,contamination<span class="op">=</span><span class="fl">0.05</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1763">
<div class="sourceCode cell-code" id="cb386"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb386-1"><a href="#cb386-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_2,clf.fit_predict(X),tab_bunny)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1764">
<div class="sourceCode cell-code" id="cb387"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb387-1"><a href="#cb387-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"LOF (Breunig et al., 2000)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-276-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.913
Precision: 0.955
Recall: 0.953
F1 Score: 0.954</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1766">
<div class="sourceCode cell-code" id="cb390"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb390-1"><a href="#cb390-1" aria-hidden="true" tabindex="-1"></a>two <span class="op">=</span> one.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  two = one.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="knn-2" class="level3">
<h3 class="anchored" data-anchor-id="knn-2">KNN</h3>
<div class="cell" data-tags="[]" data-execution_count="1767">
<div class="sourceCode cell-code" id="cb392"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb392-1"><a href="#cb392-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> KNN()</span>
<span id="cb392-2"><a href="#cb392-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'fnoise'</span>]])</span>
<span id="cb392-3"><a href="#cb392-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'knn_Clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1768">
<div class="sourceCode cell-code" id="cb393"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb393-1"><a href="#cb393-1" aria-hidden="true" tabindex="-1"></a>outlier_KNN_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1769">
<div class="sourceCode cell-code" id="cb394"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb394-1"><a href="#cb394-1" aria-hidden="true" tabindex="-1"></a>outlier_KNN_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_KNN_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1770">
<div class="sourceCode cell-code" id="cb395"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb395-1"><a href="#cb395-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_2,outlier_KNN_one,tab_bunny)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1771">
<div class="sourceCode cell-code" id="cb396"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb396-1"><a href="#cb396-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"kNN (Ramaswamy et al., 2000)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-282-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.942
Precision: 0.997
Recall: 0.942
F1 Score: 0.969</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1772">
<div class="sourceCode cell-code" id="cb399"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb399-1"><a href="#cb399-1" aria-hidden="true" tabindex="-1"></a>three <span class="op">=</span> two.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  three = two.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="cblof-1" class="level3">
<h3 class="anchored" data-anchor-id="cblof-1">CBLOF</h3>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb401"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb401-1"><a href="#cb401-1" aria-hidden="true" tabindex="-1"></a>_df1 <span class="op">=</span> load_data(<span class="st">'Bunny.pkl'</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="86">
<div class="sourceCode cell-code" id="cb402"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb402-1"><a href="#cb402-1" aria-hidden="true" tabindex="-1"></a>outlier_true_one_2 <span class="op">=</span> pd.read_csv(<span class="st">'bunny_outlier.csv'</span>).iloc[:,<span class="dv">1</span>].to_list()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb403"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb403-1"><a href="#cb403-1" aria-hidden="true" tabindex="-1"></a>_df <span class="op">=</span> pd.DataFrame({<span class="st">'x'</span>: _df1[<span class="st">'x'</span>],<span class="st">'y'</span>:_df1[<span class="st">'y'</span>],<span class="st">'z'</span>:_df1[<span class="st">'z'</span>],<span class="st">'fnoise'</span>:_df1[<span class="st">'fnoise'</span>],<span class="st">'f'</span>:_df1[<span class="st">'f'</span>],<span class="st">'noise'</span>:_df1[<span class="st">'noise'</span>]})</span></code></pre></div>
</div>
<div class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb404"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb404-1"><a href="#cb404-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> CBLOF(contamination<span class="op">=</span><span class="fl">0.05</span>,check_estimator<span class="op">=</span><span class="va">False</span>, random_state<span class="op">=</span><span class="dv">77</span>)</span>
<span id="cb404-2"><a href="#cb404-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'fnoise'</span>]])</span>
<span id="cb404-3"><a href="#cb404-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'CBLOF_Clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/csy/anaconda3/envs/pygsp/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb406"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb406-1"><a href="#cb406-1" aria-hidden="true" tabindex="-1"></a>outlier_CBLOF_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb407"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb407-1"><a href="#cb407-1" aria-hidden="true" tabindex="-1"></a>outlier_CBLOF_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_CBLOF_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="87">
<div class="sourceCode cell-code" id="cb408"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb408-1"><a href="#cb408-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_2,outlier_CBLOF_one,tab_bunny)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="88">
<div class="sourceCode cell-code" id="cb409"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb409-1"><a href="#cb409-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"CBLOF (He et al., 2003)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-291-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.974
Precision: 0.988
Recall: 0.985
F1 Score: 0.987</code></pre>
</div>
<div class="cell-output cell-output-error">
<pre><code>AttributeError: 'DataFrame' object has no attribute 'append'</code></pre>
</div>
</div>
<div class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb412"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb412-1"><a href="#cb412-1" aria-hidden="true" tabindex="-1"></a><span class="co"># four = three.append(_conf.tab)</span></span></code></pre></div>
</div>
<ul>
<li>Accuracy: 0.974</li>
<li>Precision: 0.988</li>
<li>Recall: 0.985</li>
<li>F1 Score: 0.987</li>
</ul>
</section>
<section id="ocsvm-2" class="level3">
<h3 class="anchored" data-anchor-id="ocsvm-2">OCSVM</h3>
<div class="cell" data-execution_count="1774">
<div class="sourceCode cell-code" id="cb413"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb413-1"><a href="#cb413-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> svm.OneClassSVM(nu<span class="op">=</span><span class="fl">0.1</span>, kernel<span class="op">=</span><span class="st">"rbf"</span>, gamma<span class="op">=</span><span class="fl">0.1</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1775">
<div class="sourceCode cell-code" id="cb414"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb414-1"><a href="#cb414-1" aria-hidden="true" tabindex="-1"></a>clf.fit(X)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1775">
<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-9" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>OneClassSVM(gamma=0.1, nu=0.1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-9" type="checkbox" checked=""><label for="sk-estimator-id-9" class="sk-toggleable__label sk-toggleable__label-arrow">OneClassSVM</label><div class="sk-toggleable__content"><pre>OneClassSVM(gamma=0.1, nu=0.1)</pre></div></div></div></div></div>
</div>
</div>
<div class="cell" data-execution_count="1776">
<div class="sourceCode cell-code" id="cb415"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb415-1"><a href="#cb415-1" aria-hidden="true" tabindex="-1"></a>outlier_OSVM_one <span class="op">=</span> <span class="bu">list</span>(clf.predict(X))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1777">
<div class="sourceCode cell-code" id="cb416"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb416-1"><a href="#cb416-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_2,outlier_OSVM_one,tab_bunny)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1778">
<div class="sourceCode cell-code" id="cb417"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb417-1"><a href="#cb417-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"OCSVM (Sch ̈olkopf et al., 2001)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-297-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.935
Precision: 0.992
Recall: 0.939
F1 Score: 0.965</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1779">
<div class="sourceCode cell-code" id="cb420"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb420-1"><a href="#cb420-1" aria-hidden="true" tabindex="-1"></a>five <span class="op">=</span> three.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  five = three.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="mcd" class="level3">
<h3 class="anchored" data-anchor-id="mcd">MCD</h3>
<div class="cell" data-scrolled="true" data-tags="[]" data-execution_count="1791">
<div class="sourceCode cell-code" id="cb422"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb422-1"><a href="#cb422-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> MCD(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb422-2"><a href="#cb422-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'fnoise'</span>]])</span>
<span id="cb422-3"><a href="#cb422-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'MCD_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1792">
<div class="sourceCode cell-code" id="cb423"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb423-1"><a href="#cb423-1" aria-hidden="true" tabindex="-1"></a>outlier_MCD_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1793">
<div class="sourceCode cell-code" id="cb424"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb424-1"><a href="#cb424-1" aria-hidden="true" tabindex="-1"></a>outlier_MCD_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_MCD_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1794">
<div class="sourceCode cell-code" id="cb425"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb425-1"><a href="#cb425-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_2,outlier_MCD_one,tab_bunny)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1795">
<div class="sourceCode cell-code" id="cb426"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb426-1"><a href="#cb426-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"MCD (Hardin and Rocke, 2004)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-303-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.982
Precision: 0.992
Recall: 0.989
F1 Score: 0.990</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1796">
<div class="sourceCode cell-code" id="cb429"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb429-1"><a href="#cb429-1" aria-hidden="true" tabindex="-1"></a>six <span class="op">=</span> five.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  six = five.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="feature-bagging" class="level3">
<h3 class="anchored" data-anchor-id="feature-bagging">Feature Bagging</h3>
<div class="cell" data-scrolled="true" data-tags="[]" data-execution_count="1797">
<div class="sourceCode cell-code" id="cb431"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb431-1"><a href="#cb431-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> FeatureBagging(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb431-2"><a href="#cb431-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'fnoise'</span>]])</span>
<span id="cb431-3"><a href="#cb431-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'FeatureBagging_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1798">
<div class="sourceCode cell-code" id="cb432"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb432-1"><a href="#cb432-1" aria-hidden="true" tabindex="-1"></a>outlier_FeatureBagging_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1799">
<div class="sourceCode cell-code" id="cb433"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb433-1"><a href="#cb433-1" aria-hidden="true" tabindex="-1"></a>outlier_FeatureBagging_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_FeatureBagging_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1800">
<div class="sourceCode cell-code" id="cb434"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb434-1"><a href="#cb434-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_2,outlier_FeatureBagging_one,tab_bunny)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1801">
<div class="sourceCode cell-code" id="cb435"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb435-1"><a href="#cb435-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"Feature Bagging (Lazarevic and Kumar, 2005)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-309-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.954
Precision: 0.977
Recall: 0.974
F1 Score: 0.976</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1802">
<div class="sourceCode cell-code" id="cb438"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb438-1"><a href="#cb438-1" aria-hidden="true" tabindex="-1"></a>seven <span class="op">=</span> six.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  seven = six.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="abod" class="level3">
<h3 class="anchored" data-anchor-id="abod">ABOD</h3>
<div class="cell" data-execution_count="1803">
<div class="sourceCode cell-code" id="cb440"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb440-1"><a href="#cb440-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> ABOD(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb440-2"><a href="#cb440-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'fnoise'</span>]])</span>
<span id="cb440-3"><a href="#cb440-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'ABOD_Clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1804">
<div class="sourceCode cell-code" id="cb441"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb441-1"><a href="#cb441-1" aria-hidden="true" tabindex="-1"></a>outlier_ABOD_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1805">
<div class="sourceCode cell-code" id="cb442"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb442-1"><a href="#cb442-1" aria-hidden="true" tabindex="-1"></a>outlier_ABOD_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_ABOD_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1806">
<div class="sourceCode cell-code" id="cb443"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb443-1"><a href="#cb443-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_2,outlier_ABOD_one,tab_bunny)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1807">
<div class="sourceCode cell-code" id="cb444"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb444-1"><a href="#cb444-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"ABOD (Kriegel et al., 2008)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-315-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.979
Precision: 0.990
Recall: 0.988
F1 Score: 0.989</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1809">
<div class="sourceCode cell-code" id="cb447"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb447-1"><a href="#cb447-1" aria-hidden="true" tabindex="-1"></a>eight <span class="op">=</span> seven.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  eight = seven.append(_conf.tab)</code></pre>
</div>
</div>
<p>normal fix 안 해줘서 좀 다른듯</p>
</section>
<section id="iforest" class="level3">
<h3 class="anchored" data-anchor-id="iforest">IForest</h3>
<div class="cell" data-execution_count="1810">
<div class="sourceCode cell-code" id="cb449"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb449-1"><a href="#cb449-1" aria-hidden="true" tabindex="-1"></a>od <span class="op">=</span> IForest(</span>
<span id="cb449-2"><a href="#cb449-2" aria-hidden="true" tabindex="-1"></a>    threshold<span class="op">=</span><span class="fl">0.</span>,</span>
<span id="cb449-3"><a href="#cb449-3" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">125</span></span>
<span id="cb449-4"><a href="#cb449-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1811">
<div class="sourceCode cell-code" id="cb450"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb450-1"><a href="#cb450-1" aria-hidden="true" tabindex="-1"></a>od.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'fnoise'</span>]])</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1812">
<div class="sourceCode cell-code" id="cb451"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb451-1"><a href="#cb451-1" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> od.predict(</span>
<span id="cb451-2"><a href="#cb451-2" aria-hidden="true" tabindex="-1"></a>    _df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'fnoise'</span>]],</span>
<span id="cb451-3"><a href="#cb451-3" aria-hidden="true" tabindex="-1"></a>    return_instance_score<span class="op">=</span><span class="va">True</span></span>
<span id="cb451-4"><a href="#cb451-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1813">
<div class="sourceCode cell-code" id="cb452"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb452-1"><a href="#cb452-1" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'IF_alibi'</span>] <span class="op">=</span> preds[<span class="st">'data'</span>][<span class="st">'is_outlier'</span>]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1814">
<div class="sourceCode cell-code" id="cb453"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb453-1"><a href="#cb453-1" aria-hidden="true" tabindex="-1"></a>outlier_alibi_one <span class="op">=</span> _df[<span class="st">'IF_alibi'</span>]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1815">
<div class="sourceCode cell-code" id="cb454"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb454-1"><a href="#cb454-1" aria-hidden="true" tabindex="-1"></a>outlier_alibi_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_alibi_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1816">
<div class="sourceCode cell-code" id="cb455"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb455-1"><a href="#cb455-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_2,outlier_alibi_one,tab_bunny)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1817">
<div class="sourceCode cell-code" id="cb456"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb456-1"><a href="#cb456-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"Isolation Forest (Liu et al., 2008)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-324-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.827
Precision: 0.995
Recall: 0.822
F1 Score: 0.900</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1818">
<div class="sourceCode cell-code" id="cb459"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb459-1"><a href="#cb459-1" aria-hidden="true" tabindex="-1"></a>nine <span class="op">=</span> eight.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  nine = eight.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="hbos" class="level3">
<h3 class="anchored" data-anchor-id="hbos">HBOS</h3>
<div class="cell" data-execution_count="1819">
<div class="sourceCode cell-code" id="cb461"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb461-1"><a href="#cb461-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> HBOS(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb461-2"><a href="#cb461-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'fnoise'</span>]])</span>
<span id="cb461-3"><a href="#cb461-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'HBOS_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1820">
<div class="sourceCode cell-code" id="cb462"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb462-1"><a href="#cb462-1" aria-hidden="true" tabindex="-1"></a>outlier_HBOS_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1821">
<div class="sourceCode cell-code" id="cb463"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb463-1"><a href="#cb463-1" aria-hidden="true" tabindex="-1"></a>outlier_HBOS_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_HBOS_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1822">
<div class="sourceCode cell-code" id="cb464"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb464-1"><a href="#cb464-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_2,outlier_HBOS_one,tab_bunny)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1823">
<div class="sourceCode cell-code" id="cb465"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb465-1"><a href="#cb465-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"HBOS (Goldstein and Dengel, 2012)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-330-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.919
Precision: 0.958
Recall: 0.956
F1 Score: 0.957</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1825">
<div class="sourceCode cell-code" id="cb468"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb468-1"><a href="#cb468-1" aria-hidden="true" tabindex="-1"></a>ten <span class="op">=</span> nine.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  ten = nine.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="sos" class="level3">
<h3 class="anchored" data-anchor-id="sos">SOS</h3>
<div class="cell" data-execution_count="1826">
<div class="sourceCode cell-code" id="cb470"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb470-1"><a href="#cb470-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> SOS(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb470-2"><a href="#cb470-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'fnoise'</span>]])</span>
<span id="cb470-3"><a href="#cb470-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'SOS_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1827">
<div class="sourceCode cell-code" id="cb471"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb471-1"><a href="#cb471-1" aria-hidden="true" tabindex="-1"></a>outlier_SOS_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1828">
<div class="sourceCode cell-code" id="cb472"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb472-1"><a href="#cb472-1" aria-hidden="true" tabindex="-1"></a>outlier_SOS_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_SOS_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1829">
<div class="sourceCode cell-code" id="cb473"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb473-1"><a href="#cb473-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_2,outlier_SOS_one,tab_bunny)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1830">
<div class="sourceCode cell-code" id="cb474"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb474-1"><a href="#cb474-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"SOS (Janssens et al., 2012)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-336-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.912
Precision: 0.955
Recall: 0.953
F1 Score: 0.954</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1832">
<div class="sourceCode cell-code" id="cb477"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb477-1"><a href="#cb477-1" aria-hidden="true" tabindex="-1"></a>eleven <span class="op">=</span> ten.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  eleven = ten.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="so_gaal-1" class="level3">
<h3 class="anchored" data-anchor-id="so_gaal-1">SO_GAAL</h3>
<div class="cell" data-scrolled="true" data-tags="[]" data-execution_count="1833">
<div class="sourceCode cell-code" id="cb479"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb479-1"><a href="#cb479-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> SO_GAAL(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb479-2"><a href="#cb479-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'fnoise'</span>]])</span>
<span id="cb479-3"><a href="#cb479-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'SO_GAAL_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super().__init__(name, **kwargs)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1 of 60

Testing for epoch 1 index 1:

Testing for epoch 1 index 2:

Testing for epoch 1 index 3:

Testing for epoch 1 index 4:

Testing for epoch 1 index 5:
Epoch 2 of 60

Testing for epoch 2 index 1:

Testing for epoch 2 index 2:

Testing for epoch 2 index 3:

Testing for epoch 2 index 4:

Testing for epoch 2 index 5:
Epoch 3 of 60

Testing for epoch 3 index 1:

Testing for epoch 3 index 2:

Testing for epoch 3 index 3:

Testing for epoch 3 index 4:

Testing for epoch 3 index 5:
Epoch 4 of 60

Testing for epoch 4 index 1:

Testing for epoch 4 index 2:

Testing for epoch 4 index 3:

Testing for epoch 4 index 4:

Testing for epoch 4 index 5:
Epoch 5 of 60

Testing for epoch 5 index 1:

Testing for epoch 5 index 2:

Testing for epoch 5 index 3:

Testing for epoch 5 index 4:

Testing for epoch 5 index 5:
Epoch 6 of 60

Testing for epoch 6 index 1:

Testing for epoch 6 index 2:

Testing for epoch 6 index 3:

Testing for epoch 6 index 4:

Testing for epoch 6 index 5:
Epoch 7 of 60

Testing for epoch 7 index 1:

Testing for epoch 7 index 2:

Testing for epoch 7 index 3:

Testing for epoch 7 index 4:

Testing for epoch 7 index 5:
Epoch 8 of 60

Testing for epoch 8 index 1:

Testing for epoch 8 index 2:

Testing for epoch 8 index 3:

Testing for epoch 8 index 4:

Testing for epoch 8 index 5:
Epoch 9 of 60

Testing for epoch 9 index 1:

Testing for epoch 9 index 2:

Testing for epoch 9 index 3:

Testing for epoch 9 index 4:

Testing for epoch 9 index 5:
Epoch 10 of 60

Testing for epoch 10 index 1:

Testing for epoch 10 index 2:

Testing for epoch 10 index 3:

Testing for epoch 10 index 4:

Testing for epoch 10 index 5:
Epoch 11 of 60

Testing for epoch 11 index 1:

Testing for epoch 11 index 2:

Testing for epoch 11 index 3:

Testing for epoch 11 index 4:

Testing for epoch 11 index 5:
Epoch 12 of 60

Testing for epoch 12 index 1:

Testing for epoch 12 index 2:

Testing for epoch 12 index 3:

Testing for epoch 12 index 4:

Testing for epoch 12 index 5:
Epoch 13 of 60

Testing for epoch 13 index 1:

Testing for epoch 13 index 2:

Testing for epoch 13 index 3:

Testing for epoch 13 index 4:

Testing for epoch 13 index 5:
Epoch 14 of 60

Testing for epoch 14 index 1:

Testing for epoch 14 index 2:

Testing for epoch 14 index 3:

Testing for epoch 14 index 4:

Testing for epoch 14 index 5:
Epoch 15 of 60

Testing for epoch 15 index 1:

Testing for epoch 15 index 2:

Testing for epoch 15 index 3:

Testing for epoch 15 index 4:

Testing for epoch 15 index 5:
Epoch 16 of 60

Testing for epoch 16 index 1:

Testing for epoch 16 index 2:

Testing for epoch 16 index 3:

Testing for epoch 16 index 4:

Testing for epoch 16 index 5:
Epoch 17 of 60

Testing for epoch 17 index 1:

Testing for epoch 17 index 2:

Testing for epoch 17 index 3:

Testing for epoch 17 index 4:

Testing for epoch 17 index 5:
Epoch 18 of 60

Testing for epoch 18 index 1:

Testing for epoch 18 index 2:

Testing for epoch 18 index 3:

Testing for epoch 18 index 4:

Testing for epoch 18 index 5:
Epoch 19 of 60

Testing for epoch 19 index 1:

Testing for epoch 19 index 2:

Testing for epoch 19 index 3:

Testing for epoch 19 index 4:

Testing for epoch 19 index 5:
Epoch 20 of 60

Testing for epoch 20 index 1:

Testing for epoch 20 index 2:

Testing for epoch 20 index 3:

Testing for epoch 20 index 4:

Testing for epoch 20 index 5:
Epoch 21 of 60

Testing for epoch 21 index 1:

Testing for epoch 21 index 2:

Testing for epoch 21 index 3:

Testing for epoch 21 index 4:

Testing for epoch 21 index 5:
Epoch 22 of 60

Testing for epoch 22 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 1.7853

Testing for epoch 22 index 2:
16/16 [==============================] - 0s 5ms/step - loss: 1.8346

Testing for epoch 22 index 3:
16/16 [==============================] - 0s 3ms/step - loss: 1.8320

Testing for epoch 22 index 4:
16/16 [==============================] - 0s 2ms/step - loss: 1.8046

Testing for epoch 22 index 5:
16/16 [==============================] - 0s 2ms/step - loss: 1.8184
Epoch 23 of 60

Testing for epoch 23 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.8771

Testing for epoch 23 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.8672

Testing for epoch 23 index 3:
16/16 [==============================] - 0s 3ms/step - loss: 1.8837

Testing for epoch 23 index 4:
16/16 [==============================] - 0s 2ms/step - loss: 1.8886

Testing for epoch 23 index 5:
16/16 [==============================] - 0s 2ms/step - loss: 1.9140
Epoch 24 of 60

Testing for epoch 24 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.8837

Testing for epoch 24 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.9102

Testing for epoch 24 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 1.9125

Testing for epoch 24 index 4:
16/16 [==============================] - 0s 1ms/step - loss: 2.0084

Testing for epoch 24 index 5:
16/16 [==============================] - 0s 1ms/step - loss: 1.9376
Epoch 25 of 60

Testing for epoch 25 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.9044

Testing for epoch 25 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.9835

Testing for epoch 25 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 1.9699

Testing for epoch 25 index 4:
16/16 [==============================] - 0s 2ms/step - loss: 1.9834

Testing for epoch 25 index 5:
16/16 [==============================] - 0s 1ms/step - loss: 2.0290
Epoch 26 of 60

Testing for epoch 26 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 1.9765

Testing for epoch 26 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 1.9838

Testing for epoch 26 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 1.9822

Testing for epoch 26 index 4:
16/16 [==============================] - 0s 1ms/step - loss: 2.0609

Testing for epoch 26 index 5:
16/16 [==============================] - 0s 2ms/step - loss: 2.0396
Epoch 27 of 60

Testing for epoch 27 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.0832

Testing for epoch 27 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 2.0676

Testing for epoch 27 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.0518

Testing for epoch 27 index 4:
16/16 [==============================] - 0s 1ms/step - loss: 2.0792

Testing for epoch 27 index 5:
16/16 [==============================] - 0s 2ms/step - loss: 2.1063
Epoch 28 of 60

Testing for epoch 28 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 2.1162

Testing for epoch 28 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 2.0633

Testing for epoch 28 index 3:
16/16 [==============================] - 0s 1ms/step - loss: 2.0415

Testing for epoch 28 index 4:
16/16 [==============================] - 0s 1ms/step - loss: 2.1830

Testing for epoch 28 index 5:
16/16 [==============================] - 0s 1ms/step - loss: 2.1030
Epoch 29 of 60

Testing for epoch 29 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 2.0691

Testing for epoch 29 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 2.1029

Testing for epoch 29 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.0695

Testing for epoch 29 index 4:
16/16 [==============================] - 0s 2ms/step - loss: 2.1422

Testing for epoch 29 index 5:
16/16 [==============================] - 0s 1ms/step - loss: 2.1041
Epoch 30 of 60

Testing for epoch 30 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.1561

Testing for epoch 30 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.1334

Testing for epoch 30 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.1333

Testing for epoch 30 index 4:
16/16 [==============================] - 0s 2ms/step - loss: 2.0868

Testing for epoch 30 index 5:
16/16 [==============================] - 0s 2ms/step - loss: 2.0846
Epoch 31 of 60

Testing for epoch 31 index 1:
16/16 [==============================] - 0s 3ms/step - loss: 2.1405

Testing for epoch 31 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.1730

Testing for epoch 31 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.1575

Testing for epoch 31 index 4:
16/16 [==============================] - 0s 1ms/step - loss: 2.1294

Testing for epoch 31 index 5:
16/16 [==============================] - 0s 2ms/step - loss: 2.1989
Epoch 32 of 60

Testing for epoch 32 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.1998

Testing for epoch 32 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.1295

Testing for epoch 32 index 3:
16/16 [==============================] - 0s 1ms/step - loss: 2.2162

Testing for epoch 32 index 4:
16/16 [==============================] - 0s 3ms/step - loss: 2.2034

Testing for epoch 32 index 5:
16/16 [==============================] - 0s 2ms/step - loss: 2.1361
Epoch 33 of 60

Testing for epoch 33 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.2382

Testing for epoch 33 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.2261

Testing for epoch 33 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.1818

Testing for epoch 33 index 4:
16/16 [==============================] - 0s 2ms/step - loss: 2.2120

Testing for epoch 33 index 5:
16/16 [==============================] - 0s 1ms/step - loss: 2.2132
Epoch 34 of 60

Testing for epoch 34 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.2494

Testing for epoch 34 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.2255

Testing for epoch 34 index 3:
16/16 [==============================] - 0s 1ms/step - loss: 2.2671

Testing for epoch 34 index 4:
16/16 [==============================] - 0s 2ms/step - loss: 2.2116

Testing for epoch 34 index 5:
16/16 [==============================] - 0s 2ms/step - loss: 2.2581
Epoch 35 of 60

Testing for epoch 35 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 2.2491

Testing for epoch 35 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.2208

Testing for epoch 35 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.2014

Testing for epoch 35 index 4:
16/16 [==============================] - 0s 3ms/step - loss: 2.2550

Testing for epoch 35 index 5:
16/16 [==============================] - 0s 1ms/step - loss: 2.2830
Epoch 36 of 60

Testing for epoch 36 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.2405

Testing for epoch 36 index 2:
16/16 [==============================] - 0s 3ms/step - loss: 2.3333

Testing for epoch 36 index 3:
16/16 [==============================] - 0s 3ms/step - loss: 2.2521

Testing for epoch 36 index 4:
16/16 [==============================] - 0s 1ms/step - loss: 2.2896

Testing for epoch 36 index 5:
16/16 [==============================] - 0s 2ms/step - loss: 2.3155
Epoch 37 of 60

Testing for epoch 37 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.3146

Testing for epoch 37 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.2681

Testing for epoch 37 index 3:
16/16 [==============================] - 0s 3ms/step - loss: 2.2337

Testing for epoch 37 index 4:
16/16 [==============================] - 0s 2ms/step - loss: 2.2561

Testing for epoch 37 index 5:
16/16 [==============================] - 0s 1ms/step - loss: 2.2611
Epoch 38 of 60

Testing for epoch 38 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 2.3340

Testing for epoch 38 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.2951

Testing for epoch 38 index 3:
16/16 [==============================] - 0s 1ms/step - loss: 2.2973

Testing for epoch 38 index 4:
16/16 [==============================] - 0s 2ms/step - loss: 2.3241

Testing for epoch 38 index 5:
16/16 [==============================] - 0s 1ms/step - loss: 2.3202
Epoch 39 of 60

Testing for epoch 39 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.2970

Testing for epoch 39 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 2.2818

Testing for epoch 39 index 3:
16/16 [==============================] - 0s 1ms/step - loss: 2.2771

Testing for epoch 39 index 4:
16/16 [==============================] - 0s 1ms/step - loss: 2.3100

Testing for epoch 39 index 5:
16/16 [==============================] - 0s 3ms/step - loss: 2.2902
Epoch 40 of 60

Testing for epoch 40 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.3639

Testing for epoch 40 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 2.2836

Testing for epoch 40 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.4120

Testing for epoch 40 index 4:
16/16 [==============================] - 0s 1ms/step - loss: 2.3052

Testing for epoch 40 index 5:
16/16 [==============================] - 0s 2ms/step - loss: 2.2881
Epoch 41 of 60

Testing for epoch 41 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.3652

Testing for epoch 41 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 2.3530

Testing for epoch 41 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.3983

Testing for epoch 41 index 4:
16/16 [==============================] - 0s 2ms/step - loss: 2.3804

Testing for epoch 41 index 5:
16/16 [==============================] - 0s 3ms/step - loss: 2.3145
Epoch 42 of 60

Testing for epoch 42 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.3505

Testing for epoch 42 index 2:
16/16 [==============================] - 0s 3ms/step - loss: 2.3759

Testing for epoch 42 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.3779

Testing for epoch 42 index 4:
16/16 [==============================] - 0s 2ms/step - loss: 2.4360

Testing for epoch 42 index 5:
16/16 [==============================] - 0s 2ms/step - loss: 2.3967
Epoch 43 of 60

Testing for epoch 43 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.4442

Testing for epoch 43 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.3817

Testing for epoch 43 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.4227

Testing for epoch 43 index 4:
16/16 [==============================] - 0s 2ms/step - loss: 2.3354

Testing for epoch 43 index 5:
16/16 [==============================] - 0s 2ms/step - loss: 2.3362
Epoch 44 of 60

Testing for epoch 44 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.3727

Testing for epoch 44 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 2.4077

Testing for epoch 44 index 3:
16/16 [==============================] - 0s 3ms/step - loss: 2.4266

Testing for epoch 44 index 4:
16/16 [==============================] - 0s 2ms/step - loss: 2.4087

Testing for epoch 44 index 5:
16/16 [==============================] - 0s 2ms/step - loss: 2.3740
Epoch 45 of 60

Testing for epoch 45 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 2.4019

Testing for epoch 45 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.4554

Testing for epoch 45 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.4162

Testing for epoch 45 index 4:
16/16 [==============================] - 0s 2ms/step - loss: 2.4631

Testing for epoch 45 index 5:
16/16 [==============================] - 0s 2ms/step - loss: 2.4390
Epoch 46 of 60

Testing for epoch 46 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.3997

Testing for epoch 46 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 2.4826

Testing for epoch 46 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.3973

Testing for epoch 46 index 4:
16/16 [==============================] - 0s 1ms/step - loss: 2.4596

Testing for epoch 46 index 5:
16/16 [==============================] - 0s 1ms/step - loss: 2.4296
Epoch 47 of 60

Testing for epoch 47 index 1:
16/16 [==============================] - 0s 3ms/step - loss: 2.4578

Testing for epoch 47 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.5058

Testing for epoch 47 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.4464

Testing for epoch 47 index 4:
16/16 [==============================] - 0s 1ms/step - loss: 2.4684

Testing for epoch 47 index 5:
16/16 [==============================] - 0s 1ms/step - loss: 2.4405
Epoch 48 of 60

Testing for epoch 48 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 2.4991

Testing for epoch 48 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.4709

Testing for epoch 48 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.4676

Testing for epoch 48 index 4:
16/16 [==============================] - 0s 1ms/step - loss: 2.4131

Testing for epoch 48 index 5:
16/16 [==============================] - 0s 3ms/step - loss: 2.4753
Epoch 49 of 60

Testing for epoch 49 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.5160

Testing for epoch 49 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.4963

Testing for epoch 49 index 3:
16/16 [==============================] - 0s 1ms/step - loss: 2.4678

Testing for epoch 49 index 4:
16/16 [==============================] - 0s 2ms/step - loss: 2.4248

Testing for epoch 49 index 5:
16/16 [==============================] - 0s 2ms/step - loss: 2.5513
Epoch 50 of 60

Testing for epoch 50 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.4780

Testing for epoch 50 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.4913

Testing for epoch 50 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.4956

Testing for epoch 50 index 4:
16/16 [==============================] - 0s 1ms/step - loss: 2.4918

Testing for epoch 50 index 5:
16/16 [==============================] - 0s 3ms/step - loss: 2.4777
Epoch 51 of 60

Testing for epoch 51 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.5556

Testing for epoch 51 index 2:
16/16 [==============================] - 0s 1ms/step - loss: 2.4938

Testing for epoch 51 index 3:
16/16 [==============================] - 0s 1ms/step - loss: 2.4807

Testing for epoch 51 index 4:
16/16 [==============================] - 0s 1ms/step - loss: 2.5070

Testing for epoch 51 index 5:
16/16 [==============================] - 0s 1ms/step - loss: 2.5431
Epoch 52 of 60

Testing for epoch 52 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 2.4874

Testing for epoch 52 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.5284

Testing for epoch 52 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.5150

Testing for epoch 52 index 4:
16/16 [==============================] - 0s 1ms/step - loss: 2.5187

Testing for epoch 52 index 5:
16/16 [==============================] - 0s 1ms/step - loss: 2.5245
Epoch 53 of 60

Testing for epoch 53 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.5878

Testing for epoch 53 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.5331

Testing for epoch 53 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.5031

Testing for epoch 53 index 4:
16/16 [==============================] - 0s 1ms/step - loss: 2.5649

Testing for epoch 53 index 5:
16/16 [==============================] - 0s 3ms/step - loss: 2.5189
Epoch 54 of 60

Testing for epoch 54 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.5311

Testing for epoch 54 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.5879

Testing for epoch 54 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.5670

Testing for epoch 54 index 4:
16/16 [==============================] - 0s 2ms/step - loss: 2.5522

Testing for epoch 54 index 5:
16/16 [==============================] - 0s 2ms/step - loss: 2.5572
Epoch 55 of 60

Testing for epoch 55 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.5563

Testing for epoch 55 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.5327

Testing for epoch 55 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.5742

Testing for epoch 55 index 4:
16/16 [==============================] - 0s 2ms/step - loss: 2.4747

Testing for epoch 55 index 5:
16/16 [==============================] - 0s 3ms/step - loss: 2.5711
Epoch 56 of 60

Testing for epoch 56 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.5344

Testing for epoch 56 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.5182

Testing for epoch 56 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.4722

Testing for epoch 56 index 4:
16/16 [==============================] - 0s 1ms/step - loss: 2.5704

Testing for epoch 56 index 5:
16/16 [==============================] - 0s 1ms/step - loss: 2.6122
Epoch 57 of 60

Testing for epoch 57 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 2.5826

Testing for epoch 57 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.5456

Testing for epoch 57 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.5821

Testing for epoch 57 index 4:
16/16 [==============================] - 0s 2ms/step - loss: 2.5895

Testing for epoch 57 index 5:
16/16 [==============================] - 0s 2ms/step - loss: 2.6114
Epoch 58 of 60

Testing for epoch 58 index 1:
16/16 [==============================] - 0s 2ms/step - loss: 2.5628

Testing for epoch 58 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.5592

Testing for epoch 58 index 3:
16/16 [==============================] - 0s 2ms/step - loss: 2.6494

Testing for epoch 58 index 4:
16/16 [==============================] - 0s 2ms/step - loss: 2.5955

Testing for epoch 58 index 5:
16/16 [==============================] - 0s 1ms/step - loss: 2.6131
Epoch 59 of 60

Testing for epoch 59 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 2.6084

Testing for epoch 59 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.5200

Testing for epoch 59 index 3:
16/16 [==============================] - 0s 1ms/step - loss: 2.5612

Testing for epoch 59 index 4:
16/16 [==============================] - 0s 1ms/step - loss: 2.5473

Testing for epoch 59 index 5:
16/16 [==============================] - 0s 5ms/step - loss: 2.6558
Epoch 60 of 60

Testing for epoch 60 index 1:
16/16 [==============================] - 0s 1ms/step - loss: 2.6821

Testing for epoch 60 index 2:
16/16 [==============================] - 0s 2ms/step - loss: 2.5944

Testing for epoch 60 index 3:
16/16 [==============================] - 0s 1ms/step - loss: 2.6211

Testing for epoch 60 index 4:
16/16 [==============================] - 0s 1ms/step - loss: 2.5937

Testing for epoch 60 index 5:
16/16 [==============================] - 0s 1ms/step - loss: 2.6623
79/79 [==============================] - 0s 1ms/step</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1834">
<div class="sourceCode cell-code" id="cb482"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb482-1"><a href="#cb482-1" aria-hidden="true" tabindex="-1"></a>outlier_SO_GAAL_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1835">
<div class="sourceCode cell-code" id="cb483"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb483-1"><a href="#cb483-1" aria-hidden="true" tabindex="-1"></a>outlier_SO_GAAL_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_SO_GAAL_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1836">
<div class="sourceCode cell-code" id="cb484"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb484-1"><a href="#cb484-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_2,outlier_SO_GAAL_one,tab_bunny)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1837">
<div class="sourceCode cell-code" id="cb485"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb485-1"><a href="#cb485-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"SO-GAAL (Liu et al., 2019)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-342-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.952
Precision: 0.952
Recall: 1.000
F1 Score: 0.975</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1838">
<div class="sourceCode cell-code" id="cb488"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb488-1"><a href="#cb488-1" aria-hidden="true" tabindex="-1"></a>twelve <span class="op">=</span> eleven.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  twelve = eleven.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="mo_gaal" class="level3">
<h3 class="anchored" data-anchor-id="mo_gaal">MO_GAAL</h3>
<div class="cell" data-scrolled="true" data-tags="[]" data-execution_count="1839">
<div class="sourceCode cell-code" id="cb490"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb490-1"><a href="#cb490-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> MO_GAAL(contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb490-2"><a href="#cb490-2" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'fnoise'</span>]])</span>
<span id="cb490-3"><a href="#cb490-3" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'MO_GAAL_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super().__init__(name, **kwargs)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1 of 60

Testing for epoch 1 index 1:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 1 index 2:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 1 index 3:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 1 index 4:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 1 index 5:
79/79 [==============================] - 0s 1ms/step
Epoch 2 of 60

Testing for epoch 2 index 1:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 2 index 2:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 2 index 3:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 2 index 4:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 2 index 5:
79/79 [==============================] - 0s 2ms/step
Epoch 3 of 60

Testing for epoch 3 index 1:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 3 index 2:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 3 index 3:
79/79 [==============================] - 0s 664us/step

Testing for epoch 3 index 4:
79/79 [==============================] - 0s 875us/step

Testing for epoch 3 index 5:
79/79 [==============================] - 0s 1ms/step
Epoch 4 of 60

Testing for epoch 4 index 1:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 4 index 2:
79/79 [==============================] - 0s 820us/step

Testing for epoch 4 index 3:
79/79 [==============================] - 0s 609us/step

Testing for epoch 4 index 4:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 4 index 5:
79/79 [==============================] - 0s 512us/step
Epoch 5 of 60

Testing for epoch 5 index 1:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 5 index 2:
79/79 [==============================] - 0s 684us/step

Testing for epoch 5 index 3:
79/79 [==============================] - 0s 589us/step

Testing for epoch 5 index 4:
79/79 [==============================] - 0s 742us/step

Testing for epoch 5 index 5:
79/79 [==============================] - 0s 1ms/step
Epoch 6 of 60

Testing for epoch 6 index 1:
79/79 [==============================] - 0s 3ms/step

Testing for epoch 6 index 2:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 6 index 3:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 6 index 4:
79/79 [==============================] - 0s 3ms/step

Testing for epoch 6 index 5:
79/79 [==============================] - 0s 1ms/step
Epoch 7 of 60

Testing for epoch 7 index 1:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 7 index 2:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 7 index 3:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 7 index 4:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 7 index 5:
79/79 [==============================] - 0s 1ms/step
Epoch 8 of 60

Testing for epoch 8 index 1:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 8 index 2:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 8 index 3:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 8 index 4:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 8 index 5:
79/79 [==============================] - 0s 1ms/step
Epoch 9 of 60

Testing for epoch 9 index 1:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 9 index 2:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 9 index 3:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 9 index 4:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 9 index 5:
79/79 [==============================] - 0s 2ms/step
Epoch 10 of 60

Testing for epoch 10 index 1:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 10 index 2:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 10 index 3:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 10 index 4:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 10 index 5:
79/79 [==============================] - 0s 2ms/step
Epoch 11 of 60

Testing for epoch 11 index 1:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 11 index 2:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 11 index 3:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 11 index 4:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 11 index 5:
79/79 [==============================] - 0s 1ms/step
Epoch 12 of 60

Testing for epoch 12 index 1:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 12 index 2:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 12 index 3:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 12 index 4:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 12 index 5:
79/79 [==============================] - 0s 2ms/step
Epoch 13 of 60

Testing for epoch 13 index 1:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 13 index 2:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 13 index 3:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 13 index 4:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 13 index 5:
79/79 [==============================] - 0s 1ms/step
Epoch 14 of 60

Testing for epoch 14 index 1:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 14 index 2:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 14 index 3:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 14 index 4:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 14 index 5:
79/79 [==============================] - 0s 1ms/step
Epoch 15 of 60

Testing for epoch 15 index 1:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 15 index 2:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 15 index 3:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 15 index 4:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 15 index 5:
79/79 [==============================] - 0s 976us/step
Epoch 16 of 60

Testing for epoch 16 index 1:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 16 index 2:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 16 index 3:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 16 index 4:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 16 index 5:
79/79 [==============================] - 0s 1ms/step
Epoch 17 of 60

Testing for epoch 17 index 1:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 17 index 2:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 17 index 3:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 17 index 4:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 17 index 5:
79/79 [==============================] - 0s 1ms/step
Epoch 18 of 60

Testing for epoch 18 index 1:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 18 index 2:
79/79 [==============================] - 0s 891us/step

Testing for epoch 18 index 3:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 18 index 4:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 18 index 5:
79/79 [==============================] - 0s 1ms/step
Epoch 19 of 60

Testing for epoch 19 index 1:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 19 index 2:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 19 index 3:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 19 index 4:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 19 index 5:
79/79 [==============================] - 0s 1ms/step
Epoch 20 of 60

Testing for epoch 20 index 1:
79/79 [==============================] - 0s 2ms/step

Testing for epoch 20 index 2:
79/79 [==============================] - 0s 978us/step

Testing for epoch 20 index 3:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 20 index 4:
79/79 [==============================] - 0s 969us/step

Testing for epoch 20 index 5:
79/79 [==============================] - 0s 1ms/step
Epoch 21 of 60

Testing for epoch 21 index 1:
79/79 [==============================] - 0s 1ms/step

Testing for epoch 21 index 2:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.2249
16/16 [==============================] - 0s 1ms/step - loss: 1.3848
16/16 [==============================] - 0s 2ms/step - loss: 1.5898
16/16 [==============================] - 1s 2ms/step - loss: 1.6900
16/16 [==============================] - 0s 1ms/step - loss: 1.7373
16/16 [==============================] - 0s 2ms/step - loss: 1.7695
16/16 [==============================] - 0s 1ms/step - loss: 1.7774
16/16 [==============================] - 0s 1ms/step - loss: 1.7766
16/16 [==============================] - 0s 1ms/step - loss: 1.7754
16/16 [==============================] - 0s 1ms/step - loss: 1.7754

Testing for epoch 21 index 3:
79/79 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 5ms/step - loss: 0.2249
16/16 [==============================] - 0s 2ms/step - loss: 1.4195
16/16 [==============================] - 0s 3ms/step - loss: 1.6365
16/16 [==============================] - 0s 2ms/step - loss: 1.7426
16/16 [==============================] - 0s 2ms/step - loss: 1.7927
16/16 [==============================] - 0s 2ms/step - loss: 1.8275
16/16 [==============================] - 0s 1ms/step - loss: 1.8359
16/16 [==============================] - 0s 1ms/step - loss: 1.8351
16/16 [==============================] - 0s 2ms/step - loss: 1.8338
16/16 [==============================] - 0s 2ms/step - loss: 1.8338

Testing for epoch 21 index 4:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 3ms/step - loss: 0.2255
16/16 [==============================] - 0s 2ms/step - loss: 1.4083
16/16 [==============================] - 0s 2ms/step - loss: 1.6229
16/16 [==============================] - 0s 1ms/step - loss: 1.7262
16/16 [==============================] - 0s 1ms/step - loss: 1.7739
16/16 [==============================] - 0s 2ms/step - loss: 1.8067
16/16 [==============================] - 0s 2ms/step - loss: 1.8141
16/16 [==============================] - 0s 1ms/step - loss: 1.8131
16/16 [==============================] - 0s 2ms/step - loss: 1.8118
16/16 [==============================] - 0s 2ms/step - loss: 1.8117

Testing for epoch 21 index 5:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 5ms/step - loss: 0.2193
16/16 [==============================] - 0s 2ms/step - loss: 1.4150
16/16 [==============================] - 0s 2ms/step - loss: 1.6347
16/16 [==============================] - 0s 2ms/step - loss: 1.7387
16/16 [==============================] - 0s 1ms/step - loss: 1.7855
16/16 [==============================] - 0s 2ms/step - loss: 1.8169
16/16 [==============================] - 0s 2ms/step - loss: 1.8234
16/16 [==============================] - 0s 2ms/step - loss: 1.8219
16/16 [==============================] - 0s 2ms/step - loss: 1.8205
16/16 [==============================] - 0s 2ms/step - loss: 1.8205
Epoch 22 of 60

Testing for epoch 22 index 1:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 5ms/step - loss: 0.2173
16/16 [==============================] - 0s 2ms/step - loss: 1.4357
16/16 [==============================] - 0s 2ms/step - loss: 1.6634
16/16 [==============================] - 0s 5ms/step - loss: 1.7700
16/16 [==============================] - 0s 2ms/step - loss: 1.8171
16/16 [==============================] - 0s 3ms/step - loss: 1.8488
16/16 [==============================] - 0s 5ms/step - loss: 1.8558
16/16 [==============================] - 0s 2ms/step - loss: 1.8544
16/16 [==============================] - 0s 2ms/step - loss: 1.8529
16/16 [==============================] - 0s 3ms/step - loss: 1.8529

Testing for epoch 22 index 2:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.2139
16/16 [==============================] - 0s 2ms/step - loss: 1.4286
16/16 [==============================] - 0s 2ms/step - loss: 1.6561
16/16 [==============================] - 0s 1ms/step - loss: 1.7609
16/16 [==============================] - 0s 4ms/step - loss: 1.8068
16/16 [==============================] - 0s 2ms/step - loss: 1.8372
16/16 [==============================] - 0s 6ms/step - loss: 1.8438
16/16 [==============================] - 0s 2ms/step - loss: 1.8422
16/16 [==============================] - 0s 2ms/step - loss: 1.8407
16/16 [==============================] - 0s 2ms/step - loss: 1.8407

Testing for epoch 22 index 3:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 3ms/step - loss: 0.2148
16/16 [==============================] - 0s 1ms/step - loss: 1.4293
16/16 [==============================] - 0s 2ms/step - loss: 1.6578
16/16 [==============================] - 0s 4ms/step - loss: 1.7632
16/16 [==============================] - 0s 2ms/step - loss: 1.8090
16/16 [==============================] - 0s 2ms/step - loss: 1.8394
16/16 [==============================] - 0s 2ms/step - loss: 1.8454
16/16 [==============================] - 0s 5ms/step - loss: 1.8437
16/16 [==============================] - 0s 2ms/step - loss: 1.8423
16/16 [==============================] - 0s 2ms/step - loss: 1.8423

Testing for epoch 22 index 4:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.2181
16/16 [==============================] - 0s 3ms/step - loss: 1.3837
16/16 [==============================] - 0s 2ms/step - loss: 1.6049
16/16 [==============================] - 0s 2ms/step - loss: 1.7082
16/16 [==============================] - 0s 2ms/step - loss: 1.7544
16/16 [==============================] - 0s 4ms/step - loss: 1.7860
16/16 [==============================] - 0s 3ms/step - loss: 1.7928
16/16 [==============================] - 0s 2ms/step - loss: 1.7913
16/16 [==============================] - 0s 3ms/step - loss: 1.7897
16/16 [==============================] - 0s 2ms/step - loss: 1.7897

Testing for epoch 22 index 5:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.2083
16/16 [==============================] - 0s 991us/step - loss: 1.4684
16/16 [==============================] - 0s 873us/step - loss: 1.7111
16/16 [==============================] - 0s 2ms/step - loss: 1.8243
16/16 [==============================] - 0s 820us/step - loss: 1.8730
16/16 [==============================] - 0s 947us/step - loss: 1.9051
16/16 [==============================] - 0s 771us/step - loss: 1.9109
16/16 [==============================] - 0s 1ms/step - loss: 1.9087
16/16 [==============================] - 0s 1ms/step - loss: 1.9069
16/16 [==============================] - 0s 839us/step - loss: 1.9069
Epoch 23 of 60

Testing for epoch 23 index 1:
79/79 [==============================] - 0s 564us/step
16/16 [==============================] - 0s 872us/step - loss: 0.2068
16/16 [==============================] - 0s 929us/step - loss: 1.4438
16/16 [==============================] - 0s 859us/step - loss: 1.6817
16/16 [==============================] - 0s 829us/step - loss: 1.7917
16/16 [==============================] - 0s 1ms/step - loss: 1.8384
16/16 [==============================] - 0s 672us/step - loss: 1.8677
16/16 [==============================] - 0s 2ms/step - loss: 1.8721
16/16 [==============================] - 0s 3ms/step - loss: 1.8696
16/16 [==============================] - 0s 1ms/step - loss: 1.8677
16/16 [==============================] - 0s 942us/step - loss: 1.8676

Testing for epoch 23 index 2:
79/79 [==============================] - 0s 599us/step
16/16 [==============================] - 0s 827us/step - loss: 0.2075
16/16 [==============================] - 0s 724us/step - loss: 1.4278
16/16 [==============================] - 0s 781us/step - loss: 1.6611
16/16 [==============================] - 0s 855us/step - loss: 1.7673
16/16 [==============================] - 0s 1ms/step - loss: 1.8137
16/16 [==============================] - 0s 689us/step - loss: 1.8419
16/16 [==============================] - 0s 846us/step - loss: 1.8465
16/16 [==============================] - 0s 826us/step - loss: 1.8442
16/16 [==============================] - 0s 818us/step - loss: 1.8425
16/16 [==============================] - 0s 818us/step - loss: 1.8425

Testing for epoch 23 index 3:
79/79 [==============================] - 0s 843us/step
16/16 [==============================] - 0s 2ms/step - loss: 0.2072
16/16 [==============================] - 0s 1ms/step - loss: 1.4404
16/16 [==============================] - 0s 2ms/step - loss: 1.6810
16/16 [==============================] - 0s 5ms/step - loss: 1.7908
16/16 [==============================] - 0s 1ms/step - loss: 1.8406
16/16 [==============================] - 0s 1ms/step - loss: 1.8710
16/16 [==============================] - 0s 1ms/step - loss: 1.8762
16/16 [==============================] - 0s 2ms/step - loss: 1.8739
16/16 [==============================] - 0s 2ms/step - loss: 1.8721
16/16 [==============================] - 0s 1ms/step - loss: 1.8720

Testing for epoch 23 index 4:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.2005
16/16 [==============================] - 0s 4ms/step - loss: 1.4690
16/16 [==============================] - 0s 2ms/step - loss: 1.7149
16/16 [==============================] - 0s 1ms/step - loss: 1.8240
16/16 [==============================] - 0s 1ms/step - loss: 1.8711
16/16 [==============================] - 0s 2ms/step - loss: 1.8977
16/16 [==============================] - 0s 2ms/step - loss: 1.9010
16/16 [==============================] - 0s 2ms/step - loss: 1.8979
16/16 [==============================] - 0s 2ms/step - loss: 1.8961
16/16 [==============================] - 0s 2ms/step - loss: 1.8960

Testing for epoch 23 index 5:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1965
16/16 [==============================] - 0s 933us/step - loss: 1.4857
16/16 [==============================] - 0s 2ms/step - loss: 1.7374
16/16 [==============================] - 0s 1ms/step - loss: 1.8489
16/16 [==============================] - 0s 1ms/step - loss: 1.8958
16/16 [==============================] - 0s 1ms/step - loss: 1.9208
16/16 [==============================] - 0s 2ms/step - loss: 1.9232
16/16 [==============================] - 0s 1ms/step - loss: 1.9199
16/16 [==============================] - 0s 2ms/step - loss: 1.9179
16/16 [==============================] - 0s 2ms/step - loss: 1.9179
Epoch 24 of 60

Testing for epoch 24 index 1:
79/79 [==============================] - 0s 934us/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1949
16/16 [==============================] - 0s 2ms/step - loss: 1.5125
16/16 [==============================] - 0s 1ms/step - loss: 1.7705
16/16 [==============================] - 0s 2ms/step - loss: 1.8843
16/16 [==============================] - 0s 1ms/step - loss: 1.9321
16/16 [==============================] - 0s 1ms/step - loss: 1.9573
16/16 [==============================] - 0s 934us/step - loss: 1.9592
16/16 [==============================] - 0s 2ms/step - loss: 1.9558
16/16 [==============================] - 0s 2ms/step - loss: 1.9537
16/16 [==============================] - 0s 1ms/step - loss: 1.9537

Testing for epoch 24 index 2:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1959
16/16 [==============================] - 0s 1ms/step - loss: 1.4640
16/16 [==============================] - 0s 2ms/step - loss: 1.7067
16/16 [==============================] - 0s 2ms/step - loss: 1.8128
16/16 [==============================] - 0s 2ms/step - loss: 1.8585
16/16 [==============================] - 0s 2ms/step - loss: 1.8822
16/16 [==============================] - 0s 970us/step - loss: 1.8835
16/16 [==============================] - 0s 1ms/step - loss: 1.8799
16/16 [==============================] - 0s 2ms/step - loss: 1.8778
16/16 [==============================] - 0s 1ms/step - loss: 1.8777

Testing for epoch 24 index 3:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1986
16/16 [==============================] - 0s 1ms/step - loss: 1.4867
16/16 [==============================] - 0s 4ms/step - loss: 1.7359
16/16 [==============================] - 0s 3ms/step - loss: 1.8473
16/16 [==============================] - 0s 2ms/step - loss: 1.8954
16/16 [==============================] - 0s 2ms/step - loss: 1.9193
16/16 [==============================] - 0s 2ms/step - loss: 1.9209
16/16 [==============================] - 0s 1ms/step - loss: 1.9173
16/16 [==============================] - 0s 2ms/step - loss: 1.9152
16/16 [==============================] - 0s 1ms/step - loss: 1.9151

Testing for epoch 24 index 4:
79/79 [==============================] - 0s 862us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1967
16/16 [==============================] - 0s 1ms/step - loss: 1.4806
16/16 [==============================] - 0s 2ms/step - loss: 1.7273
16/16 [==============================] - 0s 1ms/step - loss: 1.8367
16/16 [==============================] - 0s 2ms/step - loss: 1.8844
16/16 [==============================] - 0s 2ms/step - loss: 1.9072
16/16 [==============================] - 0s 1ms/step - loss: 1.9083
16/16 [==============================] - 0s 1ms/step - loss: 1.9047
16/16 [==============================] - 0s 2ms/step - loss: 1.9027
16/16 [==============================] - 0s 2ms/step - loss: 1.9027

Testing for epoch 24 index 5:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1922
16/16 [==============================] - 0s 1ms/step - loss: 1.5153
16/16 [==============================] - 0s 1ms/step - loss: 1.7681
16/16 [==============================] - 0s 1ms/step - loss: 1.8780
16/16 [==============================] - 0s 1ms/step - loss: 1.9241
16/16 [==============================] - 0s 1ms/step - loss: 1.9447
16/16 [==============================] - 0s 2ms/step - loss: 1.9445
16/16 [==============================] - 0s 1ms/step - loss: 1.9402
16/16 [==============================] - 0s 954us/step - loss: 1.9381
16/16 [==============================] - 0s 1ms/step - loss: 1.9380
Epoch 25 of 60

Testing for epoch 25 index 1:
79/79 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 5ms/step - loss: 0.1929
16/16 [==============================] - 0s 1ms/step - loss: 1.4627
16/16 [==============================] - 0s 1ms/step - loss: 1.7036
16/16 [==============================] - 0s 953us/step - loss: 1.8077
16/16 [==============================] - 0s 1000us/step - loss: 1.8516
16/16 [==============================] - 0s 2ms/step - loss: 1.8687
16/16 [==============================] - 0s 1ms/step - loss: 1.8670
16/16 [==============================] - 0s 4ms/step - loss: 1.8625
16/16 [==============================] - 0s 3ms/step - loss: 1.8602
16/16 [==============================] - 0s 2ms/step - loss: 1.8602

Testing for epoch 25 index 2:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 3ms/step - loss: 0.1918
16/16 [==============================] - 0s 1ms/step - loss: 1.5117
16/16 [==============================] - 0s 2ms/step - loss: 1.7683
16/16 [==============================] - 0s 1ms/step - loss: 1.8769
16/16 [==============================] - 0s 2ms/step - loss: 1.9240
16/16 [==============================] - 0s 2ms/step - loss: 1.9418
16/16 [==============================] - 0s 1ms/step - loss: 1.9400
16/16 [==============================] - 0s 1ms/step - loss: 1.9352
16/16 [==============================] - 0s 2ms/step - loss: 1.9327
16/16 [==============================] - 0s 2ms/step - loss: 1.9326

Testing for epoch 25 index 3:
79/79 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1905
16/16 [==============================] - 0s 1ms/step - loss: 1.5434
16/16 [==============================] - 0s 1ms/step - loss: 1.8096
16/16 [==============================] - 0s 1ms/step - loss: 1.9206
16/16 [==============================] - 0s 1ms/step - loss: 1.9683
16/16 [==============================] - 0s 5ms/step - loss: 1.9856
16/16 [==============================] - 0s 2ms/step - loss: 1.9832
16/16 [==============================] - 0s 2ms/step - loss: 1.9781
16/16 [==============================] - 0s 4ms/step - loss: 1.9756
16/16 [==============================] - 0s 2ms/step - loss: 1.9755

Testing for epoch 25 index 4:
79/79 [==============================] - 0s 623us/step
16/16 [==============================] - 0s 856us/step - loss: 0.1849
16/16 [==============================] - 0s 898us/step - loss: 1.5488
16/16 [==============================] - 0s 842us/step - loss: 1.8135
16/16 [==============================] - 0s 817us/step - loss: 1.9221
16/16 [==============================] - 0s 797us/step - loss: 1.9682
16/16 [==============================] - 0s 795us/step - loss: 1.9825
16/16 [==============================] - 0s 833us/step - loss: 1.9783
16/16 [==============================] - 0s 813us/step - loss: 1.9727
16/16 [==============================] - 0s 792us/step - loss: 1.9702
16/16 [==============================] - 0s 794us/step - loss: 1.9701

Testing for epoch 25 index 5:
79/79 [==============================] - 0s 653us/step
16/16 [==============================] - 0s 817us/step - loss: 0.1847
16/16 [==============================] - 0s 809us/step - loss: 1.5567
16/16 [==============================] - 0s 778us/step - loss: 1.8307
16/16 [==============================] - 0s 778us/step - loss: 1.9453
16/16 [==============================] - 0s 771us/step - loss: 1.9954
16/16 [==============================] - 0s 826us/step - loss: 2.0152
16/16 [==============================] - 0s 802us/step - loss: 2.0137
16/16 [==============================] - 0s 813us/step - loss: 2.0085
16/16 [==============================] - 0s 779us/step - loss: 2.0058
16/16 [==============================] - 0s 769us/step - loss: 2.0056
Epoch 26 of 60

Testing for epoch 26 index 1:
79/79 [==============================] - 0s 604us/step
16/16 [==============================] - 0s 813us/step - loss: 0.1844
16/16 [==============================] - 0s 807us/step - loss: 1.5198
16/16 [==============================] - 0s 785us/step - loss: 1.7813
16/16 [==============================] - 0s 804us/step - loss: 1.8883
16/16 [==============================] - 0s 778us/step - loss: 1.9336
16/16 [==============================] - 0s 799us/step - loss: 1.9494
16/16 [==============================] - 0s 789us/step - loss: 1.9453
16/16 [==============================] - 0s 816us/step - loss: 1.9398
16/16 [==============================] - 0s 785us/step - loss: 1.9373
16/16 [==============================] - 0s 785us/step - loss: 1.9372

Testing for epoch 26 index 2:
79/79 [==============================] - 0s 601us/step
16/16 [==============================] - 0s 806us/step - loss: 0.1859
16/16 [==============================] - 0s 815us/step - loss: 1.5434
16/16 [==============================] - 0s 803us/step - loss: 1.8104
16/16 [==============================] - 0s 770us/step - loss: 1.9219
16/16 [==============================] - 0s 847us/step - loss: 1.9699
16/16 [==============================] - 0s 794us/step - loss: 1.9883
16/16 [==============================] - 0s 779us/step - loss: 1.9855
16/16 [==============================] - 0s 801us/step - loss: 1.9802
16/16 [==============================] - 0s 835us/step - loss: 1.9775
16/16 [==============================] - 0s 1ms/step - loss: 1.9773

Testing for epoch 26 index 3:
79/79 [==============================] - 0s 583us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1817
16/16 [==============================] - 0s 1ms/step - loss: 1.5762
16/16 [==============================] - 0s 1ms/step - loss: 1.8483
16/16 [==============================] - 0s 1ms/step - loss: 1.9610
16/16 [==============================] - 0s 781us/step - loss: 2.0079
16/16 [==============================] - 0s 1ms/step - loss: 2.0228
16/16 [==============================] - 0s 1ms/step - loss: 2.0191
16/16 [==============================] - 0s 1ms/step - loss: 2.0135
16/16 [==============================] - 0s 771us/step - loss: 2.0108
16/16 [==============================] - 0s 785us/step - loss: 2.0106

Testing for epoch 26 index 4:
79/79 [==============================] - 0s 749us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1829
16/16 [==============================] - 0s 1ms/step - loss: 1.5509
16/16 [==============================] - 0s 808us/step - loss: 1.8163
16/16 [==============================] - 0s 782us/step - loss: 1.9243
16/16 [==============================] - 0s 783us/step - loss: 1.9681
16/16 [==============================] - 0s 782us/step - loss: 1.9796
16/16 [==============================] - 0s 785us/step - loss: 1.9733
16/16 [==============================] - 0s 789us/step - loss: 1.9671
16/16 [==============================] - 0s 795us/step - loss: 1.9644
16/16 [==============================] - 0s 797us/step - loss: 1.9643

Testing for epoch 26 index 5:
79/79 [==============================] - 0s 600us/step
16/16 [==============================] - 0s 834us/step - loss: 0.1804
16/16 [==============================] - 0s 785us/step - loss: 1.5986
16/16 [==============================] - 0s 791us/step - loss: 1.8789
16/16 [==============================] - 0s 810us/step - loss: 1.9937
16/16 [==============================] - 0s 786us/step - loss: 2.0401
16/16 [==============================] - 0s 775us/step - loss: 2.0543
16/16 [==============================] - 0s 813us/step - loss: 2.0491
16/16 [==============================] - 0s 841us/step - loss: 2.0430
16/16 [==============================] - 0s 787us/step - loss: 2.0402
16/16 [==============================] - 0s 802us/step - loss: 2.0401
Epoch 27 of 60

Testing for epoch 27 index 1:
79/79 [==============================] - 0s 588us/step
16/16 [==============================] - 0s 980us/step - loss: 0.1780
16/16 [==============================] - 0s 1ms/step - loss: 1.5897
16/16 [==============================] - 0s 1ms/step - loss: 1.8713
16/16 [==============================] - 0s 1ms/step - loss: 1.9847
16/16 [==============================] - 0s 785us/step - loss: 2.0309
16/16 [==============================] - 0s 800us/step - loss: 2.0444
16/16 [==============================] - 0s 801us/step - loss: 2.0382
16/16 [==============================] - 0s 800us/step - loss: 2.0316
16/16 [==============================] - 0s 775us/step - loss: 2.0287
16/16 [==============================] - 0s 772us/step - loss: 2.0286

Testing for epoch 27 index 2:
79/79 [==============================] - 0s 580us/step
16/16 [==============================] - 0s 803us/step - loss: 0.1847
16/16 [==============================] - 0s 776us/step - loss: 1.5399
16/16 [==============================] - 0s 808us/step - loss: 1.8078
16/16 [==============================] - 0s 805us/step - loss: 1.9151
16/16 [==============================] - 0s 1ms/step - loss: 1.9593
16/16 [==============================] - 0s 774us/step - loss: 1.9706
16/16 [==============================] - 0s 922us/step - loss: 1.9641
16/16 [==============================] - 0s 1ms/step - loss: 1.9579
16/16 [==============================] - 0s 1ms/step - loss: 1.9552
16/16 [==============================] - 0s 1ms/step - loss: 1.9551

Testing for epoch 27 index 3:
79/79 [==============================] - 0s 609us/step
16/16 [==============================] - 0s 795us/step - loss: 0.1802
16/16 [==============================] - 0s 1ms/step - loss: 1.5996
16/16 [==============================] - 0s 840us/step - loss: 1.8825
16/16 [==============================] - 0s 865us/step - loss: 1.9942
16/16 [==============================] - 0s 893us/step - loss: 2.0396
16/16 [==============================] - 0s 707us/step - loss: 2.0499
16/16 [==============================] - 0s 796us/step - loss: 2.0429
16/16 [==============================] - 0s 686us/step - loss: 2.0363
16/16 [==============================] - 0s 703us/step - loss: 2.0335
16/16 [==============================] - 0s 713us/step - loss: 2.0335

Testing for epoch 27 index 4:
79/79 [==============================] - 0s 763us/step
16/16 [==============================] - 0s 845us/step - loss: 0.1746
16/16 [==============================] - 0s 1ms/step - loss: 1.6245
16/16 [==============================] - 0s 1ms/step - loss: 1.9167
16/16 [==============================] - 0s 796us/step - loss: 2.0309
16/16 [==============================] - 0s 813us/step - loss: 2.0769
16/16 [==============================] - 0s 803us/step - loss: 2.0875
16/16 [==============================] - 0s 811us/step - loss: 2.0801
16/16 [==============================] - 0s 796us/step - loss: 2.0733
16/16 [==============================] - 0s 815us/step - loss: 2.0704
16/16 [==============================] - 0s 790us/step - loss: 2.0703

Testing for epoch 27 index 5:
79/79 [==============================] - 0s 581us/step
16/16 [==============================] - 0s 831us/step - loss: 0.1721
16/16 [==============================] - 0s 817us/step - loss: 1.6490
16/16 [==============================] - 0s 829us/step - loss: 1.9488
16/16 [==============================] - 0s 804us/step - loss: 2.0650
16/16 [==============================] - 0s 814us/step - loss: 2.1109
16/16 [==============================] - 0s 828us/step - loss: 2.1218
16/16 [==============================] - 0s 836us/step - loss: 2.1139
16/16 [==============================] - 0s 2ms/step - loss: 2.1069
16/16 [==============================] - 0s 2ms/step - loss: 2.1038
16/16 [==============================] - 0s 2ms/step - loss: 2.1037
Epoch 28 of 60

Testing for epoch 28 index 1:
79/79 [==============================] - 0s 577us/step
16/16 [==============================] - 0s 842us/step - loss: 0.1745
16/16 [==============================] - 0s 841us/step - loss: 1.6265
16/16 [==============================] - 0s 856us/step - loss: 1.9218
16/16 [==============================] - 0s 844us/step - loss: 2.0351
16/16 [==============================] - 0s 870us/step - loss: 2.0786
16/16 [==============================] - 0s 884us/step - loss: 2.0882
16/16 [==============================] - 0s 929us/step - loss: 2.0794
16/16 [==============================] - 0s 911us/step - loss: 2.0722
16/16 [==============================] - 0s 983us/step - loss: 2.0691
16/16 [==============================] - 0s 924us/step - loss: 2.0690

Testing for epoch 28 index 2:
79/79 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1718
16/16 [==============================] - 0s 4ms/step - loss: 1.6199
16/16 [==============================] - 0s 2ms/step - loss: 1.9153
16/16 [==============================] - 0s 1ms/step - loss: 2.0279
16/16 [==============================] - 0s 2ms/step - loss: 2.0710
16/16 [==============================] - 0s 2ms/step - loss: 2.0805
16/16 [==============================] - 0s 1ms/step - loss: 2.0718
16/16 [==============================] - 0s 2ms/step - loss: 2.0644
16/16 [==============================] - 0s 2ms/step - loss: 2.0613
16/16 [==============================] - 0s 2ms/step - loss: 2.0612

Testing for epoch 28 index 3:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1764
16/16 [==============================] - 0s 2ms/step - loss: 1.5756
16/16 [==============================] - 0s 992us/step - loss: 1.8577
16/16 [==============================] - 0s 1ms/step - loss: 1.9628
16/16 [==============================] - 0s 4ms/step - loss: 2.0023
16/16 [==============================] - 0s 1ms/step - loss: 2.0074
16/16 [==============================] - 0s 2ms/step - loss: 1.9974
16/16 [==============================] - 0s 2ms/step - loss: 1.9900
16/16 [==============================] - 0s 2ms/step - loss: 1.9869
16/16 [==============================] - 0s 2ms/step - loss: 1.9867

Testing for epoch 28 index 4:
79/79 [==============================] - 0s 897us/step
16/16 [==============================] - 0s 991us/step - loss: 0.1714
16/16 [==============================] - 0s 2ms/step - loss: 1.5748
16/16 [==============================] - 0s 2ms/step - loss: 1.8565
16/16 [==============================] - 0s 1ms/step - loss: 1.9601
16/16 [==============================] - 0s 2ms/step - loss: 1.9993
16/16 [==============================] - 0s 2ms/step - loss: 2.0048
16/16 [==============================] - 0s 2ms/step - loss: 1.9951
16/16 [==============================] - 0s 1ms/step - loss: 1.9878
16/16 [==============================] - 0s 1ms/step - loss: 1.9850
16/16 [==============================] - 0s 2ms/step - loss: 1.9849

Testing for epoch 28 index 5:
79/79 [==============================] - 0s 947us/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1700
16/16 [==============================] - 0s 1ms/step - loss: 1.6374
16/16 [==============================] - 0s 1ms/step - loss: 1.9381
16/16 [==============================] - 0s 1ms/step - loss: 2.0497
16/16 [==============================] - 0s 1ms/step - loss: 2.0926
16/16 [==============================] - 0s 1ms/step - loss: 2.0984
16/16 [==============================] - 0s 1ms/step - loss: 2.0886
16/16 [==============================] - 0s 1ms/step - loss: 2.0809
16/16 [==============================] - 0s 1ms/step - loss: 2.0778
16/16 [==============================] - 0s 2ms/step - loss: 2.0776
Epoch 29 of 60

Testing for epoch 29 index 1:
79/79 [==============================] - 0s 878us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1688
16/16 [==============================] - 0s 2ms/step - loss: 1.6160
16/16 [==============================] - 0s 2ms/step - loss: 1.9106
16/16 [==============================] - 0s 1ms/step - loss: 2.0177
16/16 [==============================] - 0s 1ms/step - loss: 2.0581
16/16 [==============================] - 0s 1ms/step - loss: 2.0626
16/16 [==============================] - 0s 2ms/step - loss: 2.0522
16/16 [==============================] - 0s 1ms/step - loss: 2.0446
16/16 [==============================] - 0s 1ms/step - loss: 2.0417
16/16 [==============================] - 0s 3ms/step - loss: 2.0417

Testing for epoch 29 index 2:
79/79 [==============================] - 0s 967us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1690
16/16 [==============================] - 0s 1ms/step - loss: 1.6719
16/16 [==============================] - 0s 1ms/step - loss: 1.9800
16/16 [==============================] - 0s 2ms/step - loss: 2.0915
16/16 [==============================] - 0s 1ms/step - loss: 2.1340
16/16 [==============================] - 0s 1ms/step - loss: 2.1376
16/16 [==============================] - 0s 1ms/step - loss: 2.1260
16/16 [==============================] - 0s 1ms/step - loss: 2.1178
16/16 [==============================] - 0s 964us/step - loss: 2.1145
16/16 [==============================] - 0s 1ms/step - loss: 2.1144

Testing for epoch 29 index 3:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1690
16/16 [==============================] - 0s 2ms/step - loss: 1.5813
16/16 [==============================] - 0s 1ms/step - loss: 1.8637
16/16 [==============================] - 0s 1ms/step - loss: 1.9638
16/16 [==============================] - 0s 2ms/step - loss: 2.0025
16/16 [==============================] - 0s 2ms/step - loss: 2.0058
16/16 [==============================] - 0s 2ms/step - loss: 1.9953
16/16 [==============================] - 0s 2ms/step - loss: 1.9879
16/16 [==============================] - 0s 2ms/step - loss: 1.9850
16/16 [==============================] - 0s 1ms/step - loss: 1.9849

Testing for epoch 29 index 4:
79/79 [==============================] - 0s 3ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1653
16/16 [==============================] - 0s 2ms/step - loss: 1.7200
16/16 [==============================] - 0s 2ms/step - loss: 2.0423
16/16 [==============================] - 0s 1ms/step - loss: 2.1567
16/16 [==============================] - 0s 1ms/step - loss: 2.2005
16/16 [==============================] - 0s 1ms/step - loss: 2.2035
16/16 [==============================] - 0s 1ms/step - loss: 2.1900
16/16 [==============================] - 0s 1ms/step - loss: 2.1809
16/16 [==============================] - 0s 2ms/step - loss: 2.1772
16/16 [==============================] - 0s 3ms/step - loss: 2.1770

Testing for epoch 29 index 5:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1656
16/16 [==============================] - 0s 2ms/step - loss: 1.6312
16/16 [==============================] - 0s 1ms/step - loss: 1.9296
16/16 [==============================] - 0s 1ms/step - loss: 2.0334
16/16 [==============================] - 0s 1ms/step - loss: 2.0736
16/16 [==============================] - 0s 1ms/step - loss: 2.0773
16/16 [==============================] - 0s 1ms/step - loss: 2.0655
16/16 [==============================] - 0s 2ms/step - loss: 2.0575
16/16 [==============================] - 0s 2ms/step - loss: 2.0543
16/16 [==============================] - 0s 2ms/step - loss: 2.0542
Epoch 30 of 60

Testing for epoch 30 index 1:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1641
16/16 [==============================] - 0s 1ms/step - loss: 1.6943
16/16 [==============================] - 0s 1ms/step - loss: 2.0115
16/16 [==============================] - 0s 1ms/step - loss: 2.1206
16/16 [==============================] - 0s 2ms/step - loss: 2.1614
16/16 [==============================] - 0s 1ms/step - loss: 2.1623
16/16 [==============================] - 0s 933us/step - loss: 2.1487
16/16 [==============================] - 0s 2ms/step - loss: 2.1400
16/16 [==============================] - 0s 2ms/step - loss: 2.1366
16/16 [==============================] - 0s 1ms/step - loss: 2.1364

Testing for epoch 30 index 2:
79/79 [==============================] - 0s 864us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1658
16/16 [==============================] - 0s 1ms/step - loss: 1.6611
16/16 [==============================] - 0s 1ms/step - loss: 1.9702
16/16 [==============================] - 0s 1ms/step - loss: 2.0763
16/16 [==============================] - 0s 1ms/step - loss: 2.1169
16/16 [==============================] - 0s 1ms/step - loss: 2.1173
16/16 [==============================] - 0s 1ms/step - loss: 2.1036
16/16 [==============================] - 0s 1ms/step - loss: 2.0951
16/16 [==============================] - 0s 1ms/step - loss: 2.0918
16/16 [==============================] - 0s 2ms/step - loss: 2.0917

Testing for epoch 30 index 3:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1678
16/16 [==============================] - 0s 2ms/step - loss: 1.6270
16/16 [==============================] - 0s 2ms/step - loss: 1.9300
16/16 [==============================] - 0s 2ms/step - loss: 2.0320
16/16 [==============================] - 0s 2ms/step - loss: 2.0718
16/16 [==============================] - 0s 2ms/step - loss: 2.0723
16/16 [==============================] - 0s 1ms/step - loss: 2.0596
16/16 [==============================] - 0s 2ms/step - loss: 2.0514
16/16 [==============================] - 0s 2ms/step - loss: 2.0481
16/16 [==============================] - 0s 2ms/step - loss: 2.0479

Testing for epoch 30 index 4:
79/79 [==============================] - 0s 932us/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1629
16/16 [==============================] - 0s 2ms/step - loss: 1.6611
16/16 [==============================] - 0s 2ms/step - loss: 1.9644
16/16 [==============================] - 0s 2ms/step - loss: 2.0613
16/16 [==============================] - 0s 1ms/step - loss: 2.0966
16/16 [==============================] - 0s 3ms/step - loss: 2.0905
16/16 [==============================] - 0s 2ms/step - loss: 2.0735
16/16 [==============================] - 0s 1ms/step - loss: 2.0643
16/16 [==============================] - 0s 2ms/step - loss: 2.0611
16/16 [==============================] - 0s 1ms/step - loss: 2.0611

Testing for epoch 30 index 5:
79/79 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1580
16/16 [==============================] - 0s 1ms/step - loss: 1.7343
16/16 [==============================] - 0s 926us/step - loss: 2.0589
16/16 [==============================] - 0s 3ms/step - loss: 2.1636
16/16 [==============================] - 0s 2ms/step - loss: 2.2026
16/16 [==============================] - 0s 960us/step - loss: 2.1988
16/16 [==============================] - 0s 2ms/step - loss: 2.1826
16/16 [==============================] - 0s 2ms/step - loss: 2.1732
16/16 [==============================] - 0s 2ms/step - loss: 2.1697
16/16 [==============================] - 0s 2ms/step - loss: 2.1695
Epoch 31 of 60

Testing for epoch 31 index 1:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1629
16/16 [==============================] - 0s 2ms/step - loss: 1.6340
16/16 [==============================] - 0s 2ms/step - loss: 1.9369
16/16 [==============================] - 0s 4ms/step - loss: 2.0366
16/16 [==============================] - 0s 1ms/step - loss: 2.0749
16/16 [==============================] - 0s 2ms/step - loss: 2.0736
16/16 [==============================] - 0s 1ms/step - loss: 2.0595
16/16 [==============================] - 0s 2ms/step - loss: 2.0507
16/16 [==============================] - 0s 1ms/step - loss: 2.0472
16/16 [==============================] - 0s 2ms/step - loss: 2.0470

Testing for epoch 31 index 2:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1602
16/16 [==============================] - 0s 2ms/step - loss: 1.6604
16/16 [==============================] - 0s 1ms/step - loss: 1.9645
16/16 [==============================] - 0s 2ms/step - loss: 2.0619
16/16 [==============================] - 0s 1ms/step - loss: 2.0977
16/16 [==============================] - 0s 2ms/step - loss: 2.0922
16/16 [==============================] - 0s 1ms/step - loss: 2.0766
16/16 [==============================] - 0s 2ms/step - loss: 2.0677
16/16 [==============================] - 0s 2ms/step - loss: 2.0645
16/16 [==============================] - 0s 2ms/step - loss: 2.0644

Testing for epoch 31 index 3:
79/79 [==============================] - 0s 814us/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1592
16/16 [==============================] - 0s 3ms/step - loss: 1.7109
16/16 [==============================] - 0s 2ms/step - loss: 2.0223
16/16 [==============================] - 0s 1ms/step - loss: 2.1224
16/16 [==============================] - 0s 1ms/step - loss: 2.1597
16/16 [==============================] - 0s 977us/step - loss: 2.1517
16/16 [==============================] - 0s 961us/step - loss: 2.1344
16/16 [==============================] - 0s 2ms/step - loss: 2.1248
16/16 [==============================] - 0s 1ms/step - loss: 2.1212
16/16 [==============================] - 0s 1ms/step - loss: 2.1210

Testing for epoch 31 index 4:
79/79 [==============================] - 0s 834us/step
16/16 [==============================] - 0s 975us/step - loss: 0.1591
16/16 [==============================] - 0s 1ms/step - loss: 1.7203
16/16 [==============================] - 0s 1ms/step - loss: 2.0343
16/16 [==============================] - 0s 4ms/step - loss: 2.1337
16/16 [==============================] - 0s 1ms/step - loss: 2.1707
16/16 [==============================] - 0s 2ms/step - loss: 2.1621
16/16 [==============================] - 0s 2ms/step - loss: 2.1441
16/16 [==============================] - 0s 2ms/step - loss: 2.1342
16/16 [==============================] - 0s 1ms/step - loss: 2.1305
16/16 [==============================] - 0s 1ms/step - loss: 2.1303

Testing for epoch 31 index 5:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1568
16/16 [==============================] - 0s 2ms/step - loss: 1.7400
16/16 [==============================] - 0s 2ms/step - loss: 2.0591
16/16 [==============================] - 0s 2ms/step - loss: 2.1605
16/16 [==============================] - 0s 2ms/step - loss: 2.1977
16/16 [==============================] - 0s 2ms/step - loss: 2.1903
16/16 [==============================] - 0s 1ms/step - loss: 2.1724
16/16 [==============================] - 0s 2ms/step - loss: 2.1626
16/16 [==============================] - 0s 2ms/step - loss: 2.1590
16/16 [==============================] - 0s 1ms/step - loss: 2.1589
Epoch 32 of 60

Testing for epoch 32 index 1:
79/79 [==============================] - 0s 870us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1585
16/16 [==============================] - 0s 2ms/step - loss: 1.7001
16/16 [==============================] - 0s 1ms/step - loss: 2.0088
16/16 [==============================] - 0s 2ms/step - loss: 2.1060
16/16 [==============================] - 0s 2ms/step - loss: 2.1420
16/16 [==============================] - 0s 2ms/step - loss: 2.1348
16/16 [==============================] - 0s 2ms/step - loss: 2.1169
16/16 [==============================] - 0s 2ms/step - loss: 2.1072
16/16 [==============================] - 0s 4ms/step - loss: 2.1037
16/16 [==============================] - 0s 2ms/step - loss: 2.1035

Testing for epoch 32 index 2:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 899us/step - loss: 0.1574
16/16 [==============================] - 0s 1ms/step - loss: 1.7923
16/16 [==============================] - 0s 1ms/step - loss: 2.1196
16/16 [==============================] - 0s 2ms/step - loss: 2.2226
16/16 [==============================] - 0s 990us/step - loss: 2.2597
16/16 [==============================] - 0s 1ms/step - loss: 2.2503
16/16 [==============================] - 0s 1ms/step - loss: 2.2299
16/16 [==============================] - 0s 962us/step - loss: 2.2193
16/16 [==============================] - 0s 1ms/step - loss: 2.2156
16/16 [==============================] - 0s 996us/step - loss: 2.2155

Testing for epoch 32 index 3:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 7ms/step - loss: 0.1557
16/16 [==============================] - 0s 2ms/step - loss: 1.7954
16/16 [==============================] - 0s 2ms/step - loss: 2.1247
16/16 [==============================] - 0s 2ms/step - loss: 2.2272
16/16 [==============================] - 0s 2ms/step - loss: 2.2635
16/16 [==============================] - 0s 2ms/step - loss: 2.2531
16/16 [==============================] - 0s 2ms/step - loss: 2.2322
16/16 [==============================] - 0s 2ms/step - loss: 2.2215
16/16 [==============================] - 0s 2ms/step - loss: 2.2178
16/16 [==============================] - 0s 2ms/step - loss: 2.2177

Testing for epoch 32 index 4:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1537
16/16 [==============================] - 0s 1ms/step - loss: 1.7869
16/16 [==============================] - 0s 1ms/step - loss: 2.1168
16/16 [==============================] - 0s 1ms/step - loss: 2.2210
16/16 [==============================] - 0s 1ms/step - loss: 2.2596
16/16 [==============================] - 0s 1ms/step - loss: 2.2526
16/16 [==============================] - 0s 992us/step - loss: 2.2329
16/16 [==============================] - 0s 969us/step - loss: 2.2226
16/16 [==============================] - 0s 1ms/step - loss: 2.2187
16/16 [==============================] - 0s 1ms/step - loss: 2.2184

Testing for epoch 32 index 5:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1510
16/16 [==============================] - 0s 2ms/step - loss: 1.7654
16/16 [==============================] - 0s 1ms/step - loss: 2.0879
16/16 [==============================] - 0s 2ms/step - loss: 2.1879
16/16 [==============================] - 0s 1ms/step - loss: 2.2230
16/16 [==============================] - 0s 1ms/step - loss: 2.2123
16/16 [==============================] - 0s 1ms/step - loss: 2.1914
16/16 [==============================] - 0s 1ms/step - loss: 2.1809
16/16 [==============================] - 0s 1ms/step - loss: 2.1770
16/16 [==============================] - 0s 1ms/step - loss: 2.1768
Epoch 33 of 60

Testing for epoch 33 index 1:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1555
16/16 [==============================] - 0s 1ms/step - loss: 1.7559
16/16 [==============================] - 0s 2ms/step - loss: 2.0784
16/16 [==============================] - 0s 2ms/step - loss: 2.1784
16/16 [==============================] - 0s 4ms/step - loss: 2.2136
16/16 [==============================] - 0s 2ms/step - loss: 2.2035
16/16 [==============================] - 0s 2ms/step - loss: 2.1825
16/16 [==============================] - 0s 1ms/step - loss: 2.1721
16/16 [==============================] - 0s 3ms/step - loss: 2.1683
16/16 [==============================] - 0s 2ms/step - loss: 2.1681

Testing for epoch 33 index 2:
79/79 [==============================] - 0s 2ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1507
16/16 [==============================] - 0s 932us/step - loss: 1.7736
16/16 [==============================] - 0s 877us/step - loss: 2.0996
16/16 [==============================] - 0s 821us/step - loss: 2.2003
16/16 [==============================] - 0s 801us/step - loss: 2.2338
16/16 [==============================] - 0s 821us/step - loss: 2.2216
16/16 [==============================] - 0s 842us/step - loss: 2.2004
16/16 [==============================] - 0s 791us/step - loss: 2.1900
16/16 [==============================] - 0s 787us/step - loss: 2.1864
16/16 [==============================] - 0s 816us/step - loss: 2.1863

Testing for epoch 33 index 3:
79/79 [==============================] - 0s 589us/step
16/16 [==============================] - 0s 798us/step - loss: 0.1522
16/16 [==============================] - 0s 829us/step - loss: 1.7884
16/16 [==============================] - 0s 806us/step - loss: 2.1161
16/16 [==============================] - 0s 793us/step - loss: 2.2152
16/16 [==============================] - 0s 777us/step - loss: 2.2465
16/16 [==============================] - 0s 780us/step - loss: 2.2315
16/16 [==============================] - 0s 794us/step - loss: 2.2087
16/16 [==============================] - 0s 783us/step - loss: 2.1979
16/16 [==============================] - 0s 824us/step - loss: 2.1942
16/16 [==============================] - 0s 1ms/step - loss: 2.1941

Testing for epoch 33 index 4:
79/79 [==============================] - 0s 600us/step
16/16 [==============================] - 0s 818us/step - loss: 0.1512
16/16 [==============================] - 0s 838us/step - loss: 1.7421
16/16 [==============================] - 0s 848us/step - loss: 2.0614
16/16 [==============================] - 0s 818us/step - loss: 2.1598
16/16 [==============================] - 0s 791us/step - loss: 2.1919
16/16 [==============================] - 0s 785us/step - loss: 2.1795
16/16 [==============================] - 0s 815us/step - loss: 2.1576
16/16 [==============================] - 0s 800us/step - loss: 2.1471
16/16 [==============================] - 0s 835us/step - loss: 2.1433
16/16 [==============================] - 0s 804us/step - loss: 2.1431

Testing for epoch 33 index 5:
79/79 [==============================] - 0s 587us/step
16/16 [==============================] - 0s 850us/step - loss: 0.1533
16/16 [==============================] - 0s 823us/step - loss: 1.7343
16/16 [==============================] - 0s 841us/step - loss: 2.0496
16/16 [==============================] - 0s 819us/step - loss: 2.1449
16/16 [==============================] - 0s 840us/step - loss: 2.1741
16/16 [==============================] - 0s 829us/step - loss: 2.1592
16/16 [==============================] - 0s 810us/step - loss: 2.1358
16/16 [==============================] - 0s 784us/step - loss: 2.1250
16/16 [==============================] - 0s 834us/step - loss: 2.1212
16/16 [==============================] - 0s 850us/step - loss: 2.1210
Epoch 34 of 60

Testing for epoch 34 index 1:
79/79 [==============================] - 0s 583us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1524
16/16 [==============================] - 0s 1ms/step - loss: 1.7155
16/16 [==============================] - 0s 1ms/step - loss: 2.0298
16/16 [==============================] - 0s 1ms/step - loss: 2.1247
16/16 [==============================] - 0s 816us/step - loss: 2.1539
16/16 [==============================] - 0s 845us/step - loss: 2.1409
16/16 [==============================] - 0s 784us/step - loss: 2.1187
16/16 [==============================] - 0s 1ms/step - loss: 2.1082
16/16 [==============================] - 0s 1ms/step - loss: 2.1044
16/16 [==============================] - 0s 970us/step - loss: 2.1041

Testing for epoch 34 index 2:
79/79 [==============================] - 0s 584us/step
16/16 [==============================] - 0s 797us/step - loss: 0.1485
16/16 [==============================] - 0s 826us/step - loss: 1.7384
16/16 [==============================] - 0s 794us/step - loss: 2.0580
16/16 [==============================] - 0s 786us/step - loss: 2.1532
16/16 [==============================] - 0s 782us/step - loss: 2.1809
16/16 [==============================] - 0s 782us/step - loss: 2.1669
16/16 [==============================] - 0s 792us/step - loss: 2.1444
16/16 [==============================] - 0s 779us/step - loss: 2.1341
16/16 [==============================] - 0s 782us/step - loss: 2.1306
16/16 [==============================] - 0s 828us/step - loss: 2.1305

Testing for epoch 34 index 3:
79/79 [==============================] - 0s 603us/step
16/16 [==============================] - 0s 809us/step - loss: 0.1477
16/16 [==============================] - 0s 1ms/step - loss: 1.7757
16/16 [==============================] - 0s 1ms/step - loss: 2.1041
16/16 [==============================] - 0s 820us/step - loss: 2.1998
16/16 [==============================] - 0s 783us/step - loss: 2.2271
16/16 [==============================] - 0s 774us/step - loss: 2.2111
16/16 [==============================] - 0s 783us/step - loss: 2.1877
16/16 [==============================] - 0s 1ms/step - loss: 2.1770
16/16 [==============================] - 0s 772us/step - loss: 2.1732
16/16 [==============================] - 0s 817us/step - loss: 2.1731

Testing for epoch 34 index 4:
79/79 [==============================] - 0s 576us/step
16/16 [==============================] - 0s 807us/step - loss: 0.1443
16/16 [==============================] - 0s 797us/step - loss: 1.8313
16/16 [==============================] - 0s 766us/step - loss: 2.1753
16/16 [==============================] - 0s 775us/step - loss: 2.2750
16/16 [==============================] - 0s 775us/step - loss: 2.3028
16/16 [==============================] - 0s 782us/step - loss: 2.2844
16/16 [==============================] - 0s 778us/step - loss: 2.2581
16/16 [==============================] - 0s 780us/step - loss: 2.2464
16/16 [==============================] - 0s 793us/step - loss: 2.2425
16/16 [==============================] - 0s 798us/step - loss: 2.2424

Testing for epoch 34 index 5:
79/79 [==============================] - 0s 587us/step
16/16 [==============================] - 0s 779us/step - loss: 0.1507
16/16 [==============================] - 0s 1ms/step - loss: 1.7991
16/16 [==============================] - 0s 1ms/step - loss: 2.1385
16/16 [==============================] - 0s 805us/step - loss: 2.2388
16/16 [==============================] - 0s 777us/step - loss: 2.2689
16/16 [==============================] - 0s 769us/step - loss: 2.2553
16/16 [==============================] - 0s 770us/step - loss: 2.2330
16/16 [==============================] - 0s 766us/step - loss: 2.2225
16/16 [==============================] - 0s 796us/step - loss: 2.2189
16/16 [==============================] - 0s 802us/step - loss: 2.2187
Epoch 35 of 60

Testing for epoch 35 index 1:
79/79 [==============================] - 0s 580us/step
16/16 [==============================] - 0s 810us/step - loss: 0.1468
16/16 [==============================] - 0s 1000us/step - loss: 1.8167
16/16 [==============================] - 0s 1ms/step - loss: 2.1570
16/16 [==============================] - 0s 783us/step - loss: 2.2540
16/16 [==============================] - 0s 816us/step - loss: 2.2807
16/16 [==============================] - 0s 980us/step - loss: 2.2621
16/16 [==============================] - 0s 787us/step - loss: 2.2360
16/16 [==============================] - 0s 814us/step - loss: 2.2244
16/16 [==============================] - 0s 805us/step - loss: 2.2204
16/16 [==============================] - 0s 832us/step - loss: 2.2203

Testing for epoch 35 index 2:
79/79 [==============================] - 0s 584us/step
16/16 [==============================] - 0s 802us/step - loss: 0.1469
16/16 [==============================] - 0s 791us/step - loss: 1.8178
16/16 [==============================] - 0s 775us/step - loss: 2.1558
16/16 [==============================] - 0s 784us/step - loss: 2.2515
16/16 [==============================] - 0s 796us/step - loss: 2.2769
16/16 [==============================] - 0s 783us/step - loss: 2.2571
16/16 [==============================] - 0s 802us/step - loss: 2.2299
16/16 [==============================] - 0s 779us/step - loss: 2.2181
16/16 [==============================] - 0s 811us/step - loss: 2.2141
16/16 [==============================] - 0s 798us/step - loss: 2.2140

Testing for epoch 35 index 3:
79/79 [==============================] - 0s 586us/step
16/16 [==============================] - 0s 814us/step - loss: 0.1499
16/16 [==============================] - 0s 823us/step - loss: 1.7804
16/16 [==============================] - 0s 814us/step - loss: 2.1084
16/16 [==============================] - 0s 789us/step - loss: 2.1988
16/16 [==============================] - 0s 779us/step - loss: 2.2227
16/16 [==============================] - 0s 772us/step - loss: 2.2030
16/16 [==============================] - 0s 794us/step - loss: 2.1772
16/16 [==============================] - 0s 787us/step - loss: 2.1661
16/16 [==============================] - 0s 822us/step - loss: 2.1624
16/16 [==============================] - 0s 797us/step - loss: 2.1623

Testing for epoch 35 index 4:
79/79 [==============================] - 0s 584us/step
16/16 [==============================] - 0s 813us/step - loss: 0.1474
16/16 [==============================] - 0s 804us/step - loss: 1.7882
16/16 [==============================] - 0s 816us/step - loss: 2.1206
16/16 [==============================] - 0s 826us/step - loss: 2.2126
16/16 [==============================] - 0s 844us/step - loss: 2.2362
16/16 [==============================] - 0s 804us/step - loss: 2.2145
16/16 [==============================] - 0s 1ms/step - loss: 2.1867
16/16 [==============================] - 0s 881us/step - loss: 2.1749
16/16 [==============================] - 0s 852us/step - loss: 2.1709
16/16 [==============================] - 0s 785us/step - loss: 2.1708

Testing for epoch 35 index 5:
79/79 [==============================] - 0s 585us/step
16/16 [==============================] - 0s 861us/step - loss: 0.1426
16/16 [==============================] - 0s 1ms/step - loss: 1.8107
16/16 [==============================] - 0s 762us/step - loss: 2.1481
16/16 [==============================] - 0s 757us/step - loss: 2.2402
16/16 [==============================] - 0s 771us/step - loss: 2.2636
16/16 [==============================] - 0s 772us/step - loss: 2.2406
16/16 [==============================] - 0s 768us/step - loss: 2.2119
16/16 [==============================] - 0s 766us/step - loss: 2.1997
16/16 [==============================] - 0s 765us/step - loss: 2.1956
16/16 [==============================] - 0s 771us/step - loss: 2.1955
Epoch 36 of 60

Testing for epoch 36 index 1:
79/79 [==============================] - 0s 577us/step
16/16 [==============================] - 0s 782us/step - loss: 0.1440
16/16 [==============================] - 0s 780us/step - loss: 1.8571
16/16 [==============================] - 0s 775us/step - loss: 2.2097
16/16 [==============================] - 0s 779us/step - loss: 2.3070
16/16 [==============================] - 0s 776us/step - loss: 2.3319
16/16 [==============================] - 0s 788us/step - loss: 2.3098
16/16 [==============================] - 0s 1ms/step - loss: 2.2808
16/16 [==============================] - 0s 1ms/step - loss: 2.2685
16/16 [==============================] - 0s 800us/step - loss: 2.2645
16/16 [==============================] - 0s 891us/step - loss: 2.2644

Testing for epoch 36 index 2:
79/79 [==============================] - 0s 693us/step
16/16 [==============================] - 0s 833us/step - loss: 0.1417
16/16 [==============================] - 0s 814us/step - loss: 1.8655
16/16 [==============================] - 0s 906us/step - loss: 2.2205
16/16 [==============================] - 0s 829us/step - loss: 2.3178
16/16 [==============================] - 0s 872us/step - loss: 2.3425
16/16 [==============================] - 0s 864us/step - loss: 2.3198
16/16 [==============================] - 0s 792us/step - loss: 2.2908
16/16 [==============================] - 0s 803us/step - loss: 2.2784
16/16 [==============================] - 0s 796us/step - loss: 2.2743
16/16 [==============================] - 0s 821us/step - loss: 2.2743

Testing for epoch 36 index 3:
79/79 [==============================] - 0s 620us/step
16/16 [==============================] - 0s 820us/step - loss: 0.1445
16/16 [==============================] - 0s 830us/step - loss: 1.8152
16/16 [==============================] - 0s 800us/step - loss: 2.1608
16/16 [==============================] - 0s 826us/step - loss: 2.2564
16/16 [==============================] - 0s 814us/step - loss: 2.2806
16/16 [==============================] - 0s 806us/step - loss: 2.2592
16/16 [==============================] - 0s 792us/step - loss: 2.2315
16/16 [==============================] - 0s 823us/step - loss: 2.2197
16/16 [==============================] - 0s 819us/step - loss: 2.2156
16/16 [==============================] - 0s 807us/step - loss: 2.2155

Testing for epoch 36 index 4:
79/79 [==============================] - 0s 588us/step
16/16 [==============================] - 0s 793us/step - loss: 0.1423
16/16 [==============================] - 0s 803us/step - loss: 1.8472
16/16 [==============================] - 0s 782us/step - loss: 2.1950
16/16 [==============================] - 0s 816us/step - loss: 2.2881
16/16 [==============================] - 0s 794us/step - loss: 2.3102
16/16 [==============================] - 0s 786us/step - loss: 2.2857
16/16 [==============================] - 0s 786us/step - loss: 2.2565
16/16 [==============================] - 0s 794us/step - loss: 2.2442
16/16 [==============================] - 0s 788us/step - loss: 2.2402
16/16 [==============================] - 0s 792us/step - loss: 2.2401

Testing for epoch 36 index 5:
79/79 [==============================] - 0s 659us/step
16/16 [==============================] - 0s 872us/step - loss: 0.1383
16/16 [==============================] - 0s 805us/step - loss: 1.8481
16/16 [==============================] - 0s 806us/step - loss: 2.2002
16/16 [==============================] - 0s 807us/step - loss: 2.2955
16/16 [==============================] - 0s 810us/step - loss: 2.3191
16/16 [==============================] - 0s 818us/step - loss: 2.2953
16/16 [==============================] - 0s 846us/step - loss: 2.2663
16/16 [==============================] - 0s 817us/step - loss: 2.2540
16/16 [==============================] - 0s 842us/step - loss: 2.2498
16/16 [==============================] - 0s 808us/step - loss: 2.2496
Epoch 37 of 60

Testing for epoch 37 index 1:
79/79 [==============================] - 0s 590us/step
16/16 [==============================] - 0s 803us/step - loss: 0.1396
16/16 [==============================] - 0s 800us/step - loss: 1.8411
16/16 [==============================] - 0s 794us/step - loss: 2.1902
16/16 [==============================] - 0s 789us/step - loss: 2.2840
16/16 [==============================] - 0s 805us/step - loss: 2.3066
16/16 [==============================] - 0s 794us/step - loss: 2.2821
16/16 [==============================] - 0s 787us/step - loss: 2.2532
16/16 [==============================] - 0s 790us/step - loss: 2.2411
16/16 [==============================] - 0s 814us/step - loss: 2.2372
16/16 [==============================] - 0s 796us/step - loss: 2.2371

Testing for epoch 37 index 2:
79/79 [==============================] - 0s 588us/step
16/16 [==============================] - 0s 798us/step - loss: 0.1395
16/16 [==============================] - 0s 832us/step - loss: 1.8573
16/16 [==============================] - 0s 835us/step - loss: 2.2095
16/16 [==============================] - 0s 826us/step - loss: 2.3029
16/16 [==============================] - 0s 804us/step - loss: 2.3241
16/16 [==============================] - 0s 805us/step - loss: 2.2983
16/16 [==============================] - 0s 796us/step - loss: 2.2682
16/16 [==============================] - 0s 807us/step - loss: 2.2556
16/16 [==============================] - 0s 1ms/step - loss: 2.2513
16/16 [==============================] - 0s 1ms/step - loss: 2.2511

Testing for epoch 37 index 3:
79/79 [==============================] - 0s 584us/step
16/16 [==============================] - 0s 798us/step - loss: 0.1401
16/16 [==============================] - 0s 796us/step - loss: 1.8239
16/16 [==============================] - 0s 798us/step - loss: 2.1752
16/16 [==============================] - 0s 804us/step - loss: 2.2721
16/16 [==============================] - 0s 838us/step - loss: 2.2980
16/16 [==============================] - 0s 841us/step - loss: 2.2774
16/16 [==============================] - 0s 820us/step - loss: 2.2510
16/16 [==============================] - 0s 694us/step - loss: 2.2395
16/16 [==============================] - 0s 737us/step - loss: 2.2355
16/16 [==============================] - 0s 689us/step - loss: 2.2353

Testing for epoch 37 index 4:
79/79 [==============================] - 0s 668us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1385
16/16 [==============================] - 0s 2ms/step - loss: 1.8566
16/16 [==============================] - 0s 2ms/step - loss: 2.2037
16/16 [==============================] - 0s 2ms/step - loss: 2.2918
16/16 [==============================] - 0s 2ms/step - loss: 2.3097
16/16 [==============================] - 0s 844us/step - loss: 2.2792
16/16 [==============================] - 0s 855us/step - loss: 2.2463
16/16 [==============================] - 0s 2ms/step - loss: 2.2331
16/16 [==============================] - 0s 2ms/step - loss: 2.2290
16/16 [==============================] - 0s 769us/step - loss: 2.2290

Testing for epoch 37 index 5:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 876us/step - loss: 0.1371
16/16 [==============================] - 0s 2ms/step - loss: 1.8841
16/16 [==============================] - 0s 781us/step - loss: 2.2415
16/16 [==============================] - 0s 754us/step - loss: 2.3349
16/16 [==============================] - 0s 831us/step - loss: 2.3552
16/16 [==============================] - 0s 788us/step - loss: 2.3269
16/16 [==============================] - 0s 832us/step - loss: 2.2956
16/16 [==============================] - 0s 797us/step - loss: 2.2825
16/16 [==============================] - 0s 2ms/step - loss: 2.2780
16/16 [==============================] - 0s 2ms/step - loss: 2.2778
Epoch 38 of 60

Testing for epoch 38 index 1:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1392
16/16 [==============================] - 0s 846us/step - loss: 1.9016
16/16 [==============================] - 0s 2ms/step - loss: 2.2615
16/16 [==============================] - 0s 2ms/step - loss: 2.3548
16/16 [==============================] - 0s 2ms/step - loss: 2.3744
16/16 [==============================] - 0s 783us/step - loss: 2.3445
16/16 [==============================] - 0s 1ms/step - loss: 2.3121
16/16 [==============================] - 0s 2ms/step - loss: 2.2989
16/16 [==============================] - 0s 2ms/step - loss: 2.2947
16/16 [==============================] - 0s 1ms/step - loss: 2.2946

Testing for epoch 38 index 2:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 804us/step - loss: 0.1370
16/16 [==============================] - 0s 2ms/step - loss: 1.8808
16/16 [==============================] - 0s 2ms/step - loss: 2.2309
16/16 [==============================] - 0s 803us/step - loss: 2.3252
16/16 [==============================] - 0s 2ms/step - loss: 2.3458
16/16 [==============================] - 0s 2ms/step - loss: 2.3187
16/16 [==============================] - 0s 2ms/step - loss: 2.2874
16/16 [==============================] - 0s 840us/step - loss: 2.2746
16/16 [==============================] - 0s 2ms/step - loss: 2.2703
16/16 [==============================] - 0s 805us/step - loss: 2.2702

Testing for epoch 38 index 3:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1335
16/16 [==============================] - 0s 2ms/step - loss: 1.9326
16/16 [==============================] - 0s 2ms/step - loss: 2.2893
16/16 [==============================] - 0s 804us/step - loss: 2.3820
16/16 [==============================] - 0s 2ms/step - loss: 2.3984
16/16 [==============================] - 0s 817us/step - loss: 2.3647
16/16 [==============================] - 0s 2ms/step - loss: 2.3292
16/16 [==============================] - 0s 2ms/step - loss: 2.3153
16/16 [==============================] - 0s 883us/step - loss: 2.3110
16/16 [==============================] - 0s 2ms/step - loss: 2.3111

Testing for epoch 38 index 4:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 849us/step - loss: 0.1333
16/16 [==============================] - 0s 805us/step - loss: 1.9265
16/16 [==============================] - 0s 793us/step - loss: 2.2845
16/16 [==============================] - 0s 815us/step - loss: 2.3802
16/16 [==============================] - 0s 808us/step - loss: 2.3993
16/16 [==============================] - 0s 818us/step - loss: 2.3696
16/16 [==============================] - 0s 2ms/step - loss: 2.3358
16/16 [==============================] - 0s 800us/step - loss: 2.3221
16/16 [==============================] - 0s 792us/step - loss: 2.3175
16/16 [==============================] - 0s 2ms/step - loss: 2.3173

Testing for epoch 38 index 5:
79/79 [==============================] - 0s 759us/step
16/16 [==============================] - 0s 844us/step - loss: 0.1361
16/16 [==============================] - 0s 833us/step - loss: 1.9262
16/16 [==============================] - 0s 799us/step - loss: 2.2858
16/16 [==============================] - 0s 2ms/step - loss: 2.3821
16/16 [==============================] - 0s 814us/step - loss: 2.4018
16/16 [==============================] - 0s 2ms/step - loss: 2.3724
16/16 [==============================] - 0s 2ms/step - loss: 2.3391
16/16 [==============================] - 0s 833us/step - loss: 2.3258
16/16 [==============================] - 0s 814us/step - loss: 2.3215
16/16 [==============================] - 0s 823us/step - loss: 2.3214
Epoch 39 of 60

Testing for epoch 39 index 1:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 833us/step - loss: 0.1330
16/16 [==============================] - 0s 1ms/step - loss: 1.8945
16/16 [==============================] - 0s 2ms/step - loss: 2.2436
16/16 [==============================] - 0s 1ms/step - loss: 2.3363
16/16 [==============================] - 0s 911us/step - loss: 2.3546
16/16 [==============================] - 0s 808us/step - loss: 2.3242
16/16 [==============================] - 0s 2ms/step - loss: 2.2909
16/16 [==============================] - 0s 2ms/step - loss: 2.2776
16/16 [==============================] - 0s 2ms/step - loss: 2.2733
16/16 [==============================] - 0s 791us/step - loss: 2.2731

Testing for epoch 39 index 2:
79/79 [==============================] - 0s 929us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1358
16/16 [==============================] - 0s 2ms/step - loss: 1.8447
16/16 [==============================] - 0s 832us/step - loss: 2.1816
16/16 [==============================] - 0s 795us/step - loss: 2.2687
16/16 [==============================] - 0s 807us/step - loss: 2.2848
16/16 [==============================] - 0s 2ms/step - loss: 2.2549
16/16 [==============================] - 0s 1ms/step - loss: 2.2232
16/16 [==============================] - 0s 919us/step - loss: 2.2103
16/16 [==============================] - 0s 2ms/step - loss: 2.2061
16/16 [==============================] - 0s 787us/step - loss: 2.2059

Testing for epoch 39 index 3:
79/79 [==============================] - 0s 533us/step
16/16 [==============================] - 0s 791us/step - loss: 0.1353
16/16 [==============================] - 0s 1ms/step - loss: 1.9032
16/16 [==============================] - 0s 2ms/step - loss: 2.2511
16/16 [==============================] - 0s 2ms/step - loss: 2.3409
16/16 [==============================] - 0s 780us/step - loss: 2.3580
16/16 [==============================] - 0s 891us/step - loss: 2.3288
16/16 [==============================] - 0s 863us/step - loss: 2.2962
16/16 [==============================] - 0s 876us/step - loss: 2.2830
16/16 [==============================] - 0s 1ms/step - loss: 2.2787
16/16 [==============================] - 0s 937us/step - loss: 2.2786

Testing for epoch 39 index 4:
79/79 [==============================] - 0s 781us/step
16/16 [==============================] - 0s 835us/step - loss: 0.1367
16/16 [==============================] - 0s 2ms/step - loss: 1.9084
16/16 [==============================] - 0s 990us/step - loss: 2.2575
16/16 [==============================] - 0s 814us/step - loss: 2.3489
16/16 [==============================] - 0s 777us/step - loss: 2.3668
16/16 [==============================] - 0s 782us/step - loss: 2.3375
16/16 [==============================] - 0s 714us/step - loss: 2.3043
16/16 [==============================] - 0s 1ms/step - loss: 2.2911
16/16 [==============================] - 0s 649us/step - loss: 2.2868
16/16 [==============================] - 0s 790us/step - loss: 2.2867

Testing for epoch 39 index 5:
79/79 [==============================] - 0s 661us/step
16/16 [==============================] - 0s 818us/step - loss: 0.1330
16/16 [==============================] - 0s 780us/step - loss: 1.9413
16/16 [==============================] - 0s 778us/step - loss: 2.3006
16/16 [==============================] - 0s 786us/step - loss: 2.3940
16/16 [==============================] - 0s 804us/step - loss: 2.4106
16/16 [==============================] - 0s 791us/step - loss: 2.3794
16/16 [==============================] - 0s 780us/step - loss: 2.3453
16/16 [==============================] - 0s 779us/step - loss: 2.3317
16/16 [==============================] - 0s 815us/step - loss: 2.3272
16/16 [==============================] - 0s 790us/step - loss: 2.3271
Epoch 40 of 60

Testing for epoch 40 index 1:
79/79 [==============================] - 0s 834us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1342
16/16 [==============================] - 0s 1ms/step - loss: 1.9180
16/16 [==============================] - 0s 1ms/step - loss: 2.2724
16/16 [==============================] - 0s 1ms/step - loss: 2.3660
16/16 [==============================] - 0s 1ms/step - loss: 2.3839
16/16 [==============================] - 0s 781us/step - loss: 2.3552
16/16 [==============================] - 0s 1ms/step - loss: 2.3228
16/16 [==============================] - 0s 788us/step - loss: 2.3097
16/16 [==============================] - 0s 800us/step - loss: 2.3054
16/16 [==============================] - 0s 800us/step - loss: 2.3052

Testing for epoch 40 index 2:
79/79 [==============================] - 0s 583us/step
16/16 [==============================] - 0s 825us/step - loss: 0.1336
16/16 [==============================] - 0s 806us/step - loss: 1.8845
16/16 [==============================] - 0s 785us/step - loss: 2.2270
16/16 [==============================] - 0s 789us/step - loss: 2.3128
16/16 [==============================] - 0s 803us/step - loss: 2.3252
16/16 [==============================] - 0s 807us/step - loss: 2.2901
16/16 [==============================] - 0s 789us/step - loss: 2.2552
16/16 [==============================] - 0s 803us/step - loss: 2.2418
16/16 [==============================] - 0s 903us/step - loss: 2.2374
16/16 [==============================] - 0s 860us/step - loss: 2.2373

Testing for epoch 40 index 3:
79/79 [==============================] - 0s 587us/step
16/16 [==============================] - 0s 800us/step - loss: 0.1321
16/16 [==============================] - 0s 817us/step - loss: 1.9400
16/16 [==============================] - 0s 819us/step - loss: 2.2933
16/16 [==============================] - 0s 825us/step - loss: 2.3799
16/16 [==============================] - 0s 819us/step - loss: 2.3924
16/16 [==============================] - 0s 885us/step - loss: 2.3564
16/16 [==============================] - 0s 882us/step - loss: 2.3207
16/16 [==============================] - 0s 823us/step - loss: 2.3070
16/16 [==============================] - 0s 865us/step - loss: 2.3027
16/16 [==============================] - 0s 803us/step - loss: 2.3027

Testing for epoch 40 index 4:
79/79 [==============================] - 0s 669us/step
16/16 [==============================] - 0s 815us/step - loss: 0.1334
16/16 [==============================] - 0s 833us/step - loss: 1.9022
16/16 [==============================] - 0s 806us/step - loss: 2.2463
16/16 [==============================] - 0s 796us/step - loss: 2.3297
16/16 [==============================] - 0s 790us/step - loss: 2.3413
16/16 [==============================] - 0s 790us/step - loss: 2.3067
16/16 [==============================] - 0s 791us/step - loss: 2.2727
16/16 [==============================] - 0s 828us/step - loss: 2.2595
16/16 [==============================] - 0s 819us/step - loss: 2.2552
16/16 [==============================] - 0s 798us/step - loss: 2.2550

Testing for epoch 40 index 5:
79/79 [==============================] - 0s 607us/step
16/16 [==============================] - 0s 790us/step - loss: 0.1269
16/16 [==============================] - 0s 804us/step - loss: 1.9772
16/16 [==============================] - 0s 796us/step - loss: 2.3427
16/16 [==============================] - 0s 779us/step - loss: 2.4319
16/16 [==============================] - 0s 792us/step - loss: 2.4454
16/16 [==============================] - 0s 806us/step - loss: 2.4078
16/16 [==============================] - 0s 804us/step - loss: 2.3714
16/16 [==============================] - 0s 817us/step - loss: 2.3572
16/16 [==============================] - 0s 854us/step - loss: 2.3526
16/16 [==============================] - 0s 889us/step - loss: 2.3524
Epoch 41 of 60

Testing for epoch 41 index 1:
79/79 [==============================] - 0s 862us/step
16/16 [==============================] - 0s 846us/step - loss: 0.1301
16/16 [==============================] - 0s 833us/step - loss: 1.9242
16/16 [==============================] - 0s 1ms/step - loss: 2.2728
16/16 [==============================] - 0s 814us/step - loss: 2.3565
16/16 [==============================] - 0s 834us/step - loss: 2.3696
16/16 [==============================] - 0s 832us/step - loss: 2.3336
16/16 [==============================] - 0s 849us/step - loss: 2.2979
16/16 [==============================] - 0s 895us/step - loss: 2.2839
16/16 [==============================] - 0s 835us/step - loss: 2.2793
16/16 [==============================] - 0s 842us/step - loss: 2.2791

Testing for epoch 41 index 2:
79/79 [==============================] - 0s 602us/step
16/16 [==============================] - 0s 832us/step - loss: 0.1314
16/16 [==============================] - 0s 845us/step - loss: 1.9663
16/16 [==============================] - 0s 874us/step - loss: 2.3249
16/16 [==============================] - 0s 816us/step - loss: 2.4099
16/16 [==============================] - 0s 824us/step - loss: 2.4224
16/16 [==============================] - 0s 826us/step - loss: 2.3848
16/16 [==============================] - 0s 849us/step - loss: 2.3482
16/16 [==============================] - 0s 800us/step - loss: 2.3341
16/16 [==============================] - 0s 856us/step - loss: 2.3296
16/16 [==============================] - 0s 797us/step - loss: 2.3295

Testing for epoch 41 index 3:
79/79 [==============================] - 0s 719us/step
16/16 [==============================] - 0s 843us/step - loss: 0.1290
16/16 [==============================] - 0s 858us/step - loss: 2.0148
16/16 [==============================] - 0s 784us/step - loss: 2.3873
16/16 [==============================] - 0s 856us/step - loss: 2.4746
16/16 [==============================] - 0s 835us/step - loss: 2.4879
16/16 [==============================] - 0s 810us/step - loss: 2.4474
16/16 [==============================] - 0s 865us/step - loss: 2.4088
16/16 [==============================] - 0s 884us/step - loss: 2.3940
16/16 [==============================] - 0s 869us/step - loss: 2.3894
16/16 [==============================] - 0s 785us/step - loss: 2.3893

Testing for epoch 41 index 4:
79/79 [==============================] - 0s 618us/step
16/16 [==============================] - 0s 814us/step - loss: 0.1312
16/16 [==============================] - 0s 807us/step - loss: 1.9137
16/16 [==============================] - 0s 1ms/step - loss: 2.2591
16/16 [==============================] - 0s 798us/step - loss: 2.3380
16/16 [==============================] - 0s 1ms/step - loss: 2.3479
16/16 [==============================] - 0s 1ms/step - loss: 2.3084
16/16 [==============================] - 0s 830us/step - loss: 2.2720
16/16 [==============================] - 0s 788us/step - loss: 2.2582
16/16 [==============================] - 0s 795us/step - loss: 2.2539
16/16 [==============================] - 0s 855us/step - loss: 2.2538

Testing for epoch 41 index 5:
79/79 [==============================] - 0s 616us/step
16/16 [==============================] - 0s 857us/step - loss: 0.1289
16/16 [==============================] - 0s 817us/step - loss: 1.9509
16/16 [==============================] - 0s 905us/step - loss: 2.3031
16/16 [==============================] - 0s 799us/step - loss: 2.3834
16/16 [==============================] - 0s 915us/step - loss: 2.3938
16/16 [==============================] - 0s 1ms/step - loss: 2.3538
16/16 [==============================] - 0s 911us/step - loss: 2.3167
16/16 [==============================] - 0s 934us/step - loss: 2.3026
16/16 [==============================] - 0s 891us/step - loss: 2.2981
16/16 [==============================] - 0s 866us/step - loss: 2.2980
Epoch 42 of 60

Testing for epoch 42 index 1:
79/79 [==============================] - 0s 584us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1312
16/16 [==============================] - 0s 3ms/step - loss: 1.9436
16/16 [==============================] - 0s 1ms/step - loss: 2.2942
16/16 [==============================] - 0s 1ms/step - loss: 2.3728
16/16 [==============================] - 0s 801us/step - loss: 2.3809
16/16 [==============================] - 0s 1ms/step - loss: 2.3390
16/16 [==============================] - 0s 881us/step - loss: 2.3013
16/16 [==============================] - 0s 818us/step - loss: 2.2872
16/16 [==============================] - 0s 807us/step - loss: 2.2829
16/16 [==============================] - 0s 794us/step - loss: 2.2830

Testing for epoch 42 index 2:
79/79 [==============================] - 0s 586us/step
16/16 [==============================] - 0s 830us/step - loss: 0.1277
16/16 [==============================] - 0s 825us/step - loss: 1.9867
16/16 [==============================] - 0s 818us/step - loss: 2.3500
16/16 [==============================] - 0s 797us/step - loss: 2.4348
16/16 [==============================] - 0s 795us/step - loss: 2.4465
16/16 [==============================] - 0s 798us/step - loss: 2.4073
16/16 [==============================] - 0s 790us/step - loss: 2.3709
16/16 [==============================] - 0s 803us/step - loss: 2.3569
16/16 [==============================] - 0s 799us/step - loss: 2.3525
16/16 [==============================] - 0s 796us/step - loss: 2.3524

Testing for epoch 42 index 3:
79/79 [==============================] - 0s 588us/step
16/16 [==============================] - 0s 802us/step - loss: 0.1334
16/16 [==============================] - 0s 821us/step - loss: 1.9570
16/16 [==============================] - 0s 806us/step - loss: 2.3121
16/16 [==============================] - 0s 808us/step - loss: 2.3937
16/16 [==============================] - 0s 805us/step - loss: 2.4031
16/16 [==============================] - 0s 824us/step - loss: 2.3619
16/16 [==============================] - 0s 808us/step - loss: 2.3245
16/16 [==============================] - 0s 827us/step - loss: 2.3104
16/16 [==============================] - 0s 827us/step - loss: 2.3060
16/16 [==============================] - 0s 813us/step - loss: 2.3059

Testing for epoch 42 index 4:
79/79 [==============================] - 0s 586us/step
16/16 [==============================] - 0s 797us/step - loss: 0.1268
16/16 [==============================] - 0s 794us/step - loss: 1.9501
16/16 [==============================] - 0s 868us/step - loss: 2.3015
16/16 [==============================] - 0s 822us/step - loss: 2.3798
16/16 [==============================] - 0s 828us/step - loss: 2.3880
16/16 [==============================] - 0s 789us/step - loss: 2.3466
16/16 [==============================] - 0s 802us/step - loss: 2.3094
16/16 [==============================] - 0s 821us/step - loss: 2.2953
16/16 [==============================] - 0s 815us/step - loss: 2.2908
16/16 [==============================] - 0s 839us/step - loss: 2.2906

Testing for epoch 42 index 5:
79/79 [==============================] - 0s 730us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1280
16/16 [==============================] - 0s 1ms/step - loss: 1.9975
16/16 [==============================] - 0s 805us/step - loss: 2.3589
16/16 [==============================] - 0s 798us/step - loss: 2.4382
16/16 [==============================] - 0s 829us/step - loss: 2.4454
16/16 [==============================] - 0s 821us/step - loss: 2.4015
16/16 [==============================] - 0s 797us/step - loss: 2.3619
16/16 [==============================] - 0s 1ms/step - loss: 2.3472
16/16 [==============================] - 0s 815us/step - loss: 2.3427
16/16 [==============================] - 0s 813us/step - loss: 2.3427
Epoch 43 of 60

Testing for epoch 43 index 1:
79/79 [==============================] - 0s 599us/step
16/16 [==============================] - 0s 792us/step - loss: 0.1275
16/16 [==============================] - 0s 1ms/step - loss: 2.0014
16/16 [==============================] - 0s 773us/step - loss: 2.3669
16/16 [==============================] - 0s 771us/step - loss: 2.4493
16/16 [==============================] - 0s 1ms/step - loss: 2.4593
16/16 [==============================] - 0s 1ms/step - loss: 2.4179
16/16 [==============================] - 0s 1ms/step - loss: 2.3796
16/16 [==============================] - 0s 1ms/step - loss: 2.3652
16/16 [==============================] - 0s 1ms/step - loss: 2.3606
16/16 [==============================] - 0s 787us/step - loss: 2.3604

Testing for epoch 43 index 2:
79/79 [==============================] - 0s 591us/step
16/16 [==============================] - 0s 797us/step - loss: 0.1274
16/16 [==============================] - 0s 800us/step - loss: 1.9885
16/16 [==============================] - 0s 800us/step - loss: 2.3486
16/16 [==============================] - 0s 793us/step - loss: 2.4281
16/16 [==============================] - 0s 810us/step - loss: 2.4365
16/16 [==============================] - 0s 805us/step - loss: 2.3939
16/16 [==============================] - 0s 806us/step - loss: 2.3558
16/16 [==============================] - 0s 830us/step - loss: 2.3415
16/16 [==============================] - 0s 830us/step - loss: 2.3370
16/16 [==============================] - 0s 812us/step - loss: 2.3370

Testing for epoch 43 index 3:
79/79 [==============================] - 0s 577us/step
16/16 [==============================] - 0s 810us/step - loss: 0.1306
16/16 [==============================] - 0s 811us/step - loss: 1.9621
16/16 [==============================] - 0s 817us/step - loss: 2.3132
16/16 [==============================] - 0s 805us/step - loss: 2.3880
16/16 [==============================] - 0s 784us/step - loss: 2.3952
16/16 [==============================] - 0s 820us/step - loss: 2.3532
16/16 [==============================] - 0s 793us/step - loss: 2.3157
16/16 [==============================] - 0s 778us/step - loss: 2.3018
16/16 [==============================] - 0s 786us/step - loss: 2.2976
16/16 [==============================] - 0s 785us/step - loss: 2.2976

Testing for epoch 43 index 4:
79/79 [==============================] - 0s 636us/step
16/16 [==============================] - 0s 829us/step - loss: 0.1253
16/16 [==============================] - 0s 825us/step - loss: 2.0501
16/16 [==============================] - 0s 816us/step - loss: 2.4197
16/16 [==============================] - 0s 851us/step - loss: 2.4972
16/16 [==============================] - 0s 822us/step - loss: 2.5028
16/16 [==============================] - 0s 876us/step - loss: 2.4551
16/16 [==============================] - 0s 910us/step - loss: 2.4127
16/16 [==============================] - 0s 879us/step - loss: 2.3973
16/16 [==============================] - 0s 871us/step - loss: 2.3927
16/16 [==============================] - 0s 818us/step - loss: 2.3928

Testing for epoch 43 index 5:
79/79 [==============================] - 0s 586us/step
16/16 [==============================] - 0s 828us/step - loss: 0.1265
16/16 [==============================] - 0s 801us/step - loss: 2.0007
16/16 [==============================] - 0s 796us/step - loss: 2.3614
16/16 [==============================] - 0s 798us/step - loss: 2.4380
16/16 [==============================] - 0s 812us/step - loss: 2.4449
16/16 [==============================] - 0s 809us/step - loss: 2.4011
16/16 [==============================] - 0s 821us/step - loss: 2.3625
16/16 [==============================] - 0s 811us/step - loss: 2.3483
16/16 [==============================] - 0s 815us/step - loss: 2.3439
16/16 [==============================] - 0s 808us/step - loss: 2.3438
Epoch 44 of 60

Testing for epoch 44 index 1:
79/79 [==============================] - 0s 592us/step
16/16 [==============================] - 0s 795us/step - loss: 0.1299
16/16 [==============================] - 0s 803us/step - loss: 1.9974
16/16 [==============================] - 0s 782us/step - loss: 2.3529
16/16 [==============================] - 0s 792us/step - loss: 2.4269
16/16 [==============================] - 0s 795us/step - loss: 2.4322
16/16 [==============================] - 0s 797us/step - loss: 2.3865
16/16 [==============================] - 0s 805us/step - loss: 2.3467
16/16 [==============================] - 0s 805us/step - loss: 2.3323
16/16 [==============================] - 0s 795us/step - loss: 2.3279
16/16 [==============================] - 0s 793us/step - loss: 2.3279

Testing for epoch 44 index 2:
79/79 [==============================] - 0s 608us/step
16/16 [==============================] - 0s 829us/step - loss: 0.1263
16/16 [==============================] - 0s 809us/step - loss: 2.0233
16/16 [==============================] - 0s 809us/step - loss: 2.3927
16/16 [==============================] - 0s 835us/step - loss: 2.4734
16/16 [==============================] - 0s 809us/step - loss: 2.4828
16/16 [==============================] - 0s 835us/step - loss: 2.4407
16/16 [==============================] - 0s 810us/step - loss: 2.4019
16/16 [==============================] - 0s 906us/step - loss: 2.3872
16/16 [==============================] - 0s 824us/step - loss: 2.3824
16/16 [==============================] - 0s 789us/step - loss: 2.3822

Testing for epoch 44 index 3:
79/79 [==============================] - 0s 595us/step
16/16 [==============================] - 0s 818us/step - loss: 0.1234
16/16 [==============================] - 0s 929us/step - loss: 2.0523
16/16 [==============================] - 0s 784us/step - loss: 2.4219
16/16 [==============================] - 0s 829us/step - loss: 2.4981
16/16 [==============================] - 0s 831us/step - loss: 2.5027
16/16 [==============================] - 0s 805us/step - loss: 2.4546
16/16 [==============================] - 0s 1ms/step - loss: 2.4132
16/16 [==============================] - 0s 1ms/step - loss: 2.3981
16/16 [==============================] - 0s 1ms/step - loss: 2.3935
16/16 [==============================] - 0s 1ms/step - loss: 2.3934

Testing for epoch 44 index 4:
79/79 [==============================] - 0s 833us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1256
16/16 [==============================] - 0s 787us/step - loss: 1.9662
16/16 [==============================] - 0s 788us/step - loss: 2.3195
16/16 [==============================] - 0s 787us/step - loss: 2.3944
16/16 [==============================] - 0s 790us/step - loss: 2.4012
16/16 [==============================] - 0s 790us/step - loss: 2.3588
16/16 [==============================] - 0s 795us/step - loss: 2.3210
16/16 [==============================] - 0s 790us/step - loss: 2.3069
16/16 [==============================] - 0s 788us/step - loss: 2.3024
16/16 [==============================] - 0s 801us/step - loss: 2.3022

Testing for epoch 44 index 5:
79/79 [==============================] - 0s 580us/step
16/16 [==============================] - 0s 804us/step - loss: 0.1243
16/16 [==============================] - 0s 807us/step - loss: 2.0090
16/16 [==============================] - 0s 838us/step - loss: 2.3705
16/16 [==============================] - 0s 823us/step - loss: 2.4464
16/16 [==============================] - 0s 817us/step - loss: 2.4520
16/16 [==============================] - 0s 803us/step - loss: 2.4062
16/16 [==============================] - 0s 809us/step - loss: 2.3657
16/16 [==============================] - 0s 829us/step - loss: 2.3510
16/16 [==============================] - 0s 824us/step - loss: 2.3466
16/16 [==============================] - 0s 796us/step - loss: 2.3466
Epoch 45 of 60

Testing for epoch 45 index 1:
79/79 [==============================] - 0s 840us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1223
16/16 [==============================] - 0s 1ms/step - loss: 2.0306
16/16 [==============================] - 0s 1ms/step - loss: 2.3982
16/16 [==============================] - 0s 1ms/step - loss: 2.4740
16/16 [==============================] - 0s 806us/step - loss: 2.4773
16/16 [==============================] - 0s 825us/step - loss: 2.4287
16/16 [==============================] - 0s 821us/step - loss: 2.3869
16/16 [==============================] - 0s 791us/step - loss: 2.3718
16/16 [==============================] - 0s 1ms/step - loss: 2.3672
16/16 [==============================] - 0s 791us/step - loss: 2.3671

Testing for epoch 45 index 2:
79/79 [==============================] - 0s 590us/step
16/16 [==============================] - 0s 825us/step - loss: 0.1210
16/16 [==============================] - 0s 791us/step - loss: 2.0704
16/16 [==============================] - 0s 1ms/step - loss: 2.4462
16/16 [==============================] - 0s 1ms/step - loss: 2.5242
16/16 [==============================] - 0s 1ms/step - loss: 2.5296
16/16 [==============================] - 0s 1ms/step - loss: 2.4812
16/16 [==============================] - 0s 1ms/step - loss: 2.4387
16/16 [==============================] - 0s 1ms/step - loss: 2.4232
16/16 [==============================] - 0s 1ms/step - loss: 2.4184
16/16 [==============================] - 0s 786us/step - loss: 2.4183

Testing for epoch 45 index 3:
79/79 [==============================] - 0s 590us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1213
16/16 [==============================] - 0s 1ms/step - loss: 2.0630
16/16 [==============================] - 0s 1ms/step - loss: 2.4339
16/16 [==============================] - 0s 791us/step - loss: 2.5090
16/16 [==============================] - 0s 1ms/step - loss: 2.5119
16/16 [==============================] - 0s 1ms/step - loss: 2.4617
16/16 [==============================] - 0s 796us/step - loss: 2.4184
16/16 [==============================] - 0s 790us/step - loss: 2.4027
16/16 [==============================] - 0s 787us/step - loss: 2.3979
16/16 [==============================] - 0s 1ms/step - loss: 2.3979

Testing for epoch 45 index 4:
79/79 [==============================] - 0s 840us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1207
16/16 [==============================] - 0s 1ms/step - loss: 2.0925
16/16 [==============================] - 0s 916us/step - loss: 2.4711
16/16 [==============================] - 0s 783us/step - loss: 2.5501
16/16 [==============================] - 0s 783us/step - loss: 2.5549
16/16 [==============================] - 0s 826us/step - loss: 2.5060
16/16 [==============================] - 0s 813us/step - loss: 2.4629
16/16 [==============================] - 0s 857us/step - loss: 2.4472
16/16 [==============================] - 0s 1ms/step - loss: 2.4425
16/16 [==============================] - 0s 1ms/step - loss: 2.4424

Testing for epoch 45 index 5:
79/79 [==============================] - 0s 591us/step
16/16 [==============================] - 0s 797us/step - loss: 0.1209
16/16 [==============================] - 0s 794us/step - loss: 2.0450
16/16 [==============================] - 0s 781us/step - loss: 2.4120
16/16 [==============================] - 0s 811us/step - loss: 2.4853
16/16 [==============================] - 0s 1ms/step - loss: 2.4875
16/16 [==============================] - 0s 1ms/step - loss: 2.4384
16/16 [==============================] - 0s 1ms/step - loss: 2.3963
16/16 [==============================] - 0s 1ms/step - loss: 2.3811
16/16 [==============================] - 0s 807us/step - loss: 2.3765
16/16 [==============================] - 0s 797us/step - loss: 2.3764
Epoch 46 of 60

Testing for epoch 46 index 1:
79/79 [==============================] - 0s 590us/step
16/16 [==============================] - 0s 792us/step - loss: 0.1157
16/16 [==============================] - 0s 803us/step - loss: 2.0892
16/16 [==============================] - 0s 783us/step - loss: 2.4688
16/16 [==============================] - 0s 779us/step - loss: 2.5448
16/16 [==============================] - 0s 790us/step - loss: 2.5463
16/16 [==============================] - 0s 791us/step - loss: 2.4953
16/16 [==============================] - 0s 788us/step - loss: 2.4526
16/16 [==============================] - 0s 786us/step - loss: 2.4370
16/16 [==============================] - 0s 788us/step - loss: 2.4322
16/16 [==============================] - 0s 792us/step - loss: 2.4321

Testing for epoch 46 index 2:
79/79 [==============================] - 0s 599us/step
16/16 [==============================] - 0s 784us/step - loss: 0.1203
16/16 [==============================] - 0s 786us/step - loss: 2.0153
16/16 [==============================] - 0s 774us/step - loss: 2.3790
16/16 [==============================] - 0s 782us/step - loss: 2.4528
16/16 [==============================] - 0s 786us/step - loss: 2.4549
16/16 [==============================] - 0s 791us/step - loss: 2.4069
16/16 [==============================] - 0s 776us/step - loss: 2.3664
16/16 [==============================] - 0s 778us/step - loss: 2.3517
16/16 [==============================] - 0s 772us/step - loss: 2.3471
16/16 [==============================] - 0s 782us/step - loss: 2.3469

Testing for epoch 46 index 3:
79/79 [==============================] - 0s 581us/step
16/16 [==============================] - 0s 799us/step - loss: 0.1176
16/16 [==============================] - 0s 784us/step - loss: 2.0705
16/16 [==============================] - 0s 780us/step - loss: 2.4423
16/16 [==============================] - 0s 778us/step - loss: 2.5156
16/16 [==============================] - 0s 787us/step - loss: 2.5160
16/16 [==============================] - 0s 775us/step - loss: 2.4639
16/16 [==============================] - 0s 780us/step - loss: 2.4200
16/16 [==============================] - 0s 783us/step - loss: 2.4040
16/16 [==============================] - 0s 779us/step - loss: 2.3992
16/16 [==============================] - 0s 780us/step - loss: 2.3991

Testing for epoch 46 index 4:
79/79 [==============================] - 0s 610us/step
16/16 [==============================] - 0s 787us/step - loss: 0.1199
16/16 [==============================] - 0s 796us/step - loss: 2.0859
16/16 [==============================] - 0s 808us/step - loss: 2.4654
16/16 [==============================] - 0s 790us/step - loss: 2.5423
16/16 [==============================] - 0s 787us/step - loss: 2.5441
16/16 [==============================] - 0s 1ms/step - loss: 2.4922
16/16 [==============================] - 0s 781us/step - loss: 2.4480
16/16 [==============================] - 0s 794us/step - loss: 2.4320
16/16 [==============================] - 0s 800us/step - loss: 2.4271
16/16 [==============================] - 0s 783us/step - loss: 2.4269

Testing for epoch 46 index 5:
79/79 [==============================] - 0s 594us/step
16/16 [==============================] - 0s 800us/step - loss: 0.1187
16/16 [==============================] - 0s 794us/step - loss: 2.0720
16/16 [==============================] - 0s 787us/step - loss: 2.4514
16/16 [==============================] - 0s 784us/step - loss: 2.5300
16/16 [==============================] - 0s 806us/step - loss: 2.5335
16/16 [==============================] - 0s 798us/step - loss: 2.4858
16/16 [==============================] - 0s 802us/step - loss: 2.4440
16/16 [==============================] - 0s 798us/step - loss: 2.4288
16/16 [==============================] - 0s 794us/step - loss: 2.4241
16/16 [==============================] - 0s 794us/step - loss: 2.4240
Epoch 47 of 60

Testing for epoch 47 index 1:
79/79 [==============================] - 0s 584us/step
16/16 [==============================] - 0s 814us/step - loss: 0.1168
16/16 [==============================] - 0s 779us/step - loss: 2.0878
16/16 [==============================] - 0s 786us/step - loss: 2.4644
16/16 [==============================] - 0s 803us/step - loss: 2.5367
16/16 [==============================] - 0s 800us/step - loss: 2.5353
16/16 [==============================] - 0s 821us/step - loss: 2.4811
16/16 [==============================] - 0s 796us/step - loss: 2.4357
16/16 [==============================] - 0s 793us/step - loss: 2.4197
16/16 [==============================] - 0s 784us/step - loss: 2.4149
16/16 [==============================] - 0s 793us/step - loss: 2.4149

Testing for epoch 47 index 2:
79/79 [==============================] - 0s 653us/step
16/16 [==============================] - 0s 799us/step - loss: 0.1129
16/16 [==============================] - 0s 863us/step - loss: 2.1291
16/16 [==============================] - 0s 866us/step - loss: 2.5159
16/16 [==============================] - 0s 877us/step - loss: 2.5921
16/16 [==============================] - 0s 791us/step - loss: 2.5924
16/16 [==============================] - 0s 785us/step - loss: 2.5399
16/16 [==============================] - 0s 793us/step - loss: 2.4954
16/16 [==============================] - 0s 794us/step - loss: 2.4795
16/16 [==============================] - 0s 808us/step - loss: 2.4746
16/16 [==============================] - 0s 813us/step - loss: 2.4745

Testing for epoch 47 index 3:
79/79 [==============================] - 0s 591us/step
16/16 [==============================] - 0s 802us/step - loss: 0.1157
16/16 [==============================] - 0s 801us/step - loss: 2.0875
16/16 [==============================] - 0s 797us/step - loss: 2.4622
16/16 [==============================] - 0s 793us/step - loss: 2.5350
16/16 [==============================] - 0s 793us/step - loss: 2.5349
16/16 [==============================] - 0s 802us/step - loss: 2.4821
16/16 [==============================] - 0s 803us/step - loss: 2.4374
16/16 [==============================] - 0s 803us/step - loss: 2.4214
16/16 [==============================] - 0s 790us/step - loss: 2.4166
16/16 [==============================] - 0s 784us/step - loss: 2.4165

Testing for epoch 47 index 4:
79/79 [==============================] - 0s 619us/step
16/16 [==============================] - 0s 791us/step - loss: 0.1148
16/16 [==============================] - 0s 780us/step - loss: 2.1379
16/16 [==============================] - 0s 809us/step - loss: 2.5248
16/16 [==============================] - 0s 780us/step - loss: 2.5999
16/16 [==============================] - 0s 872us/step - loss: 2.5989
16/16 [==============================] - 0s 866us/step - loss: 2.5435
16/16 [==============================] - 0s 863us/step - loss: 2.4969
16/16 [==============================] - 0s 863us/step - loss: 2.4802
16/16 [==============================] - 0s 787us/step - loss: 2.4751
16/16 [==============================] - 0s 859us/step - loss: 2.4750

Testing for epoch 47 index 5:
79/79 [==============================] - 0s 580us/step
16/16 [==============================] - 0s 790us/step - loss: 0.1154
16/16 [==============================] - 0s 786us/step - loss: 2.0392
16/16 [==============================] - 0s 788us/step - loss: 2.4002
16/16 [==============================] - 0s 784us/step - loss: 2.4684
16/16 [==============================] - 0s 795us/step - loss: 2.4659
16/16 [==============================] - 0s 801us/step - loss: 2.4131
16/16 [==============================] - 0s 824us/step - loss: 2.3696
16/16 [==============================] - 0s 790us/step - loss: 2.3543
16/16 [==============================] - 0s 812us/step - loss: 2.3497
16/16 [==============================] - 0s 795us/step - loss: 2.3497
Epoch 48 of 60

Testing for epoch 48 index 1:
79/79 [==============================] - 0s 653us/step
16/16 [==============================] - 0s 876us/step - loss: 0.1150
16/16 [==============================] - 0s 864us/step - loss: 2.1081
16/16 [==============================] - 0s 861us/step - loss: 2.4903
16/16 [==============================] - 0s 856us/step - loss: 2.5626
16/16 [==============================] - 0s 862us/step - loss: 2.5596
16/16 [==============================] - 0s 865us/step - loss: 2.5044
16/16 [==============================] - 0s 865us/step - loss: 2.4593
16/16 [==============================] - 0s 870us/step - loss: 2.4434
16/16 [==============================] - 0s 875us/step - loss: 2.4386
16/16 [==============================] - 0s 808us/step - loss: 2.4386

Testing for epoch 48 index 2:
79/79 [==============================] - 0s 581us/step
16/16 [==============================] - 0s 785us/step - loss: 0.1147
16/16 [==============================] - 0s 781us/step - loss: 2.1044
16/16 [==============================] - 0s 779us/step - loss: 2.4888
16/16 [==============================] - 0s 782us/step - loss: 2.5643
16/16 [==============================] - 0s 825us/step - loss: 2.5644
16/16 [==============================] - 0s 801us/step - loss: 2.5118
16/16 [==============================] - 0s 819us/step - loss: 2.4675
16/16 [==============================] - 0s 796us/step - loss: 2.4516
16/16 [==============================] - 0s 786us/step - loss: 2.4465
16/16 [==============================] - 0s 779us/step - loss: 2.4463

Testing for epoch 48 index 3:
79/79 [==============================] - 0s 581us/step
16/16 [==============================] - 0s 792us/step - loss: 0.1152
16/16 [==============================] - 0s 774us/step - loss: 2.0719
16/16 [==============================] - 0s 786us/step - loss: 2.4408
16/16 [==============================] - 0s 809us/step - loss: 2.5109
16/16 [==============================] - 0s 793us/step - loss: 2.5086
16/16 [==============================] - 0s 795us/step - loss: 2.4552
16/16 [==============================] - 0s 811us/step - loss: 2.4115
16/16 [==============================] - 0s 797us/step - loss: 2.3960
16/16 [==============================] - 0s 824us/step - loss: 2.3913
16/16 [==============================] - 0s 818us/step - loss: 2.3912

Testing for epoch 48 index 4:
79/79 [==============================] - 0s 588us/step
16/16 [==============================] - 0s 802us/step - loss: 0.1107
16/16 [==============================] - 0s 795us/step - loss: 2.1577
16/16 [==============================] - 0s 857us/step - loss: 2.5481
16/16 [==============================] - 0s 813us/step - loss: 2.6204
16/16 [==============================] - 0s 797us/step - loss: 2.6165
16/16 [==============================] - 0s 790us/step - loss: 2.5579
16/16 [==============================] - 0s 782us/step - loss: 2.5097
16/16 [==============================] - 0s 804us/step - loss: 2.4927
16/16 [==============================] - 0s 779us/step - loss: 2.4877
16/16 [==============================] - 0s 782us/step - loss: 2.4876

Testing for epoch 48 index 5:
79/79 [==============================] - 0s 586us/step
16/16 [==============================] - 0s 829us/step - loss: 0.1159
16/16 [==============================] - 0s 803us/step - loss: 2.0767
16/16 [==============================] - 0s 810us/step - loss: 2.4525
16/16 [==============================] - 0s 781us/step - loss: 2.5232
16/16 [==============================] - 0s 796us/step - loss: 2.5203
16/16 [==============================] - 0s 810us/step - loss: 2.4657
16/16 [==============================] - 0s 782us/step - loss: 2.4213
16/16 [==============================] - 0s 814us/step - loss: 2.4055
16/16 [==============================] - 0s 805us/step - loss: 2.4008
16/16 [==============================] - 0s 781us/step - loss: 2.4008
Epoch 49 of 60

Testing for epoch 49 index 1:
79/79 [==============================] - 0s 590us/step
16/16 [==============================] - 0s 804us/step - loss: 0.1130
16/16 [==============================] - 0s 806us/step - loss: 2.1188
16/16 [==============================] - 0s 784us/step - loss: 2.5027
16/16 [==============================] - 0s 783us/step - loss: 2.5749
16/16 [==============================] - 0s 777us/step - loss: 2.5720
16/16 [==============================] - 0s 790us/step - loss: 2.5164
16/16 [==============================] - 0s 787us/step - loss: 2.4706
16/16 [==============================] - 0s 823us/step - loss: 2.4543
16/16 [==============================] - 0s 811us/step - loss: 2.4493
16/16 [==============================] - 0s 819us/step - loss: 2.4491

Testing for epoch 49 index 2:
79/79 [==============================] - 0s 589us/step
16/16 [==============================] - 0s 806us/step - loss: 0.1130
16/16 [==============================] - 0s 798us/step - loss: 2.1207
16/16 [==============================] - 0s 791us/step - loss: 2.5027
16/16 [==============================] - 0s 807us/step - loss: 2.5732
16/16 [==============================] - 0s 794us/step - loss: 2.5696
16/16 [==============================] - 0s 795us/step - loss: 2.5133
16/16 [==============================] - 0s 790us/step - loss: 2.4675
16/16 [==============================] - 0s 787us/step - loss: 2.4516
16/16 [==============================] - 0s 790us/step - loss: 2.4468
16/16 [==============================] - 0s 789us/step - loss: 2.4468

Testing for epoch 49 index 3:
79/79 [==============================] - 0s 576us/step
16/16 [==============================] - 0s 786us/step - loss: 0.1157
16/16 [==============================] - 0s 771us/step - loss: 2.1044
16/16 [==============================] - 0s 773us/step - loss: 2.4790
16/16 [==============================] - 0s 775us/step - loss: 2.5455
16/16 [==============================] - 0s 783us/step - loss: 2.5407
16/16 [==============================] - 0s 848us/step - loss: 2.4851
16/16 [==============================] - 0s 846us/step - loss: 2.4397
16/16 [==============================] - 0s 861us/step - loss: 2.4239
16/16 [==============================] - 0s 847us/step - loss: 2.4191
16/16 [==============================] - 0s 868us/step - loss: 2.4191

Testing for epoch 49 index 4:
79/79 [==============================] - 0s 586us/step
16/16 [==============================] - 0s 795us/step - loss: 0.1159
16/16 [==============================] - 0s 797us/step - loss: 2.1415
16/16 [==============================] - 0s 792us/step - loss: 2.5249
16/16 [==============================] - 0s 821us/step - loss: 2.5930
16/16 [==============================] - 0s 888us/step - loss: 2.5871
16/16 [==============================] - 0s 879us/step - loss: 2.5281
16/16 [==============================] - 0s 867us/step - loss: 2.4804
16/16 [==============================] - 0s 893us/step - loss: 2.4640
16/16 [==============================] - 0s 804us/step - loss: 2.4592
16/16 [==============================] - 0s 808us/step - loss: 2.4592

Testing for epoch 49 index 5:
79/79 [==============================] - 0s 590us/step
16/16 [==============================] - 0s 821us/step - loss: 0.1108
16/16 [==============================] - 0s 839us/step - loss: 2.1666
16/16 [==============================] - 0s 802us/step - loss: 2.5501
16/16 [==============================] - 0s 670us/step - loss: 2.6150
16/16 [==============================] - 0s 783us/step - loss: 2.6061
16/16 [==============================] - 0s 2ms/step - loss: 2.5432
16/16 [==============================] - 0s 748us/step - loss: 2.4931
16/16 [==============================] - 0s 2ms/step - loss: 2.4759
16/16 [==============================] - 0s 746us/step - loss: 2.4708
16/16 [==============================] - 0s 2ms/step - loss: 2.4707
Epoch 50 of 60

Testing for epoch 50 index 1:
79/79 [==============================] - 0s 834us/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1092
16/16 [==============================] - 0s 2ms/step - loss: 2.1786
16/16 [==============================] - 0s 2ms/step - loss: 2.5691
16/16 [==============================] - 0s 783us/step - loss: 2.6374
16/16 [==============================] - 0s 1ms/step - loss: 2.6307
16/16 [==============================] - 0s 983us/step - loss: 2.5707
16/16 [==============================] - 0s 1ms/step - loss: 2.5218
16/16 [==============================] - 0s 2ms/step - loss: 2.5047
16/16 [==============================] - 0s 827us/step - loss: 2.4996
16/16 [==============================] - 0s 820us/step - loss: 2.4996

Testing for epoch 50 index 2:
79/79 [==============================] - 0s 577us/step
16/16 [==============================] - 0s 814us/step - loss: 0.1106
16/16 [==============================] - 0s 812us/step - loss: 2.1213
16/16 [==============================] - 0s 2ms/step - loss: 2.4972
16/16 [==============================] - 0s 805us/step - loss: 2.5617
16/16 [==============================] - 0s 2ms/step - loss: 2.5540
16/16 [==============================] - 0s 779us/step - loss: 2.4949
16/16 [==============================] - 0s 798us/step - loss: 2.4473
16/16 [==============================] - 0s 802us/step - loss: 2.4311
16/16 [==============================] - 0s 827us/step - loss: 2.4263
16/16 [==============================] - 0s 827us/step - loss: 2.4263

Testing for epoch 50 index 3:
79/79 [==============================] - 0s 574us/step
16/16 [==============================] - 0s 825us/step - loss: 0.1134
16/16 [==============================] - 0s 833us/step - loss: 2.1640
16/16 [==============================] - 0s 831us/step - loss: 2.5466
16/16 [==============================] - 0s 806us/step - loss: 2.6094
16/16 [==============================] - 0s 2ms/step - loss: 2.5990
16/16 [==============================] - 0s 2ms/step - loss: 2.5356
16/16 [==============================] - 0s 2ms/step - loss: 2.4853
16/16 [==============================] - 0s 2ms/step - loss: 2.4681
16/16 [==============================] - 0s 830us/step - loss: 2.4631
16/16 [==============================] - 0s 2ms/step - loss: 2.4631

Testing for epoch 50 index 4:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1106
16/16 [==============================] - 0s 2ms/step - loss: 2.1669
16/16 [==============================] - 0s 2ms/step - loss: 2.5499
16/16 [==============================] - 0s 2ms/step - loss: 2.6154
16/16 [==============================] - 0s 919us/step - loss: 2.6077
16/16 [==============================] - 0s 1ms/step - loss: 2.5490
16/16 [==============================] - 0s 2ms/step - loss: 2.5020
16/16 [==============================] - 0s 2ms/step - loss: 2.4856
16/16 [==============================] - 0s 2ms/step - loss: 2.4805
16/16 [==============================] - 0s 1ms/step - loss: 2.4803

Testing for epoch 50 index 5:
79/79 [==============================] - 0s 853us/step
16/16 [==============================] - 0s 803us/step - loss: 0.1134
16/16 [==============================] - 0s 1ms/step - loss: 2.1299
16/16 [==============================] - 0s 1ms/step - loss: 2.5127
16/16 [==============================] - 0s 1ms/step - loss: 2.5788
16/16 [==============================] - 0s 818us/step - loss: 2.5713
16/16 [==============================] - 0s 828us/step - loss: 2.5120
16/16 [==============================] - 0s 806us/step - loss: 2.4655
16/16 [==============================] - 0s 1ms/step - loss: 2.4496
16/16 [==============================] - 0s 825us/step - loss: 2.4450
16/16 [==============================] - 0s 851us/step - loss: 2.4451
Epoch 51 of 60

Testing for epoch 51 index 1:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 799us/step - loss: 0.1102
16/16 [==============================] - 0s 2ms/step - loss: 2.1080
16/16 [==============================] - 0s 1ms/step - loss: 2.4813
16/16 [==============================] - 0s 801us/step - loss: 2.5444
16/16 [==============================] - 0s 1ms/step - loss: 2.5367
16/16 [==============================] - 0s 1ms/step - loss: 2.4800
16/16 [==============================] - 0s 779us/step - loss: 2.4343
16/16 [==============================] - 0s 780us/step - loss: 2.4184
16/16 [==============================] - 0s 809us/step - loss: 2.4137
16/16 [==============================] - 0s 816us/step - loss: 2.4137

Testing for epoch 51 index 2:
79/79 [==============================] - 0s 544us/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1094
16/16 [==============================] - 0s 2ms/step - loss: 2.1884
16/16 [==============================] - 0s 825us/step - loss: 2.5791
16/16 [==============================] - 0s 1ms/step - loss: 2.6449
16/16 [==============================] - 0s 837us/step - loss: 2.6358
16/16 [==============================] - 0s 1ms/step - loss: 2.5744
16/16 [==============================] - 0s 792us/step - loss: 2.5257
16/16 [==============================] - 0s 2ms/step - loss: 2.5088
16/16 [==============================] - 0s 807us/step - loss: 2.5036
16/16 [==============================] - 0s 2ms/step - loss: 2.5035

Testing for epoch 51 index 3:
79/79 [==============================] - 0s 575us/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1135
16/16 [==============================] - 0s 918us/step - loss: 2.1417
16/16 [==============================] - 0s 778us/step - loss: 2.5163
16/16 [==============================] - 0s 839us/step - loss: 2.5775
16/16 [==============================] - 0s 1ms/step - loss: 2.5668
16/16 [==============================] - 0s 817us/step - loss: 2.5060
16/16 [==============================] - 0s 820us/step - loss: 2.4582
16/16 [==============================] - 0s 2ms/step - loss: 2.4420
16/16 [==============================] - 0s 2ms/step - loss: 2.4372
16/16 [==============================] - 0s 2ms/step - loss: 2.4371

Testing for epoch 51 index 4:
79/79 [==============================] - 0s 660us/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1133
16/16 [==============================] - 0s 2ms/step - loss: 2.1446
16/16 [==============================] - 0s 780us/step - loss: 2.5259
16/16 [==============================] - 0s 2ms/step - loss: 2.5907
16/16 [==============================] - 0s 2ms/step - loss: 2.5826
16/16 [==============================] - 0s 2ms/step - loss: 2.5259
16/16 [==============================] - 0s 778us/step - loss: 2.4802
16/16 [==============================] - 0s 945us/step - loss: 2.4642
16/16 [==============================] - 0s 2ms/step - loss: 2.4593
16/16 [==============================] - 0s 937us/step - loss: 2.4592

Testing for epoch 51 index 5:
79/79 [==============================] - 0s 1ms/step
16/16 [==============================] - 0s 867us/step - loss: 0.1097
16/16 [==============================] - 0s 2ms/step - loss: 2.1334
16/16 [==============================] - 0s 2ms/step - loss: 2.5077
16/16 [==============================] - 0s 1ms/step - loss: 2.5687
16/16 [==============================] - 0s 814us/step - loss: 2.5580
16/16 [==============================] - 0s 1ms/step - loss: 2.4978
16/16 [==============================] - 0s 1ms/step - loss: 2.4505
16/16 [==============================] - 0s 2ms/step - loss: 2.4342
16/16 [==============================] - 0s 804us/step - loss: 2.4293
16/16 [==============================] - 0s 831us/step - loss: 2.4292
Epoch 52 of 60

Testing for epoch 52 index 1:
79/79 [==============================] - 0s 964us/step
16/16 [==============================] - 0s 2ms/step - loss: 0.1128
16/16 [==============================] - 0s 2ms/step - loss: 2.1843
16/16 [==============================] - 0s 1ms/step - loss: 2.5731
16/16 [==============================] - 0s 992us/step - loss: 2.6381
16/16 [==============================] - 0s 2ms/step - loss: 2.6285
16/16 [==============================] - 0s 2ms/step - loss: 2.5692
16/16 [==============================] - 0s 815us/step - loss: 2.5219
16/16 [==============================] - 0s 944us/step - loss: 2.5053
16/16 [==============================] - 0s 740us/step - loss: 2.5000
16/16 [==============================] - 0s 799us/step - loss: 2.4998

Testing for epoch 52 index 2:
79/79 [==============================] - 0s 498us/step
16/16 [==============================] - 0s 814us/step - loss: 0.1151
16/16 [==============================] - 0s 662us/step - loss: 2.1366
16/16 [==============================] - 0s 786us/step - loss: 2.5143
16/16 [==============================] - 0s 832us/step - loss: 2.5790
16/16 [==============================] - 0s 828us/step - loss: 2.5708
16/16 [==============================] - 0s 772us/step - loss: 2.5126
16/16 [==============================] - 0s 763us/step - loss: 2.4669
16/16 [==============================] - 0s 761us/step - loss: 2.4509
16/16 [==============================] - 0s 762us/step - loss: 2.4460
16/16 [==============================] - 0s 765us/step - loss: 2.4459

Testing for epoch 52 index 3:
79/79 [==============================] - 0s 651us/step
16/16 [==============================] - 0s 801us/step - loss: 0.1137
16/16 [==============================] - 0s 776us/step - loss: 2.1214
16/16 [==============================] - 0s 777us/step - loss: 2.4961
16/16 [==============================] - 0s 810us/step - loss: 2.5559
16/16 [==============================] - 0s 1ms/step - loss: 2.5443
16/16 [==============================] - 0s 1ms/step - loss: 2.4830
16/16 [==============================] - 0s 1ms/step - loss: 2.4348
16/16 [==============================] - 0s 1ms/step - loss: 2.4184
16/16 [==============================] - 0s 823us/step - loss: 2.4137
16/16 [==============================] - 0s 786us/step - loss: 2.4137

Testing for epoch 52 index 4:
79/79 [==============================] - 0s 809us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1077
16/16 [==============================] - 0s 1ms/step - loss: 2.2251
16/16 [==============================] - 0s 765us/step - loss: 2.6189
16/16 [==============================] - 0s 1ms/step - loss: 2.6792
16/16 [==============================] - 0s 1ms/step - loss: 2.6652
16/16 [==============================] - 0s 1ms/step - loss: 2.5992
16/16 [==============================] - 0s 1ms/step - loss: 2.5479
16/16 [==============================] - 0s 787us/step - loss: 2.5304
16/16 [==============================] - 0s 1ms/step - loss: 2.5253
16/16 [==============================] - 0s 1ms/step - loss: 2.5253

Testing for epoch 52 index 5:
79/79 [==============================] - 0s 575us/step
16/16 [==============================] - 0s 801us/step - loss: 0.1081
16/16 [==============================] - 0s 787us/step - loss: 2.1911
16/16 [==============================] - 0s 801us/step - loss: 2.5848
16/16 [==============================] - 0s 776us/step - loss: 2.6481
16/16 [==============================] - 0s 819us/step - loss: 2.6369
16/16 [==============================] - 0s 784us/step - loss: 2.5737
16/16 [==============================] - 0s 778us/step - loss: 2.5244
16/16 [==============================] - 0s 774us/step - loss: 2.5073
16/16 [==============================] - 0s 830us/step - loss: 2.5021
16/16 [==============================] - 0s 783us/step - loss: 2.5019
Epoch 53 of 60

Testing for epoch 53 index 1:
79/79 [==============================] - 0s 581us/step
16/16 [==============================] - 0s 783us/step - loss: 0.1105
16/16 [==============================] - 0s 798us/step - loss: 2.2424
16/16 [==============================] - 0s 801us/step - loss: 2.6434
16/16 [==============================] - 0s 782us/step - loss: 2.7071
16/16 [==============================] - 0s 809us/step - loss: 2.6960
16/16 [==============================] - 0s 781us/step - loss: 2.6329
16/16 [==============================] - 0s 796us/step - loss: 2.5825
16/16 [==============================] - 0s 785us/step - loss: 2.5653
16/16 [==============================] - 0s 779us/step - loss: 2.5603
16/16 [==============================] - 0s 780us/step - loss: 2.5603

Testing for epoch 53 index 2:
79/79 [==============================] - 0s 666us/step
16/16 [==============================] - 0s 800us/step - loss: 0.1065
16/16 [==============================] - 0s 773us/step - loss: 2.2126
16/16 [==============================] - 0s 780us/step - loss: 2.5990
16/16 [==============================] - 0s 799us/step - loss: 2.6537
16/16 [==============================] - 0s 775us/step - loss: 2.6362
16/16 [==============================] - 0s 778us/step - loss: 2.5665
16/16 [==============================] - 0s 777us/step - loss: 2.5130
16/16 [==============================] - 0s 782us/step - loss: 2.4951
16/16 [==============================] - 0s 793us/step - loss: 2.4897
16/16 [==============================] - 0s 783us/step - loss: 2.4896

Testing for epoch 53 index 3:
79/79 [==============================] - 0s 596us/step
16/16 [==============================] - 0s 787us/step - loss: 0.1109
16/16 [==============================] - 0s 783us/step - loss: 2.2409
16/16 [==============================] - 0s 826us/step - loss: 2.6359
16/16 [==============================] - 0s 802us/step - loss: 2.6951
16/16 [==============================] - 0s 784us/step - loss: 2.6803
16/16 [==============================] - 0s 818us/step - loss: 2.6154
16/16 [==============================] - 0s 854us/step - loss: 2.5654
16/16 [==============================] - 0s 824us/step - loss: 2.5480
16/16 [==============================] - 0s 878us/step - loss: 2.5427
16/16 [==============================] - 0s 823us/step - loss: 2.5425

Testing for epoch 53 index 4:
79/79 [==============================] - 0s 593us/step
16/16 [==============================] - 0s 822us/step - loss: 0.1077
16/16 [==============================] - 0s 774us/step - loss: 2.1988
16/16 [==============================] - 0s 791us/step - loss: 2.5828
16/16 [==============================] - 0s 824us/step - loss: 2.6395
16/16 [==============================] - 0s 777us/step - loss: 2.6247
16/16 [==============================] - 0s 786us/step - loss: 2.5585
16/16 [==============================] - 0s 791us/step - loss: 2.5071
16/16 [==============================] - 0s 796us/step - loss: 2.4895
16/16 [==============================] - 0s 784us/step - loss: 2.4842
16/16 [==============================] - 0s 781us/step - loss: 2.4840

Testing for epoch 53 index 5:
79/79 [==============================] - 0s 607us/step
16/16 [==============================] - 0s 793us/step - loss: 0.1101
16/16 [==============================] - 0s 823us/step - loss: 2.1742
16/16 [==============================] - 0s 806us/step - loss: 2.5541
16/16 [==============================] - 0s 815us/step - loss: 2.6106
16/16 [==============================] - 0s 786us/step - loss: 2.5974
16/16 [==============================] - 0s 779us/step - loss: 2.5359
16/16 [==============================] - 0s 777us/step - loss: 2.4884
16/16 [==============================] - 0s 821us/step - loss: 2.4719
16/16 [==============================] - 0s 831us/step - loss: 2.4668
16/16 [==============================] - 0s 794us/step - loss: 2.4666
Epoch 54 of 60

Testing for epoch 54 index 1:
79/79 [==============================] - 0s 584us/step
16/16 [==============================] - 0s 824us/step - loss: 0.1084
16/16 [==============================] - 0s 791us/step - loss: 2.1517
16/16 [==============================] - 0s 804us/step - loss: 2.5217
16/16 [==============================] - 0s 781us/step - loss: 2.5735
16/16 [==============================] - 0s 773us/step - loss: 2.5572
16/16 [==============================] - 0s 860us/step - loss: 2.4921
16/16 [==============================] - 0s 798us/step - loss: 2.4427
16/16 [==============================] - 0s 783us/step - loss: 2.4259
16/16 [==============================] - 0s 788us/step - loss: 2.4209
16/16 [==============================] - 0s 779us/step - loss: 2.4208

Testing for epoch 54 index 2:
79/79 [==============================] - 0s 601us/step
16/16 [==============================] - 0s 841us/step - loss: 0.1095
16/16 [==============================] - 0s 791us/step - loss: 2.1899
16/16 [==============================] - 0s 857us/step - loss: 2.5646
16/16 [==============================] - 0s 832us/step - loss: 2.6178
16/16 [==============================] - 0s 805us/step - loss: 2.5997
16/16 [==============================] - 0s 811us/step - loss: 2.5326
16/16 [==============================] - 0s 1ms/step - loss: 2.4819
16/16 [==============================] - 0s 799us/step - loss: 2.4647
16/16 [==============================] - 0s 810us/step - loss: 2.4596
16/16 [==============================] - 0s 818us/step - loss: 2.4596

Testing for epoch 54 index 3:
79/79 [==============================] - 0s 592us/step
16/16 [==============================] - 0s 786us/step - loss: 0.1083
16/16 [==============================] - 0s 802us/step - loss: 2.1131
16/16 [==============================] - 0s 796us/step - loss: 2.4631
16/16 [==============================] - 0s 776us/step - loss: 2.5087
16/16 [==============================] - 0s 791us/step - loss: 2.4890
16/16 [==============================] - 0s 833us/step - loss: 2.4226
16/16 [==============================] - 0s 784us/step - loss: 2.3731
16/16 [==============================] - 0s 772us/step - loss: 2.3567
16/16 [==============================] - 0s 772us/step - loss: 2.3520
16/16 [==============================] - 0s 798us/step - loss: 2.3520

Testing for epoch 54 index 4:
79/79 [==============================] - 0s 592us/step
16/16 [==============================] - 0s 813us/step - loss: 0.1097
16/16 [==============================] - 0s 820us/step - loss: 2.2147
16/16 [==============================] - 0s 790us/step - loss: 2.5931
16/16 [==============================] - 0s 786us/step - loss: 2.6463
16/16 [==============================] - 0s 793us/step - loss: 2.6278
16/16 [==============================] - 0s 815us/step - loss: 2.5607
16/16 [==============================] - 0s 786us/step - loss: 2.5099
16/16 [==============================] - 0s 800us/step - loss: 2.4930
16/16 [==============================] - 0s 804us/step - loss: 2.4882
16/16 [==============================] - 0s 803us/step - loss: 2.4882

Testing for epoch 54 index 5:
79/79 [==============================] - 0s 598us/step
16/16 [==============================] - 0s 865us/step - loss: 0.1090
16/16 [==============================] - 0s 821us/step - loss: 2.2205
16/16 [==============================] - 0s 781us/step - loss: 2.5931
16/16 [==============================] - 0s 1ms/step - loss: 2.6411
16/16 [==============================] - 0s 1ms/step - loss: 2.6206
16/16 [==============================] - 0s 783us/step - loss: 2.5518
16/16 [==============================] - 0s 789us/step - loss: 2.5000
16/16 [==============================] - 0s 791us/step - loss: 2.4825
16/16 [==============================] - 0s 786us/step - loss: 2.4773
16/16 [==============================] - 0s 784us/step - loss: 2.4773
Epoch 55 of 60

Testing for epoch 55 index 1:
79/79 [==============================] - 0s 581us/step
16/16 [==============================] - 0s 788us/step - loss: 0.1066
16/16 [==============================] - 0s 821us/step - loss: 2.2507
16/16 [==============================] - 0s 780us/step - loss: 2.6376
16/16 [==============================] - 0s 776us/step - loss: 2.6928
16/16 [==============================] - 0s 775us/step - loss: 2.6741
16/16 [==============================] - 0s 780us/step - loss: 2.6065
16/16 [==============================] - 0s 778us/step - loss: 2.5550
16/16 [==============================] - 0s 908us/step - loss: 2.5376
16/16 [==============================] - 0s 1ms/step - loss: 2.5325
16/16 [==============================] - 0s 1ms/step - loss: 2.5324

Testing for epoch 55 index 2:
79/79 [==============================] - 0s 827us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1066
16/16 [==============================] - 0s 830us/step - loss: 2.2226
16/16 [==============================] - 0s 782us/step - loss: 2.5906
16/16 [==============================] - 0s 780us/step - loss: 2.6408
16/16 [==============================] - 0s 1ms/step - loss: 2.6207
16/16 [==============================] - 0s 1ms/step - loss: 2.5530
16/16 [==============================] - 0s 1ms/step - loss: 2.5026
16/16 [==============================] - 0s 1ms/step - loss: 2.4856
16/16 [==============================] - 0s 1ms/step - loss: 2.4805
16/16 [==============================] - 0s 1ms/step - loss: 2.4805

Testing for epoch 55 index 3:
79/79 [==============================] - 0s 733us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1096
16/16 [==============================] - 0s 1ms/step - loss: 2.2626
16/16 [==============================] - 0s 1ms/step - loss: 2.6387
16/16 [==============================] - 0s 1ms/step - loss: 2.6881
16/16 [==============================] - 0s 1ms/step - loss: 2.6669
16/16 [==============================] - 0s 1ms/step - loss: 2.5970
16/16 [==============================] - 0s 1ms/step - loss: 2.5450
16/16 [==============================] - 0s 1ms/step - loss: 2.5276
16/16 [==============================] - 0s 1ms/step - loss: 2.5226
16/16 [==============================] - 0s 799us/step - loss: 2.5226

Testing for epoch 55 index 4:
79/79 [==============================] - 0s 586us/step
16/16 [==============================] - 0s 795us/step - loss: 0.1016
16/16 [==============================] - 0s 783us/step - loss: 2.2888
16/16 [==============================] - 0s 782us/step - loss: 2.6711
16/16 [==============================] - 0s 815us/step - loss: 2.7212
16/16 [==============================] - 0s 1ms/step - loss: 2.6985
16/16 [==============================] - 0s 848us/step - loss: 2.6265
16/16 [==============================] - 0s 837us/step - loss: 2.5724
16/16 [==============================] - 0s 788us/step - loss: 2.5543
16/16 [==============================] - 0s 828us/step - loss: 2.5490
16/16 [==============================] - 0s 827us/step - loss: 2.5490

Testing for epoch 55 index 5:
79/79 [==============================] - 0s 833us/step
16/16 [==============================] - 0s 924us/step - loss: 0.1076
16/16 [==============================] - 0s 874us/step - loss: 2.3363
16/16 [==============================] - 0s 793us/step - loss: 2.7287
16/16 [==============================] - 0s 798us/step - loss: 2.7820
16/16 [==============================] - 0s 796us/step - loss: 2.7605
16/16 [==============================] - 0s 836us/step - loss: 2.6894
16/16 [==============================] - 0s 801us/step - loss: 2.6356
16/16 [==============================] - 0s 797us/step - loss: 2.6175
16/16 [==============================] - 0s 791us/step - loss: 2.6122
16/16 [==============================] - 0s 794us/step - loss: 2.6122
Epoch 56 of 60

Testing for epoch 56 index 1:
79/79 [==============================] - 0s 588us/step
16/16 [==============================] - 0s 785us/step - loss: 0.1053
16/16 [==============================] - 0s 823us/step - loss: 2.2654
16/16 [==============================] - 0s 784us/step - loss: 2.6376
16/16 [==============================] - 0s 796us/step - loss: 2.6831
16/16 [==============================] - 0s 798us/step - loss: 2.6585
16/16 [==============================] - 0s 784us/step - loss: 2.5853
16/16 [==============================] - 0s 784us/step - loss: 2.5317
16/16 [==============================] - 0s 776us/step - loss: 2.5138
16/16 [==============================] - 0s 788us/step - loss: 2.5086
16/16 [==============================] - 0s 780us/step - loss: 2.5085

Testing for epoch 56 index 2:
79/79 [==============================] - 0s 601us/step
16/16 [==============================] - 0s 798us/step - loss: 0.1091
16/16 [==============================] - 0s 846us/step - loss: 2.2312
16/16 [==============================] - 0s 782us/step - loss: 2.6027
16/16 [==============================] - 0s 806us/step - loss: 2.6531
16/16 [==============================] - 0s 791us/step - loss: 2.6318
16/16 [==============================] - 0s 771us/step - loss: 2.5612
16/16 [==============================] - 0s 795us/step - loss: 2.5079
16/16 [==============================] - 0s 779us/step - loss: 2.4899
16/16 [==============================] - 0s 788us/step - loss: 2.4844
16/16 [==============================] - 0s 784us/step - loss: 2.4842

Testing for epoch 56 index 3:
79/79 [==============================] - 0s 596us/step
16/16 [==============================] - 0s 818us/step - loss: 0.1080
16/16 [==============================] - 0s 804us/step - loss: 2.2179
16/16 [==============================] - 0s 792us/step - loss: 2.5805
16/16 [==============================] - 0s 778us/step - loss: 2.6276
16/16 [==============================] - 0s 777us/step - loss: 2.6063
16/16 [==============================] - 0s 784us/step - loss: 2.5380
16/16 [==============================] - 0s 779us/step - loss: 2.4867
16/16 [==============================] - 0s 795us/step - loss: 2.4695
16/16 [==============================] - 0s 798us/step - loss: 2.4644
16/16 [==============================] - 0s 791us/step - loss: 2.4644

Testing for epoch 56 index 4:
79/79 [==============================] - 0s 709us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1050
16/16 [==============================] - 0s 1ms/step - loss: 2.2906
16/16 [==============================] - 0s 1ms/step - loss: 2.6671
16/16 [==============================] - 0s 1ms/step - loss: 2.7144
16/16 [==============================] - 0s 1ms/step - loss: 2.6909
16/16 [==============================] - 0s 1ms/step - loss: 2.6183
16/16 [==============================] - 0s 787us/step - loss: 2.5653
16/16 [==============================] - 0s 783us/step - loss: 2.5474
16/16 [==============================] - 0s 800us/step - loss: 2.5423
16/16 [==============================] - 0s 1ms/step - loss: 2.5422

Testing for epoch 56 index 5:
79/79 [==============================] - 0s 823us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1082
16/16 [==============================] - 0s 831us/step - loss: 2.2431
16/16 [==============================] - 0s 843us/step - loss: 2.6105
16/16 [==============================] - 0s 781us/step - loss: 2.6582
16/16 [==============================] - 0s 786us/step - loss: 2.6367
16/16 [==============================] - 0s 790us/step - loss: 2.5673
16/16 [==============================] - 0s 809us/step - loss: 2.5156
16/16 [==============================] - 0s 805us/step - loss: 2.4986
16/16 [==============================] - 0s 807us/step - loss: 2.4937
16/16 [==============================] - 0s 804us/step - loss: 2.4937
Epoch 57 of 60

Testing for epoch 57 index 1:
79/79 [==============================] - 0s 582us/step
16/16 [==============================] - 0s 792us/step - loss: 0.1051
16/16 [==============================] - 0s 794us/step - loss: 2.3166
16/16 [==============================] - 0s 852us/step - loss: 2.6952
16/16 [==============================] - 0s 812us/step - loss: 2.7417
16/16 [==============================] - 0s 788us/step - loss: 2.7163
16/16 [==============================] - 0s 781us/step - loss: 2.6402
16/16 [==============================] - 0s 775us/step - loss: 2.5838
16/16 [==============================] - 0s 787us/step - loss: 2.5650
16/16 [==============================] - 0s 783us/step - loss: 2.5596
16/16 [==============================] - 0s 808us/step - loss: 2.5596

Testing for epoch 57 index 2:
79/79 [==============================] - 0s 614us/step
16/16 [==============================] - 0s 798us/step - loss: 0.1056
16/16 [==============================] - 0s 1ms/step - loss: 2.1999
16/16 [==============================] - 0s 1ms/step - loss: 2.5532
16/16 [==============================] - 0s 1ms/step - loss: 2.5957
16/16 [==============================] - 0s 1ms/step - loss: 2.5734
16/16 [==============================] - 0s 930us/step - loss: 2.5046
16/16 [==============================] - 0s 902us/step - loss: 2.4545
16/16 [==============================] - 0s 1ms/step - loss: 2.4380
16/16 [==============================] - 0s 1ms/step - loss: 2.4332
16/16 [==============================] - 0s 1ms/step - loss: 2.4332

Testing for epoch 57 index 3:
79/79 [==============================] - 0s 665us/step
16/16 [==============================] - 0s 791us/step - loss: 0.1134
16/16 [==============================] - 0s 842us/step - loss: 2.1538
16/16 [==============================] - 0s 780us/step - loss: 2.5023
16/16 [==============================] - 0s 775us/step - loss: 2.5460
16/16 [==============================] - 0s 788us/step - loss: 2.5254
16/16 [==============================] - 0s 775us/step - loss: 2.4601
16/16 [==============================] - 0s 775us/step - loss: 2.4117
16/16 [==============================] - 0s 789us/step - loss: 2.3954
16/16 [==============================] - 0s 788us/step - loss: 2.3907
16/16 [==============================] - 0s 787us/step - loss: 2.3906

Testing for epoch 57 index 4:
79/79 [==============================] - 0s 909us/step
16/16 [==============================] - 0s 804us/step - loss: 0.1055
16/16 [==============================] - 0s 809us/step - loss: 2.3141
16/16 [==============================] - 0s 817us/step - loss: 2.6890
16/16 [==============================] - 0s 861us/step - loss: 2.7333
16/16 [==============================] - 0s 789us/step - loss: 2.7077
16/16 [==============================] - 0s 777us/step - loss: 2.6323
16/16 [==============================] - 0s 780us/step - loss: 2.5774
16/16 [==============================] - 0s 780us/step - loss: 2.5593
16/16 [==============================] - 0s 802us/step - loss: 2.5541
16/16 [==============================] - 0s 790us/step - loss: 2.5541

Testing for epoch 57 index 5:
79/79 [==============================] - 0s 604us/step
16/16 [==============================] - 0s 801us/step - loss: 0.1056
16/16 [==============================] - 0s 789us/step - loss: 2.2966
16/16 [==============================] - 0s 796us/step - loss: 2.6697
16/16 [==============================] - 0s 781us/step - loss: 2.7157
16/16 [==============================] - 0s 781us/step - loss: 2.6915
16/16 [==============================] - 0s 1ms/step - loss: 2.6184
16/16 [==============================] - 0s 1ms/step - loss: 2.5641
16/16 [==============================] - 0s 803us/step - loss: 2.5462
16/16 [==============================] - 0s 776us/step - loss: 2.5410
16/16 [==============================] - 0s 805us/step - loss: 2.5409
Epoch 58 of 60

Testing for epoch 58 index 1:
79/79 [==============================] - 0s 585us/step
16/16 [==============================] - 0s 781us/step - loss: 0.1061
16/16 [==============================] - 0s 785us/step - loss: 2.3267
16/16 [==============================] - 0s 777us/step - loss: 2.7069
16/16 [==============================] - 0s 784us/step - loss: 2.7544
16/16 [==============================] - 0s 782us/step - loss: 2.7306
16/16 [==============================] - 0s 1ms/step - loss: 2.6573
16/16 [==============================] - 0s 816us/step - loss: 2.6031
16/16 [==============================] - 0s 1ms/step - loss: 2.5850
16/16 [==============================] - 0s 1ms/step - loss: 2.5797
16/16 [==============================] - 0s 1ms/step - loss: 2.5796

Testing for epoch 58 index 2:
79/79 [==============================] - 0s 578us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.1053
16/16 [==============================] - 0s 1ms/step - loss: 2.2433
16/16 [==============================] - 0s 778us/step - loss: 2.6007
16/16 [==============================] - 0s 778us/step - loss: 2.6389
16/16 [==============================] - 0s 782us/step - loss: 2.6116
16/16 [==============================] - 0s 784us/step - loss: 2.5366
16/16 [==============================] - 0s 777us/step - loss: 2.4824
16/16 [==============================] - 0s 773us/step - loss: 2.4647
16/16 [==============================] - 0s 780us/step - loss: 2.4596
16/16 [==============================] - 0s 792us/step - loss: 2.4596

Testing for epoch 58 index 3:
79/79 [==============================] - 0s 586us/step
16/16 [==============================] - 0s 796us/step - loss: 0.1060
16/16 [==============================] - 0s 813us/step - loss: 2.2387
16/16 [==============================] - 0s 788us/step - loss: 2.5997
16/16 [==============================] - 0s 780us/step - loss: 2.6431
16/16 [==============================] - 0s 827us/step - loss: 2.6195
16/16 [==============================] - 0s 818us/step - loss: 2.5483
16/16 [==============================] - 0s 820us/step - loss: 2.4955
16/16 [==============================] - 0s 774us/step - loss: 2.4780
16/16 [==============================] - 0s 791us/step - loss: 2.4729
16/16 [==============================] - 0s 801us/step - loss: 2.4729

Testing for epoch 58 index 4:
79/79 [==============================] - 0s 586us/step
16/16 [==============================] - 0s 812us/step - loss: 0.1044
16/16 [==============================] - 0s 792us/step - loss: 2.3186
16/16 [==============================] - 0s 778us/step - loss: 2.6976
16/16 [==============================] - 0s 779us/step - loss: 2.7451
16/16 [==============================] - 0s 783us/step - loss: 2.7220
16/16 [==============================] - 0s 795us/step - loss: 2.6492
16/16 [==============================] - 0s 787us/step - loss: 2.5949
16/16 [==============================] - 0s 792us/step - loss: 2.5768
16/16 [==============================] - 0s 1ms/step - loss: 2.5714
16/16 [==============================] - 0s 807us/step - loss: 2.5713

Testing for epoch 58 index 5:
79/79 [==============================] - 0s 593us/step
16/16 [==============================] - 0s 783us/step - loss: 0.1039
16/16 [==============================] - 0s 794us/step - loss: 2.2733
16/16 [==============================] - 0s 768us/step - loss: 2.6396
16/16 [==============================] - 0s 790us/step - loss: 2.6824
16/16 [==============================] - 0s 774us/step - loss: 2.6577
16/16 [==============================] - 0s 780us/step - loss: 2.5860
16/16 [==============================] - 0s 1ms/step - loss: 2.5336
16/16 [==============================] - 0s 1ms/step - loss: 2.5162
16/16 [==============================] - 0s 1ms/step - loss: 2.5110
16/16 [==============================] - 0s 1ms/step - loss: 2.5109
Epoch 59 of 60

Testing for epoch 59 index 1:
79/79 [==============================] - 0s 574us/step
16/16 [==============================] - 0s 785us/step - loss: 0.1015
16/16 [==============================] - 0s 799us/step - loss: 2.2663
16/16 [==============================] - 0s 786us/step - loss: 2.6320
16/16 [==============================] - 0s 785us/step - loss: 2.6746
16/16 [==============================] - 0s 782us/step - loss: 2.6500
16/16 [==============================] - 0s 779us/step - loss: 2.5779
16/16 [==============================] - 0s 776us/step - loss: 2.5255
16/16 [==============================] - 0s 781us/step - loss: 2.5079
16/16 [==============================] - 0s 780us/step - loss: 2.5025
16/16 [==============================] - 0s 807us/step - loss: 2.5023

Testing for epoch 59 index 2:
79/79 [==============================] - 0s 578us/step
16/16 [==============================] - 0s 787us/step - loss: 0.1025
16/16 [==============================] - 0s 807us/step - loss: 2.3515
16/16 [==============================] - 0s 777us/step - loss: 2.7275
16/16 [==============================] - 0s 774us/step - loss: 2.7688
16/16 [==============================] - 0s 1ms/step - loss: 2.7405
16/16 [==============================] - 0s 1ms/step - loss: 2.6618
16/16 [==============================] - 0s 776us/step - loss: 2.6044
16/16 [==============================] - 0s 1ms/step - loss: 2.5857
16/16 [==============================] - 0s 780us/step - loss: 2.5804
16/16 [==============================] - 0s 786us/step - loss: 2.5804

Testing for epoch 59 index 3:
79/79 [==============================] - 0s 602us/step
16/16 [==============================] - 0s 787us/step - loss: 0.1027
16/16 [==============================] - 0s 791us/step - loss: 2.2619
16/16 [==============================] - 0s 780us/step - loss: 2.6166
16/16 [==============================] - 0s 783us/step - loss: 2.6521
16/16 [==============================] - 0s 819us/step - loss: 2.6231
16/16 [==============================] - 0s 796us/step - loss: 2.5477
16/16 [==============================] - 0s 827us/step - loss: 2.4932
16/16 [==============================] - 0s 795us/step - loss: 2.4756
16/16 [==============================] - 0s 786us/step - loss: 2.4707
16/16 [==============================] - 0s 778us/step - loss: 2.4708

Testing for epoch 59 index 4:
79/79 [==============================] - 0s 608us/step
16/16 [==============================] - 0s 808us/step - loss: 0.1013
16/16 [==============================] - 0s 800us/step - loss: 2.2814
16/16 [==============================] - 0s 797us/step - loss: 2.6450
16/16 [==============================] - 0s 797us/step - loss: 2.6843
16/16 [==============================] - 0s 797us/step - loss: 2.6560
16/16 [==============================] - 0s 801us/step - loss: 2.5800
16/16 [==============================] - 0s 792us/step - loss: 2.5259
16/16 [==============================] - 0s 796us/step - loss: 2.5080
16/16 [==============================] - 0s 814us/step - loss: 2.5027
16/16 [==============================] - 0s 820us/step - loss: 2.5026

Testing for epoch 59 index 5:
79/79 [==============================] - 0s 657us/step
16/16 [==============================] - 0s 795us/step - loss: 0.1035
16/16 [==============================] - 0s 785us/step - loss: 2.3241
16/16 [==============================] - 0s 810us/step - loss: 2.7005
16/16 [==============================] - 0s 802us/step - loss: 2.7443
16/16 [==============================] - 0s 788us/step - loss: 2.7189
16/16 [==============================] - 0s 785us/step - loss: 2.6445
16/16 [==============================] - 0s 783us/step - loss: 2.5895
16/16 [==============================] - 0s 787us/step - loss: 2.5711
16/16 [==============================] - 0s 794us/step - loss: 2.5655
16/16 [==============================] - 0s 1ms/step - loss: 2.5654
Epoch 60 of 60

Testing for epoch 60 index 1:
79/79 [==============================] - 0s 828us/step
16/16 [==============================] - 0s 1ms/step - loss: 0.0991
16/16 [==============================] - 0s 797us/step - loss: 2.3587
16/16 [==============================] - 0s 1ms/step - loss: 2.7350
16/16 [==============================] - 0s 1ms/step - loss: 2.7747
16/16 [==============================] - 0s 1ms/step - loss: 2.7463
16/16 [==============================] - 0s 1ms/step - loss: 2.6678
16/16 [==============================] - 0s 1ms/step - loss: 2.6113
16/16 [==============================] - 0s 1ms/step - loss: 2.5929
16/16 [==============================] - 0s 1ms/step - loss: 2.5877
16/16 [==============================] - 0s 809us/step - loss: 2.5878

Testing for epoch 60 index 2:
79/79 [==============================] - 0s 576us/step
16/16 [==============================] - 0s 828us/step - loss: 0.1038
16/16 [==============================] - 0s 830us/step - loss: 2.2804
16/16 [==============================] - 0s 783us/step - loss: 2.6366
16/16 [==============================] - 0s 788us/step - loss: 2.6709
16/16 [==============================] - 0s 1ms/step - loss: 2.6408
16/16 [==============================] - 0s 1ms/step - loss: 2.5626
16/16 [==============================] - 0s 1ms/step - loss: 2.5067
16/16 [==============================] - 0s 1ms/step - loss: 2.4886
16/16 [==============================] - 0s 1ms/step - loss: 2.4834
16/16 [==============================] - 0s 803us/step - loss: 2.4834

Testing for epoch 60 index 3:
79/79 [==============================] - 0s 812us/step
16/16 [==============================] - 0s 788us/step - loss: 0.0991
16/16 [==============================] - 0s 1ms/step - loss: 2.3934
16/16 [==============================] - 0s 1ms/step - loss: 2.7744
16/16 [==============================] - 0s 1ms/step - loss: 2.8152
16/16 [==============================] - 0s 1ms/step - loss: 2.7874
16/16 [==============================] - 0s 1ms/step - loss: 2.7085
16/16 [==============================] - 0s 777us/step - loss: 2.6519
16/16 [==============================] - 0s 1ms/step - loss: 2.6333
16/16 [==============================] - 0s 778us/step - loss: 2.6279
16/16 [==============================] - 0s 1ms/step - loss: 2.6278

Testing for epoch 60 index 4:
79/79 [==============================] - 0s 579us/step
16/16 [==============================] - 0s 793us/step - loss: 0.0991
16/16 [==============================] - 0s 784us/step - loss: 2.3763
16/16 [==============================] - 0s 782us/step - loss: 2.7524
16/16 [==============================] - 0s 782us/step - loss: 2.7925
16/16 [==============================] - 0s 1ms/step - loss: 2.7641
16/16 [==============================] - 0s 1ms/step - loss: 2.6876
16/16 [==============================] - 0s 1ms/step - loss: 2.6322
16/16 [==============================] - 0s 1ms/step - loss: 2.6139
16/16 [==============================] - 0s 1ms/step - loss: 2.6085
16/16 [==============================] - 0s 1ms/step - loss: 2.6085

Testing for epoch 60 index 5:
79/79 [==============================] - 0s 581us/step
16/16 [==============================] - 0s 786us/step - loss: 0.1005
16/16 [==============================] - 0s 785us/step - loss: 2.3451
16/16 [==============================] - 0s 785us/step - loss: 2.7153
16/16 [==============================] - 0s 771us/step - loss: 2.7525
16/16 [==============================] - 0s 774us/step - loss: 2.7234
16/16 [==============================] - 0s 789us/step - loss: 2.6445
16/16 [==============================] - 0s 784us/step - loss: 2.5884
16/16 [==============================] - 0s 789us/step - loss: 2.5702
16/16 [==============================] - 0s 789us/step - loss: 2.5652
16/16 [==============================] - 0s 802us/step - loss: 2.5653
79/79 [==============================] - 0s 580us/step</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1840">
<div class="sourceCode cell-code" id="cb493"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb493-1"><a href="#cb493-1" aria-hidden="true" tabindex="-1"></a>outlier_MO_GAAL_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1841">
<div class="sourceCode cell-code" id="cb494"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb494-1"><a href="#cb494-1" aria-hidden="true" tabindex="-1"></a>outlier_MO_GAAL_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_MO_GAAL_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1842">
<div class="sourceCode cell-code" id="cb495"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb495-1"><a href="#cb495-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_2,outlier_MO_GAAL_one,tab_bunny)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1843">
<div class="sourceCode cell-code" id="cb496"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb496-1"><a href="#cb496-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"MO-GAAL (Liu et al., 2019)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-348-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.952
Precision: 0.952
Recall: 1.000
F1 Score: 0.975</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1844">
<div class="sourceCode cell-code" id="cb499"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb499-1"><a href="#cb499-1" aria-hidden="true" tabindex="-1"></a>thirteen <span class="op">=</span> twelve.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  thirteen = twelve.append(_conf.tab)</code></pre>
</div>
</div>
</section>
<section id="lscp" class="level3">
<h3 class="anchored" data-anchor-id="lscp">LSCP</h3>
<div class="cell" data-execution_count="1845">
<div class="sourceCode cell-code" id="cb501"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb501-1"><a href="#cb501-1" aria-hidden="true" tabindex="-1"></a>detectors <span class="op">=</span> [KNN(), LOF(), OCSVM()]</span>
<span id="cb501-2"><a href="#cb501-2" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LSCP(detectors,contamination<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb501-3"><a href="#cb501-3" aria-hidden="true" tabindex="-1"></a>clf.fit(_df[[<span class="st">'x'</span>, <span class="st">'y'</span>,<span class="st">'fnoise'</span>]])</span>
<span id="cb501-4"><a href="#cb501-4" aria-hidden="true" tabindex="-1"></a>_df[<span class="st">'LSCP_clf'</span>] <span class="op">=</span> clf.labels_</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/pyod/models/lscp.py:382: UserWarning: The number of histogram bins is greater than the number of classifiers, reducing n_bins to n_clf.
  warnings.warn(</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1846">
<div class="sourceCode cell-code" id="cb503"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb503-1"><a href="#cb503-1" aria-hidden="true" tabindex="-1"></a>outlier_LSCP_one <span class="op">=</span> <span class="bu">list</span>(clf.labels_)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1847">
<div class="sourceCode cell-code" id="cb504"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb504-1"><a href="#cb504-1" aria-hidden="true" tabindex="-1"></a>outlier_LSCP_one <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">0</span>  <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>,outlier_LSCP_one))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1848">
<div class="sourceCode cell-code" id="cb505"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb505-1"><a href="#cb505-1" aria-hidden="true" tabindex="-1"></a>_conf <span class="op">=</span> Conf_matrx(outlier_true_one_2,outlier_LSCP_one,tab_bunny)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="1849">
<div class="sourceCode cell-code" id="cb506"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb506-1"><a href="#cb506-1" aria-hidden="true" tabindex="-1"></a>_conf.conf(<span class="st">"LSCP (Zhao et al., 2019)"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2023-07-03-other_outlier_detection-Copy1_files/figure-html/cell-354-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.978
Precision: 0.990
Recall: 0.987
F1 Score: 0.989</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  self.tab = self.tab.append(pd.DataFrame({"Accuracy":[self.acc],"Precision":[self.pre],"Recall":[self.rec],"F1":[self.f1]},index = [name]))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1850">
<div class="sourceCode cell-code" id="cb509"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb509-1"><a href="#cb509-1" aria-hidden="true" tabindex="-1"></a>fourteen <span class="op">=</span> thirteen.append(_conf.tab)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  fourteen = thirteen.append(_conf.tab)</code></pre>
</div>
</div>
</section>
</section>
<section id="bunny-result" class="level2">
<h2 class="anchored" data-anchor-id="bunny-result">Bunny Result</h2>
<div class="cell" data-execution_count="1851">
<div class="sourceCode cell-code" id="cb511"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb511-1"><a href="#cb511-1" aria-hidden="true" tabindex="-1"></a>fourteen.<span class="bu">round</span>(<span class="dv">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1851">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Accuracy</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>GODE</th>
      <td>0.988</td>
      <td>0.995</td>
      <td>0.993</td>
      <td>0.994</td>
    </tr>
    <tr>
      <th>LOF (Breunig et al., 2000)</th>
      <td>0.913</td>
      <td>0.955</td>
      <td>0.953</td>
      <td>0.954</td>
    </tr>
    <tr>
      <th>kNN (Ramaswamy et al., 2000)</th>
      <td>0.942</td>
      <td>0.997</td>
      <td>0.942</td>
      <td>0.969</td>
    </tr>
    <tr>
      <th>OCSVM (Sch ̈olkopf et al., 2001)</th>
      <td>0.935</td>
      <td>0.992</td>
      <td>0.939</td>
      <td>0.965</td>
    </tr>
    <tr>
      <th>MCD (Hardin and Rocke, 2004)</th>
      <td>0.982</td>
      <td>0.992</td>
      <td>0.989</td>
      <td>0.990</td>
    </tr>
    <tr>
      <th>Feature Bagging (Lazarevic and Kumar, 2005)</th>
      <td>0.954</td>
      <td>0.977</td>
      <td>0.974</td>
      <td>0.976</td>
    </tr>
    <tr>
      <th>ABOD (Kriegel et al., 2008)</th>
      <td>0.979</td>
      <td>0.990</td>
      <td>0.988</td>
      <td>0.989</td>
    </tr>
    <tr>
      <th>Isolation Forest (Liu et al., 2008)</th>
      <td>0.827</td>
      <td>0.995</td>
      <td>0.822</td>
      <td>0.900</td>
    </tr>
    <tr>
      <th>HBOS (Goldstein and Dengel, 2012)</th>
      <td>0.919</td>
      <td>0.958</td>
      <td>0.956</td>
      <td>0.957</td>
    </tr>
    <tr>
      <th>SOS (Janssens et al., 2012)</th>
      <td>0.912</td>
      <td>0.955</td>
      <td>0.953</td>
      <td>0.954</td>
    </tr>
    <tr>
      <th>SO-GAAL (Liu et al., 2019)</th>
      <td>0.952</td>
      <td>0.952</td>
      <td>1.000</td>
      <td>0.975</td>
    </tr>
    <tr>
      <th>MO-GAAL (Liu et al., 2019)</th>
      <td>0.952</td>
      <td>0.952</td>
      <td>1.000</td>
      <td>0.975</td>
    </tr>
    <tr>
      <th>LSCP (Zhao et al., 2019)</th>
      <td>0.978</td>
      <td>0.990</td>
      <td>0.987</td>
      <td>0.989</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">Bunny 5%</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">GODE</td>
<td style="text-align: center;">0.988</td>
<td style="text-align: center;">0.995</td>
<td style="text-align: center;">0.993</td>
<td style="text-align: center;">0.994</td>
</tr>
<tr class="even">
<td style="text-align: center;">LOF (Breunig et al., 2000)</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.955</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">0.954</td>
</tr>
<tr class="odd">
<td style="text-align: center;">KNN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">CBLOF</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">OCSVM (Sch ̈olkopf et al., 2001)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">MCD (Hardin and Rocke, 2004)</td>
<td style="text-align: center;">0.982</td>
<td style="text-align: center;">0.992</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.990</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Feature Bagging (Lazarevic and Kumar, 2005)</td>
<td style="text-align: center;">0.954</td>
<td style="text-align: center;">0.977</td>
<td style="text-align: center;">0.975</td>
<td style="text-align: center;">0.976</td>
</tr>
<tr class="even">
<td style="text-align: center;">ABOD (Kriegel et al., 2008)</td>
<td style="text-align: center;">0.977</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.987</td>
<td style="text-align: center;">0.988</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Isolation Forest (Liu et al., 2008)</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">0.996</td>
<td style="text-align: center;">0.795</td>
<td style="text-align: center;">0.884</td>
</tr>
<tr class="even">
<td style="text-align: center;">HBOS (Goldstein and Dengel, 2012)</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.957</td>
</tr>
<tr class="odd">
<td style="text-align: center;">SOS (Janssens et al., 2012)</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.955</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">0.954</td>
</tr>
<tr class="even">
<td style="text-align: center;">SO-GAAL (Liu et al., 2019)</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.975</td>
</tr>
<tr class="odd">
<td style="text-align: center;">MO-GAAL (Liu et al., 2019)</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.975</td>
</tr>
<tr class="even">
<td style="text-align: center;">LSCP (Zhao et al., 2019)</td>
<td style="text-align: center;">0.980</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.988</td>
<td style="text-align: center;">0.989</td>
</tr>
</tbody>
</table>



</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="seoyeonc/blog" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>