{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b8a18c36-7705-4cf3-a9a5-358a4a93c041",
   "metadata": {
    "id": "cac470df-29e7-4148-9bbd-d8b9a32fa570",
    "tags": []
   },
   "source": [
    "---\n",
    "title: \"CAM_downsize\"\n",
    "author: \"SEOYEON CHOI\"\n",
    "date: \"2023-08-10\"\n",
    "categories:\n",
    "  - CAM\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d83b071-7964-4407-ab86-95162852c56e",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b518da-7b1f-47c3-b5af-c578ca1a7ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from fastai.vision.all import *\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageFile\n",
    "from PIL import Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from torchvision.utils import save_image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189ca5b6-ac1a-49e4-9dc7-a42d1523a25d",
   "metadata": {},
   "source": [
    "#### (1) 랜덤박스가 들어간 개 고양이 그림 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f1ba35b-bdbb-4487-94f3-91e34393b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=untar_data(URLs.PETS)/'images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19050cc0-c59b-4d4a-8e7b-b2a3d56c2d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#7390) [Path('/home/csy/.fastai/data/oxford-iiit-pet/images/Bombay_13.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/beagle_193.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/Ragdoll_8.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/boxer_106.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/keeshond_56.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_162.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/saint_bernard_136.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_76.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/pug_173.jpg'),Path('/home/csy/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_117.jpg')...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70016aa0-0066-47a4-8c7a-5d5234ce7041",
   "metadata": {},
   "outputs": [],
   "source": [
    "files=get_image_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a884697-3fc8-4552-bc54-a74366aab5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_func(f):\n",
    "    if f[0].isupper():\n",
    "        return 'cat' \n",
    "    else: \n",
    "        return 'dog' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfeb0c53-11b1-4f08-b15b-9725da0016eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls=ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a85a5-f520-49cf-8bfb-4ee63da4e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/fastai/vision/learner.py:288: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n",
      "  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n",
      "/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/csy/anaconda3/envs/temp_csy/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.174851</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>33:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='6' class='' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      25.00% [6/24 00:54&lt;02:42 0.0684]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrnr=cnn_learner(dls,resnet34,metrics=error_rate)\n",
    "lrnr.fine_tune(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "473e4380-c443-49d7-9a58-2b5eb6bcffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net1=lrnr.model[0]\n",
    "net2=lrnr.model[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9a1eefa-a1f3-4e6d-b5d7-dcbd76c8c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.AdaptiveAvgPool2d(output_size=1), \n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(2,out_features=2,bias=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b20514d-0c94-483f-b092-4f8d3fc66bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=torch.nn.Sequential(net1,net2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d7ccfbf-df3d-4e2b-9522-35ee3698e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr2=Learner(dls,net,metrics=accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c2ffb-7f85-4896-b799-e884ad44f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr2.fine_tune(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3793c8d7-7235-402b-aa72-11d3e4ea61f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(lrnr2)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf71f6-39f8-4fdd-aac6-a0d123cb857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.print_classification_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae17c54-589e-4aa9-979b-50a79496dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=untar_data(URLs.PETS)/'images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97d31d2-309e-4bc2-bb46-20ec20ad77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if str(list(path.ls())[103]).split('/')[-1].split('.')[-1]==\"jpg\" :\n",
    "    print(\"jpg\")\n",
    "#name=str(list(path.ls())[i]).split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88fb2a-02a5-4515-8535-e3d90a62162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e068895-e175-446e-ab9e-1afa94f5937f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 320)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PILImage.create(get_image_files(path)[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8357e388-a15f-403d-9145-804a08664b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76800"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "240*320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7cb06f7-1e0c-40b6-8ddf-8af0c4164721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512*512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664dcd23-ec0c-4eeb-8210-f27f74bd3938",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7393) :\n",
    "    img = PILImage.create(get_image_files(path)[i])\n",
    "    img = img.resize([512,512], resample=None, box=None, reducing_gap=None)\n",
    "    name = str(list(path.ls())[i]).split('/')[-1]\n",
    "    fname = name.split('.')[-1]\n",
    "    if fname!=\"jpg\" : \n",
    "        print(name)\n",
    "    else : pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5662b860-d973-4973-8c1f-198a72aea8c1",
   "metadata": {},
   "source": [
    "`.mat` 파일 같은 이상한 거 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b317c08b-e227-4c02-96be-b51a9e038528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.remove(r\"/home/csy/.fastai/data/oxford-iiit-pet/images/Abyssinian_100.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1326b-9280-4b47-b051-b1a0d2c340a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.remove(r\"/home/csy/.fastai/data/oxford-iiit-pet/images/Abyssinian_102.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4623edde-4f3a-436b-a692-c971d287374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.remove(r\"/home/csy/.fastai/data/oxford-iiit-pet/images/Abyssinian_101.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0360f001-f8ef-483d-b8f4-0464458ec512",
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512337c8-ca62-46d8-b8a4-bcd8c2d710ce",
   "metadata": {},
   "source": [
    "### 랜덤박스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24b8730-5756-4b4e-8229-711946aa10cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(\"random_pet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2caed24-f705-4270-b5d0-fb8cd7f138a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(path.ls())) :\n",
    "    img = PILImage.create(get_image_files(path)[i])\n",
    "    img = img.resize([512,512], resample=None, box=None, reducing_gap=None)\n",
    "    (w, h) = (img.shape[0], img.shape[1])\n",
    "    a = random.uniform(0, w*0.7)\n",
    "    b = random.uniform(0, h*0.9)\n",
    "    shape = [(a, b), (a+100, b+50)]\n",
    "    font = ImageFont.truetype(\"DejaVuSans.ttf\", round(h*0.08))\n",
    "    name = str(list(path.ls())[i]).split('/')[-1]\n",
    "    fname = name.split('.')[-1]\n",
    "    if name[0].isupper() == True :\n",
    "        img1 = ImageDraw.Draw(img)  \n",
    "        img1.rectangle(shape, fill =\"white\", outline =\"black\")\n",
    "        ImageDraw.Draw(img).text((a, b), 'CAT', (0,0,0), font=font)\n",
    "        img.save(\"random_pet/\"+name)\n",
    "    else: \n",
    "        img1 = ImageDraw.Draw(img)  \n",
    "        img1.rectangle(shape, fill =\"black\", outline =\"black\")\n",
    "        ImageDraw.Draw(img).text((a, b), 'DOG', (255,255,255), font=font)\n",
    "        img.save(\"random_pet/\"+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15145b1d-6a03-4135-a43f-9ea5e84a743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_r=Path('random_pet')   #랜덤박스넣은사진"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b771802-8427-4348-ae25-b21d7e36aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "files=get_image_files(path_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3513d4af-ceed-49e9-be0c-906febfb54fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_r=ImageDataLoaders.from_name_func(path_r,files,label_func,item_tfms=Resize(512)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d5abe3-5edb-47ef-b211-398c9995962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr_r1=cnn_learner(dls_r,resnet34,metrics=error_rate)\n",
    "lrnr_r1.fine_tune(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ae1745-d459-429d-bb39-2a7e6ac03e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_1=lrnr_r1.model[0]\n",
    "net_2=lrnr_r1.model[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6540410-81ee-4cec-8e8b-f9ae20fc43c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_2 = torch.nn.Sequential(\n",
    "    torch.nn.AdaptiveAvgPool2d(output_size=1), \n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(512,out_features=2,bias=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f23173-c028-4ab8-addf-07a95b837359",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_r=torch.nn.Sequential(net_1,net_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ea828-adcc-42ac-afb8-397550452892",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr_r2=Learner(dls_r,net_r,metrics=accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9091da62-09e0-4f8b-96d5-770306f3a816",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr_r2.fine_tune(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfbc071-ead2-4e57-802a-b9f79b082476",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(lrnr_r2)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35682c7-798f-45f3-bdd0-8b5d8c567c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, = first(dls_r.test_dl([PILImage.create(get_image_files(path_r)[7389])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ed34e-dd14-4e9e-972f-92ca4a0073f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "camimg = torch.einsum('ij,jkl -> ikl', net_2[2].weight, net_1(x).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5b4e65-2935-4bef-b845-60865db50402",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_2[2].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ead559a-4d46-4b3e-8fbb-b7bf21e02b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_1(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ad78e3-1d50-4422-8295-38d0605d994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "512*16*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f89fa-4abd-4585-bf20-0d832e4ec40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "2048*8*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ea6ac-bfa9-4e5b-8c47-4876c81f1663",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_1(x).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f5cb1-f848-4904-a214-d68d2b52bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23e74f-e347-472f-9ee1-375a2943d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "camimg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf81b8f4-abbb-4d3d-aba1-fcb71e9ac2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2) \n",
    "# \n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax1)\n",
    "ax1.imshow(camimg[0].to(\"cpu\").detach(),alpha=0.3,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n",
    "#\n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax2)\n",
    "ax2.imshow(camimg[1].to(\"cpu\").detach(),alpha=0.3,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d8a84-6c21-49fa-ae50-b1bc8bbce5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서연 수정 code\n",
    "fig, (ax1,ax2) = plt.subplots(1,2) \n",
    "# \n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax1)\n",
    "ax1.imshow(camimg[0].to(\"cpu\").detach(),alpha=0.7,extent=(0,511,511,0),interpolation='bilinear',cmap='bone')\n",
    "#\n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax2)\n",
    "ax2.imshow(camimg[1].to(\"cpu\").detach(),alpha=0.7,extent=(0,511,511,0),interpolation='bilinear',cmap='bone')\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9988eb23-010b-450b-8602-9cb6f667ee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서연 수정 code\n",
    "fig, (ax1,ax2) = plt.subplots(1,2) \n",
    "# \n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax1)\n",
    "ax1.imshow(camimg[0].to(\"cpu\").detach(),alpha=0.7,extent=(0,511,511,0),interpolation='bilinear',cmap='bone')\n",
    "#\n",
    "dls_r.train.decode((x,))[0].squeeze().show(ax=ax2)\n",
    "ax2.imshow(camimg[1].to(\"cpu\").detach(),alpha=0.7,extent=(0,511,511,0),interpolation='bilinear',cmap='bone')\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b922e5-6404-49a3-ab5f-bdd624655af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,5) \n",
    "k=0 \n",
    "for i in range(5):\n",
    "    for j in range(5): \n",
    "        x, = first(dls_r.test_dl([PILImage.create(get_image_files(path_r)[k])]))\n",
    "        camimg = torch.einsum('ij,jkl -> ikl', net_2[2].weight, net_1(x).squeeze())\n",
    "        a,b = net_r(x).tolist()[0]\n",
    "        catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) ,  np.exp(b)/ (np.exp(a)+np.exp(b)) \n",
    "        if catprob>dogprob: \n",
    "            dls_r.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n",
    "            ax[i][j].imshow(camimg[0].to(\"cpu\").detach(),alpha=0.7,extent=(0,512,512,0),interpolation='bilinear',cmap='bone')\n",
    "            ax[i][j].set_title(\"cat(%s)\" % catprob.round(5))\n",
    "        else: \n",
    "            dls_r.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n",
    "            ax[i][j].imshow(camimg[1].to(\"cpu\").detach(),alpha=0.7,extent=(0,512,512,0),interpolation='bilinear',cmap='bone')\n",
    "            ax[i][j].set_title(\"dog(%s)\" % dogprob.round(5))\n",
    "        k=k+1 \n",
    "fig.set_figwidth(16)            \n",
    "fig.set_figheight(16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3720ba18-e83d-400b-8370-88a8c9950501",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,5) \n",
    "k=0 \n",
    "for i in range(5):\n",
    "    for j in range(5): \n",
    "        x, = first(dls_r.test_dl([PILImage.create(get_image_files(path_r)[k])]))\n",
    "        camimg = torch.einsum('ij,jkl -> ikl', net_2[2].weight, net_1(x).squeeze())\n",
    "        a,b = net_r(x).tolist()[0]\n",
    "        catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) ,  np.exp(b)/ (np.exp(a)+np.exp(b))\n",
    "        if catprob>dogprob: \n",
    "            test=camimg[0]-torch.min(camimg[0])\n",
    "            A1=torch.exp(-0.1*test)\n",
    "            X1=np.array(A1.to(\"cpu\").detach(),dtype=np.float32)\n",
    "            Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "            x1=x.squeeze().to('cpu')*Y1-torch.min(x.squeeze().to('cpu'))*Y1\n",
    "            (x1*0.25).squeeze().show(ax=ax[i][j])\n",
    "            ax[i][j].set_title(\"cat(%s)\" % catprob.round(5))\n",
    "        else: \n",
    "            test=camimg[1]-torch.min(camimg[1])\n",
    "            A1=torch.exp(-0.1*test)\n",
    "            X1=np.array(A1.to(\"cpu\").detach(),dtype=np.float32)\n",
    "            Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "            x1=x.squeeze().to('cpu')*Y1-torch.min(x.squeeze().to('cpu'))*Y1\n",
    "            (x1*0.25).squeeze().show(ax=ax[i][j])\n",
    "            ax[i][j].set_title(\"dog(%s)\" % dogprob.round(5))\n",
    "        k=k+1 \n",
    "fig.set_figwidth(16)            \n",
    "fig.set_figheight(16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc23cad-8af5-4ea7-9565-394601153de8",
   "metadata": {},
   "source": [
    "- `.mat`파일 있나 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af0110-f56d-4280-8ece-3d73891dd480",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(path_r.ls())) :\n",
    "    img = PILImage.create(get_image_files(path_r)[i])\n",
    "    img = img.resize([512,512], resample=None, box=None, reducing_gap=None)\n",
    "    name = str(list(path.ls())[i]).split('/')[-1]\n",
    "    fname = name.split('.')[-1]\n",
    "    if fname!=\"jpg\" : \n",
    "        print(name)\n",
    "    else : pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27953ec-32eb-4378-9a6f-7d70e899ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, = first(dls_r.test_dl([PILImage.create(get_image_files(path_r)[1])]))\n",
    "camimg = torch.einsum('ij,jkl -> ikl', net_2[2].weight, net_1(x).squeeze())\n",
    "a,b = net_r(x).tolist()[0]\n",
    "catprob, dogprob = np.exp(a)/ (np.exp(a)+np.exp(b)) ,  np.exp(b)/ (np.exp(a)+np.exp(b))\n",
    "if catprob>dogprob: \n",
    "    test=camimg[0]-torch.min(camimg[0])\n",
    "    A1=torch.exp(-0.07*test)\n",
    "    X1=np.array(A1.to(\"cpu\").detach(),dtype=np.float32)\n",
    "    Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "    x1=x.squeeze().to('cpu')*Y1-torch.min(x.squeeze().to('cpu'))*Y1\n",
    "    (x1*0.25).squeeze().show()\n",
    "else: \n",
    "        test=camimg[1]-torch.min(camimg[1])\n",
    "        A1=torch.exp(-0.07*test)\n",
    "        X1=np.array(A1.to(\"cpu\").detach(),dtype=np.float32)\n",
    "        Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "        x1=x.squeeze().to('cpu')*Y1-torch.min(x.squeeze().to('cpu'))*Y1\n",
    "        (x1*0.25).squeeze().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb5145-f0ce-4c16-81fd-113884479e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #저장 참고\n",
    "# np_arr = np.array(tensor, dtype=np.uint8)\n",
    "# img = PIL.Image.fromarray(np_arr)\n",
    "# img.save('path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b89b4-1868-4b40-ad1e-9f806ad5ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = str(list(path.ls())[1]).split('/')[-1]\n",
    "# res1=(x1*0.35).squeeze()\n",
    "# res1.show()\n",
    "# save_image(res1, \"pet3_mode1_res/\"+name)\n",
    "#res1.save(\"pet3_mode1_res/\"+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535d051f-ba1c-42ac-a68e-4a4fc4359e2c",
   "metadata": {},
   "source": [
    "### 원본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d964a0-9942-4334-ae0c-e0d3f71d3d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(\"original_pet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e2b139-b9e8-4ef0-bd97-b59c7ee82335",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(path.ls())) :\n",
    "    img = PILImage.create(get_image_files(path)[i])\n",
    "    img = img.resize([512,512], resample=None, box=None, reducing_gap=None)\n",
    "    (w, h) = (img.shape[0], img.shape[1])\n",
    "    # a = random.uniform(0, w*0.7)\n",
    "    # b = random.uniform(0, h*0.9)\n",
    "    shape = [(a, b), (a+100, b+50)]\n",
    "    # font = ImageFont.truetype(\"DejaVuSans.ttf\", round(h*0.08))\n",
    "    name = str(list(path.ls())[i]).split('/')[-1]\n",
    "    fname = name.split('.')[-1]\n",
    "    if name[0].isupper() == True :\n",
    "        img1 = ImageDraw.Draw(img)  \n",
    "        # img1.rectangle(shape, fill =\"white\", outline =\"black\")\n",
    "        # ImageDraw.Draw(img).text((a, b), 'CAT', (0,0,0), font=font)\n",
    "        img.save(\"original_pet/\"+name)\n",
    "    else: \n",
    "        img1 = ImageDraw.Draw(img)  \n",
    "        # img1.rectangle(shape, fill =\"black\", outline =\"black\")\n",
    "        # ImageDraw.Draw(img).text((a, b), 'DOG', (255,255,255), font=font)\n",
    "        img.save(\"original_pet/\"+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f4398-d135-4207-912b-2b8ffd533e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_o=Path('original_pet')   #랜덤박스넣은사진"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd1a160-f447-4611-beba-f0ec9da19ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_o=get_image_files(path_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c77f02-7fe4-410d-ad22-2287b53a7486",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_o=ImageDataLoaders.from_name_func(path_o,files_o,label_func,item_tfms=Resize(512)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d5bcd-8d84-4019-b348-99f062e9305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr_o1=cnn_learner(dls_o,resnet34,metrics=error_rate)\n",
    "lrnr_o1.fine_tune(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c119b8d-a7fc-4f8b-96d7-f72080efd5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_o1=lrnr_o1.model[0]\n",
    "net_o2=lrnr_o1.model[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae980b6e-e1b5-4821-8665-bb4065e044c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_o2 = torch.nn.Sequential(\n",
    "    torch.nn.AdaptiveAvgPool2d(output_size=1), \n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(512,out_features=2,bias=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b94d6-edc1-4638-b672-c397d5d08f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_o=torch.nn.Sequential(net_o1,net_o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e99345-5b9a-4c75-9586-91eba8711498",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr_o2=Learner(dls_o,net_o,metrics=accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb994cde-42b8-43e1-9712-1879de29426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr_o2.fine_tune(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa00a083-8e5e-4316-a006-84871359bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_o = ClassificationInterpretation.from_learner(lrnr_o2)\n",
    "interp_o.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f167908-2c98-48cd-9d44-261bcf91420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_o, = first(dls_o.test_dl([PILImage.create(get_image_files(path_o)[7389])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94cdebf-943b-4b0a-aca5-106538b5e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "camimg_o = torch.einsum('ij,jkl -> ikl', net_o2[2].weight, net_o1(x).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a4d0f-3ef5-41b2-89ac-a68e599af0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad42e02-57ed-4523-b834-b469c76a4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "camimg_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a476ee42-fdf8-4353-b8fe-3c7a717b4388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서연 수정 code\n",
    "fig, (ax1,ax2) = plt.subplots(1,2) \n",
    "# \n",
    "dls_o.train.decode((x_o,))[0].squeeze().show(ax=ax1)\n",
    "ax1.imshow(camimg_o[0].to(\"cpu\").detach(),alpha=0.7,extent=(0,511,511,0),interpolation='bilinear',cmap='bone')\n",
    "#\n",
    "dls_r.train.decode((x_o,))[0].squeeze().show(ax=ax2)\n",
    "ax2.imshow(camimg_o[1].to(\"cpu\").detach(),alpha=0.7,extent=(0,511,511,0),interpolation='bilinear',cmap='bone')\n",
    "fig.set_figwidth(8)            \n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0ce733-3353-4f6a-8eb1-6cd8199115c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,5) \n",
    "k=0 \n",
    "for i in range(5):\n",
    "    for j in range(5): \n",
    "        x_o, = first(dls_o.test_dl([PILImage.create(get_image_files(path_o)[k])]))\n",
    "        camimg_o = torch.einsum('ij,jkl -> ikl', net_o2[2].weight, net_o1(x_o).squeeze())\n",
    "        a_o,b_o = net_r(x_o).tolist()[0]\n",
    "        catprob, dogprob = np.exp(a_o)/ (np.exp(a_o)+np.exp(b_o)) ,  np.exp(b_o)/ (np.exp(a_o)+np.exp(b_o)) \n",
    "        if catprob>dogprob: \n",
    "            dls_o.train.decode((x_o,))[0].squeeze().show(ax=ax[i][j])\n",
    "            ax[i][j].imshow(camimg_o[0].to(\"cpu\").detach(),alpha=0.7,extent=(0,512,512,0),interpolation='bilinear',cmap='bone')\n",
    "            ax[i][j].set_title(\"cat(%s)\" % catprob.round(5))\n",
    "        else: \n",
    "            dls_o.train.decode((x_o,))[0].squeeze().show(ax=ax[i][j])\n",
    "            ax[i][j].imshow(camimg_o[1].to(\"cpu\").detach(),alpha=0.7,extent=(0,512,512,0),interpolation='bilinear',cmap='bone')\n",
    "            ax[i][j].set_title(\"dog(%s)\" % dogprob.round(5))\n",
    "        k=k+1 \n",
    "fig.set_figwidth(16)            \n",
    "fig.set_figheight(16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f3aa7-4932-49e4-9fa8-ba791cd5afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,5) \n",
    "k=0 \n",
    "for i in range(5):\n",
    "    for j in range(5): \n",
    "        x_o, = first(dls_o.test_dl([PILImage.create(get_image_files(path_o)[k])]))\n",
    "        camimg_o = torch.einsum('ij,jkl -> ikl', net_o2[2].weight, net_o1(x).squeeze())\n",
    "        a_o,b_o = net_o(x_o).tolist()[0]\n",
    "        catprob, dogprob = np.exp(a_o)/ (np.exp(a_o)+np.exp(b_o)) ,  np.exp(b_o)/ (np.exp(a_o)+np.exp(b_o))\n",
    "        if catprob>dogprob: \n",
    "            test=camimg_o[0]-torch.min(camimg_o[0])\n",
    "            A1=torch.exp(-0.1*test)\n",
    "            X1=np.array(A1.to(\"cpu\").detach(),dtype=np.float32)\n",
    "            Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "            x1=x_o.squeeze().to('cpu')*Y1-torch.min(x_o.squeeze().to('cpu'))*Y1\n",
    "            (x1*0.25).squeeze().show(ax=ax[i][j])\n",
    "            ax[i][j].set_title(\"cat(%s)\" % catprob.round(5))\n",
    "        else: \n",
    "            test=camimg_o[1]-torch.min(camimg_o[1])\n",
    "            A1=torch.exp(-0.1*test)\n",
    "            X1=np.array(A1.to(\"cpu\").detach(),dtype=np.float32)\n",
    "            Y1=torch.Tensor(cv2.resize(X1,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "            x1=x_o.squeeze().to('cpu')*Y1-torch.min(x_o.squeeze().to('cpu'))*Y1\n",
    "            (x1*0.25).squeeze().show(ax=ax[i][j])\n",
    "            ax[i][j].set_title(\"dog(%s)\" % dogprob.round(5))\n",
    "        k=k+1 \n",
    "fig.set_figwidth(16)            \n",
    "fig.set_figheight(16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664dfc2b-7bfa-4a43-93ae-3a5605b5a1ec",
   "metadata": {},
   "source": [
    "- `.mat`파일 있나 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151075d8-943f-4021-828a-ce372a9d0e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(path_o.ls())) :\n",
    "    img = PILImage.create(get_image_files(path_o)[i])\n",
    "    img = img.resize([512,512], resample=None, box=None, reducing_gap=None)\n",
    "    name = str(list(path_o.ls())[i]).split('/')[-1]\n",
    "    fname = name.split('.')[-1]\n",
    "    if fname!=\"jpg\" : \n",
    "        print(name)\n",
    "    else : pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df04a9ea-f00e-4306-9044-775a6560154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_o, = first(dls_o.test_dl([PILImage.create(get_image_files(path_o)[1])]))\n",
    "camimg_o = torch.einsum('ij,jkl -> ikl', net_o2[2].weight, net_o1(x).squeeze())\n",
    "a_o,b_o = net_o(x_o).tolist()[0]\n",
    "catprob_o, dogprob_o = np.exp(a_o)/ (np.exp(a_o)+np.exp(b_o)) ,  np.exp(b_o)/ (np.exp(a_o)+np.exp(b_o))\n",
    "if catprob_o>dogprob_o: \n",
    "    test_o=camimg_o[0]-torch.min(camimg_o[0])\n",
    "    A1_o=torch.exp(-0.07*test_o)\n",
    "    X1_o=np.array(A1_o.to(\"cpu\").detach(),dtype=np.float32)\n",
    "    Y1_o=torch.Tensor(cv2.resize(X1_o,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "    x1_o=x_o.squeeze().to('cpu')*Y1_o-torch.min(x_o.squeeze().to('cpu'))*Y1_o\n",
    "    (x1_o*0.25).squeeze().show()\n",
    "else: \n",
    "        test_o=camimg_o[1]-torch.min(camimg_o[1])\n",
    "        A1_o=torch.exp(-0.07*test_o)\n",
    "        X1_o=np.array(A1_o.to(\"cpu\").detach(),dtype=np.float32)\n",
    "        Y1_o=torch.Tensor(cv2.resize(X1_o,(512,512),interpolation=cv2.INTER_LINEAR))\n",
    "        x1_o=x_o.squeeze().to('cpu')*Y1-torch.min(x_o.squeeze().to('cpu'))*Y1_o\n",
    "        (x1_o*0.25).squeeze().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc0cce8-7b17-4f9d-ac92-e31d0c1118c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #저장 참고\n",
    "# np_arr = np.array(tensor, dtype=np.uint8)\n",
    "# img = PIL.Image.fromarray(np_arr)\n",
    "# img.save('path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ed3765-0bac-4637-9f6e-90e1716ab43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = str(list(path.ls())[1]).split('/')[-1]\n",
    "# res1=(x1*0.35).squeeze()\n",
    "# res1.show()\n",
    "# save_image(res1, \"pet3_mode1_res/\"+name)\n",
    "#res1.save(\"pet3_mode1_res/\"+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9345f7f-a8f5-4c9b-a98c-70c192450a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555cafc6-11e5-4eac-96b1-b93114afd2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
