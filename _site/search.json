[
  {
    "objectID": "posts/Study/index.html",
    "href": "posts/Study/index.html",
    "title": "Study",
    "section": "",
    "text": "About Study"
  },
  {
    "objectID": "posts/Study/2022-12-31-Space-study.html",
    "href": "posts/Study/2022-12-31-Space-study.html",
    "title": "Study for Spaces",
    "section": "",
    "text": "Spaces"
  },
  {
    "objectID": "posts/Study/2022-12-31-Space-study.html#내적공간",
    "href": "posts/Study/2022-12-31-Space-study.html#내적공간",
    "title": "Study for Spaces",
    "section": "내적공간",
    "text": "내적공간\n\\[|a||b| \\cos \\theta\\]\n\n길이 + 각의 개념\n\nProjection"
  },
  {
    "objectID": "posts/Study/2022-12-31-Space-study.html#바나흐공간",
    "href": "posts/Study/2022-12-31-Space-study.html#바나흐공간",
    "title": "Study for Spaces",
    "section": "바나흐공간",
    "text": "바나흐공간\n\\[|---|\\]\n\n길이 + 극한의 개념"
  },
  {
    "objectID": "posts/Study/2022-12-31-Space-study.html#노름공간",
    "href": "posts/Study/2022-12-31-Space-study.html#노름공간",
    "title": "Study for Spaces",
    "section": "노름공간",
    "text": "노름공간\n\\[|   |\\]\n\n길이의 개념"
  },
  {
    "objectID": "posts/Study/2022-12-31-Space-study.html#힐베르트공간유클리드-비유클리드-모두-존재",
    "href": "posts/Study/2022-12-31-Space-study.html#힐베르트공간유클리드-비유클리드-모두-존재",
    "title": "Study for Spaces",
    "section": "힐베르트공간(유클리드 + 비유클리드 모두 존재)",
    "text": "힐베르트공간(유클리드 + 비유클리드 모두 존재)\n퓨리에 해석 - 길이 + 각 + 극한의 개념"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html",
    "href": "posts/GODE/2022-09-02-paper_simulation.html",
    "title": "Simulation",
    "section": "",
    "text": "Simulation"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#imports",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#imports",
    "title": "Simulation",
    "section": "imports",
    "text": "imports\n\nimport rpy2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport plotly.graph_objects as go\n\n\nimport tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport plotly.express as px\nimport warnings\nwarnings.simplefilter(\"ignore\", np.ComplexWarning)\nfrom haversine import haversine\nfrom IPython.display import HTML\n\n\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr\n\n\nfrom plotly.subplots import make_subplots"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#ebayesthresh",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#ebayesthresh",
    "title": "Simulation",
    "section": "EbayesThresh",
    "text": "EbayesThresh\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(EbayesThresh)\nset.seed(1)\nx <- rnorm(1000) + sample(c( runif(25,-7,7), rep(0,975)))\n#plot(x,type='l')\n#mu <- EbayesThresh::ebayesthresh(x,sdev=2)\n#lines(mu,col=2,lty=2,lwd=2)\n\n\nR + python\n- R환경에 있던 x를 가지고 오기\n\n%R -o x \n\n- R환경에 있는 ebayesthresh 함수를 가지고 오기\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\n\n\nxhat = np.array(ebayesthresh(FloatVector(x)))\n\n\n#plt.plot(x)\n#plt.plot(xhat)"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#시도-1",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#시도-1",
    "title": "Simulation",
    "section": "시도 1",
    "text": "시도 1\n\n_x = np.linspace(0,2,1000)\n_y1 = 5*_x\n_y = _y1 + x # x is epsilon\n\n\ndf1=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\nw=np.zeros((1000,1000))\n\n\nfor i in range(1000):\n    for j in range(1000):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nclass SIMUL:\n    def __init__(self,df):\n        self.df = df\n        self.y = df.y.to_numpy()\n        self.y1 = df.y1.to_numpy()\n        self.x = df.x.to_numpy()\n        self.n = len(self.y)\n        self.W = w\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)      \n    def fit(self,sd=5): # fit with ebayesthresh\n        self._eigen()\n        self.ybar = self.Psi.T @ self.y # fbar := graph fourier transform of f\n        self.power = self.ybar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.ybar**2),sd=sd))\n        self.ybar_threshed = np.where(self.power_threshed>0,self.ybar,0)\n        self.yhat = self.Psi@self.ybar_threshed\n        self.df = self.df.assign(yHat = self.yhat)\n        self.df = self.df.assign(Residual = self.df.y- self.df.yHat)\n        self.differ=(np.abs(self.y-self.yhat)-np.min(np.abs(self.y-self.yhat)))/(np.max(np.abs(self.y-self.yhat))-np.min(np.abs(self.y-self.yhat))) #color 표현은 위핸 표준화\n        self.df = self.df.assign(differ = self.differ)\n        #with plt.style.context('seaborn-dark'):\n            #plt.figure(figsize=(16,10))\n            #plt.scatter(self.x,self.y,c=self.differ3,cmap='Purples',s=50)\n            #plt.plot(self.x,self.yhat, 'k--')\n    def vis(self,ref=60):\n        fig = go.Figure()\n        fig.add_scatter(x=self.x,y=self.y,mode=\"markers\",marker=dict(size=2, color=\"#9fc5e8\"),name='y',opacity=0.7)\n        fig.add_scatter(x=self.x,y=self.yhat,mode=\"markers\",marker=dict(size=2, color=\"#000000\"),name='yhat',opacity=0.7)\n        fig.add_scatter(x=self.df.query('Residual**2>@ref')['x'],y=self.df.query('Residual**2>@ref')['y'],mode=\"markers\",marker=dict(size=3, color=\"#f20505\"),name='R square',opacity=1)\n        fig.add_trace(go.Scatter(x=self.x,y=self.y1,mode='lines',line_color='#0000FF',name='underline'))\n        fig.update_layout(width=1000,height=1000,autosize=False,margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n    def sub(self):\n        fig, axs = plt.subplots(2,2,figsize=(16,10))\n\n        axs[0,0].plot(self.power)\n        axs[0,0].plot(self.power_threshed)\n        axs[0,0].set_title('power_threshed')\n\n        axs[0,1].plot(self.power[1:])\n        axs[0,1].plot(self.power_threshed[1:])\n        axs[0,1].set_title('power_threshed 1:')\n\n        axs[1,0].plot(self.power[2:])\n        axs[1,0].plot(self.power_threshed[2:])\n        axs[1,0].set_title('power_threshed 2:')\n\n        axs[1,1].plot((self.df.Residual)**2)\n        axs[1,1].set_title('Residual square')\n\n        plt.tight_layout()\n        plt.show()\n    def subvis(self,ref=60):\n        fig = make_subplots(rows=2, cols=2, subplot_titles=(\"y\", \"yhat\", \"Residual Square\", \"Graph\"))\n                            \n        fig.add_scatter(x=self.x,y=self.y, mode=\"markers\",marker=dict(size=3, color=\"#9fc5e8\"),name='y',opacity=0.7,row=1,col=1)\n        fig.add_trace(go.Scatter(x=self.x,y=self.y1,mode='lines',line_color='#000000',name='underline'),row=1,col=1)\n        \n        fig.add_scatter(x=self.x,y=self.yhat, mode=\"markers\",marker=dict(size=3, color=\"#999999\"),name='yhat',opacity=0.7,row=1,col=2)\n        fig.add_trace(go.Scatter(x=self.x,y=self.y1,mode='lines',line_color='#000000',name='underline'),row=1,col=2)\n        \n        fig.add_scatter(x=self.df.query('Residual**2>@ref')['x'],y=self.df.query('Residual**2>@ref')['y'], mode=\"markers\",marker=dict(size=3, color=\"#f20505\"),name='R square',opacity=1,row=2,col=1)\n        fig.add_trace(go.Scatter(x=self.x,y=self.y1,mode='lines',line_color='#000000',name='underline'),row=2,col=1)\n        \n        fig.add_scatter(x=self.x,y=self.y, mode=\"markers\",marker=dict(size=3, color=\"#9fc5e8\"),name='y',opacity=0.7,row=2,col=2)        \n        fig.add_scatter(x=self.x,y=self.yhat, mode=\"markers\",marker=dict(size=3, color=\"#999999\"),name='yhat',opacity=0.7,row=2,col=2)        \n        fig.add_scatter(x=self.df.query('Residual**2>@ref')['x'],y=self.df.query('Residual**2>@ref')['y'], mode=\"markers\",marker=dict(size=3, color=\"#f20505\"),name='R square',opacity=1,row=2,col=2)\n        fig.add_trace(go.Scatter(x=self.x,y=self.y1,mode='lines',line_color='#000000',name='underline'),row=2,col=2)\n        \n        fig.update_xaxes(range=[0, 2], row=1, col=1)\n        fig.update_yaxes(range=[-5, 15], row=1, col=1)\n        \n        fig.update_xaxes(range=[0, 2], row=1, col=2)\n        fig.update_yaxes(range=[-5, 15], row=1, col=2)\n        \n        fig.update_xaxes(range=[0, 2], row=2, col=1)\n        fig.update_yaxes(range=[-5, 15], row=2, col=1)\n        \n        fig.update_xaxes(range=[0, 2], row=2, col=2)\n        fig.update_yaxes(range=[-5, 15], row=2, col=2)\n        \n        fig.update_layout(width=1000,height=1000,autosize=False,showlegend=False,title_text=\"The result\")\n        \n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n\n\nclass SIMUL2(SIMUL):\n    def fit2(self,sd=5,ref=60,cuts=0,cutf=995):\n        self.fit()\n        with plt.style.context('seaborn-dark'):\n            plt.figure(figsize=(16,10))\n            plt.scatter(self.x,self.y,c=self.differ3,cmap='Purples',s=50)\n            plt.scatter(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],color='red',s=50)\n            plt.plot(self.x,self.y1,'b--')\n            plt.plot(self.x[cuts:cutf],self.yhat[cuts:cutf], 'k--')\n    def fit3(self,sd=5,ref=30,ymin=-5,ymax=20,cuts=0,cutf=995):\n        self.fit()\n        with plt.style.context('seaborn-dark'):\n            fig, axs = plt.subplots(2,2,figsize=(16,10))\n\n            axs[0,0].scatter(self.x,self.y,c=self.differ,cmap='Purples',s=50)\n            axs[0,0].set_title('y')\n            axs[0,0].set_ylim([ymin,ymax])\n            \n\n            axs[0,1].plot(self.x[cuts:cutf],self.yhat[cuts:cutf], 'k')\n            axs[0,1].plot(self.x[cuts:cutf],self.y1[cuts:cutf], 'b',alpha=0.5)\n            axs[0,1].set_title('yhat')\n            axs[0,1].set_ylim([ymin,ymax])\n\n            axs[1,0].scatter(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],color='red',s=50,marker='*')\n            axs[1,0].plot(self.x[cuts:cutf],self.y1[cuts:cutf], 'b',alpha=0.5)\n            axs[1,0].set_title('Residual square')\n            axs[1,0].set_ylim([ymin,ymax])\n\n            axs[1,1].scatter(self.x,self.y,c=self.differ,cmap='Purples',s=50)\n            axs[1,1].plot(self.x[cuts:cutf],self.yhat[cuts:cutf], 'k')\n            axs[1,1].scatter(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],color='red',s=50,marker='*')\n            axs[1,1].set_title('Graph')\n            axs[1,1].set_ylim([ymin,ymax])\n\n            plt.tight_layout()\n            plt.show()\n\n\n_simul = SIMUL2(df1)\n\n\n_simul.fit3(sd=5,ref=20,ymin=-10,ymax=15)\n\n\n\n\n\n#_simul.vis()\n\n\n_simul.sub()\n\n\n\n\n\n#_simul.subvis()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#시도-2",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#시도-2",
    "title": "Simulation",
    "section": "시도 2",
    "text": "시도 2\n\n_x = np.linspace(0,2,1000)\n_y1 = 5*_x**2\n_y = _y1 + x # x is epsilon\n\n\ndf2=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul = SIMUL2(df2)\n\n\n_simul.fit3(sd=6,ref=20,ymin=-10,ymax=25)\n\n\n\n\n\n#_simul.vis()\n\n\n_simul.sub()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#시도-3",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#시도-3",
    "title": "Simulation",
    "section": "시도 3",
    "text": "시도 3\n\n_x = np.linspace(0,2,1000)\n_y1 = 5*_x**3 \n_y = _y1 + x # x is epsilon\n\n\ndf3=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul = SIMUL2(df3)\n\n\n_simul.fit3(ymin=-10,ymax=45)\n\n\n\n\n\n#_simul.vis()\n\n\n_simul.sub()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#시도-4",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#시도-4",
    "title": "Simulation",
    "section": "시도 4",
    "text": "시도 4\n\n_x = np.linspace(0,2,1000)\n_y1 = -2+ 3*np.cos(_x) + 1*np.cos(2*_x) + 5*np.cos(5*_x)\n_y = _y1 + x\n\n\n# _x = np.linspace(0,2,1000)\n# _y1 = 5*np.sin(_x) \n# _y = _y1 + x # x is epsilon\n\n\ndf4=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul = SIMUL2(df4)\n\n\n_simul.fit3(ref=10,ymin=-15,ymax=10)\n\n\n\n\n\n#_simul.vis()\n\n\n_simul.sub()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#시도-5",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#시도-5",
    "title": "Simulation",
    "section": "시도 5",
    "text": "시도 5\n\n# _x = np.linspace(0,2,1000)\n# _y1 =  3*np.cos(_x) + 1*np.cos(_x**2) + 0.5*np.cos(5*_x) \n# _y = _y1 + x # x is epsilon\n\n\n_x = np.linspace(0,2,1000)\n_y1 =  3*np.sin(_x) + 1*np.sin(_x**2) + 5*np.sin(5*_x) \n_y = _y1 + x # x is epsilon\n\n\ndf5=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul = SIMUL2(df5)\n\n\n_simul.fit3(ref=15,ymin=-10,ymax=15,cuts=5)\n\n\n\n\n\n#_simul.vis()\n\n\n_simul.sub()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#d-시도-1",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#d-시도-1",
    "title": "Simulation",
    "section": "3D 시도 1",
    "text": "3D 시도 1\n\n### Example 2\nnp.random.seed(777)\npi=np.pi\nn=1000\nang=np.linspace(-pi,pi-2*pi/n,n)\nr=2+np.sin(np.linspace(0,6*pi,n))\nvx=r*np.cos(ang)\nvy=r*np.sin(ang)\nf1=10*np.sin(np.linspace(0,3*pi,n))\nf = f1 + x\n\n# 1. \np=plt.figure(figsize=(12,4), dpi=200)  # Make figure object \n\n# 2. \nax=p.add_subplot(1,1,1, projection='3d')\nax.grid(False)\nax.ticklabel_format(style='sci', axis='x',scilimits=(0,0))\nax.ticklabel_format(style='sci', axis='y',scilimits=(0,0))\nax.ticklabel_format(style='sci', axis='z',scilimits=(0,0))\ntop = f\nbottom = np.zeros_like(top)\nwidth=depth=0.05\n#ax.bar3d(vx, vy, bottom, width, depth, top, shade=False)\nax.scatter3D(vx,vy,f,zdir='z',s=10,marker='.')\nax.scatter3D(vx,vy,f1,zdir='z',s=10,marker='.')\nax.bar3d(vx, vy, bottom, width, depth, 0, color='Black',shade=False)\nax.set_xlim(-3,3)\nax.set_ylim(-3,3)\nax.set_zlim(-10,10)\n\ndf = pd.DataFrame({'x' : vx, 'y' : vy, 'f' : f, 'f1' : f1})\n\n\nclass SIMUL:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.f.to_numpy()\n        self.f1 = df.f1.to_numpy()\n        self.x = df.x.to_numpy()\n        self.y = df.y.to_numpy()\n        self.n = len(self.f)\n        self.theta= None\n    def get_distance(self):\n        self.D = np.zeros([self.n,self.n])\n        locations = np.stack([self.x, self.y],axis=1)\n        for i in tqdm.tqdm(range(self.n)):\n            for j in range(i,self.n):\n                self.D[i,j]=np.linalg.norm(locations[i]-locations[j])\n        self.D = self.D + self.D.T\n    def get_weightmatrix(self,theta=1,beta=0.5,kappa=4000):\n        self.theta = theta\n        dist = np.where(self.D < kappa,self.D,0)\n        self.W = np.exp(-(dist/self.theta)**2)\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)       \n    def fit(self,sd=5,ref=60): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.f # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2),sd=sd))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(fHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.f- self.df.fHat)\n        self.dif=(np.abs(self.f-self.fhat)-np.min(np.abs(self.f-self.fhat)))/(np.max(np.abs(self.f-self.fhat))-np.min(np.abs(self.f-self.fhat)))\n        self.df = self.df.assign(dif = self.dif)\n        self.bottom = np.zeros_like(self.f)\n        self.width=0.05\n        self.depth=0.05\n        \n        fig, axs = plt.subplots(2,2,figsize=(16,16),subplot_kw={\"projection\":\"3d\"})\n        axs[0,0].grid(False)\n        axs[0,0].scatter3D(self.x,self.y,self.f,c=self.dif,cmap='winter',zdir='z',s=50,marker='.',alpha=0.2)\n        axs[0,0].plot3D(self.x,self.y,[0]*1000,'black')\n        axs[0,0].set_xlim(-3,3)\n        axs[0,0].set_ylim(-3,3)\n        axs[0,0].set_zlim(-10,10)\n        axs[0,0].view_init(elev=20., azim=40)\n        \n        axs[0,1].grid(False)\n        axs[0,1].scatter3D(self.x,self.y,self.fhat,color='black',zdir='z',s=50,marker='.',alpha=0.2)\n        axs[0,1].plot3D(self.x,self.y,self.f1,'blue')\n        axs[0,1].plot3D(self.x,self.y,[0]*1000,'black')\n        axs[0,1].set_xlim(-3,3)\n        axs[0,1].set_ylim(-3,3)\n        axs[0,1].set_zlim(-10,10)\n        axs[0,1].view_init(elev=20., azim=40)\n        \n        axs[1,0].grid(False)\n        axs[1,0].scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['f'],color='red',zdir='z',s=100,marker='.',alpha=1)\n        axs[1,0].plot3D(self.x,self.y,self.f1,'blue')\n        axs[1,0].plot3D(self.x,self.y,[0]*1000,'black')\n        axs[1,0].set_xlim(-3,3)\n        axs[1,0].set_ylim(-3,3)\n        axs[1,0].set_zlim(-10,10)\n        axs[1,0].view_init(elev=20., azim=40)\n        \n        axs[1,1].grid(False)\n        axs[1,1].scatter3D(self.x,self.y,self.f,c=self.dif,cmap='winter',zdir='z',s=50,marker='.',alpha=0.2)\n        axs[1,1].scatter3D(self.x,self.y,self.fhat,color='black',zdir='z',s=50,marker='.',alpha=0.2)\n        axs[1,1].scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['f'],color='red',zdir='z',s=100,marker='.',alpha=1)\n        axs[1,1].plot3D(self.x,self.y,self.f1,'black')\n        axs[1,1].plot3D(self.x,self.y,[0]*1000,'black')\n        axs[1,1].set_xlim(-3,3)\n        axs[1,1].set_ylim(-3,3)\n        axs[1,1].set_zlim(-10,10)\n        axs[1,1].view_init(elev=20., azim=40)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        # p = plt.figure(figsize=(16,16))\n        # ax = p.add_subplot(1,1,1, projection='3d')\n        # ax.grid(False)\n        # ax.ticklabel_format(style='sci', axis='x',scilimits=(0,0))\n        # ax.ticklabel_format(style='sci', axis='y',scilimits=(0,0))\n        # ax.ticklabel_format(style='sci', axis='z',scilimits=(0,0))\n        # ax.scatter3D(self.x,self.y,self.f,c=self.dif,cmap='winter',zdir='z',s=50,marker='.',alpha=0.2)\n        # ax.scatter3D(self.x,self.y,self.fhat,color='black',zdir='z',s=50,marker='.',alpha=0.2)\n        # #ax.plot3D(self.x,self.y,self.fhat,'black')\n        # ax.scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['f'],color='red',zdir='z',s=100,marker='.',alpha=1)\n        # ax.plot3D(self.x,self.y,self.f1,'black')\n        # ax.plot3D(self.x,self.y,[0]*1000,'black')\n        # ax.set_xlim(-3,3)\n        # ax.set_ylim(-3,3)\n        # ax.set_zlim(-10,10)\n    def vis(self,ref=60):\n        fig = go.Figure()\n        fig.add_scatter3d(x=self.x,y=self.y,z=self.f, mode=\"markers\",marker=dict(size=3, color=\"#9fc5e8\"),name='f',opacity=0.2)\n        fig.add_scatter3d(x=self.x,y=self.y,z=self.fhat, mode=\"markers\",marker=dict(size=3, color=\"#999999\"),name='fhat',opacity=0.2)\n        #fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=self.fhat,mode='lines',line_color='#000000'))\n        fig.add_scatter3d(x=self.df.query('Residual**2>@ref')['x'],y=self.df.query('Residual**2>@ref')['y'],z=self.df.query('Residual**2>@ref')['f'], mode=\"markers\",marker=dict(size=3, color=\"#f20505\"),name='R square',opacity=1)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=self.f1,mode='lines',line_color='#000000',name='underline'))\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=[0]*1000,mode='lines',line_color='#000000',name='z=0'))\n        fig.update_layout(width=1000,height=1000,autosize=False,margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n    def sub(self):\n        fig, axs = plt.subplots(2,2,figsize=(16,10))\n\n        axs[0,0].plot(_simul.power)\n        axs[0,0].plot(_simul.power_threshed)\n        axs[0,0].set_title('power_threshed')\n\n        axs[0,1].plot(_simul.power[1:])\n        axs[0,1].plot(_simul.power_threshed[1:])\n        axs[0,1].set_title('power_threshed 1:')\n\n        axs[1,0].plot(_simul.power[2:])\n        axs[1,0].plot(_simul.power_threshed[2:])\n        axs[1,0].set_title('power_threshed 2:')\n\n        axs[1,1].plot((_simul.df.Residual)**2)\n        axs[1,1].set_title('Residual square')\n\n        plt.tight_layout()\n        plt.show()\n    def subvis(self,ref=60):\n        fig = make_subplots(2,2,specs=[[{'type': 'surface'}, {'type': 'surface'}],[{'type': 'surface'}, {'type': 'surface'}]],subplot_titles=(\"f\", \"fhat\", \"Residual Square\", \"Graph\"))\n        \n        fig.add_scatter3d(x=self.x,y=self.y,z=self.f, mode=\"markers\",marker=dict(size=3, color=\"#9fc5e8\"),name='f',opacity=0.2,row=1,col=1)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=self.f1,mode='lines',line_color='#000000',name='underline'),row=1,col=1)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=[0]*1000,mode='lines',line_color='#000000',name='z=0'),row=1,col=1)\n        \n        fig.add_scatter3d(x=self.x,y=self.y,z=self.fhat, mode=\"markers\",marker=dict(size=3, color=\"#999999\"),name='fhat',opacity=0.2,row=1,col=2)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=self.f1,mode='lines',line_color='#000000',name='underline'),row=1,col=2)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=[0]*1000,mode='lines',line_color='#000000',name='z=0'),row=1,col=2)\n        \n        fig.add_scatter3d(x=self.df.query('Residual**2>@ref')['x'],y=self.df.query('Residual**2>@ref')['y'],z=self.df.query('Residual**2>@ref')['f'], mode=\"markers\",marker=dict(size=3, color=\"#f20505\"),name='R square',opacity=1,row=2,col=1)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=self.f1,mode='lines',line_color='#000000',name='underline'),row=2,col=1)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=[0]*1000,mode='lines',line_color='#000000',name='z=0'),row=2,col=1)\n        \n        fig.add_scatter3d(x=self.x,y=self.y,z=self.f, mode=\"markers\",marker=dict(size=3, color=\"#9fc5e8\"),name='f',opacity=0.2,row=2,col=2)        \n        fig.add_scatter3d(x=self.x,y=self.y,z=self.fhat, mode=\"markers\",marker=dict(size=3, color=\"#999999\"),name='fhat',opacity=0.2,row=2,col=2)        \n        fig.add_scatter3d(x=self.df.query('Residual**2>@ref')['x'],y=self.df.query('Residual**2>@ref')['y'],z=self.df.query('Residual**2>@ref')['f'], mode=\"markers\",marker=dict(size=3, color=\"#f20505\"),name='R square',opacity=1,row=2,col=2)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=self.f1,mode='lines',line_color='#000000',name='underline'),row=2,col=2)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=[0]*1000,mode='lines',line_color='#000000',name='z=0'),row=2,col=2)\n        \n        fig.update_layout(scene = dict(xaxis = dict(range=[-3,3],),\n                                         yaxis = dict(range=[-3,3],),\n                                         zaxis = dict(range=[-10,10],),),\n                                      width=1000,height=1000,autosize=False)\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n\n\n_simul = SIMUL(df)\n\n\n_simul.get_distance()\n\n100%|██████████| 1000/1000 [00:01<00:00, 532.20it/s]\n\n\n\n_simul.D[_simul.D>0].mean()\n\n2.6888234729389295\n\n\n\nplt.hist(_simul.D[_simul.D>0])\n\n(array([ 66308.,  64352.,  68358., 177302., 166964., 114648.,  94344.,\n        111136.,  75508.,  60080.]),\n array([0.00628415, 0.54637775, 1.08647135, 1.62656495, 2.16665855,\n        2.70675214, 3.24684574, 3.78693934, 4.32703294, 4.86712654,\n        5.40722013]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n\n_simul.get_weightmatrix(theta=(2.6888234729389295),kappa=2500) \n\n\n_simul.fit(sd=5,ref=20)\n\n\n\n\n\n#_simul.vis(ref=20)\n\n\n#_simul.subvis(ref=20)\n\n\n_simul.sub()\n\n\n\n\n\n#_simul.subvis()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#d-시도-2",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#d-시도-2",
    "title": "Simulation",
    "section": "3D 시도 2",
    "text": "3D 시도 2\n\n### Example 2\nnp.random.seed(777)\npi=np.pi\nn=1000\nang=np.linspace(-pi,pi-2*pi/n,n)\nr=2+np.sin(np.linspace(0,8*pi,n))\nvx=r*np.cos(ang)\nvy=r*np.sin(ang)\nf1=10*np.sin(np.linspace(0,3*pi,n))\nf = f1 + x\n\n\ndf1 = pd.DataFrame({'x' : vx, 'y' : vy, 'f' : f,'f1':f1})\n\n\n_simul = SIMUL(df1)\n\n\n_simul.get_distance()\n\n100%|██████████| 1000/1000 [00:01<00:00, 521.69it/s]\n\n\n\n_simul.D[_simul.D>0].mean()\n\n2.6984753461932702\n\n\n\nplt.hist(_simul.D[_simul.D>0])\n\n(array([ 63450.,  64118., 146970., 169756., 138202., 126198., 162650.,\n         75642.,  28416.,  23598.]),\n array([0.0062838 , 0.60565122, 1.20501864, 1.80438605, 2.40375347,\n        3.00312089, 3.6024883 , 4.20185572, 4.80122314, 5.40059055,\n        5.99995797]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n\n_simul.get_weightmatrix(theta=(2.6984753461932702),kappa=2500) \n\n\n_simul.fit(sd=5,ref=30)\n\n\n\n\n\n#_simul.vis(ref=50)\n\n\n_simul.sub()\n\n\n\n\n\n#_simul.subvis()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#d-시도-3",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#d-시도-3",
    "title": "Simulation",
    "section": "3D 시도 3",
    "text": "3D 시도 3\n\n### Example 2\nnp.random.seed(777)\npi=np.pi\nn=1000\nang=np.linspace(-pi,pi-2*pi/n,n)\nr=2+np.sin(np.linspace(0,6*pi,n))\nvx=r*np.cos(ang)\nvy=r*np.sin(ang)\nf1=10*np.sin(np.linspace(0,6*pi,n))\nf = f1 + x\n\n\ndf2 = pd.DataFrame({'x' : vx, 'y' : vy, 'f' : f,'f1':f1})\n\n\n_simul = SIMUL(df2)\n\n\n_simul.get_distance()\n\n100%|██████████| 1000/1000 [00:02<00:00, 463.80it/s]\n\n\n\n_simul.D[_simul.D>0].mean()\n\n2.6888234729389295\n\n\n\nplt.hist(_simul.D[_simul.D>0])\n\n(array([ 66308.,  64352.,  68358., 177302., 166964., 114648.,  94344.,\n        111136.,  75508.,  60080.]),\n array([0.00628415, 0.54637775, 1.08647135, 1.62656495, 2.16665855,\n        2.70675214, 3.24684574, 3.78693934, 4.32703294, 4.86712654,\n        5.40722013]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n\n_simul.get_weightmatrix(theta=(2.6984753461932702),kappa=2500) \n\n\n_simul.fit(sd=5,ref=30)\n\n\n\n\n\n#_simul.vis(ref=50)\n\n\n_simul.sub()\n\n\n\n\n\n#_simul.subvis()"
  },
  {
    "objectID": "posts/GODE/2022-12-27-DFT_study.html",
    "href": "posts/GODE/2022-12-27-DFT_study.html",
    "title": "Discrete Fourier Transform",
    "section": "",
    "text": "DFT\nhttps://miruetoto.quarto.pub/yechan/posts/CGSP/2022-12-24-CGSP-Chap-8-3-DFT.html#fnref1\nhttps://miruetoto.github.io/yechan/%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88/2019/11/24/(%EB%85%B8%ED%8A%B8)-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88-%EC%B6%94%EB%A1%A0-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88.html"
  },
  {
    "objectID": "posts/GODE/2022-12-27-DFT_study.html#import",
    "href": "posts/GODE/2022-12-27-DFT_study.html#import",
    "title": "Discrete Fourier Transform",
    "section": "import",
    "text": "import\n\nimport numpy as np\n\n\nForward operator A\n\nA = np.array([[0, 0, 0, 0, 1],\n    [1, 0, 0, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, 0, 1, 0, 0],\n    [0, 0, 0, 1, 0]])\nA\n\narray([[0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0]])\n\n\n\nnp.transpose(A)@A\n\narray([[1, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 1]])\n\n\n\nnote: A is orthogonal matrix\n\n\ns = np.array([[1],[22],[333],[4444],[55555]])\ns\n\narray([[    1],\n       [   22],\n       [  333],\n       [ 4444],\n       [55555]])\n\n\n\nA@s\n\narray([[55555],\n       [    1],\n       [   22],\n       [  333],\n       [ 4444]])\n\n\n\nA@A@s\n\narray([[ 4444],\n       [55555],\n       [    1],\n       [   22],\n       [  333]])\n\n\n\nA@A@A@s\n\narray([[  333],\n       [ 4444],\n       [55555],\n       [    1],\n       [   22]])\n\n\n\nnote : thus A is a forward operator,A* is a backward operator.\n\n\n\nDFT\n\\(A = DFT^* \\Lambda DFT\\)\n\nλ, ψ = np.linalg.eig(A)\nλ, ψ\n\n(array([-0.80901699+0.58778525j, -0.80901699-0.58778525j,\n         0.30901699+0.95105652j,  0.30901699-0.95105652j,\n         1.        +0.j        ]),\n array([[-0.3618034+0.26286556j, -0.3618034-0.26286556j,\n         -0.3618034-0.26286556j, -0.3618034+0.26286556j,\n          0.4472136+0.j        ],\n        [ 0.4472136+0.j        ,  0.4472136-0.j        ,\n         -0.3618034+0.26286556j, -0.3618034-0.26286556j,\n          0.4472136+0.j        ],\n        [-0.3618034-0.26286556j, -0.3618034+0.26286556j,\n          0.1381966+0.4253254j ,  0.1381966-0.4253254j ,\n          0.4472136+0.j        ],\n        [ 0.1381966+0.4253254j ,  0.1381966-0.4253254j ,\n          0.4472136+0.j        ,  0.4472136-0.j        ,\n          0.4472136+0.j        ],\n        [ 0.1381966-0.4253254j ,  0.1381966+0.4253254j ,\n          0.1381966-0.4253254j ,  0.1381966+0.4253254j ,\n          0.4472136+0.j        ]]))\n\n\n\nλ.shape, ψ.shape\n\n((5,), (5, 5))\n\n\n\nA \n\narray([[0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0]])\n\n\n\n(ψ @ np.diag(λ) @ ψ.transpose()).round(2)\n\narray([[-0.  +0.j,  0.45+0.j,  0.28+0.j,  0.72+0.j, -0.45+0.j],\n       [ 0.45+0.j,  0.28+0.j,  0.72+0.j, -0.45+0.j, -0.  +0.j],\n       [ 0.28+0.j,  0.72+0.j, -0.45+0.j,  0.  +0.j,  0.45+0.j],\n       [ 0.72+0.j, -0.45+0.j,  0.  +0.j,  0.45+0.j,  0.28+0.j],\n       [-0.45+0.j, -0.  +0.j,  0.45+0.j,  0.28+0.j,  0.72+0.j]])\n\n\n?\ndefine \\(\\psi^* = DFT\\)\n\nDFT = np.transpose(ψ)\nDFT\n\narray([[-0.3618034+0.26286556j,  0.4472136+0.j        ,\n        -0.3618034-0.26286556j,  0.1381966+0.4253254j ,\n         0.1381966-0.4253254j ],\n       [-0.3618034-0.26286556j,  0.4472136-0.j        ,\n        -0.3618034+0.26286556j,  0.1381966-0.4253254j ,\n         0.1381966+0.4253254j ],\n       [-0.3618034-0.26286556j, -0.3618034+0.26286556j,\n         0.1381966+0.4253254j ,  0.4472136+0.j        ,\n         0.1381966-0.4253254j ],\n       [-0.3618034+0.26286556j, -0.3618034-0.26286556j,\n         0.1381966-0.4253254j ,  0.4472136-0.j        ,\n         0.1381966+0.4253254j ],\n       [ 0.4472136+0.j        ,  0.4472136+0.j        ,\n         0.4472136+0.j        ,  0.4472136+0.j        ,\n         0.4472136+0.j        ]])\n\n\n\nλ[3,]\n\n(0.30901699437494734-0.9510565162951535j)\n\n\n\na=np.array([1,2,3,4])\nnp.diag(np.diag(a))\n\narray([1, 2, 3, 4])\n\n\n\nλ\n\narray([-0.80901699+0.58778525j, -0.80901699-0.58778525j,\n        0.30901699+0.95105652j,  0.30901699-0.95105652j,\n        1.        +0.j        ])\n\n\n\n(np.matrix(ψ)@np.matrix(np.diag(λ))@np.matrix(ψ).H).round(3)\n\nmatrix([[-0.+0.j,  0.+0.j, -0.+0.j,  0.+0.j,  1.+0.j],\n        [ 1.+0.j, -0.+0.j,  0.+0.j, -0.+0.j, -0.+0.j],\n        [-0.+0.j,  1.+0.j, -0.+0.j,  0.+0.j,  0.+0.j],\n        [ 0.+0.j, -0.+0.j,  1.+0.j,  0.+0.j, -0.+0.j],\n        [-0.+0.j, -0.+0.j,  0.+0.j,  1.+0.j, -0.+0.j]])\n\n\n\nnp.matrix(ψ).H\n\nmatrix([[-0.3618034-0.26286556j,  0.4472136-0.j        ,\n         -0.3618034+0.26286556j,  0.1381966-0.4253254j ,\n          0.1381966+0.4253254j ],\n        [-0.3618034+0.26286556j,  0.4472136+0.j        ,\n         -0.3618034-0.26286556j,  0.1381966+0.4253254j ,\n          0.1381966-0.4253254j ],\n        [-0.3618034+0.26286556j, -0.3618034-0.26286556j,\n          0.1381966-0.4253254j ,  0.4472136-0.j        ,\n          0.1381966+0.4253254j ],\n        [-0.3618034-0.26286556j, -0.3618034+0.26286556j,\n          0.1381966+0.4253254j ,  0.4472136+0.j        ,\n          0.1381966-0.4253254j ],\n        [ 0.4472136-0.j        ,  0.4472136-0.j        ,\n          0.4472136-0.j        ,  0.4472136-0.j        ,\n          0.4472136-0.j        ]])\n\n\n\n\nSpectral components and Frequencies\n\\[\\{ 1,\\psi_1, \\psi_2, \\psi_3,\\dots, \\psi_{N-1} \\}\\]\nThese vectors are called spectral components.\nIn Physics and in operator theory, these eigenvalues are the frequencies of the signal.\nEigenvalues of \\(A\\)"
  },
  {
    "objectID": "posts/GODE/Untitled.html",
    "href": "posts/GODE/Untitled.html",
    "title": "Seoyeon's Blog for study",
    "section": "",
    "text": "import numpy as np\n\n\na=10\nb=20\nn=200\n\n\narr1 = np.array([a+(b-a)/(n-1) * (i-1) for i in range(1,n+1)])\n\n\\[a+\\frac{(b-a)i}{n-1}\\] for \\(i=1,2,3,\\dots, n\\)\n\narr2 = np.linspace(a,b,n)\n\n\narr1[:5]\n\narray([10.        , 10.05025126, 10.10050251, 10.15075377, 10.20100503])\n\n\n\narr2[:5]\n\narray([10.        , 10.05025126, 10.10050251, 10.15075377, 10.20100503])"
  },
  {
    "objectID": "posts/GODE/index.html",
    "href": "posts/GODE/index.html",
    "title": "GODE",
    "section": "",
    "text": "About GODE paper"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html",
    "title": "Earthquake",
    "section": "",
    "text": "Real analysis"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html#imports",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html#imports",
    "title": "Earthquake",
    "section": "imports",
    "text": "imports\n\nimport tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport plotly.express as px\nimport warnings\nwarnings.simplefilter(\"ignore\", np.ComplexWarning)\nfrom haversine import haversine\nfrom IPython.display import HTML\nimport plotly.graph_objects as go\n\n\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html#load-data-and-clean-it",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html#load-data-and-clean-it",
    "title": "Earthquake",
    "section": "load data and clean it",
    "text": "load data and clean it\n- load\n\ndf= pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      Date\n      Latitude\n      Longitude\n      Magnitude\n    \n  \n  \n    \n      0\n      01/02/1965\n      19.2460\n      145.6160\n      6.0\n    \n    \n      1\n      01/04/1965\n      1.8630\n      127.3520\n      5.8\n    \n    \n      2\n      01/05/1965\n      -20.5790\n      -173.9720\n      6.2\n    \n    \n      3\n      01/08/1965\n      -59.0760\n      -23.5570\n      5.8\n    \n    \n      4\n      01/09/1965\n      11.9380\n      126.4270\n      5.8\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      23407\n      12/28/2016\n      38.3917\n      -118.8941\n      5.6\n    \n    \n      23408\n      12/28/2016\n      38.3777\n      -118.8957\n      5.5\n    \n    \n      23409\n      12/28/2016\n      36.9179\n      140.4262\n      5.9\n    \n    \n      23410\n      12/29/2016\n      -9.0283\n      118.6639\n      6.3\n    \n    \n      23411\n      12/30/2016\n      37.3973\n      141.4103\n      5.5\n    \n  \n\n23412 rows × 4 columns\n\n\n\n\ndf_korea= pd.read_csv('earthquake_korea2.csv').iloc[:,[1,2,5,6]].rename(columns={'규모':'Magnitude'})\n\nhttps://www.weather.go.kr/w/eqk-vol/search/korea.do?schOption=&xls=0&startTm=2012-01-02&endTm=2022-06-17&startSize=2&endSize=&startLat=&endLat=&startLon=&endLon=&lat=&lon=&dist=&keyword=&dpType=m\n\ndf_global= pd.concat([pd.read_csv('00_05.csv'),pd.read_csv('05_10.csv'),pd.read_csv('10_15.csv'),pd.read_csv('15_20.csv')]).iloc[:,[0,1,2,4]].rename(columns={'latitude':'Latitude','longitude':'Longitude','mag':'Magnitude'}).reset_index().iloc[:,1:]\n\nhttps://www.usgs.gov/programs/earthquake-hazards/lists-maps-and-statistics\n- cleaning\n\ndf.Date[df.Date == '1975-02-23T02:58:41.000Z']\n\n3378    1975-02-23T02:58:41.000Z\nName: Date, dtype: object\n\n\n\ndf.iloc[3378,0] = '02/03/1975'\n\n\ndf.Date[df.Date == '1985-04-28T02:53:41.530Z']\n\n7512    1985-04-28T02:53:41.530Z\nName: Date, dtype: object\n\n\n\ndf.iloc[7512,0] = '04/28/1985'\n\n\ndf.Date[df.Date == '2011-03-13T02:23:34.520Z']\n\n20650    2011-03-13T02:23:34.520Z\nName: Date, dtype: object\n\n\n\ndf.iloc[20650,0] = '03/13/2011'\n\n\ndf= df.assign(Year=list(map(lambda x: x.split('/')[-1], df.Date))).iloc[:,1:]\ndf\n\n\n\n\n\n  \n    \n      \n      Latitude\n      Longitude\n      Magnitude\n      Year\n    \n  \n  \n    \n      0\n      19.2460\n      145.6160\n      6.0\n      1965\n    \n    \n      1\n      1.8630\n      127.3520\n      5.8\n      1965\n    \n    \n      2\n      -20.5790\n      -173.9720\n      6.2\n      1965\n    \n    \n      3\n      -59.0760\n      -23.5570\n      5.8\n      1965\n    \n    \n      4\n      11.9380\n      126.4270\n      5.8\n      1965\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      23407\n      38.3917\n      -118.8941\n      5.6\n      2016\n    \n    \n      23408\n      38.3777\n      -118.8957\n      5.5\n      2016\n    \n    \n      23409\n      36.9179\n      140.4262\n      5.9\n      2016\n    \n    \n      23410\n      -9.0283\n      118.6639\n      6.3\n      2016\n    \n    \n      23411\n      37.3973\n      141.4103\n      5.5\n      2016\n    \n  \n\n23412 rows × 4 columns\n\n\n\n\ndf.Year = df.Year.astype(np.float64)\n\n\ndf_korea = df_korea.assign(Year=list(map(lambda x: x.split('/')[0], df_korea.발생시각))).iloc[:,1:]\ndf_korea = df_korea.assign(Latitude=list(map(lambda x: x.split(' ')[0], df_korea.위도))).iloc[:,[0,2,3,4]]\ndf_korea = df_korea.assign(Longitude=list(map(lambda x: x.split(' ')[0], df_korea.경도))).iloc[:,[0,2,3,4]]\n\n\ndf_global = df_global.assign(Year=list(map(lambda x: x.split('-')[0], df_global.time))).iloc[:,1:]\n\n\ndf_korea.Year = df_korea.Year.astype(np.float64)\ndf_korea.Latitude = df_korea.Latitude.astype(np.float64)\ndf_korea.Longitude = df_korea.Longitude.astype(np.float64)\ndf_global.Year = df_global.Year.astype(np.float64)"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html#define-class",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html#define-class",
    "title": "Earthquake",
    "section": "define class",
    "text": "define class\n\nclass MooYaHo:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.Magnitude.to_numpy()\n        self.year = df.Year.to_numpy()\n        self.lat = df.Latitude.to_numpy()\n        self.long = df.Longitude.to_numpy()\n        self.n = len(self.f)\n        \n        self.theta= None\n    def get_distance(self):\n        self.D = np.zeros([self.n,self.n])\n        locations = np.stack([self.lat, self.long],axis=1)\n        for i in tqdm.tqdm(range(self.n)):\n            for j in range(i,self.n): \n                self.D[i,j]=haversine(locations[i],locations[j])\n        self.D = self.D+self.D.T\n    def get_weightmatrix(self,theta=1,beta=0.5,kappa=4000):\n        self.theta = theta\n        dist = np.where(self.D<kappa,self.D,0)\n        self.W = np.exp(-(dist/self.theta)**2)\n\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)        \n    def fit(self,m):\n        self._eigen()\n        self.fhat = self.Psi[:,0:m]@self.Psi[:,0:m].T@self.f\n        self.df = self.df.assign(MagnitudeHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.Magnitude- self.df.MagnitudeHat)\n        plt.plot(self.f,'.')\n        plt.plot(self.fhat,'x')\n        \n    def vis(self,MagThresh=7,ResThresh=1):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=5,\n                        center=dict(lat=37, lon=160), \n                        zoom=1.5,\n                        height=900,\n                        opacity = 0.4,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 8,\n                      marker_color= 'red',\n                      opacity = 0.6\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= 8,\n                      marker_color= 'blue',\n                      opacity = 0.5\n                      )\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n    def visf(self):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=5,\n                        center=dict(lat=37, lon=160), \n                        zoom=1.5,\n                        height=900,\n                        opacity = 0.7,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n    def visfhat(self):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='MagnitudeHat', \n                        radius=5,\n                        center=dict(lat=37, lon=160), \n                        zoom=1.5,\n                        height=900,\n                        opacity = 0.7,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n    def visres(self,MagThresh=7,ResThresh=1):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z=[0] * len(self.df), \n                        radius=5,\n                        center=dict(lat=37, lon=160), \n                        zoom=1.5,\n                        height=900,\n                        opacity = 0.7,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= 8,\n                      marker_color= 'blue',\n                      opacity = 0.7\n                      )\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n\n\nclass MooYaHo2(MooYaHo): # ebayesthresh 기능추가\n    def fit2(self,ref=0.5): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.f # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2)))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(MagnitudeHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.Magnitude- self.df.MagnitudeHat)\n        self.con = np.where(self.df.Residual>0.7,1,0)\n        #plt.plot(self.f,'.')\n        #plt.plot(self.fhat,'x')\n\n#         fig, axs = plt.subplots(2,2,figsize=(16,10))\n\n#         axs[0,0].plot(self.f,'b')\n#         axs[0,0].set_title('Magnitude')\n#         axs[0,0].set_ylim([4.5,9])\n\n#         axs[0,1].plot(self.fhat,'k')\n#         axs[0,1].set_title('MagnitudeHat')\n#         axs[0,1].set_ylim([4.5,9])\n\n#         axs[1,0].plot(self.con,'r*')\n#         axs[1,0].set_title('Residual square')\n\n#         axs[1,1].plot(self.f,'b')\n#         axs[1,1].plot(self.fhat,'k')\n#         axs[1,1].plot(self.con,'r*')\n#         axs[1,1].set_title('Graph')\n#         axs[1,1].set_ylim([4.5,9])\n\n#         plt.tight_layout()\n#         plt.show()\n\n\nclass MooYaHo3(MooYaHo2):\n    def vis(self,MagThresh=7,ResThresh=1):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=5,\n                        center=dict(lat=37, lon=126), \n                        zoom=5.7,\n                        height=900,\n                        opacity = 0.3,\n                        mapbox_style=\"stamen-terrain\")\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 8,\n                      marker_color= 'red',\n                      opacity = 0.5\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= 8,\n                      marker_color= 'blue',\n                      opacity = 0.5\n                      )\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n\n\n       ebayesthresh = importr('EbayesThresh').ebayesthresh"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html#analysis_df_global20102015",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html#analysis_df_global20102015",
    "title": "Earthquake",
    "section": "analysis_df_global(2010~2015)",
    "text": "analysis_df_global(2010~2015)\n- make instance for analysis\n\nmoo_global=MooYaHo2(df_global.query(\"2010 <= Year < 2015\"))\n\n- get distance\n\nmoo_global.get_distance()\n\n100%|██████████| 12498/12498 [03:14<00:00, 64.19it/s] \n\n\n\nmoo_global.D[moo_global.D>0].mean()\n\n8810.865423093777\n\n\n\nplt.hist(moo_global.D[moo_global.D>0])\n\n(array([14176290., 16005894., 21186674., 22331128., 19394182., 17548252.,\n        16668048., 13316436., 12973260.,  2582550.]),\n array([8.97930163e-02, 2.00141141e+03, 4.00273303e+03, 6.00405465e+03,\n        8.00537626e+03, 1.00066979e+04, 1.20080195e+04, 1.40093411e+04,\n        1.60106627e+04, 1.80119844e+04, 2.00133060e+04]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n- weight matrix\n\nmoo_global.get_weightmatrix(theta=(8810.865423093777),kappa=2500) \n\n- fit\n\nmoo_global.fit2()\n\n\nmoo_global.df.sort_values(\"Residual\",ascending=False).iloc[:40,:]\n\n\n\n\n\n  \n    \n      \n      Latitude\n      Longitude\n      Magnitude\n      Year\n      MagnitudeHat\n      Residual\n    \n  \n  \n    \n      11094\n      -36.1220\n      -72.8980\n      8.8\n      2010.0\n      7.754649\n      1.045351\n    \n    \n      10727\n      -36.2170\n      -73.2570\n      6.7\n      2010.0\n      5.750555\n      0.949445\n    \n    \n      27513\n      0.8020\n      92.4630\n      8.2\n      2012.0\n      7.253721\n      0.946279\n    \n    \n      26109\n      -10.9280\n      166.0180\n      7.1\n      2013.0\n      6.194467\n      0.905533\n    \n    \n      30291\n      38.0580\n      144.5900\n      7.7\n      2011.0\n      6.804163\n      0.895837\n    \n    \n      31673\n      -17.5410\n      168.0690\n      7.3\n      2010.0\n      6.431399\n      0.868601\n    \n    \n      27527\n      2.3270\n      93.0630\n      8.6\n      2012.0\n      7.736731\n      0.863269\n    \n    \n      30311\n      38.2970\n      142.3730\n      9.1\n      2011.0\n      8.255988\n      0.844012\n    \n    \n      24388\n      -19.6097\n      -70.7691\n      8.2\n      2014.0\n      7.394885\n      0.805115\n    \n    \n      28408\n      -28.9930\n      -176.2380\n      7.4\n      2011.0\n      6.596810\n      0.803190\n    \n    \n      23863\n      -55.4703\n      -28.3669\n      6.9\n      2014.0\n      6.097310\n      0.802690\n    \n    \n      32803\n      -36.1220\n      -72.8980\n      8.8\n      2010.0\n      7.999521\n      0.800479\n    \n    \n      27672\n      -22.1410\n      170.3400\n      6.6\n      2012.0\n      5.830724\n      0.769276\n    \n    \n      24958\n      -60.2738\n      -46.4011\n      7.7\n      2013.0\n      6.938609\n      0.761391\n    \n    \n      31196\n      -56.5860\n      -142.2920\n      6.4\n      2010.0\n      5.641422\n      0.758578\n    \n    \n      26943\n      2.1900\n      126.8370\n      6.6\n      2012.0\n      5.849246\n      0.750754\n    \n    \n      9118\n      -19.7020\n      167.9470\n      7.3\n      2010.0\n      6.550810\n      0.749190\n    \n    \n      32001\n      7.8810\n      91.9360\n      7.5\n      2010.0\n      6.785405\n      0.714595\n    \n    \n      31229\n      -3.4870\n      100.0820\n      7.8\n      2010.0\n      7.086996\n      0.713004\n    \n    \n      29640\n      38.2760\n      141.5880\n      7.1\n      2011.0\n      6.387718\n      0.712282\n    \n    \n      11356\n      18.4430\n      -72.5710\n      7.0\n      2010.0\n      6.288026\n      0.711974\n    \n    \n      30296\n      36.2810\n      141.1110\n      7.9\n      2011.0\n      7.197845\n      0.702155\n    \n    \n      28574\n      -21.6110\n      -179.5280\n      7.3\n      2011.0\n      6.601124\n      0.698876\n    \n    \n      23633\n      -32.6953\n      -71.4416\n      6.4\n      2014.0\n      5.705553\n      0.694447\n    \n    \n      25517\n      10.7010\n      -42.5940\n      6.6\n      2013.0\n      5.908930\n      0.691070\n    \n    \n      28001\n      -10.6170\n      165.1600\n      6.4\n      2012.0\n      5.722428\n      0.677572\n    \n    \n      25773\n      30.3080\n      102.8880\n      6.6\n      2013.0\n      5.922831\n      0.677169\n    \n    \n      29004\n      38.0340\n      143.2640\n      7.0\n      2011.0\n      6.332486\n      0.667514\n    \n    \n      23815\n      14.7240\n      -92.4614\n      6.9\n      2014.0\n      6.237697\n      0.662303\n    \n    \n      24360\n      -20.3113\n      -70.5756\n      6.5\n      2014.0\n      5.839327\n      0.660673\n    \n    \n      25633\n      -23.0090\n      -177.2320\n      7.4\n      2013.0\n      6.743190\n      0.656810\n    \n    \n      30256\n      36.8230\n      141.8240\n      6.1\n      2011.0\n      5.446211\n      0.653789\n    \n    \n      9520\n      -3.4870\n      100.0820\n      7.8\n      2010.0\n      7.146346\n      0.653654\n    \n    \n      29133\n      52.0500\n      -171.8360\n      7.3\n      2011.0\n      6.646652\n      0.653348\n    \n    \n      32492\n      -34.2900\n      -71.8910\n      6.9\n      2010.0\n      6.247025\n      0.652975\n    \n    \n      24066\n      4.2485\n      92.7574\n      6.0\n      2014.0\n      5.353450\n      0.646550\n    \n    \n      24359\n      -20.5709\n      -70.4931\n      7.7\n      2014.0\n      7.056532\n      0.643468\n    \n    \n      26083\n      -10.9940\n      165.7410\n      6.6\n      2013.0\n      5.958927\n      0.641073\n    \n    \n      32768\n      -37.7730\n      -75.0480\n      7.4\n      2010.0\n      6.762054\n      0.637946\n    \n    \n      10845\n      -36.6650\n      -73.3740\n      6.6\n      2010.0\n      5.963286\n      0.636714\n    \n  \n\n\n\n\n(2010~2014 시도) - 21번째 Ouest Department, Haiti 아이티 지진 2010년 진도 7.0 - 24번쨰 Puchuncavi, Valparaíso, Chile 칠레 지진 2014년 진도 6.4 - 28번째 Baoxing County, Yaan, Sichuan, China 중국 쓰촨성 지진 2013년 진도 6.6\n(2010~2015 시도_결과 좋지 않음?!) - 23번째 2010년 West New Britain Province, Papua New Guinea 진도 7.3 - 24번째 2011년 Kuzawa Terayama, Tanagura, Higashishirakawa District, Fukushima 963-5671, Japan 진도 6.6 - 29번째 2015년 Kishim, Afghanistan 진도 7.5\n- vis\n\n#moo_global.visf()\n\n\n#moo_global.visfhat()\n\n\n#moo_global.visres()\n\n\n#moo_global.vis(MagThresh=6.9,ResThresh=0.5)"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html#analysis_df_global20152020",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html#analysis_df_global20152020",
    "title": "Earthquake",
    "section": "analysis_df_global(2015~2020)",
    "text": "analysis_df_global(2015~2020)\n- make instance for analysis\n\nmoo_global=MooYaHo2(df_global.query(\"2015 <= Year <= 2020\"))\n\n- get distance\n\nmoo_global.get_distance()\n\n100%|██████████| 11239/11239 [02:38<00:00, 71.12it/s] \n\n\n\nmoo_global.D[moo_global.D>0].mean()\n\n8814.318793468068\n\n\n\nplt.hist(moo_global.D[moo_global.D>0])\n\n(array([10894274., 13618924., 16426520., 17583818., 16025000., 15684642.,\n        13794372., 10946494.,  9072574.,  2254138.]),\n array([2.54728455e-02, 2.00123511e+03, 4.00244475e+03, 6.00365439e+03,\n        8.00486402e+03, 1.00060737e+04, 1.20072833e+04, 1.40084929e+04,\n        1.60097026e+04, 1.80109122e+04, 2.00121218e+04]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n- weight matrix\n\nmoo_global.get_weightmatrix(theta=(8814.318793468068),kappa=2500) \n\n- fit\n\nmoo_global.fit2()\n\n\nmoo_global.df.sort_values(\"Residual\",ascending=False).iloc[:30,:]\n\n\n\n\n\n  \n    \n      \n      Latitude\n      Longitude\n      Magnitude\n      Year\n      MagnitudeHat\n      Residual\n    \n  \n  \n    \n      41735\n      -31.5729\n      -71.6744\n      8.3\n      2015.0\n      7.247889\n      1.052111\n    \n    \n      36993\n      -18.1125\n      -178.1530\n      8.2\n      2018.0\n      7.151339\n      1.048661\n    \n    \n      21952\n      -31.5729\n      -71.6744\n      8.3\n      2015.0\n      7.377015\n      0.922985\n    \n    \n      36363\n      -21.9496\n      169.4266\n      7.5\n      2018.0\n      6.641801\n      0.858199\n    \n    \n      39771\n      -10.6812\n      161.3273\n      7.8\n      2016.0\n      6.952781\n      0.847219\n    \n    \n      41015\n      -4.9521\n      94.3299\n      7.8\n      2016.0\n      6.982741\n      0.817259\n    \n    \n      33896\n      -33.2927\n      -177.8571\n      7.4\n      2020.0\n      6.584337\n      0.815663\n    \n    \n      39932\n      -42.7373\n      173.0540\n      7.8\n      2016.0\n      6.994644\n      0.805356\n    \n    \n      21719\n      -8.3381\n      124.8754\n      6.5\n      2015.0\n      5.698561\n      0.801439\n    \n    \n      33404\n      54.6020\n      -159.6258\n      7.6\n      2020.0\n      6.825914\n      0.774086\n    \n    \n      33593\n      -27.9686\n      -71.3062\n      6.8\n      2020.0\n      6.068130\n      0.731870\n    \n    \n      21559\n      38.2107\n      72.7797\n      7.2\n      2015.0\n      6.476211\n      0.723789\n    \n    \n      36263\n      55.0999\n      164.6993\n      7.3\n      2018.0\n      6.591954\n      0.708046\n    \n    \n      42106\n      -36.3601\n      -73.8120\n      6.4\n      2015.0\n      5.712923\n      0.687077\n    \n    \n      37769\n      -6.0699\n      142.7536\n      7.5\n      2018.0\n      6.826692\n      0.673308\n    \n    \n      36848\n      -18.4743\n      179.3502\n      7.9\n      2018.0\n      7.233195\n      0.666805\n    \n    \n      39584\n      -43.4064\n      -73.9413\n      7.6\n      2016.0\n      6.936485\n      0.663515\n    \n    \n      22246\n      -9.3070\n      158.4030\n      6.7\n      2015.0\n      6.036540\n      0.663460\n    \n    \n      36301\n      -58.5446\n      -26.3856\n      7.1\n      2018.0\n      6.448234\n      0.651766\n    \n    \n      42271\n      27.8087\n      86.0655\n      7.3\n      2015.0\n      6.651371\n      0.648629\n    \n    \n      40675\n      -56.2409\n      -26.9353\n      7.2\n      2016.0\n      6.551931\n      0.648069\n    \n    \n      42622\n      -10.7598\n      164.1216\n      6.1\n      2015.0\n      5.469452\n      0.630548\n    \n    \n      35487\n      -30.6441\n      -178.0995\n      7.3\n      2019.0\n      6.673190\n      0.626810\n    \n    \n      22521\n      -7.2175\n      154.5567\n      7.1\n      2015.0\n      6.493164\n      0.606836\n    \n    \n      42475\n      -4.7294\n      152.5623\n      7.5\n      2015.0\n      6.897835\n      0.602165\n    \n    \n      36101\n      -30.0404\n      -71.3815\n      6.7\n      2019.0\n      6.098721\n      0.601279\n    \n    \n      37927\n      56.0039\n      -149.1658\n      7.9\n      2018.0\n      7.307120\n      0.592880\n    \n    \n      38520\n      15.0222\n      -93.8993\n      8.2\n      2017.0\n      7.608529\n      0.591471\n    \n    \n      21912\n      -31.5173\n      -71.8040\n      6.7\n      2015.0\n      6.110265\n      0.589735\n    \n    \n      41887\n      -9.3438\n      158.0525\n      6.6\n      2015.0\n      6.012902\n      0.587098\n    \n  \n\n\n\n\n바다 아닌 거 - 8번째 2016년 Rotherham, New Zealand 뉴질랜드 카이코우라 지진 진도 7.8 - 9번째 2015년 Langkuru Utara, Pureman, Alor Regency, East Nusa Tenggara, Indonesia 수마트라 진도 6.5 - 15번째 2018년 Hela Province, Papua New Guinea 파푸아뉴기니 진도 7.5 - 20번째 2015년 Kalinchok, Nepal 네팔 진도 7.3 - 26번째 2019년 Coquimbo, Chile 칠레 코킴보주 진도 6.7\n- vis\n\n#moo_global.vis(MagThresh=7,ResThresh=0.3)\n\n\n\npd.read_html('https://en.wikipedia.org/wiki/Lists_of_21st-century_earthquakes',encoding='utf-8')[0].query('Magnitude<=7')# List of deadliest earthquakes\n\n\n\n\n\n  \n    \n      \n      Rank\n      Fatalities\n      Magnitude\n      Location\n      Event\n      Date\n    \n  \n  \n    \n      1\n      2\n      220000\n      7.0\n      Haiti\n      2010 Haiti earthquake\n      January 12, 2010\n    \n    \n      4\n      5\n      26271\n      6.6\n      Iran\n      2003 Bam earthquake\n      December 26, 2003\n    \n    \n      8\n      9\n      5782\n      6.4\n      Indonesia\n      2006 Yogyakarta earthquake\n      May 26, 2006\n    \n    \n      10\n      11\n      2968\n      6.9\n      China\n      2010 Yushu earthquake\n      April 13, 2010\n    \n    \n      11\n      12\n      2266\n      6.8\n      Algeria\n      2003 Boumerdès earthquake\n      May 21, 2003\n    \n    \n      14\n      15\n      1163\n      6.0\n      Afghanistan\n      June 2022 Afghanistan earthquake\n      June 21, 2022\n    \n  \n\n\n\n\n\npd.read_html('https://en.wikipedia.org/wiki/Lists_of_21st-century_earthquakes',encoding='utf-8')[3] # Deadliest earthquakes by year\n\n\n\n\n\n  \n    \n      \n      Year\n      Fatalities\n      Magnitude\n      Event\n      Location\n      Date\n      Death toll\n    \n  \n  \n    \n      0\n      2001\n      20085\n      7.7\n      2001 Gujarat earthquake\n      India\n      January 26\n      21357\n    \n    \n      1\n      2002\n      1000\n      6.1\n      2002 Hindu Kush earthquakes\n      Afghanistan\n      March 25\n      1685\n    \n    \n      2\n      2003\n      26271\n      6.6\n      2003 Bam earthquake\n      Iran\n      December 26\n      33819\n    \n    \n      3\n      2004\n      227898\n      9.1\n      2004 Indian Ocean earthquake and tsunami\n      Indonesia, Indian Ocean\n      December 26\n      227898\n    \n    \n      4\n      2005\n      87351\n      7.6\n      2005 Kashmir earthquake\n      Pakistan\n      October 8\n      87992\n    \n    \n      5\n      2006\n      5782\n      6.4\n      2006 Yogyakarta earthquake\n      Indonesia\n      May 26\n      6605\n    \n    \n      6\n      2007\n      519\n      8.0\n      2007 Peru earthquake\n      Peru\n      August 15\n      708\n    \n    \n      7\n      2008\n      87587\n      7.9\n      2008 Sichuan earthquake\n      China\n      May 12\n      88708\n    \n    \n      8\n      2009\n      1115\n      7.6\n      2009 Sumatra earthquakes\n      Indonesia\n      September 30\n      1790\n    \n    \n      9\n      2010\n      160000\n      7.0\n      2010 Haiti earthquake\n      Haiti\n      January 12\n      164627\n    \n    \n      10\n      2011\n      20896\n      9.0\n      2011 Tōhoku earthquake and tsunami\n      Japan\n      March 11\n      21492\n    \n    \n      11\n      2012\n      306\n      6.4\n      2012 East Azerbaijan earthquakes\n      Iran\n      August 11\n      689\n    \n    \n      12\n      2013\n      825\n      7.7\n      2013 Balochistan earthquakes\n      Pakistan\n      September 24\n      1572\n    \n    \n      13\n      2014\n      729\n      6.1\n      2014 Ludian earthquake\n      China\n      August 3\n      756\n    \n    \n      14\n      2015\n      8964\n      7.8\n      2015 Nepal earthquake\n      Nepal\n      April 25\n      9624\n    \n    \n      15\n      2016\n      673\n      7.8\n      2016 Ecuador earthquake\n      Ecuador\n      April 16\n      1339\n    \n    \n      16\n      2017\n      630\n      7.3\n      2017 Iran–Iraq earthquake\n      Iran\n      November 12\n      1232\n    \n    \n      17\n      2018\n      4340\n      7.5\n      2018 Sulawesi earthquake and tsunami\n      Indonesia\n      September 28\n      5239\n    \n    \n      18\n      2019\n      51\n      6.4\n      2019 Albania earthquake\n      Albania\n      November 26\n      288\n    \n    \n      19\n      2020\n      119\n      7.0\n      2020 Aegean Sea earthquake\n      Turkey/ Greece\n      October 30\n      207\n    \n    \n      20\n      2021\n      2248\n      7.2\n      2021 Haiti earthquake\n      Haiti\n      August 14\n      2476\n    \n    \n      21\n      2022\n      1163\n      6.0\n      June 2022 Afghanistan earthquake\n      Afghanistan\n      June 21\n      1405\n    \n  \n\n\n\n\n\n\nclass eachlocation(MooYaHo2):\n    def haiti(self,MagThresh=7,ResThresh=1,adjzoom=5,adjmarkersize = 40):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=15,\n                        center=dict(lat=18.4430, lon=-72.5710), \n                        zoom= adjzoom,\n                        height=900,\n                        opacity = 0.8,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-3,3])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 5,\n                      marker_color= 'blue',\n                      opacity = 0.1\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= adjmarkersize,\n                      marker_color= 'red',\n                      opacity = 0.8\n                      )\n        fig.add_trace(go.Scattermapbox(\n                    lat=self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                    lon=self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                    mode='markers',\n                    marker=go.scattermapbox.Marker(\n                        size=20,\n                        color='rgb(255, 255, 255)',\n                        opacity=0.4\n                    )\n                ))\n        return fig \n    def lquique(self,MagThresh=7,ResThresh=1,adjzoom=5, adjmarkersize= 40):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=15,\n                        center=dict(lat=-32.6953, lon=-71.4416), \n                        zoom=adjzoom,\n                        height=900,\n                        opacity = 0.8,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 5,\n                      marker_color= 'blue',\n                      opacity = 0.1\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= adjmarkersize,\n                      marker_color= 'red',\n                      opacity = 0.8\n                      )\n        fig.add_trace(go.Scattermapbox(\n                    lat=self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                    lon=self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                    mode='markers',\n                    marker=go.scattermapbox.Marker(\n                        size=20,\n                        color='rgb(255, 255, 255)',\n                        opacity=0.8\n                    )\n                ))\n        return fig \n    def sichuan(self,MagThresh=7,ResThresh=1,adjzoom=5,adjmarkersize=40):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=15,\n                        center=dict(lat=30.3080, lon=102.8880), \n                        zoom=adjzoom,\n                        height=900,\n                        opacity = 0.6,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 5,\n                      marker_color= 'blue',\n                      opacity = 0.1\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= adjmarkersize,\n                      marker_color= 'red',\n                      opacity = 0.8\n                      )\n        fig.add_trace(go.Scattermapbox(\n                    lat=self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                    lon=self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                    mode='markers',\n                    marker=go.scattermapbox.Marker(\n                        size=20,\n                        color='rgb(255, 255, 255)',\n                        opacity=0.8\n                    )\n                ))\n        return fig \n\n\neach_location=eachlocation(df_global.query(\"2010 <= Year < 2015\"))\n\n- get distance\n\neach_location.get_distance()\n\n100%|██████████| 12498/12498 [03:20<00:00, 62.38it/s] \n\n\n\neach_location.D[each_location.D>0].mean()\n\n8810.865423093777\n\n\n\nplt.hist(each_location.D[each_location.D>0])\n\n(array([14176290., 16005894., 21186674., 22331128., 19394182., 17548252.,\n        16668048., 13316436., 12973260.,  2582550.]),\n array([8.97930163e-02, 2.00141141e+03, 4.00273303e+03, 6.00405465e+03,\n        8.00537626e+03, 1.00066979e+04, 1.20080195e+04, 1.40093411e+04,\n        1.60106627e+04, 1.80119844e+04, 2.00133060e+04]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n- weight matrix\n\neach_location.get_weightmatrix(theta=(8810.865423093777),kappa=2500) \n\n- fit\n\neach_location.fit2()\n\n\neach_location.haiti(MagThresh=6.9,ResThresh=0.5)\n\n\n                                                \n\n\n\neach_location.lquique(MagThresh=8,ResThresh=0.4,adjzoom=4.3)\n\n\n                                                \n\n\n\neach_location.sichuan(MagThresh=6.5,ResThresh=0.4)"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html#칠레",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html#칠레",
    "title": "Earthquake",
    "section": "칠레",
    "text": "칠레\n\ndf_chile_ex= pd.concat([pd.read_csv('05_10.csv'),pd.read_csv('10_15.csv')]).iloc[:,[0,1,2,4]].rename(columns={'latitude':'Latitude','longitude':'Longitude','mag':'Magnitude'}).reset_index().iloc[:,1:]\n\n\ndf_chile = df_chile_ex.assign(Year=list(map(lambda x: x.split('-')[0], df_chile_ex.time))).iloc[:,1:]\n\n\ndf_chile = df_chile.assign(Month=list(map(lambda x: x.split('-')[1], df_chile_ex.time)))\n\n\ndf_chile.Year = df_chile.Year.astype(np.float64)\ndf_chile.Month = df_chile.Month.astype(np.float64)\n\n\nchile_location=eachlocation(df_chile.query(\"2010 <= Year < 2015\"))\n\n\nchile_location.get_distance()\n\n100%|██████████| 12498/12498 [03:18<00:00, 62.95it/s]  \n\n\n\nchile_location.get_weightmatrix(theta=(chile_location.D[chile_location.D>0].mean()),kappa=2500) \n\n\nchile_location.fit2()\n\n아이티\n\nchile_location.df.assign(Residual2 = chile_location.df.Residual**2).reset_index().iloc[2324:2330,:]\n\n\n\n\n\n  \n    \n      \n      index\n      Latitude\n      Longitude\n      Magnitude\n      Year\n      Month\n      MagnitudeHat\n      Residual\n      Residual2\n    \n  \n  \n    \n      2324\n      2324\n      18.463\n      -72.626\n      5.0\n      2010.0\n      1.0\n      5.334262\n      -0.334262\n      0.111731\n    \n    \n      2325\n      2325\n      18.387\n      -72.784\n      6.0\n      2010.0\n      1.0\n      5.719565\n      0.280435\n      0.078644\n    \n    \n      2326\n      2326\n      18.443\n      -72.571\n      7.0\n      2010.0\n      1.0\n      6.288026\n      0.711974\n      0.506907\n    \n    \n      2327\n      2327\n      -5.417\n      133.731\n      5.5\n      2010.0\n      1.0\n      5.530625\n      -0.030625\n      0.000938\n    \n    \n      2328\n      2328\n      15.437\n      -88.761\n      5.1\n      2010.0\n      1.0\n      5.125565\n      -0.025565\n      0.000654\n    \n    \n      2329\n      2329\n      -16.861\n      -174.228\n      5.3\n      2010.0\n      1.0\n      5.471571\n      -0.171571\n      0.029437\n    \n  \n\n\n\n\n칠레\n\nchile_location.df.assign(Residual2 = chile_location.df.Residual**2).reset_index().query(\"-56.5 < Latitude & Latitude <-17.4 & -81.5 < Longitude & Longitude < -61.5 & Year == 2014 & Month == 8\")\n\n\n\n\n\n  \n    \n      \n      index\n      Latitude\n      Longitude\n      Magnitude\n      Year\n      Month\n      MagnitudeHat\n      Residual\n      Residual2\n    \n  \n  \n    \n      2997\n      14603\n      -32.6953\n      -71.4416\n      6.4\n      2014.0\n      8.0\n      5.705553\n      0.694447\n      0.482257\n    \n    \n      2999\n      14605\n      -20.1745\n      -69.0385\n      5.6\n      2014.0\n      8.0\n      5.497368\n      0.102632\n      0.010533\n    \n    \n      3032\n      14638\n      -20.1580\n      -70.0230\n      5.3\n      2014.0\n      8.0\n      5.291126\n      0.008874\n      0.000079\n    \n    \n      3046\n      14652\n      -23.9047\n      -66.7371\n      5.0\n      2014.0\n      8.0\n      4.909951\n      0.090049\n      0.008109\n    \n    \n      3057\n      14663\n      -33.7770\n      -72.2030\n      5.2\n      2014.0\n      8.0\n      5.382720\n      -0.182720\n      0.033387\n    \n  \n\n\n\n\n중국\n\nchile_location.df.assign(Residual2 = chile_location.df.Residual**2).reset_index().iloc[5136:5142,:]\n\n\n\n\n\n  \n    \n      \n      index\n      Latitude\n      Longitude\n      Magnitude\n      Year\n      Month\n      MagnitudeHat\n      Residual\n      Residual2\n    \n  \n  \n    \n      5136\n      16742\n      30.209\n      102.862\n      5.0\n      2013.0\n      4.0\n      5.027420\n      -0.027420\n      0.000752\n    \n    \n      5137\n      16743\n      30.308\n      102.888\n      6.6\n      2013.0\n      4.0\n      5.922831\n      0.677169\n      0.458558\n    \n    \n      5138\n      16744\n      39.693\n      143.258\n      5.0\n      2013.0\n      4.0\n      4.758333\n      0.241667\n      0.058403\n    \n    \n      5139\n      16745\n      49.965\n      157.652\n      6.1\n      2013.0\n      4.0\n      5.797293\n      0.302707\n      0.091632\n    \n    \n      5140\n      16746\n      -11.976\n      121.632\n      5.8\n      2013.0\n      4.0\n      5.854233\n      -0.054233\n      0.002941\n    \n    \n      5141\n      16747\n      -14.966\n      166.857\n      5.2\n      2013.0\n      4.0\n      5.228670\n      -0.028670\n      0.000822"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html",
    "title": "Class code for Comparison Study",
    "section": "",
    "text": "Simulation\nex - The Stanford bunny data generated using the pygsp package is a common graphics 3D test model and NN-graph. It has 2503 data. We use filter.Heat in this package and it calculate data by \\(\\hat{g}(x) = \\exp(\\frac{-\\tau x}{\\lambda_{max}})\\) and \\(\\tau\\) is 75. We use Chebyshev polynomial approximation on this filter. We make zero vector whixh size is 2503, and put -3000 to one value to use a Lanczos approximation. A Lanczos approximation will resize signals by flattened.\nref: https://pygsp.readthedocs.io/en/v0.5.1/reference/filters.html\n\\(r = 5 + \\cos(\\frac{12\\pi - (-\\pi)}{n})\\times i , (i=1,2,\\dots , n)\\)\n\\(r \\cos(\\frac{\\pi - 2\\times \\pi /n - (-\\pi) }{n}\\times i)),(i=1,2,\\dots ,n)\\)\n\\(r \\sin((\\frac{\\pi - 2\\times \\pi /n - (-\\pi) }{n}\\times i)),(i=1,2,\\dots ,n)\\)\n\\(f = 10 \\times (\\frac{6 \\pi}{n} \\times i),(i=1,2, \\dots , n)\\)"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#import",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#import",
    "title": "Class code for Comparison Study",
    "section": "Import",
    "text": "Import\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.linear_model import SGDOneClassSVM\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.pipeline import make_pipeline\n\nimport pandas as pd\nfrom sklearn.neighbors import LocalOutlierFactor\n\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr\n\nfrom sklearn.datasets import fetch_kddcup99, fetch_covtype, fetch_openml\nfrom sklearn.preprocessing import LabelBinarizer\n\nimport tqdm\n\nfrom pygsp import graphs, filters, plotting, utils\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\nimport plotly.graph_objects as go\nfrom IPython.display import HTML\n\nimport plotly.express as px\n\nfrom sklearn.covariance import EmpiricalCovariance, MinCovDet\n\nfrom alibi_detect.od import IForest\n# from pyod.models.iforest import IForest\n\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nimport seaborn as sns\n\nfrom PyNomaly import loop\n\nfrom sklearn import svm\n\nfrom pyod.models.lscp import LSCP\nfrom pyod.models.hbos import HBOS\n\nfrom pyod.models.so_gaal import SO_GAAL\nfrom pyod.models.mcd import MCD\nfrom pyod.models.mo_gaal import MO_GAAL\nfrom pyod.models.knn import KNN\nfrom pyod.models.lof import LOF\nfrom pyod.models.ocsvm import OCSVM\n\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.sos import SOS"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#class-code",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#class-code",
    "title": "Class code for Comparison Study",
    "section": "Class Code",
    "text": "Class Code\n\ntab_linear = pd.DataFrame(columns=[\"Accuracy\",\"Precision\",\"Recall\",\"F1\"])\ntab_orbit = pd.DataFrame(columns=[\"Accuracy\",\"Precision\",\"Recall\",\"F1\"])\ntab_bunny = pd.DataFrame(columns=[\"Accuracy\",\"Precision\",\"Recall\",\"F1\"])\n\n\nclass Conf_matrx:\n    def __init__(self,original,compare,tab):\n        self.original = original\n        self.compare = compare\n        self.tab = tab\n    def conf(self,name):\n        self.conf_matrix = confusion_matrix(self.original, self.compare)\n        \n        fig, ax = plt.subplots(figsize=(5, 5))\n        ax.matshow(self.conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n        for i in range(self.conf_matrix.shape[0]):\n            for j in range(self.conf_matrix.shape[1]):\n                ax.text(x=j, y=i,s=self.conf_matrix[i, j], va='center', ha='center', size='xx-large')\n        plt.xlabel('Predictions', fontsize=18)\n        plt.ylabel('Actuals', fontsize=18)\n        plt.title('Confusion Matrix', fontsize=18)\n        plt.show()\n        \n        self.acc = accuracy_score(self.original, self.compare)\n        self.pre = precision_score(self.original, self.compare)\n        self.rec = recall_score(self.original, self.compare)\n        self.f1 = f1_score(self.original, self.compare)\n        \n        print('Accuracy: %.3f' % self.acc)\n        print('Precision: %.3f' % self.pre)\n        print('Recall: %.3f' % self.rec)\n        print('F1 Score: %.3f' % self.f1)\n        \n        self.tab = self.tab.append(pd.DataFrame({\"Accuracy\":[self.acc],\"Precision\":[self.pre],\"Recall\":[self.rec],\"F1\":[self.f1]},index = [name]))\n\n\nclass Linear:\n    def __init__(self,df):\n        self.df = df\n        self.y = df.y.to_numpy()\n        #self.y1 = df.y1.to_numpy()\n        self.x = df.x.to_numpy()\n        self.n = len(self.y)\n        self.W = w\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)      \n    def fit(self,sd=20): # fit with ebayesthresh\n        self._eigen()\n        self.ybar = self.Psi.T @ self.y # fbar := graph fourier transform of f\n        self.power = self.ybar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.ybar**2),sd=sd))\n        self.ybar_threshed = np.where(self.power_threshed>0,self.ybar,0)\n        self.yhat = self.Psi@self.ybar_threshed\n        self.df = self.df.assign(yHat = self.yhat)\n        self.df = self.df.assign(Residual = self.df.y- self.df.yHat)\n\n\nclass Orbit:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.f.to_numpy()\n        self.x = df.x.to_numpy()\n        self.y = df.y.to_numpy()\n        self.n = len(self.f)\n        self.theta= None\n    def get_distance(self):\n        self.D = np.zeros([self.n,self.n])\n        locations = np.stack([self.x, self.y],axis=1)\n        for i in tqdm.tqdm(range(self.n)):\n            for j in range(i,self.n):\n                self.D[i,j]=np.linalg.norm(locations[i]-locations[j])\n        self.D = self.D + self.D.T\n    def get_weightmatrix(self,theta=1,beta=0.5,kappa=4000):\n        self.theta = theta\n        dist = np.where(self.D < kappa,self.D,0)\n        self.W = np.exp(-(dist/self.theta)**2)\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)       \n    def fit(self,sd=5,ref=20): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.f # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2),sd=sd))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(fHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.f- self.df.fHat)\n        self.bottom = np.zeros_like(self.f)\n        self.width=0.05\n        self.depth=0.05\n\n\nclass BUNNY:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.f.to_numpy()\n        self.z = df.z.to_numpy()\n        self.x = df.x.to_numpy()\n        self.y = df.y.to_numpy()\n        self.noise = df.noise.to_numpy()\n        self.fnoise = self.f + self.noise\n        self.W = _W\n        self.n = len(self.f)\n        self.theta= None\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)       \n    def fit(self,sd=5,ref=6): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.fnoise # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2),sd=sd))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(fnoise = self.fnoise)\n        self.df = self.df.assign(fHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.f + self.df.noise - self.df.fHat)\n        self.bottom = np.zeros_like(self.f)\n        self.width=0.05\n        self.depth=0.05"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#linear-ebayesthresh",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#linear-ebayesthresh",
    "title": "Class code for Comparison Study",
    "section": "Linear EbayesThresh",
    "text": "Linear EbayesThresh\n\n%load_ext rpy2.ipython\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython\n\n\n\n%%R\nlibrary(EbayesThresh)\nset.seed(1)\nepsilon = rnorm(1000)\nsignal_1 = sample(c(runif(25,-2,-1.5), runif(25,1.5,2), rep(0,950)))\nindex_of_trueoutlier_1 = which(signal_1!=0)\nindex_of_trueoutlier_1\nx_1=signal_1+epsilon\n\n\n%R -o x_1\n%R -o index_of_trueoutlier_1\n%R -o signal_1\n\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\n\n\noutlier_true_index_1 = index_of_trueoutlier_1\n\n\noutlier_true_value_1 = x_1[index_of_trueoutlier_1]\n\n\noutlier_true_one_1 = signal_1.copy()\n\n\noutlier_true_one_1 = list(map(lambda x: -1 if x!=0 else 1,outlier_true_one_1))"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#linear",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#linear",
    "title": "Class code for Comparison Study",
    "section": "Linear",
    "text": "Linear\n\n_x_1 = np.linspace(0,2,1000)\n_y1_1 = 5*_x_1\n_y_1 = _y1_1 + x_1 # x is epsilon\n\n\n_df=pd.DataFrame({'x':_x_1, 'y':_y_1})\n\n\nX = np.array(_df)\n\n\nGODE\n\nw=np.zeros((1000,1000))\n\n\nfor i in range(1000):\n    for j in range(1000):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\n_Linear = Linear(_df)\n\n\n_Linear.fit(sd=5)\n\n\noutlier_simul_one = (_Linear.df['Residual']**2).tolist()\n\n\noutlier_simul_one = list(map(lambda x: -1 if x > 20 else 1,outlier_simul_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_simul_one,tab_linear)\n\n\n_conf.conf(\"GODE\")\n\n\n\n\nAccuracy: 0.950\nPrecision: 0.950\nRecall: 1.000\nF1 Score: 0.974\n\n\n\none = _conf.tab\n\n\n\nLOF\n\nclf = LocalOutlierFactor(n_neighbors=2)\n\n\n_conf = Conf_matrx(outlier_true_one_1,clf.fit_predict(X),tab_linear)\n\n\n_conf.conf(\"LOF (Breunig et al., 2000)\")\n\n\n\n\nAccuracy: 0.890\nPrecision: 0.973\nRecall: 0.909\nF1 Score: 0.940\n\n\n\ntwo = one.append(_conf.tab)\n\n\n\nKNN\n\nfrom pyod.models.knn import KNN\n\n\nclf = KNN()\nclf.fit(_df[['x', 'y']])\n_df['knn_Clf'] = clf.labels_\n\n\noutlier_KNN_one = list(clf.labels_)\n\n\noutlier_KNN_one = list(map(lambda x: 1 if x==0  else -1,outlier_KNN_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_KNN_one,tab_linear)\n\n\n_conf.conf(\"kNN (Ramaswamy et al., 2000)\")\n\n\n\n\nAccuracy: 0.912\nPrecision: 0.979\nRecall: 0.927\nF1 Score: 0.952\n\n\n\nthree = two.append(_conf.tab)\n\n\n\nCBLOF\n\nclf = CBLOF(contamination=0.05,check_estimator=False, random_state=77)\nclf.fit(_df[['x', 'y']])\n_df['CBLOF_Clf'] = clf.labels_\n\n\noutlier_CBLOF_one = list(clf.labels_)\n\n\noutlier_CBLOF_one = list(map(lambda x: 1 if x==0  else -1,outlier_CBLOF_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_CBLOF_one,tab_linear)\n\n\n_conf.conf(\"CBLOF (He et al., 2003)\")\n\n\n\n\nAccuracy: 0.920\nPrecision: 0.958\nRecall: 0.958\nF1 Score: 0.958\n\n\n\nfour = three.append(_conf.tab)\n\n\n\nOCSVM\n\nclf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n\n\nclf.fit(X)\n\nOneClassSVM(gamma=0.1, nu=0.1)\n\n\n\noutlier_OSVM_one = list(clf.predict(X))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_OSVM_one,tab_linear)\n\n\n_conf.conf(\"OCSVM (Sch ̈olkopf et al., 2001)\")\n\n\n\n\nAccuracy: 0.909\nPrecision: 0.978\nRecall: 0.925\nF1 Score: 0.951\n\n\n\nfive = four.append(_conf.tab)\n\n\n\nMCD\n\nclf = MCD()\nclf.fit(_df[['x', 'y']])\n_df['MCD_clf'] = clf.labels_\n\n\noutlier_MCD_one = list(clf.labels_)\n\n\noutlier_MCD_one = list(map(lambda x: 1 if x==0  else -1,outlier_MCD_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_MCD_one,tab_linear)\n\n\n_conf.conf(\"MCD (Hardin and Rocke, 2004)\")\n\n\n\n\nAccuracy: 0.918\nPrecision: 0.982\nRecall: 0.931\nF1 Score: 0.956\n\n\n\nsix = five.append(_conf.tab)\n\n\n\nFeature Bagging\n\nclf = FeatureBagging()\nclf.fit(_df[['x', 'y']])\n_df['FeatureBagging_clf'] = clf.labels_\n\n\noutlier_FeatureBagging_one = list(clf.labels_)\n\n\noutlier_FeatureBagging_one = list(map(lambda x: 1 if x==0  else -1,outlier_FeatureBagging_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_FeatureBagging_one,tab_linear)\n\n\n_conf.conf(\"Feature Bagging (Lazarevic and Kumar, 2005)\")\n\n\n\n\nAccuracy: 0.918\nPrecision: 0.982\nRecall: 0.931\nF1 Score: 0.956\n\n\n\nseven = six.append(_conf.tab)\n\n\n\nABOD\n\nclf = ABOD(contamination=0.05)\nclf.fit(_df[['x', 'y']])\n_df['ABOD_Clf'] = clf.labels_\n\n\noutlier_ABOD_one = list(clf.labels_)\n\n\noutlier_ABOD_one = list(map(lambda x: 1 if x==0  else -1,outlier_ABOD_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_ABOD_one,tab_linear)\n\n\n_conf.conf(\"ABOD (Kriegel et al., 2008)\")\n\n\n\n\nAccuracy: 0.946\nPrecision: 0.972\nRecall: 0.972\nF1 Score: 0.972\n\n\n\neight = seven.append(_conf.tab)\n\n\n\nIForest\n\nod = IForest(\n    threshold=0.,\n    n_estimators=100\n)\n\n\nod.fit(_df[['x', 'y']])\n\n\npreds = od.predict(\n    _df[['x', 'y']],\n    return_instance_score=True\n)\n\n\n_df['IF_alibi'] = preds['data']['is_outlier']\n\n\noutlier_alibi_one = _df['IF_alibi']\n\n\noutlier_alibi_one = list(map(lambda x: 1 if x==0  else -1,outlier_alibi_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_alibi_one,tab_linear)\n\n\n_conf.conf(\"Isolation Forest (Liu et al., 2008)\")\n\n\n\n\nAccuracy: 0.800\nPrecision: 0.984\nRecall: 0.802\nF1 Score: 0.884\n\n\n\nnine = eight.append(_conf.tab)\n\n\n\nHBOS\n\nclf = HBOS()\nclf.fit(_df[['x', 'y']])\n_df['HBOS_clf'] = clf.labels_\n\n\noutlier_HBOS_one = list(clf.labels_)\n\n\noutlier_HBOS_one = list(map(lambda x: 1 if x==0  else -1,outlier_HBOS_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_HBOS_one,tab_linear)\n\n\n_conf.conf(\"HBOS (Goldstein and Dengel, 2012)\")\n\n\n\n\nAccuracy: 0.889\nPrecision: 0.960\nRecall: 0.921\nF1 Score: 0.940\n\n\n\nten = nine.append(_conf.tab)\n\n\n\nSOS\n\noutlier_SOS_one = list(clf.labels_)\n\n\noutlier_SOS_one = list(map(lambda x: 1 if x==0  else -1,outlier_SOS_one))\n\n\nclf = SOS()\nclf.fit(_df[['x', 'y']])\n_df['SOS_clf'] = clf.labels_\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_SOS_one,tab_linear)\n\n\n_conf.conf(\"SOS (Janssens et al., 2012)\")\n\n\n\n\nAccuracy: 0.889\nPrecision: 0.960\nRecall: 0.921\nF1 Score: 0.940\n\n\n\neleven = ten.append(_conf.tab)\n\n\n\nSO_GAAL\n\nclf = SO_GAAL()\nclf.fit(_df[['x', 'y']])\n_df['SO_GAAL_clf'] = clf.labels_\n\nEpoch 1 of 60\n\nTesting for epoch 1 index 1:\n\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n\n\n\nTesting for epoch 1 index 2:\nEpoch 2 of 60\n\nTesting for epoch 2 index 1:\n\nTesting for epoch 2 index 2:\nEpoch 3 of 60\n\nTesting for epoch 3 index 1:\n\nTesting for epoch 3 index 2:\nEpoch 4 of 60\n\nTesting for epoch 4 index 1:\n\nTesting for epoch 4 index 2:\nEpoch 5 of 60\n\nTesting for epoch 5 index 1:\n\nTesting for epoch 5 index 2:\nEpoch 6 of 60\n\nTesting for epoch 6 index 1:\n\nTesting for epoch 6 index 2:\nEpoch 7 of 60\n\nTesting for epoch 7 index 1:\n\nTesting for epoch 7 index 2:\nEpoch 8 of 60\n\nTesting for epoch 8 index 1:\n\nTesting for epoch 8 index 2:\nEpoch 9 of 60\n\nTesting for epoch 9 index 1:\n\nTesting for epoch 9 index 2:\nEpoch 10 of 60\n\nTesting for epoch 10 index 1:\n\nTesting for epoch 10 index 2:\nEpoch 11 of 60\n\nTesting for epoch 11 index 1:\n\nTesting for epoch 11 index 2:\nEpoch 12 of 60\n\nTesting for epoch 12 index 1:\n\nTesting for epoch 12 index 2:\nEpoch 13 of 60\n\nTesting for epoch 13 index 1:\n\nTesting for epoch 13 index 2:\nEpoch 14 of 60\n\nTesting for epoch 14 index 1:\n\nTesting for epoch 14 index 2:\nEpoch 15 of 60\n\nTesting for epoch 15 index 1:\n\nTesting for epoch 15 index 2:\nEpoch 16 of 60\n\nTesting for epoch 16 index 1:\n\nTesting for epoch 16 index 2:\nEpoch 17 of 60\n\nTesting for epoch 17 index 1:\n\nTesting for epoch 17 index 2:\nEpoch 18 of 60\n\nTesting for epoch 18 index 1:\n\nTesting for epoch 18 index 2:\nEpoch 19 of 60\n\nTesting for epoch 19 index 1:\n\nTesting for epoch 19 index 2:\nEpoch 20 of 60\n\nTesting for epoch 20 index 1:\n\nTesting for epoch 20 index 2:\nEpoch 21 of 60\n\nTesting for epoch 21 index 1:\n\nTesting for epoch 21 index 2:\nEpoch 22 of 60\n\nTesting for epoch 22 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.3973\n\nTesting for epoch 22 index 2:\n16/16 [==============================] - 0s 679us/step - loss: 1.4180\nEpoch 23 of 60\n\nTesting for epoch 23 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4019\n\nTesting for epoch 23 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4210\nEpoch 24 of 60\n\nTesting for epoch 24 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4234\n\nTesting for epoch 24 index 2:\n16/16 [==============================] - 0s 691us/step - loss: 1.4552\nEpoch 25 of 60\n\nTesting for epoch 25 index 1:\n16/16 [==============================] - 0s 663us/step - loss: 1.4271\n\nTesting for epoch 25 index 2:\n16/16 [==============================] - 0s 767us/step - loss: 1.4613\nEpoch 26 of 60\n\nTesting for epoch 26 index 1:\n16/16 [==============================] - 0s 602us/step - loss: 1.4776\n\nTesting for epoch 26 index 2:\n16/16 [==============================] - 0s 715us/step - loss: 1.4349\nEpoch 27 of 60\n\nTesting for epoch 27 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4333\n\nTesting for epoch 27 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4840\nEpoch 28 of 60\n\nTesting for epoch 28 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5092\n\nTesting for epoch 28 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4956\nEpoch 29 of 60\n\nTesting for epoch 29 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5026\n\nTesting for epoch 29 index 2:\n16/16 [==============================] - 0s 831us/step - loss: 1.5576\nEpoch 30 of 60\n\nTesting for epoch 30 index 1:\n16/16 [==============================] - 0s 611us/step - loss: 1.5602\n\nTesting for epoch 30 index 2:\n16/16 [==============================] - 0s 638us/step - loss: 1.4791\nEpoch 31 of 60\n\nTesting for epoch 31 index 1:\n16/16 [==============================] - 0s 619us/step - loss: 1.5625\n\nTesting for epoch 31 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5635\nEpoch 32 of 60\n\nTesting for epoch 32 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5925\n\nTesting for epoch 32 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5807\nEpoch 33 of 60\n\nTesting for epoch 33 index 1:\n16/16 [==============================] - 0s 844us/step - loss: 1.5739\n\nTesting for epoch 33 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6122\nEpoch 34 of 60\n\nTesting for epoch 34 index 1:\n16/16 [==============================] - 0s 657us/step - loss: 1.6156\n\nTesting for epoch 34 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6021\nEpoch 35 of 60\n\nTesting for epoch 35 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6237\n\nTesting for epoch 35 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6302\nEpoch 36 of 60\n\nTesting for epoch 36 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6586\n\nTesting for epoch 36 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 1.6349\nEpoch 37 of 60\n\nTesting for epoch 37 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6708\n\nTesting for epoch 37 index 2:\n16/16 [==============================] - 0s 726us/step - loss: 1.7010\nEpoch 38 of 60\n\nTesting for epoch 38 index 1:\n16/16 [==============================] - 0s 826us/step - loss: 1.6865\n\nTesting for epoch 38 index 2:\n16/16 [==============================] - 0s 820us/step - loss: 1.6874\nEpoch 39 of 60\n\nTesting for epoch 39 index 1:\n16/16 [==============================] - 0s 680us/step - loss: 1.7410\n\nTesting for epoch 39 index 2:\n16/16 [==============================] - 0s 663us/step - loss: 1.7334\nEpoch 40 of 60\n\nTesting for epoch 40 index 1:\n16/16 [==============================] - 0s 637us/step - loss: 1.6871\n\nTesting for epoch 40 index 2:\n16/16 [==============================] - 0s 621us/step - loss: 1.7771\nEpoch 41 of 60\n\nTesting for epoch 41 index 1:\n16/16 [==============================] - 0s 629us/step - loss: 1.7724\n\nTesting for epoch 41 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.7815\nEpoch 42 of 60\n\nTesting for epoch 42 index 1:\n16/16 [==============================] - 0s 647us/step - loss: 1.7470\n\nTesting for epoch 42 index 2:\n16/16 [==============================] - 0s 612us/step - loss: 1.7897\nEpoch 43 of 60\n\nTesting for epoch 43 index 1:\n16/16 [==============================] - 0s 660us/step - loss: 1.8400\n\nTesting for epoch 43 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8351\nEpoch 44 of 60\n\nTesting for epoch 44 index 1:\n16/16 [==============================] - 0s 689us/step - loss: 1.8388\n\nTesting for epoch 44 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8174\nEpoch 45 of 60\n\nTesting for epoch 45 index 1:\n16/16 [==============================] - 0s 974us/step - loss: 1.8131\n\nTesting for epoch 45 index 2:\n16/16 [==============================] - 0s 712us/step - loss: 1.8391\nEpoch 46 of 60\n\nTesting for epoch 46 index 1:\n16/16 [==============================] - 0s 635us/step - loss: 1.7937\n\nTesting for epoch 46 index 2:\n16/16 [==============================] - 0s 971us/step - loss: 1.8550\nEpoch 47 of 60\n\nTesting for epoch 47 index 1:\n16/16 [==============================] - 0s 628us/step - loss: 1.8632\n\nTesting for epoch 47 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8457\nEpoch 48 of 60\n\nTesting for epoch 48 index 1:\n16/16 [==============================] - 0s 628us/step - loss: 1.8924\n\nTesting for epoch 48 index 2:\n16/16 [==============================] - 0s 640us/step - loss: 1.8481\nEpoch 49 of 60\n\nTesting for epoch 49 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8722\n\nTesting for epoch 49 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9405\nEpoch 50 of 60\n\nTesting for epoch 50 index 1:\n16/16 [==============================] - 0s 640us/step - loss: 1.9428\n\nTesting for epoch 50 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8585\nEpoch 51 of 60\n\nTesting for epoch 51 index 1:\n16/16 [==============================] - 0s 638us/step - loss: 1.8806\n\nTesting for epoch 51 index 2:\n16/16 [==============================] - 0s 608us/step - loss: 1.9145\nEpoch 52 of 60\n\nTesting for epoch 52 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9380\n\nTesting for epoch 52 index 2:\n16/16 [==============================] - 0s 615us/step - loss: 1.8934\nEpoch 53 of 60\n\nTesting for epoch 53 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9282\n\nTesting for epoch 53 index 2:\n16/16 [==============================] - 0s 651us/step - loss: 1.8956\nEpoch 54 of 60\n\nTesting for epoch 54 index 1:\n16/16 [==============================] - 0s 630us/step - loss: 1.8997\n\nTesting for epoch 54 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9230\nEpoch 55 of 60\n\nTesting for epoch 55 index 1:\n16/16 [==============================] - 0s 671us/step - loss: 1.9290\n\nTesting for epoch 55 index 2:\n16/16 [==============================] - 0s 885us/step - loss: 1.9631\nEpoch 56 of 60\n\nTesting for epoch 56 index 1:\n16/16 [==============================] - 0s 643us/step - loss: 1.9394\n\nTesting for epoch 56 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9368\nEpoch 57 of 60\n\nTesting for epoch 57 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9715\n\nTesting for epoch 57 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9327\nEpoch 58 of 60\n\nTesting for epoch 58 index 1:\n16/16 [==============================] - 0s 690us/step - loss: 1.9782\n\nTesting for epoch 58 index 2:\n16/16 [==============================] - 0s 612us/step - loss: 1.9637\nEpoch 59 of 60\n\nTesting for epoch 59 index 1:\n16/16 [==============================] - 0s 575us/step - loss: 1.9657\n\nTesting for epoch 59 index 2:\n16/16 [==============================] - 0s 890us/step - loss: 1.9923\nEpoch 60 of 60\n\nTesting for epoch 60 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9824\n\nTesting for epoch 60 index 2:\n16/16 [==============================] - 0s 658us/step - loss: 2.0536\n\n\n\noutlier_SO_GAAL_one = list(clf.labels_)\n\n\noutlier_SO_GAAL_one = list(map(lambda x: 1 if x==0  else -1,outlier_SO_GAAL_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_SO_GAAL_one,tab_linear)\n\n\n_conf.conf(\"SO-GAAL (Liu et al., 2019)\")\n\n\n\n\nAccuracy: 0.868\nPrecision: 0.954\nRecall: 0.904\nF1 Score: 0.929\n\n\n\ntwelve = eleven.append(_conf.tab)\n\n\n\nMO_GAAL\n\nclf = MO_GAAL()\nclf.fit(_df[['x', 'y']])\n_df['MO_GAAL_clf'] = clf.labels_\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n\n\nEpoch 1 of 60\n\nTesting for epoch 1 index 1:\n\nTesting for epoch 1 index 2:\nEpoch 2 of 60\n\nTesting for epoch 2 index 1:\n\nTesting for epoch 2 index 2:\nEpoch 3 of 60\n\nTesting for epoch 3 index 1:\n\nTesting for epoch 3 index 2:\nEpoch 4 of 60\n\nTesting for epoch 4 index 1:\n\nTesting for epoch 4 index 2:\nEpoch 5 of 60\n\nTesting for epoch 5 index 1:\n\nTesting for epoch 5 index 2:\nEpoch 6 of 60\n\nTesting for epoch 6 index 1:\n\nTesting for epoch 6 index 2:\nEpoch 7 of 60\n\nTesting for epoch 7 index 1:\n\nTesting for epoch 7 index 2:\nEpoch 8 of 60\n\nTesting for epoch 8 index 1:\n\nTesting for epoch 8 index 2:\nEpoch 9 of 60\n\nTesting for epoch 9 index 1:\n\nTesting for epoch 9 index 2:\nEpoch 10 of 60\n\nTesting for epoch 10 index 1:\n\nTesting for epoch 10 index 2:\nEpoch 11 of 60\n\nTesting for epoch 11 index 1:\n\nTesting for epoch 11 index 2:\nEpoch 12 of 60\n\nTesting for epoch 12 index 1:\n\nTesting for epoch 12 index 2:\nEpoch 13 of 60\n\nTesting for epoch 13 index 1:\n\nTesting for epoch 13 index 2:\nEpoch 14 of 60\n\nTesting for epoch 14 index 1:\n\nTesting for epoch 14 index 2:\nEpoch 15 of 60\n\nTesting for epoch 15 index 1:\n\nTesting for epoch 15 index 2:\nEpoch 16 of 60\n\nTesting for epoch 16 index 1:\n\nTesting for epoch 16 index 2:\nEpoch 17 of 60\n\nTesting for epoch 17 index 1:\n\nTesting for epoch 17 index 2:\nEpoch 18 of 60\n\nTesting for epoch 18 index 1:\n\nTesting for epoch 18 index 2:\nEpoch 19 of 60\n\nTesting for epoch 19 index 1:\n\nTesting for epoch 19 index 2:\nEpoch 20 of 60\n\nTesting for epoch 20 index 1:\n\nTesting for epoch 20 index 2:\nEpoch 21 of 60\n\nTesting for epoch 21 index 1:\n\nTesting for epoch 21 index 2:\n16/16 [==============================] - 0s 674us/step - loss: 0.5119\n16/16 [==============================] - 0s 1ms/step - loss: 0.8174\n16/16 [==============================] - 0s 1ms/step - loss: 1.0584\n16/16 [==============================] - 0s 1ms/step - loss: 1.2057\n16/16 [==============================] - 0s 1ms/step - loss: 1.2512\n16/16 [==============================] - 0s 653us/step - loss: 1.2690\n16/16 [==============================] - 0s 640us/step - loss: 1.2744\n16/16 [==============================] - 0s 846us/step - loss: 1.2761\n16/16 [==============================] - 0s 782us/step - loss: 1.2766\n16/16 [==============================] - 0s 1ms/step - loss: 1.2766\nEpoch 22 of 60\n\nTesting for epoch 22 index 1:\n16/16 [==============================] - 0s 665us/step - loss: 0.5016\n16/16 [==============================] - 0s 1ms/step - loss: 0.8168\n16/16 [==============================] - 0s 729us/step - loss: 1.0701\n16/16 [==============================] - 0s 619us/step - loss: 1.2239\n16/16 [==============================] - 0s 952us/step - loss: 1.2703\n16/16 [==============================] - 0s 680us/step - loss: 1.2884\n16/16 [==============================] - 0s 1ms/step - loss: 1.2938\n16/16 [==============================] - 0s 1ms/step - loss: 1.2955\n16/16 [==============================] - 0s 674us/step - loss: 1.2959\n16/16 [==============================] - 0s 680us/step - loss: 1.2959\n\nTesting for epoch 22 index 2:\n16/16 [==============================] - 0s 638us/step - loss: 0.4978\n16/16 [==============================] - 0s 1ms/step - loss: 0.8174\n16/16 [==============================] - 0s 988us/step - loss: 1.0777\n16/16 [==============================] - 0s 683us/step - loss: 1.2388\n16/16 [==============================] - 0s 1ms/step - loss: 1.2871\n16/16 [==============================] - 0s 1ms/step - loss: 1.3063\n16/16 [==============================] - 0s 899us/step - loss: 1.3121\n16/16 [==============================] - 0s 701us/step - loss: 1.3140\n16/16 [==============================] - 0s 674us/step - loss: 1.3144\n16/16 [==============================] - 0s 894us/step - loss: 1.3144\nEpoch 23 of 60\n\nTesting for epoch 23 index 1:\n16/16 [==============================] - 0s 629us/step - loss: 0.4856\n16/16 [==============================] - 0s 911us/step - loss: 0.8190\n16/16 [==============================] - 0s 1ms/step - loss: 1.0913\n16/16 [==============================] - 0s 1ms/step - loss: 1.2605\n16/16 [==============================] - 0s 1ms/step - loss: 1.3112\n16/16 [==============================] - 0s 1ms/step - loss: 1.3310\n16/16 [==============================] - 0s 2ms/step - loss: 1.3370\n16/16 [==============================] - 0s 1ms/step - loss: 1.3389\n16/16 [==============================] - 0s 745us/step - loss: 1.3393\n16/16 [==============================] - 0s 964us/step - loss: 1.3393\n\nTesting for epoch 23 index 2:\n16/16 [==============================] - 0s 640us/step - loss: 0.4780\n16/16 [==============================] - 0s 851us/step - loss: 0.8265\n16/16 [==============================] - 0s 1ms/step - loss: 1.1094\n16/16 [==============================] - 0s 1ms/step - loss: 1.2901\n16/16 [==============================] - 0s 702us/step - loss: 1.3448\n16/16 [==============================] - 0s 939us/step - loss: 1.3665\n16/16 [==============================] - 0s 854us/step - loss: 1.3731\n16/16 [==============================] - 0s 872us/step - loss: 1.3753\n16/16 [==============================] - 0s 633us/step - loss: 1.3759\n16/16 [==============================] - 0s 1ms/step - loss: 1.3759\nEpoch 24 of 60\n\nTesting for epoch 24 index 1:\n16/16 [==============================] - 0s 709us/step - loss: 0.4680\n16/16 [==============================] - 0s 964us/step - loss: 0.8342\n16/16 [==============================] - 0s 717us/step - loss: 1.1328\n16/16 [==============================] - 0s 631us/step - loss: 1.3245\n16/16 [==============================] - 0s 1ms/step - loss: 1.3825\n16/16 [==============================] - 0s 1ms/step - loss: 1.4056\n16/16 [==============================] - 0s 1ms/step - loss: 1.4125\n16/16 [==============================] - 0s 675us/step - loss: 1.4148\n16/16 [==============================] - 0s 1ms/step - loss: 1.4154\n16/16 [==============================] - 0s 1ms/step - loss: 1.4154\n\nTesting for epoch 24 index 2:\n16/16 [==============================] - 0s 683us/step - loss: 0.4700\n16/16 [==============================] - 0s 1ms/step - loss: 0.8212\n16/16 [==============================] - 0s 608us/step - loss: 1.1067\n16/16 [==============================] - 0s 1ms/step - loss: 1.2919\n16/16 [==============================] - 0s 645us/step - loss: 1.3484\n16/16 [==============================] - 0s 655us/step - loss: 1.3713\n16/16 [==============================] - 0s 1ms/step - loss: 1.3780\n16/16 [==============================] - 0s 707us/step - loss: 1.3802\n16/16 [==============================] - 0s 1ms/step - loss: 1.3807\n16/16 [==============================] - 0s 1ms/step - loss: 1.3807\nEpoch 25 of 60\n\nTesting for epoch 25 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4568\n16/16 [==============================] - 0s 1ms/step - loss: 0.8264\n16/16 [==============================] - 0s 866us/step - loss: 1.1304\n16/16 [==============================] - 0s 737us/step - loss: 1.3292\n16/16 [==============================] - 0s 1ms/step - loss: 1.3891\n16/16 [==============================] - 0s 859us/step - loss: 1.4139\n16/16 [==============================] - 0s 664us/step - loss: 1.4209\n16/16 [==============================] - 0s 1ms/step - loss: 1.4233\n16/16 [==============================] - 0s 632us/step - loss: 1.4239\n16/16 [==============================] - 0s 2ms/step - loss: 1.4239\n\nTesting for epoch 25 index 2:\n16/16 [==============================] - 0s 660us/step - loss: 0.4472\n16/16 [==============================] - 0s 613us/step - loss: 0.8308\n16/16 [==============================] - 0s 633us/step - loss: 1.1472\n16/16 [==============================] - 0s 640us/step - loss: 1.3580\n16/16 [==============================] - 0s 1ms/step - loss: 1.4212\n16/16 [==============================] - 0s 644us/step - loss: 1.4477\n16/16 [==============================] - 0s 621us/step - loss: 1.4553\n16/16 [==============================] - 0s 601us/step - loss: 1.4579\n16/16 [==============================] - 0s 799us/step - loss: 1.4585\n16/16 [==============================] - 0s 663us/step - loss: 1.4585\nEpoch 26 of 60\n\nTesting for epoch 26 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4458\n16/16 [==============================] - 0s 639us/step - loss: 0.8332\n16/16 [==============================] - 0s 622us/step - loss: 1.1594\n16/16 [==============================] - 0s 620us/step - loss: 1.3754\n16/16 [==============================] - 0s 987us/step - loss: 1.4394\n16/16 [==============================] - 0s 652us/step - loss: 1.4660\n16/16 [==============================] - 0s 641us/step - loss: 1.4735\n16/16 [==============================] - 0s 628us/step - loss: 1.4761\n16/16 [==============================] - 0s 1ms/step - loss: 1.4766\n16/16 [==============================] - 0s 597us/step - loss: 1.4766\n\nTesting for epoch 26 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4323\n16/16 [==============================] - 0s 1ms/step - loss: 0.8289\n16/16 [==============================] - 0s 1ms/step - loss: 1.1745\n16/16 [==============================] - 0s 2ms/step - loss: 1.4047\n16/16 [==============================] - 0s 1ms/step - loss: 1.4730\n16/16 [==============================] - 0s 835us/step - loss: 1.5017\n16/16 [==============================] - 0s 684us/step - loss: 1.5100\n16/16 [==============================] - 0s 643us/step - loss: 1.5128\n16/16 [==============================] - 0s 1ms/step - loss: 1.5135\n16/16 [==============================] - 0s 1ms/step - loss: 1.5135\nEpoch 27 of 60\n\nTesting for epoch 27 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4311\n16/16 [==============================] - 0s 693us/step - loss: 0.8327\n16/16 [==============================] - 0s 639us/step - loss: 1.1867\n16/16 [==============================] - 0s 606us/step - loss: 1.4217\n16/16 [==============================] - 0s 816us/step - loss: 1.4904\n16/16 [==============================] - 0s 828us/step - loss: 1.5189\n16/16 [==============================] - 0s 1ms/step - loss: 1.5270\n16/16 [==============================] - 0s 1ms/step - loss: 1.5298\n16/16 [==============================] - 0s 1ms/step - loss: 1.5303\n16/16 [==============================] - 0s 779us/step - loss: 1.5303\n\nTesting for epoch 27 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4224\n16/16 [==============================] - 0s 1ms/step - loss: 0.8208\n16/16 [==============================] - 0s 998us/step - loss: 1.1776\n16/16 [==============================] - 0s 1ms/step - loss: 1.4157\n16/16 [==============================] - 0s 635us/step - loss: 1.4850\n16/16 [==============================] - 0s 1ms/step - loss: 1.5136\n16/16 [==============================] - 0s 1ms/step - loss: 1.5217\n16/16 [==============================] - 0s 640us/step - loss: 1.5245\n16/16 [==============================] - 0s 590us/step - loss: 1.5251\n16/16 [==============================] - 0s 1ms/step - loss: 1.5251\nEpoch 28 of 60\n\nTesting for epoch 28 index 1:\n16/16 [==============================] - 0s 707us/step - loss: 0.4220\n16/16 [==============================] - 0s 790us/step - loss: 0.8241\n16/16 [==============================] - 0s 1ms/step - loss: 1.1871\n16/16 [==============================] - 0s 829us/step - loss: 1.4277\n16/16 [==============================] - 0s 796us/step - loss: 1.4965\n16/16 [==============================] - 0s 1ms/step - loss: 1.5243\n16/16 [==============================] - 0s 1ms/step - loss: 1.5321\n16/16 [==============================] - 0s 1ms/step - loss: 1.5347\n16/16 [==============================] - 0s 611us/step - loss: 1.5352\n16/16 [==============================] - 0s 607us/step - loss: 1.5351\n\nTesting for epoch 28 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4162\n16/16 [==============================] - 0s 765us/step - loss: 0.8240\n16/16 [==============================] - 0s 1ms/step - loss: 1.1967\n16/16 [==============================] - 0s 1ms/step - loss: 1.4447\n16/16 [==============================] - 0s 1ms/step - loss: 1.5154\n16/16 [==============================] - 0s 718us/step - loss: 1.5437\n16/16 [==============================] - 0s 1ms/step - loss: 1.5517\n16/16 [==============================] - 0s 1ms/step - loss: 1.5543\n16/16 [==============================] - 0s 659us/step - loss: 1.5548\n16/16 [==============================] - 0s 2ms/step - loss: 1.5547\nEpoch 29 of 60\n\nTesting for epoch 29 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4156\n16/16 [==============================] - 0s 843us/step - loss: 0.8177\n16/16 [==============================] - 0s 623us/step - loss: 1.1876\n16/16 [==============================] - 0s 1ms/step - loss: 1.4313\n16/16 [==============================] - 0s 1ms/step - loss: 1.4993\n16/16 [==============================] - 0s 1ms/step - loss: 1.5259\n16/16 [==============================] - 0s 723us/step - loss: 1.5332\n16/16 [==============================] - 0s 640us/step - loss: 1.5355\n16/16 [==============================] - 0s 625us/step - loss: 1.5358\n16/16 [==============================] - 0s 634us/step - loss: 1.5357\n\nTesting for epoch 29 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4090\n16/16 [==============================] - 0s 1ms/step - loss: 0.8233\n16/16 [==============================] - 0s 1ms/step - loss: 1.2093\n16/16 [==============================] - 0s 643us/step - loss: 1.4641\n16/16 [==============================] - 0s 627us/step - loss: 1.5348\n16/16 [==============================] - 0s 668us/step - loss: 1.5623\n16/16 [==============================] - 0s 885us/step - loss: 1.5697\n16/16 [==============================] - 0s 887us/step - loss: 1.5721\n16/16 [==============================] - 0s 640us/step - loss: 1.5724\n16/16 [==============================] - 0s 1ms/step - loss: 1.5724\nEpoch 30 of 60\n\nTesting for epoch 30 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4023\n16/16 [==============================] - 0s 847us/step - loss: 0.8249\n16/16 [==============================] - 0s 1ms/step - loss: 1.2211\n16/16 [==============================] - 0s 669us/step - loss: 1.4795\n16/16 [==============================] - 0s 837us/step - loss: 1.5497\n16/16 [==============================] - 0s 1ms/step - loss: 1.5763\n16/16 [==============================] - 0s 792us/step - loss: 1.5833\n16/16 [==============================] - 0s 1ms/step - loss: 1.5854\n16/16 [==============================] - 0s 821us/step - loss: 1.5856\n16/16 [==============================] - 0s 654us/step - loss: 1.5855\n\nTesting for epoch 30 index 2:\n16/16 [==============================] - 0s 815us/step - loss: 0.4039\n16/16 [==============================] - 0s 621us/step - loss: 0.8273\n16/16 [==============================] - 0s 636us/step - loss: 1.2286\n16/16 [==============================] - 0s 1ms/step - loss: 1.4900\n16/16 [==============================] - 0s 1ms/step - loss: 1.5605\n16/16 [==============================] - 0s 1ms/step - loss: 1.5869\n16/16 [==============================] - 0s 1ms/step - loss: 1.5938\n16/16 [==============================] - 0s 2ms/step - loss: 1.5958\n16/16 [==============================] - 0s 1ms/step - loss: 1.5960\n16/16 [==============================] - 0s 721us/step - loss: 1.5958\nEpoch 31 of 60\n\nTesting for epoch 31 index 1:\n16/16 [==============================] - 0s 822us/step - loss: 0.3922\n16/16 [==============================] - 0s 618us/step - loss: 0.8303\n16/16 [==============================] - 0s 979us/step - loss: 1.2484\n16/16 [==============================] - 0s 594us/step - loss: 1.5177\n16/16 [==============================] - 0s 584us/step - loss: 1.5887\n16/16 [==============================] - 0s 886us/step - loss: 1.6148\n16/16 [==============================] - 0s 616us/step - loss: 1.6214\n16/16 [==============================] - 0s 986us/step - loss: 1.6232\n16/16 [==============================] - 0s 634us/step - loss: 1.6234\n16/16 [==============================] - 0s 647us/step - loss: 1.6232\n\nTesting for epoch 31 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3917\n16/16 [==============================] - 0s 638us/step - loss: 0.8412\n16/16 [==============================] - 0s 1ms/step - loss: 1.2745\n16/16 [==============================] - 0s 1ms/step - loss: 1.5525\n16/16 [==============================] - 0s 597us/step - loss: 1.6251\n16/16 [==============================] - 0s 1ms/step - loss: 1.6514\n16/16 [==============================] - 0s 1ms/step - loss: 1.6580\n16/16 [==============================] - 0s 1ms/step - loss: 1.6598\n16/16 [==============================] - 0s 1ms/step - loss: 1.6598\n16/16 [==============================] - 0s 875us/step - loss: 1.6597\nEpoch 32 of 60\n\nTesting for epoch 32 index 1:\n16/16 [==============================] - 0s 643us/step - loss: 0.3861\n16/16 [==============================] - 0s 1ms/step - loss: 0.8410\n16/16 [==============================] - 0s 1ms/step - loss: 1.2819\n16/16 [==============================] - 0s 747us/step - loss: 1.5604\n16/16 [==============================] - 0s 1ms/step - loss: 1.6314\n16/16 [==============================] - 0s 2ms/step - loss: 1.6566\n16/16 [==============================] - 0s 1ms/step - loss: 1.6625\n16/16 [==============================] - 0s 1ms/step - loss: 1.6641\n16/16 [==============================] - 0s 682us/step - loss: 1.6641\n16/16 [==============================] - 0s 868us/step - loss: 1.6639\n\nTesting for epoch 32 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3785\n16/16 [==============================] - 0s 640us/step - loss: 0.8366\n16/16 [==============================] - 0s 620us/step - loss: 1.2831\n16/16 [==============================] - 0s 630us/step - loss: 1.5628\n16/16 [==============================] - 0s 569us/step - loss: 1.6327\n16/16 [==============================] - 0s 1ms/step - loss: 1.6570\n16/16 [==============================] - 0s 1ms/step - loss: 1.6626\n16/16 [==============================] - 0s 671us/step - loss: 1.6639\n16/16 [==============================] - 0s 1ms/step - loss: 1.6638\n16/16 [==============================] - 0s 1ms/step - loss: 1.6636\nEpoch 33 of 60\n\nTesting for epoch 33 index 1:\n16/16 [==============================] - 0s 686us/step - loss: 0.3794\n16/16 [==============================] - 0s 1ms/step - loss: 0.8368\n16/16 [==============================] - 0s 590us/step - loss: 1.2836\n16/16 [==============================] - 0s 1ms/step - loss: 1.5578\n16/16 [==============================] - 0s 1ms/step - loss: 1.6242\n16/16 [==============================] - 0s 1ms/step - loss: 1.6467\n16/16 [==============================] - 0s 1ms/step - loss: 1.6516\n16/16 [==============================] - 0s 880us/step - loss: 1.6526\n16/16 [==============================] - 0s 1ms/step - loss: 1.6524\n16/16 [==============================] - 0s 744us/step - loss: 1.6521\n\nTesting for epoch 33 index 2:\n16/16 [==============================] - 0s 639us/step - loss: 0.3767\n16/16 [==============================] - 0s 675us/step - loss: 0.8386\n16/16 [==============================] - 0s 636us/step - loss: 1.2925\n16/16 [==============================] - 0s 667us/step - loss: 1.5686\n16/16 [==============================] - 0s 570us/step - loss: 1.6342\n16/16 [==============================] - 0s 650us/step - loss: 1.6560\n16/16 [==============================] - 0s 1ms/step - loss: 1.6605\n16/16 [==============================] - 0s 1ms/step - loss: 1.6614\n16/16 [==============================] - 0s 828us/step - loss: 1.6611\n16/16 [==============================] - 0s 754us/step - loss: 1.6608\nEpoch 34 of 60\n\nTesting for epoch 34 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3785\n16/16 [==============================] - 0s 606us/step - loss: 0.8568\n16/16 [==============================] - 0s 1ms/step - loss: 1.3267\n16/16 [==============================] - 0s 1ms/step - loss: 1.6075\n16/16 [==============================] - 0s 632us/step - loss: 1.6723\n16/16 [==============================] - 0s 700us/step - loss: 1.6932\n16/16 [==============================] - 0s 814us/step - loss: 1.6974\n16/16 [==============================] - 0s 1ms/step - loss: 1.6980\n16/16 [==============================] - 0s 2ms/step - loss: 1.6977\n16/16 [==============================] - 0s 1ms/step - loss: 1.6974\n\nTesting for epoch 34 index 2:\n16/16 [==============================] - 0s 611us/step - loss: 0.3640\n16/16 [==============================] - 0s 586us/step - loss: 0.8516\n16/16 [==============================] - 0s 613us/step - loss: 1.3334\n16/16 [==============================] - 0s 631us/step - loss: 1.6188\n16/16 [==============================] - 0s 635us/step - loss: 1.6834\n16/16 [==============================] - 0s 1ms/step - loss: 1.7039\n16/16 [==============================] - 0s 600us/step - loss: 1.7078\n16/16 [==============================] - 0s 784us/step - loss: 1.7083\n16/16 [==============================] - 0s 1ms/step - loss: 1.7080\n16/16 [==============================] - 0s 654us/step - loss: 1.7076\nEpoch 35 of 60\n\nTesting for epoch 35 index 1:\n16/16 [==============================] - 0s 642us/step - loss: 0.3623\n16/16 [==============================] - 0s 870us/step - loss: 0.8627\n16/16 [==============================] - 0s 1ms/step - loss: 1.3550\n16/16 [==============================] - 0s 935us/step - loss: 1.6411\n16/16 [==============================] - 0s 631us/step - loss: 1.7039\n16/16 [==============================] - 0s 1ms/step - loss: 1.7231\n16/16 [==============================] - 0s 659us/step - loss: 1.7264\n16/16 [==============================] - 0s 1ms/step - loss: 1.7267\n16/16 [==============================] - 0s 1ms/step - loss: 1.7262\n16/16 [==============================] - 0s 706us/step - loss: 1.7259\n\nTesting for epoch 35 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3604\n16/16 [==============================] - 0s 635us/step - loss: 0.8668\n16/16 [==============================] - 0s 889us/step - loss: 1.3676\n16/16 [==============================] - 0s 1ms/step - loss: 1.6572\n16/16 [==============================] - 0s 947us/step - loss: 1.7200\n16/16 [==============================] - 0s 2ms/step - loss: 1.7389\n16/16 [==============================] - 0s 772us/step - loss: 1.7421\n16/16 [==============================] - 0s 1ms/step - loss: 1.7424\n16/16 [==============================] - 0s 1ms/step - loss: 1.7419\n16/16 [==============================] - 0s 744us/step - loss: 1.7415\nEpoch 36 of 60\n\nTesting for epoch 36 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3571\n16/16 [==============================] - 0s 680us/step - loss: 0.8755\n16/16 [==============================] - 0s 751us/step - loss: 1.3899\n16/16 [==============================] - 0s 1ms/step - loss: 1.6814\n16/16 [==============================] - 0s 1ms/step - loss: 1.7429\n16/16 [==============================] - 0s 1ms/step - loss: 1.7609\n16/16 [==============================] - 0s 681us/step - loss: 1.7637\n16/16 [==============================] - 0s 677us/step - loss: 1.7638\n16/16 [==============================] - 0s 646us/step - loss: 1.7633\n16/16 [==============================] - 0s 1ms/step - loss: 1.7629\n\nTesting for epoch 36 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3680\n16/16 [==============================] - 0s 623us/step - loss: 0.8560\n16/16 [==============================] - 0s 652us/step - loss: 1.3413\n16/16 [==============================] - 0s 605us/step - loss: 1.6123\n16/16 [==============================] - 0s 622us/step - loss: 1.6680\n16/16 [==============================] - 0s 808us/step - loss: 1.6837\n16/16 [==============================] - 0s 1ms/step - loss: 1.6859\n16/16 [==============================] - 0s 889us/step - loss: 1.6858\n16/16 [==============================] - 0s 633us/step - loss: 1.6852\n16/16 [==============================] - 0s 626us/step - loss: 1.6848\nEpoch 37 of 60\n\nTesting for epoch 37 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3582\n16/16 [==============================] - 0s 660us/step - loss: 0.8667\n16/16 [==============================] - 0s 649us/step - loss: 1.3768\n16/16 [==============================] - 0s 1000us/step - loss: 1.6562\n16/16 [==============================] - 0s 1ms/step - loss: 1.7123\n16/16 [==============================] - 0s 634us/step - loss: 1.7277\n16/16 [==============================] - 0s 685us/step - loss: 1.7297\n16/16 [==============================] - 0s 1ms/step - loss: 1.7295\n16/16 [==============================] - 0s 1ms/step - loss: 1.7288\n16/16 [==============================] - 0s 628us/step - loss: 1.7284\n\nTesting for epoch 37 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3583\n16/16 [==============================] - 0s 1ms/step - loss: 0.8671\n16/16 [==============================] - 0s 1ms/step - loss: 1.3803\n16/16 [==============================] - 0s 1ms/step - loss: 1.6596\n16/16 [==============================] - 0s 1ms/step - loss: 1.7150\n16/16 [==============================] - 0s 692us/step - loss: 1.7298\n16/16 [==============================] - 0s 979us/step - loss: 1.7317\n16/16 [==============================] - 0s 1ms/step - loss: 1.7314\n16/16 [==============================] - 0s 1ms/step - loss: 1.7308\n16/16 [==============================] - 0s 1ms/step - loss: 1.7304\nEpoch 38 of 60\n\nTesting for epoch 38 index 1:\n16/16 [==============================] - 0s 863us/step - loss: 0.3471\n16/16 [==============================] - 0s 661us/step - loss: 0.8780\n16/16 [==============================] - 0s 659us/step - loss: 1.4219\n16/16 [==============================] - 0s 1ms/step - loss: 1.7117\n16/16 [==============================] - 0s 1ms/step - loss: 1.7680\n16/16 [==============================] - 0s 1ms/step - loss: 1.7827\n16/16 [==============================] - 0s 824us/step - loss: 1.7845\n16/16 [==============================] - 0s 1ms/step - loss: 1.7841\n16/16 [==============================] - 0s 1ms/step - loss: 1.7834\n16/16 [==============================] - 0s 2ms/step - loss: 1.7830\n\nTesting for epoch 38 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3466\n16/16 [==============================] - 0s 680us/step - loss: 0.8801\n16/16 [==============================] - 0s 1ms/step - loss: 1.4285\n16/16 [==============================] - 0s 615us/step - loss: 1.7186\n16/16 [==============================] - 0s 1ms/step - loss: 1.7739\n16/16 [==============================] - 0s 1ms/step - loss: 1.7880\n16/16 [==============================] - 0s 1ms/step - loss: 1.7895\n16/16 [==============================] - 0s 619us/step - loss: 1.7890\n16/16 [==============================] - 0s 1ms/step - loss: 1.7882\n16/16 [==============================] - 0s 587us/step - loss: 1.7878\nEpoch 39 of 60\n\nTesting for epoch 39 index 1:\n16/16 [==============================] - 0s 861us/step - loss: 0.3492\n16/16 [==============================] - 0s 879us/step - loss: 0.8719\n16/16 [==============================] - 0s 664us/step - loss: 1.4147\n16/16 [==============================] - 0s 643us/step - loss: 1.6946\n16/16 [==============================] - 0s 1ms/step - loss: 1.7465\n16/16 [==============================] - 0s 621us/step - loss: 1.7591\n16/16 [==============================] - 0s 594us/step - loss: 1.7602\n16/16 [==============================] - 0s 612us/step - loss: 1.7596\n16/16 [==============================] - 0s 594us/step - loss: 1.7588\n16/16 [==============================] - 0s 660us/step - loss: 1.7584\n\nTesting for epoch 39 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3448\n16/16 [==============================] - 0s 891us/step - loss: 0.8840\n16/16 [==============================] - 0s 1ms/step - loss: 1.4475\n16/16 [==============================] - 0s 634us/step - loss: 1.7374\n16/16 [==============================] - 0s 1ms/step - loss: 1.7907\n16/16 [==============================] - 0s 1ms/step - loss: 1.8035\n16/16 [==============================] - 0s 698us/step - loss: 1.8046\n16/16 [==============================] - 0s 660us/step - loss: 1.8040\n16/16 [==============================] - 0s 828us/step - loss: 1.8032\n16/16 [==============================] - 0s 1ms/step - loss: 1.8028\nEpoch 40 of 60\n\nTesting for epoch 40 index 1:\n16/16 [==============================] - 0s 604us/step - loss: 0.3427\n16/16 [==============================] - 0s 629us/step - loss: 0.8792\n16/16 [==============================] - 0s 613us/step - loss: 1.4449\n16/16 [==============================] - 0s 605us/step - loss: 1.7294\n16/16 [==============================] - 0s 1ms/step - loss: 1.7803\n16/16 [==============================] - 0s 1ms/step - loss: 1.7920\n16/16 [==============================] - 0s 670us/step - loss: 1.7928\n16/16 [==============================] - 0s 1ms/step - loss: 1.7921\n16/16 [==============================] - 0s 702us/step - loss: 1.7912\n16/16 [==============================] - 0s 978us/step - loss: 1.7908\n\nTesting for epoch 40 index 2:\n16/16 [==============================] - 0s 884us/step - loss: 0.3356\n16/16 [==============================] - 0s 1ms/step - loss: 0.8885\n16/16 [==============================] - 0s 850us/step - loss: 1.4743\n16/16 [==============================] - 0s 730us/step - loss: 1.7694\n16/16 [==============================] - 0s 1ms/step - loss: 1.8221\n16/16 [==============================] - 0s 944us/step - loss: 1.8343\n16/16 [==============================] - 0s 932us/step - loss: 1.8352\n16/16 [==============================] - 0s 696us/step - loss: 1.8345\n16/16 [==============================] - 0s 1ms/step - loss: 1.8337\n16/16 [==============================] - 0s 1ms/step - loss: 1.8333\nEpoch 41 of 60\n\nTesting for epoch 41 index 1:\n16/16 [==============================] - 0s 612us/step - loss: 0.3393\n16/16 [==============================] - 0s 595us/step - loss: 0.8775\n16/16 [==============================] - 0s 1ms/step - loss: 1.4502\n16/16 [==============================] - 0s 1ms/step - loss: 1.7321\n16/16 [==============================] - 0s 1ms/step - loss: 1.7808\n16/16 [==============================] - 0s 1ms/step - loss: 1.7913\n16/16 [==============================] - 0s 1ms/step - loss: 1.7917\n16/16 [==============================] - 0s 663us/step - loss: 1.7909\n16/16 [==============================] - 0s 688us/step - loss: 1.7900\n16/16 [==============================] - 0s 659us/step - loss: 1.7895\n\nTesting for epoch 41 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3349\n16/16 [==============================] - 0s 913us/step - loss: 0.8782\n16/16 [==============================] - 0s 683us/step - loss: 1.4573\n16/16 [==============================] - 0s 2ms/step - loss: 1.7431\n16/16 [==============================] - 0s 1ms/step - loss: 1.7923\n16/16 [==============================] - 0s 1ms/step - loss: 1.8028\n16/16 [==============================] - 0s 647us/step - loss: 1.8033\n16/16 [==============================] - 0s 633us/step - loss: 1.8024\n16/16 [==============================] - 0s 573us/step - loss: 1.8015\n16/16 [==============================] - 0s 2ms/step - loss: 1.8011\nEpoch 42 of 60\n\nTesting for epoch 42 index 1:\n16/16 [==============================] - 0s 712us/step - loss: 0.3296\n16/16 [==============================] - 0s 757us/step - loss: 0.8910\n16/16 [==============================] - 0s 1ms/step - loss: 1.4933\n16/16 [==============================] - 0s 676us/step - loss: 1.7869\n16/16 [==============================] - 0s 639us/step - loss: 1.8366\n16/16 [==============================] - 0s 622us/step - loss: 1.8470\n16/16 [==============================] - 0s 605us/step - loss: 1.8474\n16/16 [==============================] - 0s 1ms/step - loss: 1.8464\n16/16 [==============================] - 0s 1ms/step - loss: 1.8456\n16/16 [==============================] - 0s 1ms/step - loss: 1.8451\n\nTesting for epoch 42 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3304\n16/16 [==============================] - 0s 1ms/step - loss: 0.8862\n16/16 [==============================] - 0s 704us/step - loss: 1.4818\n16/16 [==============================] - 0s 1ms/step - loss: 1.7733\n16/16 [==============================] - 0s 679us/step - loss: 1.8222\n16/16 [==============================] - 0s 644us/step - loss: 1.8324\n16/16 [==============================] - 0s 1ms/step - loss: 1.8327\n16/16 [==============================] - 0s 1ms/step - loss: 1.8318\n16/16 [==============================] - 0s 653us/step - loss: 1.8309\n16/16 [==============================] - 0s 1ms/step - loss: 1.8304\nEpoch 43 of 60\n\nTesting for epoch 43 index 1:\n16/16 [==============================] - 0s 769us/step - loss: 0.3289\n16/16 [==============================] - 0s 1ms/step - loss: 0.8923\n16/16 [==============================] - 0s 889us/step - loss: 1.4981\n16/16 [==============================] - 0s 676us/step - loss: 1.7912\n16/16 [==============================] - 0s 576us/step - loss: 1.8395\n16/16 [==============================] - 0s 594us/step - loss: 1.8492\n16/16 [==============================] - 0s 614us/step - loss: 1.8493\n16/16 [==============================] - 0s 754us/step - loss: 1.8483\n16/16 [==============================] - 0s 1ms/step - loss: 1.8474\n16/16 [==============================] - 0s 620us/step - loss: 1.8469\n\nTesting for epoch 43 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3236\n16/16 [==============================] - 0s 669us/step - loss: 0.8897\n16/16 [==============================] - 0s 1ms/step - loss: 1.4962\n16/16 [==============================] - 0s 622us/step - loss: 1.7918\n16/16 [==============================] - 0s 636us/step - loss: 1.8402\n16/16 [==============================] - 0s 609us/step - loss: 1.8499\n16/16 [==============================] - 0s 589us/step - loss: 1.8499\n16/16 [==============================] - 0s 1ms/step - loss: 1.8488\n16/16 [==============================] - 0s 1ms/step - loss: 1.8479\n16/16 [==============================] - 0s 1ms/step - loss: 1.8474\nEpoch 44 of 60\n\nTesting for epoch 44 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3240\n16/16 [==============================] - 0s 1ms/step - loss: 0.9065\n16/16 [==============================] - 0s 1ms/step - loss: 1.5340\n16/16 [==============================] - 0s 699us/step - loss: 1.8380\n16/16 [==============================] - 0s 644us/step - loss: 1.8875\n16/16 [==============================] - 0s 603us/step - loss: 1.8973\n16/16 [==============================] - 0s 1ms/step - loss: 1.8974\n16/16 [==============================] - 0s 1ms/step - loss: 1.8964\n16/16 [==============================] - 0s 1ms/step - loss: 1.8954\n16/16 [==============================] - 0s 773us/step - loss: 1.8950\n\nTesting for epoch 44 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3120\n16/16 [==============================] - 0s 980us/step - loss: 0.9140\n16/16 [==============================] - 0s 1ms/step - loss: 1.5655\n16/16 [==============================] - 0s 1ms/step - loss: 1.8840\n16/16 [==============================] - 0s 1ms/step - loss: 1.9360\n16/16 [==============================] - 0s 677us/step - loss: 1.9465\n16/16 [==============================] - 0s 1ms/step - loss: 1.9468\n16/16 [==============================] - 0s 736us/step - loss: 1.9458\n16/16 [==============================] - 0s 645us/step - loss: 1.9449\n16/16 [==============================] - 0s 648us/step - loss: 1.9444\nEpoch 45 of 60\n\nTesting for epoch 45 index 1:\n16/16 [==============================] - 0s 673us/step - loss: 0.3280\n16/16 [==============================] - 0s 651us/step - loss: 0.8989\n16/16 [==============================] - 0s 1ms/step - loss: 1.5171\n16/16 [==============================] - 0s 626us/step - loss: 1.8154\n16/16 [==============================] - 0s 1ms/step - loss: 1.8629\n16/16 [==============================] - 0s 629us/step - loss: 1.8720\n16/16 [==============================] - 0s 1ms/step - loss: 1.8720\n16/16 [==============================] - 0s 1ms/step - loss: 1.8709\n16/16 [==============================] - 0s 1ms/step - loss: 1.8700\n16/16 [==============================] - 0s 632us/step - loss: 1.8696\n\nTesting for epoch 45 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3253\n16/16 [==============================] - 0s 816us/step - loss: 0.8820\n16/16 [==============================] - 0s 782us/step - loss: 1.4859\n16/16 [==============================] - 0s 1ms/step - loss: 1.7760\n16/16 [==============================] - 0s 1ms/step - loss: 1.8211\n16/16 [==============================] - 0s 672us/step - loss: 1.8292\n16/16 [==============================] - 0s 2ms/step - loss: 1.8287\n16/16 [==============================] - 0s 1ms/step - loss: 1.8275\n16/16 [==============================] - 0s 2ms/step - loss: 1.8265\n16/16 [==============================] - 0s 705us/step - loss: 1.8259\nEpoch 46 of 60\n\nTesting for epoch 46 index 1:\n16/16 [==============================] - 0s 629us/step - loss: 0.3018\n16/16 [==============================] - 0s 609us/step - loss: 0.9113\n16/16 [==============================] - 0s 1ms/step - loss: 1.5796\n16/16 [==============================] - 0s 1ms/step - loss: 1.8997\n16/16 [==============================] - 0s 1ms/step - loss: 1.9496\n16/16 [==============================] - 0s 657us/step - loss: 1.9589\n16/16 [==============================] - 0s 591us/step - loss: 1.9587\n16/16 [==============================] - 0s 1ms/step - loss: 1.9575\n16/16 [==============================] - 0s 2ms/step - loss: 1.9565\n16/16 [==============================] - 0s 893us/step - loss: 1.9560\n\nTesting for epoch 46 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3160\n16/16 [==============================] - 0s 649us/step - loss: 0.8929\n16/16 [==============================] - 0s 663us/step - loss: 1.5280\n16/16 [==============================] - 0s 668us/step - loss: 1.8325\n16/16 [==============================] - 0s 1ms/step - loss: 1.8797\n16/16 [==============================] - 0s 698us/step - loss: 1.8884\n16/16 [==============================] - 0s 700us/step - loss: 1.8881\n16/16 [==============================] - 0s 631us/step - loss: 1.8869\n16/16 [==============================] - 0s 1ms/step - loss: 1.8859\n16/16 [==============================] - 0s 649us/step - loss: 1.8854\nEpoch 47 of 60\n\nTesting for epoch 47 index 1:\n16/16 [==============================] - 0s 684us/step - loss: 0.3158\n16/16 [==============================] - 0s 627us/step - loss: 0.9064\n16/16 [==============================] - 0s 1ms/step - loss: 1.5588\n16/16 [==============================] - 0s 632us/step - loss: 1.8678\n16/16 [==============================] - 0s 618us/step - loss: 1.9149\n16/16 [==============================] - 0s 585us/step - loss: 1.9232\n16/16 [==============================] - 0s 1ms/step - loss: 1.9228\n16/16 [==============================] - 0s 1ms/step - loss: 1.9215\n16/16 [==============================] - 0s 1ms/step - loss: 1.9205\n16/16 [==============================] - 0s 694us/step - loss: 1.9200\n\nTesting for epoch 47 index 2:\n16/16 [==============================] - 0s 695us/step - loss: 0.3114\n16/16 [==============================] - 0s 805us/step - loss: 0.8972\n16/16 [==============================] - 0s 899us/step - loss: 1.5487\n16/16 [==============================] - 0s 1ms/step - loss: 1.8577\n16/16 [==============================] - 0s 705us/step - loss: 1.9042\n16/16 [==============================] - 0s 597us/step - loss: 1.9123\n16/16 [==============================] - 0s 630us/step - loss: 1.9118\n16/16 [==============================] - 0s 706us/step - loss: 1.9105\n16/16 [==============================] - 0s 1ms/step - loss: 1.9094\n16/16 [==============================] - 0s 790us/step - loss: 1.9089\nEpoch 48 of 60\n\nTesting for epoch 48 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3079\n16/16 [==============================] - 0s 1ms/step - loss: 0.9138\n16/16 [==============================] - 0s 610us/step - loss: 1.5910\n16/16 [==============================] - 0s 1ms/step - loss: 1.9092\n16/16 [==============================] - 0s 680us/step - loss: 1.9566\n16/16 [==============================] - 0s 609us/step - loss: 1.9647\n16/16 [==============================] - 0s 617us/step - loss: 1.9641\n16/16 [==============================] - 0s 620us/step - loss: 1.9628\n16/16 [==============================] - 0s 621us/step - loss: 1.9617\n16/16 [==============================] - 0s 639us/step - loss: 1.9612\n\nTesting for epoch 48 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2996\n16/16 [==============================] - 0s 668us/step - loss: 0.9044\n16/16 [==============================] - 0s 1ms/step - loss: 1.5879\n16/16 [==============================] - 0s 1ms/step - loss: 1.9109\n16/16 [==============================] - 0s 1ms/step - loss: 1.9594\n16/16 [==============================] - 0s 1ms/step - loss: 1.9680\n16/16 [==============================] - 0s 677us/step - loss: 1.9676\n16/16 [==============================] - 0s 640us/step - loss: 1.9664\n16/16 [==============================] - 0s 630us/step - loss: 1.9654\n16/16 [==============================] - 0s 612us/step - loss: 1.9649\nEpoch 49 of 60\n\nTesting for epoch 49 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3126\n16/16 [==============================] - 0s 1ms/step - loss: 0.8881\n16/16 [==============================] - 0s 985us/step - loss: 1.5372\n16/16 [==============================] - 0s 2ms/step - loss: 1.8393\n16/16 [==============================] - 0s 621us/step - loss: 1.8833\n16/16 [==============================] - 0s 753us/step - loss: 1.8904\n16/16 [==============================] - 0s 1ms/step - loss: 1.8896\n16/16 [==============================] - 0s 602us/step - loss: 1.8883\n16/16 [==============================] - 0s 689us/step - loss: 1.8872\n16/16 [==============================] - 0s 1ms/step - loss: 1.8867\n\nTesting for epoch 49 index 2:\n16/16 [==============================] - 0s 658us/step - loss: 0.3055\n16/16 [==============================] - 0s 641us/step - loss: 0.8975\n16/16 [==============================] - 0s 577us/step - loss: 1.5715\n16/16 [==============================] - 0s 612us/step - loss: 1.8861\n16/16 [==============================] - 0s 605us/step - loss: 1.9320\n16/16 [==============================] - 0s 997us/step - loss: 1.9395\n16/16 [==============================] - 0s 1ms/step - loss: 1.9388\n16/16 [==============================] - 0s 644us/step - loss: 1.9374\n16/16 [==============================] - 0s 952us/step - loss: 1.9364\n16/16 [==============================] - 0s 628us/step - loss: 1.9358\nEpoch 50 of 60\n\nTesting for epoch 50 index 1:\n16/16 [==============================] - 0s 796us/step - loss: 0.2940\n16/16 [==============================] - 0s 649us/step - loss: 0.9002\n16/16 [==============================] - 0s 1ms/step - loss: 1.5930\n16/16 [==============================] - 0s 964us/step - loss: 1.9135\n16/16 [==============================] - 0s 1ms/step - loss: 1.9596\n16/16 [==============================] - 0s 674us/step - loss: 1.9670\n16/16 [==============================] - 0s 652us/step - loss: 1.9662\n16/16 [==============================] - 0s 642us/step - loss: 1.9648\n16/16 [==============================] - 0s 1ms/step - loss: 1.9638\n16/16 [==============================] - 0s 1ms/step - loss: 1.9633\n\nTesting for epoch 50 index 2:\n16/16 [==============================] - 0s 665us/step - loss: 0.3044\n16/16 [==============================] - 0s 665us/step - loss: 0.9019\n16/16 [==============================] - 0s 1ms/step - loss: 1.5888\n16/16 [==============================] - 0s 665us/step - loss: 1.9064\n16/16 [==============================] - 0s 1ms/step - loss: 1.9517\n16/16 [==============================] - 0s 660us/step - loss: 1.9588\n16/16 [==============================] - 0s 691us/step - loss: 1.9579\n16/16 [==============================] - 0s 1ms/step - loss: 1.9565\n16/16 [==============================] - 0s 1ms/step - loss: 1.9554\n16/16 [==============================] - 0s 693us/step - loss: 1.9549\nEpoch 51 of 60\n\nTesting for epoch 51 index 1:\n16/16 [==============================] - 0s 939us/step - loss: 0.3020\n16/16 [==============================] - 0s 739us/step - loss: 0.8987\n16/16 [==============================] - 0s 698us/step - loss: 1.5860\n16/16 [==============================] - 0s 1ms/step - loss: 1.9003\n16/16 [==============================] - 0s 652us/step - loss: 1.9445\n16/16 [==============================] - 0s 807us/step - loss: 1.9511\n16/16 [==============================] - 0s 1ms/step - loss: 1.9502\n16/16 [==============================] - 0s 1ms/step - loss: 1.9488\n16/16 [==============================] - 0s 669us/step - loss: 1.9477\n16/16 [==============================] - 0s 642us/step - loss: 1.9472\n\nTesting for epoch 51 index 2:\n16/16 [==============================] - 0s 919us/step - loss: 0.2888\n16/16 [==============================] - 0s 799us/step - loss: 0.9191\n16/16 [==============================] - 0s 639us/step - loss: 1.6532\n16/16 [==============================] - 0s 1ms/step - loss: 1.9907\n16/16 [==============================] - 0s 617us/step - loss: 2.0383\n16/16 [==============================] - 0s 1000us/step - loss: 2.0458\n16/16 [==============================] - 0s 1ms/step - loss: 2.0449\n16/16 [==============================] - 0s 617us/step - loss: 2.0435\n16/16 [==============================] - 0s 1ms/step - loss: 2.0424\n16/16 [==============================] - 0s 617us/step - loss: 2.0419\nEpoch 52 of 60\n\nTesting for epoch 52 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2896\n16/16 [==============================] - 0s 692us/step - loss: 0.9198\n16/16 [==============================] - 0s 662us/step - loss: 1.6543\n16/16 [==============================] - 0s 644us/step - loss: 1.9879\n16/16 [==============================] - 0s 1ms/step - loss: 2.0342\n16/16 [==============================] - 0s 642us/step - loss: 2.0410\n16/16 [==============================] - 0s 1ms/step - loss: 2.0401\n16/16 [==============================] - 0s 1ms/step - loss: 2.0387\n16/16 [==============================] - 0s 1ms/step - loss: 2.0376\n16/16 [==============================] - 0s 1ms/step - loss: 2.0371\n\nTesting for epoch 52 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2887\n16/16 [==============================] - 0s 655us/step - loss: 0.9164\n16/16 [==============================] - 0s 625us/step - loss: 1.6524\n16/16 [==============================] - 0s 664us/step - loss: 1.9863\n16/16 [==============================] - 0s 1ms/step - loss: 2.0320\n16/16 [==============================] - 0s 1ms/step - loss: 2.0386\n16/16 [==============================] - 0s 695us/step - loss: 2.0375\n16/16 [==============================] - 0s 671us/step - loss: 2.0360\n16/16 [==============================] - 0s 593us/step - loss: 2.0348\n16/16 [==============================] - 0s 592us/step - loss: 2.0343\nEpoch 53 of 60\n\nTesting for epoch 53 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2903\n16/16 [==============================] - 0s 652us/step - loss: 0.9111\n16/16 [==============================] - 0s 634us/step - loss: 1.6388\n16/16 [==============================] - 0s 820us/step - loss: 1.9646\n16/16 [==============================] - 0s 1ms/step - loss: 2.0082\n16/16 [==============================] - 0s 730us/step - loss: 2.0139\n16/16 [==============================] - 0s 592us/step - loss: 2.0126\n16/16 [==============================] - 0s 590us/step - loss: 2.0110\n16/16 [==============================] - 0s 854us/step - loss: 2.0098\n16/16 [==============================] - 0s 639us/step - loss: 2.0093\n\nTesting for epoch 53 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2907\n16/16 [==============================] - 0s 1ms/step - loss: 0.9147\n16/16 [==============================] - 0s 719us/step - loss: 1.6530\n16/16 [==============================] - 0s 658us/step - loss: 1.9850\n16/16 [==============================] - 0s 634us/step - loss: 2.0298\n16/16 [==============================] - 0s 653us/step - loss: 2.0361\n16/16 [==============================] - 0s 2ms/step - loss: 2.0350\n16/16 [==============================] - 0s 1ms/step - loss: 2.0335\n16/16 [==============================] - 0s 625us/step - loss: 2.0324\n16/16 [==============================] - 0s 2ms/step - loss: 2.0319\nEpoch 54 of 60\n\nTesting for epoch 54 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2956\n16/16 [==============================] - 0s 1ms/step - loss: 0.9136\n16/16 [==============================] - 0s 1ms/step - loss: 1.6438\n16/16 [==============================] - 0s 1ms/step - loss: 1.9674\n16/16 [==============================] - 0s 1ms/step - loss: 2.0101\n16/16 [==============================] - 0s 1ms/step - loss: 2.0156\n16/16 [==============================] - 0s 1ms/step - loss: 2.0143\n16/16 [==============================] - 0s 1ms/step - loss: 2.0127\n16/16 [==============================] - 0s 1ms/step - loss: 2.0116\n16/16 [==============================] - 0s 1ms/step - loss: 2.0111\n\nTesting for epoch 54 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2939\n16/16 [==============================] - 0s 1ms/step - loss: 0.9110\n16/16 [==============================] - 0s 1ms/step - loss: 1.6463\n16/16 [==============================] - 0s 1ms/step - loss: 1.9729\n16/16 [==============================] - 0s 2ms/step - loss: 2.0161\n16/16 [==============================] - 0s 2ms/step - loss: 2.0218\n16/16 [==============================] - 0s 1ms/step - loss: 2.0206\n16/16 [==============================] - 0s 1ms/step - loss: 2.0191\n16/16 [==============================] - 0s 1ms/step - loss: 2.0180\n16/16 [==============================] - 0s 679us/step - loss: 2.0175\nEpoch 55 of 60\n\nTesting for epoch 55 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2919\n16/16 [==============================] - 0s 2ms/step - loss: 0.9017\n16/16 [==============================] - 0s 1ms/step - loss: 1.6263\n16/16 [==============================] - 0s 1ms/step - loss: 1.9428\n16/16 [==============================] - 0s 1ms/step - loss: 1.9832\n16/16 [==============================] - 0s 1ms/step - loss: 1.9878\n16/16 [==============================] - 0s 694us/step - loss: 1.9863\n16/16 [==============================] - 0s 642us/step - loss: 1.9846\n16/16 [==============================] - 0s 593us/step - loss: 1.9834\n16/16 [==============================] - 0s 607us/step - loss: 1.9829\n\nTesting for epoch 55 index 2:\n16/16 [==============================] - 0s 627us/step - loss: 0.2932\n16/16 [==============================] - 0s 619us/step - loss: 0.8949\n16/16 [==============================] - 0s 621us/step - loss: 1.6136\n16/16 [==============================] - 0s 623us/step - loss: 1.9271\n16/16 [==============================] - 0s 610us/step - loss: 1.9666\n16/16 [==============================] - 0s 1ms/step - loss: 1.9708\n16/16 [==============================] - 0s 1ms/step - loss: 1.9692\n16/16 [==============================] - 0s 629us/step - loss: 1.9675\n16/16 [==============================] - 0s 1ms/step - loss: 1.9663\n16/16 [==============================] - 0s 580us/step - loss: 1.9658\nEpoch 56 of 60\n\nTesting for epoch 56 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2877\n16/16 [==============================] - 0s 1ms/step - loss: 0.9139\n16/16 [==============================] - 0s 1ms/step - loss: 1.6633\n16/16 [==============================] - 0s 1ms/step - loss: 1.9868\n16/16 [==============================] - 0s 1ms/step - loss: 2.0271\n16/16 [==============================] - 0s 654us/step - loss: 2.0314\n16/16 [==============================] - 0s 683us/step - loss: 2.0298\n16/16 [==============================] - 0s 615us/step - loss: 2.0281\n16/16 [==============================] - 0s 1ms/step - loss: 2.0269\n16/16 [==============================] - 0s 663us/step - loss: 2.0263\n\nTesting for epoch 56 index 2:\n16/16 [==============================] - 0s 742us/step - loss: 0.2883\n16/16 [==============================] - 0s 641us/step - loss: 0.8940\n16/16 [==============================] - 0s 1ms/step - loss: 1.6224\n16/16 [==============================] - 0s 634us/step - loss: 1.9361\n16/16 [==============================] - 0s 617us/step - loss: 1.9749\n16/16 [==============================] - 0s 1ms/step - loss: 1.9788\n16/16 [==============================] - 0s 628us/step - loss: 1.9771\n16/16 [==============================] - 0s 628us/step - loss: 1.9755\n16/16 [==============================] - 0s 1ms/step - loss: 1.9743\n16/16 [==============================] - 0s 802us/step - loss: 1.9737\nEpoch 57 of 60\n\nTesting for epoch 57 index 1:\n16/16 [==============================] - 0s 624us/step - loss: 0.2843\n16/16 [==============================] - 0s 1ms/step - loss: 0.9090\n16/16 [==============================] - 0s 1ms/step - loss: 1.6600\n16/16 [==============================] - 0s 712us/step - loss: 1.9791\n16/16 [==============================] - 0s 648us/step - loss: 2.0175\n16/16 [==============================] - 0s 647us/step - loss: 2.0210\n16/16 [==============================] - 0s 613us/step - loss: 2.0192\n16/16 [==============================] - 0s 629us/step - loss: 2.0174\n16/16 [==============================] - 0s 1ms/step - loss: 2.0162\n16/16 [==============================] - 0s 859us/step - loss: 2.0156\n\nTesting for epoch 57 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2836\n16/16 [==============================] - 0s 1ms/step - loss: 0.9146\n16/16 [==============================] - 0s 2ms/step - loss: 1.6782\n16/16 [==============================] - 0s 1ms/step - loss: 2.0028\n16/16 [==============================] - 0s 1ms/step - loss: 2.0419\n16/16 [==============================] - 0s 1ms/step - loss: 2.0457\n16/16 [==============================] - 0s 1ms/step - loss: 2.0439\n16/16 [==============================] - 0s 1ms/step - loss: 2.0422\n16/16 [==============================] - 0s 676us/step - loss: 2.0410\n16/16 [==============================] - 0s 2ms/step - loss: 2.0405\nEpoch 58 of 60\n\nTesting for epoch 58 index 1:\n16/16 [==============================] - 0s 684us/step - loss: 0.2791\n16/16 [==============================] - 0s 1ms/step - loss: 0.9413\n16/16 [==============================] - 0s 1ms/step - loss: 1.7429\n16/16 [==============================] - 0s 1ms/step - loss: 2.0793\n16/16 [==============================] - 0s 748us/step - loss: 2.1192\n16/16 [==============================] - 0s 1ms/step - loss: 2.1229\n16/16 [==============================] - 0s 657us/step - loss: 2.1211\n16/16 [==============================] - 0s 826us/step - loss: 2.1193\n16/16 [==============================] - 0s 654us/step - loss: 2.1181\n16/16 [==============================] - 0s 662us/step - loss: 2.1176\n\nTesting for epoch 58 index 2:\n16/16 [==============================] - 0s 623us/step - loss: 0.2745\n16/16 [==============================] - 0s 636us/step - loss: 0.9399\n16/16 [==============================] - 0s 604us/step - loss: 1.7502\n16/16 [==============================] - 0s 1ms/step - loss: 2.0900\n16/16 [==============================] - 0s 1ms/step - loss: 2.1304\n16/16 [==============================] - 0s 1ms/step - loss: 2.1343\n16/16 [==============================] - 0s 2ms/step - loss: 2.1327\n16/16 [==============================] - 0s 1ms/step - loss: 2.1310\n16/16 [==============================] - 0s 1ms/step - loss: 2.1299\n16/16 [==============================] - 0s 1ms/step - loss: 2.1293\nEpoch 59 of 60\n\nTesting for epoch 59 index 1:\n16/16 [==============================] - 0s 850us/step - loss: 0.2705\n16/16 [==============================] - 0s 624us/step - loss: 0.9431\n16/16 [==============================] - 0s 626us/step - loss: 1.7595\n16/16 [==============================] - 0s 946us/step - loss: 2.0960\n16/16 [==============================] - 0s 631us/step - loss: 2.1341\n16/16 [==============================] - 0s 653us/step - loss: 2.1372\n16/16 [==============================] - 0s 1ms/step - loss: 2.1353\n16/16 [==============================] - 0s 745us/step - loss: 2.1335\n16/16 [==============================] - 0s 1ms/step - loss: 2.1322\n16/16 [==============================] - 0s 1ms/step - loss: 2.1317\n\nTesting for epoch 59 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 0.2706\n16/16 [==============================] - 0s 1ms/step - loss: 0.9454\n16/16 [==============================] - 0s 1ms/step - loss: 1.7683\n16/16 [==============================] - 0s 1ms/step - loss: 2.1066\n16/16 [==============================] - 0s 1ms/step - loss: 2.1448\n16/16 [==============================] - 0s 1ms/step - loss: 2.1478\n16/16 [==============================] - 0s 1ms/step - loss: 2.1459\n16/16 [==============================] - 0s 1ms/step - loss: 2.1441\n16/16 [==============================] - 0s 676us/step - loss: 2.1429\n16/16 [==============================] - 0s 1ms/step - loss: 2.1423\nEpoch 60 of 60\n\nTesting for epoch 60 index 1:\n16/16 [==============================] - 0s 648us/step - loss: 0.2763\n16/16 [==============================] - 0s 1ms/step - loss: 0.9427\n16/16 [==============================] - 0s 690us/step - loss: 1.7522\n16/16 [==============================] - 0s 1ms/step - loss: 2.0797\n16/16 [==============================] - 0s 717us/step - loss: 2.1151\n16/16 [==============================] - 0s 650us/step - loss: 2.1173\n16/16 [==============================] - 0s 1ms/step - loss: 2.1151\n16/16 [==============================] - 0s 1ms/step - loss: 2.1133\n16/16 [==============================] - 0s 674us/step - loss: 2.1120\n16/16 [==============================] - 0s 1ms/step - loss: 2.1114\n\nTesting for epoch 60 index 2:\n16/16 [==============================] - 0s 660us/step - loss: 0.2810\n16/16 [==============================] - 0s 668us/step - loss: 0.9270\n16/16 [==============================] - 0s 615us/step - loss: 1.7161\n16/16 [==============================] - 0s 637us/step - loss: 2.0351\n16/16 [==============================] - 0s 1ms/step - loss: 2.0696\n16/16 [==============================] - 0s 694us/step - loss: 2.0717\n16/16 [==============================] - 0s 1ms/step - loss: 2.0696\n16/16 [==============================] - 0s 641us/step - loss: 2.0678\n16/16 [==============================] - 0s 599us/step - loss: 2.0666\n16/16 [==============================] - 0s 624us/step - loss: 2.0660\n\n\n\noutlier_MO_GAAL_one = list(clf.labels_)\n\n\noutlier_MO_GAAL_one = list(map(lambda x: 1 if x==0  else -1,outlier_MO_GAAL_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_MO_GAAL_one,tab_linear)\n\n\n_conf.conf(\"MO-GAAL (Liu et al., 2019)\")\n\n\n\n\nAccuracy: 0.879\nPrecision: 0.955\nRecall: 0.916\nF1 Score: 0.935\n\n\n\nthirteen = twelve.append(_conf.tab)\n\n\n\nLSCP\n\ndetectors = [KNN(), LOF(), OCSVM()]\nclf = LSCP(detectors)\nclf.fit(_df[['x', 'y']])\n_df['LSCP_clf'] = clf.labels_\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/pyod/models/lscp.py:382: UserWarning: The number of histogram bins is greater than the number of classifiers, reducing n_bins to n_clf.\n  warnings.warn(\n\n\n\noutlier_LSCP_one = list(clf.labels_)\n\n\noutlier_LSCP_one = list(map(lambda x: 1 if x==0  else -1,outlier_LSCP_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_LSCP_one,tab_linear)\n\n\n_conf.conf(\"LSCP (Zhao et al., 2019)\")\n\n\n\n\nAccuracy: 0.908\nPrecision: 0.977\nRecall: 0.925\nF1 Score: 0.950\n\n\n\nfourteen_linear = thirteen.append(_conf.tab)"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#linear-result",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#linear-result",
    "title": "Class code for Comparison Study",
    "section": "Linear Result",
    "text": "Linear Result\n\nround(fourteen_linear,3)\n\n\n\n\n\n  \n    \n      \n      Accuracy\n      Precision\n      Recall\n      F1\n    \n  \n  \n    \n      GODE\n      0.959\n      0.960\n      0.999\n      0.979\n    \n    \n      LOF (Breunig et al., 2000)\n      0.890\n      0.973\n      0.909\n      0.940\n    \n    \n      kNN (Ramaswamy et al., 2000)\n      0.912\n      0.979\n      0.927\n      0.952\n    \n    \n      CBLOF (He et al., 2003)\n      0.920\n      0.958\n      0.958\n      0.958\n    \n    \n      OCSVM (Sch ̈olkopf et al., 2001)\n      0.909\n      0.978\n      0.925\n      0.951\n    \n    \n      MCD (Hardin and Rocke, 2004)\n      0.918\n      0.982\n      0.931\n      0.956\n    \n    \n      Feature Bagging (Lazarevic and Kumar, 2005)\n      0.918\n      0.982\n      0.931\n      0.956\n    \n    \n      ABOD (Kriegel et al., 2008)\n      0.946\n      0.972\n      0.972\n      0.972\n    \n    \n      Isolation Forest (Liu et al., 2008)\n      0.800\n      0.984\n      0.802\n      0.884\n    \n    \n      HBOS (Goldstein and Dengel, 2012)\n      0.889\n      0.960\n      0.921\n      0.940\n    \n    \n      SOS (Janssens et al., 2012)\n      0.889\n      0.960\n      0.921\n      0.940\n    \n    \n      SO-GAAL (Liu et al., 2019)\n      0.868\n      0.954\n      0.904\n      0.929\n    \n    \n      MO-GAAL (Liu et al., 2019)\n      0.879\n      0.955\n      0.916\n      0.935\n    \n    \n      LSCP (Zhao et al., 2019)\n      0.908\n      0.977\n      0.925\n      0.950"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#orbit-ebayesthresh",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#orbit-ebayesthresh",
    "title": "Class code for Comparison Study",
    "section": "Orbit EbayesThresh",
    "text": "Orbit EbayesThresh\n\n%load_ext rpy2.ipython\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython\n\n\n\n%%R\nlibrary(EbayesThresh)\nset.seed(1)\nepsilon = rnorm(1000)\nsignal = sample(c(runif(25,-7,-5), runif(25,5,7), rep(0,950)))\nindex_of_trueoutlier = which(signal!=0)\nindex_of_trueoutlier\nx=signal+epsilon\nplot(1:1000,x)\npoints(index_of_trueoutlier,x[index_of_trueoutlier],col=2,cex=4)\n\n#plot(x,type='l')\n#mu <- EbayesThresh::ebayesthresh(x,sdev=2)\n#lines(mu,col=2,lty=2,lwd=2)\n\n\n\n\n\n%R -o x\n%R -o index_of_trueoutlier\n%R -o signal\n\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\n\n\nxhat = np.array(ebayesthresh(FloatVector(x)))\n\n\n# plt.plot(x)\n# plt.plot(xhat)\n\n\noutlier_true_index = index_of_trueoutlier\n\n\noutlier_true_value = x[index_of_trueoutlier]\n\npackage와 비교를 위해 outlier는 -1, inlier는 1로 표시\n\noutlier_true_one = signal.copy()\n\n\noutlier_true_one = list(map(lambda x: -1 if x!=0 else 1,outlier_true_one))"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#orbit",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#orbit",
    "title": "Class code for Comparison Study",
    "section": "Orbit",
    "text": "Orbit\n\nnp.random.seed(777)\npi=np.pi\nn=1000\nang=np.linspace(-pi,pi-2*pi/n,n)\nr=5+np.cos(np.linspace(0,12*pi,n))\nvx=r*np.cos(ang)\nvy=r*np.sin(ang)\nf1=10*np.sin(np.linspace(0,6*pi,n))\nf = f1 + x\n\n\n_df = pd.DataFrame({'x' : vx, 'y' : vy, 'f' : f})\n\n\nX = np.array(_df)\n\n\nGODE\n\n_Orbit = Orbit(_df)\n\n\n_Orbit.get_distance()\n\n100%|██████████| 1000/1000 [00:02<00:00, 497.54it/s]\n\n\n\n_Orbit.get_weightmatrix(theta=(_Orbit.D[_Orbit.D>0].mean()),kappa=2500) \n\n\n_Orbit.fit(sd=15,ref=20)\n\n\noutlier_simul_one = (_Orbit.df['Residual']**2).tolist()\n\n\noutlier_simul_one = list(map(lambda x: -1 if x > 20 else 1,outlier_simul_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_simul_one,tab_orbit)\n\n\n_conf.conf(\"GODE\")\n\n\n\n\nAccuracy: 0.997\nPrecision: 0.997\nRecall: 1.000\nF1 Score: 0.998\n\n\n\none = _conf.tab\n\n\n\nLOF\n\nclf = LocalOutlierFactor(n_neighbors=2)\n\n\n_conf = Conf_matrx(outlier_true_one,clf.fit_predict(X),tab_orbit)\n\n\n_conf.conf(\"LOF (Breunig et al., 2000)\")\n\n\n\n\nAccuracy: 0.886\nPrecision: 0.987\nRecall: 0.892\nF1 Score: 0.937\n\n\n\ntwo = one.append(_conf.tab)\n\n\n\nKNN\n\nclf = KNN()\nclf.fit(_df[['x', 'y','f']])\n_df['knn_clf'] = clf.labels_\n\n\noutlier_KNN_one = list(clf.labels_)\n\n\noutlier_KNN_one = list(map(lambda x: 1 if x==0  else -1,outlier_KNN_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_KNN_one,tab_orbit)\n\n\n_conf.conf(\"kNN (Ramaswamy et al., 2000)\")\n\n\n\n\nAccuracy: 0.948\nPrecision: 0.999\nRecall: 0.946\nF1 Score: 0.972\n\n\n\nthree = two.append(_conf.tab)\n\n\n\nCBLOF\n\nclf = CBLOF(contamination=0.05,check_estimator=False, random_state=77)\nclf.fit(_df[['x', 'y','f']])\n_df['CBLOF_Clf'] = clf.labels_\n\n\noutlier_CBLOF_one = list(clf.labels_)\n\n\noutlier_CBLOF_one = list(map(lambda x: 1 if x==0  else -1,outlier_CBLOF_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_CBLOF_one,tab_orbit)\n\n\n_conf.conf(\"CBLOF (He et al., 2003)\")\n\n\n\n\nAccuracy: 0.918\nPrecision: 0.957\nRecall: 0.957\nF1 Score: 0.957\n\n\n\nfour = three.append(_conf.tab)\n\n\n\nOCSVM\n\nclf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n\n\nclf.fit(X)\n\nOneClassSVM(gamma=0.1, nu=0.1)\n\n\n\noutlier_OSVM_one = list(clf.predict(X))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_OSVM_one,tab_orbit)\n\n\n_conf.conf(\"OCSVM (Sch ̈olkopf et al., 2001)\")\n\n\n\n\nAccuracy: 0.923\nPrecision: 0.988\nRecall: 0.931\nF1 Score: 0.958\n\n\n\nfive = four.append(_conf.tab)\n\n\n\nMCD\n\nclf = MCD()\nclf.fit(_df[['x', 'y','f']])\n_df['MCD_clf'] = clf.labels_\n\n\noutlier_MCD_one = list(clf.labels_)\n\n\noutlier_MCD_one = list(map(lambda x: 1 if x==0  else -1,outlier_MCD_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_MCD_one,tab_orbit)\n\n\n_conf.conf(\"MCD (Hardin and Rocke, 2004)\")\n\n\n\n\nAccuracy: 0.866\nPrecision: 0.953\nRecall: 0.903\nF1 Score: 0.928\n\n\n\nsix = five.append(_conf.tab)\n\n\n\nFeature Bagging\n\nclf = FeatureBagging()\nclf.fit(_df[['x', 'y','f']])\n_df['FeatureBagging_clf'] = clf.labels_\n\n\noutlier_FeatureBagging_one = list(clf.labels_)\n\n\noutlier_FeatureBagging_one = list(map(lambda x: 1 if x==0  else -1,outlier_FeatureBagging_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_FeatureBagging_one,tab_orbit)\n\n\n_conf.conf(\"Feature Bagging (Lazarevic and Kumar, 2005)\")\n\n\n\n\nAccuracy: 0.912\nPrecision: 0.979\nRecall: 0.927\nF1 Score: 0.952\n\n\n\nseven = six.append(_conf.tab)\n\n\n\nABOD\n\nclf = ABOD(contamination=0.05)\nclf.fit(_df[['x', 'y','f']])\n_df['ABOD_Clf'] = clf.labels_\n\n\noutlier_ABOD_one = list(clf.labels_)\n\n\noutlier_ABOD_one = list(map(lambda x: 1 if x==0  else -1,outlier_ABOD_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_ABOD_one,tab_orbit)\n\n\n_conf.conf(\"ABOD (Kriegel et al., 2008)\")\n\n\n\n\nAccuracy: 0.988\nPrecision: 0.994\nRecall: 0.994\nF1 Score: 0.994\n\n\n\neight = seven.append(_conf.tab)\n\n\n\nIForest\n\nod = IForest(\n    threshold=0.,\n    n_estimators=100\n)\n\n\nod.fit(_df[['x', 'y','f']])\n\n\npreds = od.predict(\n    _df[['x', 'y','f']],\n    return_instance_score=True\n)\n\n\n_df['IF_alibi'] = preds['data']['is_outlier']\n\n\noutlier_alibi_one = _df['IF_alibi']\n\n\noutlier_alibi_one = list(map(lambda x: 1 if x==0  else -1,outlier_alibi_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_alibi_one,tab_orbit)\n\n\n_conf.conf(\"Isolation Forest (Liu et al., 2008)\")\n\n\n\n\nAccuracy: 0.378\nPrecision: 0.997\nRecall: 0.346\nF1 Score: 0.514\n\n\n\nnine = eight.append(_conf.tab)\n\n\n\nHBOS\n\nclf = HBOS()\nclf.fit(_df[['x', 'y','f']])\n_df['HBOS_clf'] = clf.labels_\n\n\noutlier_HBOS_one = list(clf.labels_)\n\n\noutlier_HBOS_one = list(map(lambda x: 1 if x==0  else -1,outlier_HBOS_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_HBOS_one,tab_orbit)\n\n\n_conf.conf(\"HBOS (Goldstein and Dengel, 2012)\")\n\n\n\n\nAccuracy: 0.881\nPrecision: 0.961\nRecall: 0.912\nF1 Score: 0.936\n\n\n\nten = nine.append(_conf.tab)\n\n\n\nSOS\n\noutlier_SOS_one = list(clf.labels_)\n\n\noutlier_SOS_one = list(map(lambda x: 1 if x==0  else -1,outlier_SOS_one))\n\n\nclf = SOS()\nclf.fit(_df[['x', 'y','f']])\n_df['SOS_clf'] = clf.labels_\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_SOS_one,tab_orbit)\n\n\n_conf.conf(\"SOS (Janssens et al., 2012)\")\n\n\n\n\nAccuracy: 0.881\nPrecision: 0.961\nRecall: 0.912\nF1 Score: 0.936\n\n\n\neleven = ten.append(_conf.tab)\n\n\n\nSO_GAAL\n\nclf = SO_GAAL()\nclf.fit(_df[['x', 'y','f']])\n_df['SO_GAAL_clf'] = clf.labels_\n\nEpoch 1 of 60\n\nTesting for epoch 1 index 1:\n\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n\n\n\nTesting for epoch 1 index 2:\nEpoch 2 of 60\n\nTesting for epoch 2 index 1:\n\nTesting for epoch 2 index 2:\nEpoch 3 of 60\n\nTesting for epoch 3 index 1:\n\nTesting for epoch 3 index 2:\nEpoch 4 of 60\n\nTesting for epoch 4 index 1:\n\nTesting for epoch 4 index 2:\nEpoch 5 of 60\n\nTesting for epoch 5 index 1:\n\nTesting for epoch 5 index 2:\nEpoch 6 of 60\n\nTesting for epoch 6 index 1:\n\nTesting for epoch 6 index 2:\nEpoch 7 of 60\n\nTesting for epoch 7 index 1:\n\nTesting for epoch 7 index 2:\nEpoch 8 of 60\n\nTesting for epoch 8 index 1:\n\nTesting for epoch 8 index 2:\nEpoch 9 of 60\n\nTesting for epoch 9 index 1:\n\nTesting for epoch 9 index 2:\nEpoch 10 of 60\n\nTesting for epoch 10 index 1:\n\nTesting for epoch 10 index 2:\nEpoch 11 of 60\n\nTesting for epoch 11 index 1:\n\nTesting for epoch 11 index 2:\nEpoch 12 of 60\n\nTesting for epoch 12 index 1:\n\nTesting for epoch 12 index 2:\nEpoch 13 of 60\n\nTesting for epoch 13 index 1:\n\nTesting for epoch 13 index 2:\nEpoch 14 of 60\n\nTesting for epoch 14 index 1:\n\nTesting for epoch 14 index 2:\nEpoch 15 of 60\n\nTesting for epoch 15 index 1:\n\nTesting for epoch 15 index 2:\nEpoch 16 of 60\n\nTesting for epoch 16 index 1:\n\nTesting for epoch 16 index 2:\nEpoch 17 of 60\n\nTesting for epoch 17 index 1:\n\nTesting for epoch 17 index 2:\nEpoch 18 of 60\n\nTesting for epoch 18 index 1:\n\nTesting for epoch 18 index 2:\nEpoch 19 of 60\n\nTesting for epoch 19 index 1:\n\nTesting for epoch 19 index 2:\nEpoch 20 of 60\n\nTesting for epoch 20 index 1:\n\nTesting for epoch 20 index 2:\nEpoch 21 of 60\n\nTesting for epoch 21 index 1:\n\nTesting for epoch 21 index 2:\nEpoch 22 of 60\n\nTesting for epoch 22 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2135\n\nTesting for epoch 22 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2178\nEpoch 23 of 60\n\nTesting for epoch 23 index 1:\n16/16 [==============================] - 0s 891us/step - loss: 1.2227\n\nTesting for epoch 23 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2138\nEpoch 24 of 60\n\nTesting for epoch 24 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2244\n\nTesting for epoch 24 index 2:\n16/16 [==============================] - 0s 917us/step - loss: 1.2068\nEpoch 25 of 60\n\nTesting for epoch 25 index 1:\n16/16 [==============================] - 0s 624us/step - loss: 1.2319\n\nTesting for epoch 25 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2260\nEpoch 26 of 60\n\nTesting for epoch 26 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2357\n\nTesting for epoch 26 index 2:\n16/16 [==============================] - 0s 985us/step - loss: 1.2294\nEpoch 27 of 60\n\nTesting for epoch 27 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2426\n\nTesting for epoch 27 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2583\nEpoch 28 of 60\n\nTesting for epoch 28 index 1:\n16/16 [==============================] - 0s 815us/step - loss: 1.2599\n\nTesting for epoch 28 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2752\nEpoch 29 of 60\n\nTesting for epoch 29 index 1:\n16/16 [==============================] - 0s 639us/step - loss: 1.3019\n\nTesting for epoch 29 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2905\nEpoch 30 of 60\n\nTesting for epoch 30 index 1:\n16/16 [==============================] - 0s 635us/step - loss: 1.3191\n\nTesting for epoch 30 index 2:\n16/16 [==============================] - 0s 781us/step - loss: 1.3229\nEpoch 31 of 60\n\nTesting for epoch 31 index 1:\n16/16 [==============================] - 0s 604us/step - loss: 1.3371\n\nTesting for epoch 31 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.3418\nEpoch 32 of 60\n\nTesting for epoch 32 index 1:\n16/16 [==============================] - 0s 803us/step - loss: 1.3589\n\nTesting for epoch 32 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.3819\nEpoch 33 of 60\n\nTesting for epoch 33 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.3966\n\nTesting for epoch 33 index 2:\n16/16 [==============================] - 0s 956us/step - loss: 1.3947\nEpoch 34 of 60\n\nTesting for epoch 34 index 1:\n16/16 [==============================] - 0s 609us/step - loss: 1.4201\n\nTesting for epoch 34 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4322\nEpoch 35 of 60\n\nTesting for epoch 35 index 1:\n16/16 [==============================] - 0s 617us/step - loss: 1.4333\n\nTesting for epoch 35 index 2:\n16/16 [==============================] - 0s 626us/step - loss: 1.4465\nEpoch 36 of 60\n\nTesting for epoch 36 index 1:\n16/16 [==============================] - 0s 640us/step - loss: 1.4560\n\nTesting for epoch 36 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4823\nEpoch 37 of 60\n\nTesting for epoch 37 index 1:\n16/16 [==============================] - 0s 932us/step - loss: 1.4888\n\nTesting for epoch 37 index 2:\n16/16 [==============================] - 0s 782us/step - loss: 1.5030\nEpoch 38 of 60\n\nTesting for epoch 38 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5161\n\nTesting for epoch 38 index 2:\n16/16 [==============================] - 0s 599us/step - loss: 1.5196\nEpoch 39 of 60\n\nTesting for epoch 39 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5412\n\nTesting for epoch 39 index 2:\n16/16 [==============================] - 0s 877us/step - loss: 1.5368\nEpoch 40 of 60\n\nTesting for epoch 40 index 1:\n16/16 [==============================] - 0s 579us/step - loss: 1.5523\n\nTesting for epoch 40 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5574\nEpoch 41 of 60\n\nTesting for epoch 41 index 1:\n16/16 [==============================] - 0s 981us/step - loss: 1.5684\n\nTesting for epoch 41 index 2:\n16/16 [==============================] - 0s 643us/step - loss: 1.5748\nEpoch 42 of 60\n\nTesting for epoch 42 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5725\n\nTesting for epoch 42 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5772\nEpoch 43 of 60\n\nTesting for epoch 43 index 1:\n16/16 [==============================] - 0s 648us/step - loss: 1.5934\n\nTesting for epoch 43 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6053\nEpoch 44 of 60\n\nTesting for epoch 44 index 1:\n16/16 [==============================] - 0s 907us/step - loss: 1.6078\n\nTesting for epoch 44 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6025\nEpoch 45 of 60\n\nTesting for epoch 45 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6277\n\nTesting for epoch 45 index 2:\n16/16 [==============================] - 0s 615us/step - loss: 1.6348\nEpoch 46 of 60\n\nTesting for epoch 46 index 1:\n16/16 [==============================] - 0s 815us/step - loss: 1.6427\n\nTesting for epoch 46 index 2:\n16/16 [==============================] - 0s 606us/step - loss: 1.6405\nEpoch 47 of 60\n\nTesting for epoch 47 index 1:\n16/16 [==============================] - 0s 619us/step - loss: 1.6498\n\nTesting for epoch 47 index 2:\n16/16 [==============================] - 0s 812us/step - loss: 1.6603\nEpoch 48 of 60\n\nTesting for epoch 48 index 1:\n16/16 [==============================] - 0s 614us/step - loss: 1.6775\n\nTesting for epoch 48 index 2:\n16/16 [==============================] - 0s 650us/step - loss: 1.6890\nEpoch 49 of 60\n\nTesting for epoch 49 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6979\n\nTesting for epoch 49 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6971\nEpoch 50 of 60\n\nTesting for epoch 50 index 1:\n16/16 [==============================] - 0s 624us/step - loss: 1.7076\n\nTesting for epoch 50 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.7120\nEpoch 51 of 60\n\nTesting for epoch 51 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.7271\n\nTesting for epoch 51 index 2:\n16/16 [==============================] - 0s 863us/step - loss: 1.7406\nEpoch 52 of 60\n\nTesting for epoch 52 index 1:\n16/16 [==============================] - 0s 623us/step - loss: 1.7534\n\nTesting for epoch 52 index 2:\n16/16 [==============================] - 0s 677us/step - loss: 1.7597\nEpoch 53 of 60\n\nTesting for epoch 53 index 1:\n16/16 [==============================] - 0s 741us/step - loss: 1.7555\n\nTesting for epoch 53 index 2:\n16/16 [==============================] - 0s 678us/step - loss: 1.7716\nEpoch 54 of 60\n\nTesting for epoch 54 index 1:\n16/16 [==============================] - 0s 827us/step - loss: 1.7776\n\nTesting for epoch 54 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.7776\nEpoch 55 of 60\n\nTesting for epoch 55 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8009\n\nTesting for epoch 55 index 2:\n16/16 [==============================] - 0s 626us/step - loss: 1.8053\nEpoch 56 of 60\n\nTesting for epoch 56 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8205\n\nTesting for epoch 56 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8218\nEpoch 57 of 60\n\nTesting for epoch 57 index 1:\n16/16 [==============================] - 0s 666us/step - loss: 1.8259\n\nTesting for epoch 57 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8307\nEpoch 58 of 60\n\nTesting for epoch 58 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8576\n\nTesting for epoch 58 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8445\nEpoch 59 of 60\n\nTesting for epoch 59 index 1:\n16/16 [==============================] - 0s 742us/step - loss: 1.8687\n\nTesting for epoch 59 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8710\nEpoch 60 of 60\n\nTesting for epoch 60 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8824\n\nTesting for epoch 60 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8924\n\n\n\noutlier_SO_GAAL_one = list(clf.labels_)\n\n\noutlier_SO_GAAL_one = list(map(lambda x: 1 if x==0  else -1,outlier_SO_GAAL_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_SO_GAAL_one,tab_orbit)\n\n\n_conf.conf(\"SO-GAAL (Liu et al., 2019)\")\n\n\n\n\nAccuracy: 0.876\nPrecision: 0.959\nRecall: 0.908\nF1 Score: 0.933\n\n\n\ntwelve = eleven.append(_conf.tab)\n\n\n\nMO_GAAL\n\nclf = MO_GAAL()\nclf.fit(_df[['x', 'y','f']])\n_df['MO_GAAL_clf'] = clf.labels_\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n\n\nEpoch 1 of 60\n\nTesting for epoch 1 index 1:\n\nTesting for epoch 1 index 2:\nEpoch 2 of 60\n\nTesting for epoch 2 index 1:\n\nTesting for epoch 2 index 2:\nEpoch 3 of 60\n\nTesting for epoch 3 index 1:\n\nTesting for epoch 3 index 2:\nEpoch 4 of 60\n\nTesting for epoch 4 index 1:\n\nTesting for epoch 4 index 2:\nEpoch 5 of 60\n\nTesting for epoch 5 index 1:\n\nTesting for epoch 5 index 2:\nEpoch 6 of 60\n\nTesting for epoch 6 index 1:\n\nTesting for epoch 6 index 2:\nEpoch 7 of 60\n\nTesting for epoch 7 index 1:\n\nTesting for epoch 7 index 2:\nEpoch 8 of 60\n\nTesting for epoch 8 index 1:\n\nTesting for epoch 8 index 2:\nEpoch 9 of 60\n\nTesting for epoch 9 index 1:\n\nTesting for epoch 9 index 2:\nEpoch 10 of 60\n\nTesting for epoch 10 index 1:\n\nTesting for epoch 10 index 2:\nEpoch 11 of 60\n\nTesting for epoch 11 index 1:\n\nTesting for epoch 11 index 2:\nEpoch 12 of 60\n\nTesting for epoch 12 index 1:\n\nTesting for epoch 12 index 2:\nEpoch 13 of 60\n\nTesting for epoch 13 index 1:\n\nTesting for epoch 13 index 2:\nEpoch 14 of 60\n\nTesting for epoch 14 index 1:\n\nTesting for epoch 14 index 2:\nEpoch 15 of 60\n\nTesting for epoch 15 index 1:\n\nTesting for epoch 15 index 2:\nEpoch 16 of 60\n\nTesting for epoch 16 index 1:\n\nTesting for epoch 16 index 2:\nEpoch 17 of 60\n\nTesting for epoch 17 index 1:\n\nTesting for epoch 17 index 2:\nEpoch 18 of 60\n\nTesting for epoch 18 index 1:\n\nTesting for epoch 18 index 2:\nEpoch 19 of 60\n\nTesting for epoch 19 index 1:\n\nTesting for epoch 19 index 2:\nEpoch 20 of 60\n\nTesting for epoch 20 index 1:\n\nTesting for epoch 20 index 2:\nEpoch 21 of 60\n\nTesting for epoch 21 index 1:\n\nTesting for epoch 21 index 2:\n16/16 [==============================] - 0s 638us/step - loss: 0.5986\n16/16 [==============================] - 0s 1ms/step - loss: 1.2168\n16/16 [==============================] - 0s 643us/step - loss: 1.2657\n16/16 [==============================] - 0s 637us/step - loss: 1.2688\n16/16 [==============================] - 0s 1ms/step - loss: 1.2695\n16/16 [==============================] - 0s 1ms/step - loss: 1.2696\n16/16 [==============================] - 0s 653us/step - loss: 1.2696\n16/16 [==============================] - 0s 649us/step - loss: 1.2696\n16/16 [==============================] - 0s 711us/step - loss: 1.2696\n16/16 [==============================] - 0s 1ms/step - loss: 1.2696\nEpoch 22 of 60\n\nTesting for epoch 22 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.6392\n16/16 [==============================] - 0s 664us/step - loss: 1.2086\n16/16 [==============================] - 0s 711us/step - loss: 1.2461\n16/16 [==============================] - 0s 2ms/step - loss: 1.2488\n16/16 [==============================] - 0s 707us/step - loss: 1.2493\n16/16 [==============================] - 0s 628us/step - loss: 1.2494\n16/16 [==============================] - 0s 642us/step - loss: 1.2494\n16/16 [==============================] - 0s 698us/step - loss: 1.2494\n16/16 [==============================] - 0s 674us/step - loss: 1.2494\n16/16 [==============================] - 0s 788us/step - loss: 1.2494\n\nTesting for epoch 22 index 2:\n16/16 [==============================] - 0s 663us/step - loss: 0.6763\n16/16 [==============================] - 0s 1ms/step - loss: 1.2250\n16/16 [==============================] - 0s 1ms/step - loss: 1.2559\n16/16 [==============================] - 0s 642us/step - loss: 1.2583\n16/16 [==============================] - 0s 1ms/step - loss: 1.2588\n16/16 [==============================] - 0s 672us/step - loss: 1.2589\n16/16 [==============================] - 0s 629us/step - loss: 1.2589\n16/16 [==============================] - 0s 1ms/step - loss: 1.2589\n16/16 [==============================] - 0s 1ms/step - loss: 1.2589\n16/16 [==============================] - 0s 1ms/step - loss: 1.2589\nEpoch 23 of 60\n\nTesting for epoch 23 index 1:\n16/16 [==============================] - 0s 658us/step - loss: 0.7044\n16/16 [==============================] - 0s 682us/step - loss: 1.2426\n16/16 [==============================] - 0s 661us/step - loss: 1.2710\n16/16 [==============================] - 0s 1ms/step - loss: 1.2733\n16/16 [==============================] - 0s 757us/step - loss: 1.2738\n16/16 [==============================] - 0s 725us/step - loss: 1.2739\n16/16 [==============================] - 0s 1ms/step - loss: 1.2739\n16/16 [==============================] - 0s 1ms/step - loss: 1.2739\n16/16 [==============================] - 0s 638us/step - loss: 1.2739\n16/16 [==============================] - 0s 648us/step - loss: 1.2739\n\nTesting for epoch 23 index 2:\n16/16 [==============================] - 0s 660us/step - loss: 0.7203\n16/16 [==============================] - 0s 878us/step - loss: 1.2467\n16/16 [==============================] - 0s 647us/step - loss: 1.2715\n16/16 [==============================] - 0s 1ms/step - loss: 1.2737\n16/16 [==============================] - 0s 1ms/step - loss: 1.2741\n16/16 [==============================] - 0s 637us/step - loss: 1.2742\n16/16 [==============================] - 0s 656us/step - loss: 1.2742\n16/16 [==============================] - 0s 1ms/step - loss: 1.2742\n16/16 [==============================] - 0s 645us/step - loss: 1.2742\n16/16 [==============================] - 0s 689us/step - loss: 1.2742\nEpoch 24 of 60\n\nTesting for epoch 24 index 1:\n16/16 [==============================] - 0s 656us/step - loss: 0.7260\n16/16 [==============================] - 0s 1ms/step - loss: 1.2580\n16/16 [==============================] - 0s 655us/step - loss: 1.2817\n16/16 [==============================] - 0s 1ms/step - loss: 1.2839\n16/16 [==============================] - 0s 846us/step - loss: 1.2844\n16/16 [==============================] - 0s 696us/step - loss: 1.2844\n16/16 [==============================] - 0s 926us/step - loss: 1.2845\n16/16 [==============================] - 0s 661us/step - loss: 1.2845\n16/16 [==============================] - 0s 1ms/step - loss: 1.2845\n16/16 [==============================] - 0s 1ms/step - loss: 1.2845\n\nTesting for epoch 24 index 2:\n16/16 [==============================] - 0s 641us/step - loss: 0.7292\n16/16 [==============================] - 0s 632us/step - loss: 1.2735\n16/16 [==============================] - 0s 780us/step - loss: 1.2970\n16/16 [==============================] - 0s 1ms/step - loss: 1.2995\n16/16 [==============================] - 0s 637us/step - loss: 1.2999\n16/16 [==============================] - 0s 1ms/step - loss: 1.3000\n16/16 [==============================] - 0s 1ms/step - loss: 1.3000\n16/16 [==============================] - 0s 981us/step - loss: 1.3000\n16/16 [==============================] - 0s 1ms/step - loss: 1.3000\n16/16 [==============================] - 0s 1ms/step - loss: 1.3000\nEpoch 25 of 60\n\nTesting for epoch 25 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.7228\n16/16 [==============================] - 0s 1ms/step - loss: 1.2928\n16/16 [==============================] - 0s 1ms/step - loss: 1.3171\n16/16 [==============================] - 0s 821us/step - loss: 1.3198\n16/16 [==============================] - 0s 611us/step - loss: 1.3203\n16/16 [==============================] - 0s 690us/step - loss: 1.3204\n16/16 [==============================] - 0s 647us/step - loss: 1.3204\n16/16 [==============================] - 0s 1ms/step - loss: 1.3204\n16/16 [==============================] - 0s 974us/step - loss: 1.3204\n16/16 [==============================] - 0s 589us/step - loss: 1.3204\n\nTesting for epoch 25 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.7136\n16/16 [==============================] - 0s 1ms/step - loss: 1.3031\n16/16 [==============================] - 0s 643us/step - loss: 1.3286\n16/16 [==============================] - 0s 1ms/step - loss: 1.3313\n16/16 [==============================] - 0s 948us/step - loss: 1.3319\n16/16 [==============================] - 0s 1ms/step - loss: 1.3320\n16/16 [==============================] - 0s 802us/step - loss: 1.3320\n16/16 [==============================] - 0s 1ms/step - loss: 1.3320\n16/16 [==============================] - 0s 1ms/step - loss: 1.3320\n16/16 [==============================] - 0s 837us/step - loss: 1.3320\nEpoch 26 of 60\n\nTesting for epoch 26 index 1:\n16/16 [==============================] - 0s 631us/step - loss: 0.6966\n16/16 [==============================] - 0s 1ms/step - loss: 1.3288\n16/16 [==============================] - 0s 820us/step - loss: 1.3566\n16/16 [==============================] - 0s 934us/step - loss: 1.3598\n16/16 [==============================] - 0s 1ms/step - loss: 1.3604\n16/16 [==============================] - 0s 1ms/step - loss: 1.3605\n16/16 [==============================] - 0s 1ms/step - loss: 1.3605\n16/16 [==============================] - 0s 635us/step - loss: 1.3605\n16/16 [==============================] - 0s 865us/step - loss: 1.3605\n16/16 [==============================] - 0s 630us/step - loss: 1.3605\n\nTesting for epoch 26 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.6781\n16/16 [==============================] - 0s 1ms/step - loss: 1.3420\n16/16 [==============================] - 0s 862us/step - loss: 1.3719\n16/16 [==============================] - 0s 635us/step - loss: 1.3756\n16/16 [==============================] - 0s 611us/step - loss: 1.3763\n16/16 [==============================] - 0s 626us/step - loss: 1.3764\n16/16 [==============================] - 0s 796us/step - loss: 1.3764\n16/16 [==============================] - 0s 1ms/step - loss: 1.3764\n16/16 [==============================] - 0s 920us/step - loss: 1.3764\n16/16 [==============================] - 0s 596us/step - loss: 1.3764\nEpoch 27 of 60\n\nTesting for epoch 27 index 1:\n16/16 [==============================] - 0s 629us/step - loss: 0.6549\n16/16 [==============================] - 0s 1ms/step - loss: 1.3709\n16/16 [==============================] - 0s 712us/step - loss: 1.4048\n16/16 [==============================] - 0s 724us/step - loss: 1.4090\n16/16 [==============================] - 0s 749us/step - loss: 1.4098\n16/16 [==============================] - 0s 653us/step - loss: 1.4099\n16/16 [==============================] - 0s 629us/step - loss: 1.4099\n16/16 [==============================] - 0s 1ms/step - loss: 1.4099\n16/16 [==============================] - 0s 723us/step - loss: 1.4099\n16/16 [==============================] - 0s 634us/step - loss: 1.4099\n\nTesting for epoch 27 index 2:\n16/16 [==============================] - 0s 625us/step - loss: 0.6334\n16/16 [==============================] - 0s 618us/step - loss: 1.3962\n16/16 [==============================] - 0s 603us/step - loss: 1.4358\n16/16 [==============================] - 0s 1ms/step - loss: 1.4403\n16/16 [==============================] - 0s 1ms/step - loss: 1.4413\n16/16 [==============================] - 0s 1ms/step - loss: 1.4414\n16/16 [==============================] - 0s 893us/step - loss: 1.4415\n16/16 [==============================] - 0s 598us/step - loss: 1.4415\n16/16 [==============================] - 0s 1ms/step - loss: 1.4414\n16/16 [==============================] - 0s 1ms/step - loss: 1.4414\nEpoch 28 of 60\n\nTesting for epoch 28 index 1:\n16/16 [==============================] - 0s 662us/step - loss: 0.6050\n16/16 [==============================] - 0s 1ms/step - loss: 1.4078\n16/16 [==============================] - 0s 1ms/step - loss: 1.4521\n16/16 [==============================] - 0s 818us/step - loss: 1.4572\n16/16 [==============================] - 0s 993us/step - loss: 1.4584\n16/16 [==============================] - 0s 1ms/step - loss: 1.4585\n16/16 [==============================] - 0s 1ms/step - loss: 1.4586\n16/16 [==============================] - 0s 1ms/step - loss: 1.4586\n16/16 [==============================] - 0s 765us/step - loss: 1.4585\n16/16 [==============================] - 0s 1ms/step - loss: 1.4585\n\nTesting for epoch 28 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.5843\n16/16 [==============================] - 0s 739us/step - loss: 1.4360\n16/16 [==============================] - 0s 1ms/step - loss: 1.4867\n16/16 [==============================] - 0s 582us/step - loss: 1.4928\n16/16 [==============================] - 0s 1ms/step - loss: 1.4941\n16/16 [==============================] - 0s 684us/step - loss: 1.4943\n16/16 [==============================] - 0s 884us/step - loss: 1.4943\n16/16 [==============================] - 0s 746us/step - loss: 1.4943\n16/16 [==============================] - 0s 997us/step - loss: 1.4943\n16/16 [==============================] - 0s 1ms/step - loss: 1.4942\nEpoch 29 of 60\n\nTesting for epoch 29 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.5581\n16/16 [==============================] - 0s 1ms/step - loss: 1.4546\n16/16 [==============================] - 0s 1ms/step - loss: 1.5115\n16/16 [==============================] - 0s 1ms/step - loss: 1.5182\n16/16 [==============================] - 0s 1ms/step - loss: 1.5197\n16/16 [==============================] - 0s 1ms/step - loss: 1.5199\n16/16 [==============================] - 0s 1ms/step - loss: 1.5199\n16/16 [==============================] - 0s 612us/step - loss: 1.5199\n16/16 [==============================] - 0s 650us/step - loss: 1.5199\n16/16 [==============================] - 0s 642us/step - loss: 1.5199\n\nTesting for epoch 29 index 2:\n16/16 [==============================] - 0s 601us/step - loss: 0.5402\n16/16 [==============================] - 0s 900us/step - loss: 1.4761\n16/16 [==============================] - 0s 1ms/step - loss: 1.5388\n16/16 [==============================] - 0s 1ms/step - loss: 1.5458\n16/16 [==============================] - 0s 1ms/step - loss: 1.5476\n16/16 [==============================] - 0s 1ms/step - loss: 1.5478\n16/16 [==============================] - 0s 1ms/step - loss: 1.5478\n16/16 [==============================] - 0s 593us/step - loss: 1.5478\n16/16 [==============================] - 0s 1ms/step - loss: 1.5478\n16/16 [==============================] - 0s 1ms/step - loss: 1.5478\nEpoch 30 of 60\n\nTesting for epoch 30 index 1:\n16/16 [==============================] - 0s 769us/step - loss: 0.5184\n16/16 [==============================] - 0s 606us/step - loss: 1.5000\n16/16 [==============================] - 0s 908us/step - loss: 1.5668\n16/16 [==============================] - 0s 1ms/step - loss: 1.5748\n16/16 [==============================] - 0s 1ms/step - loss: 1.5767\n16/16 [==============================] - 0s 1ms/step - loss: 1.5769\n16/16 [==============================] - 0s 1ms/step - loss: 1.5769\n16/16 [==============================] - 0s 705us/step - loss: 1.5769\n16/16 [==============================] - 0s 613us/step - loss: 1.5769\n16/16 [==============================] - 0s 671us/step - loss: 1.5768\n\nTesting for epoch 30 index 2:\n16/16 [==============================] - 0s 603us/step - loss: 0.5062\n16/16 [==============================] - 0s 629us/step - loss: 1.5280\n16/16 [==============================] - 0s 751us/step - loss: 1.5999\n16/16 [==============================] - 0s 615us/step - loss: 1.6088\n16/16 [==============================] - 0s 929us/step - loss: 1.6109\n16/16 [==============================] - 0s 1ms/step - loss: 1.6112\n16/16 [==============================] - 0s 1ms/step - loss: 1.6112\n16/16 [==============================] - 0s 613us/step - loss: 1.6112\n16/16 [==============================] - 0s 1ms/step - loss: 1.6112\n16/16 [==============================] - 0s 876us/step - loss: 1.6112\nEpoch 31 of 60\n\nTesting for epoch 31 index 1:\n16/16 [==============================] - 0s 663us/step - loss: 0.4910\n16/16 [==============================] - 0s 1ms/step - loss: 1.5266\n16/16 [==============================] - 0s 1ms/step - loss: 1.6021\n16/16 [==============================] - 0s 1ms/step - loss: 1.6113\n16/16 [==============================] - 0s 763us/step - loss: 1.6135\n16/16 [==============================] - 0s 956us/step - loss: 1.6138\n16/16 [==============================] - 0s 611us/step - loss: 1.6139\n16/16 [==============================] - 0s 613us/step - loss: 1.6139\n16/16 [==============================] - 0s 1ms/step - loss: 1.6138\n16/16 [==============================] - 0s 718us/step - loss: 1.6138\n\nTesting for epoch 31 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4859\n16/16 [==============================] - 0s 622us/step - loss: 1.5514\n16/16 [==============================] - 0s 608us/step - loss: 1.6294\n16/16 [==============================] - 0s 1ms/step - loss: 1.6390\n16/16 [==============================] - 0s 634us/step - loss: 1.6414\n16/16 [==============================] - 0s 593us/step - loss: 1.6417\n16/16 [==============================] - 0s 656us/step - loss: 1.6417\n16/16 [==============================] - 0s 606us/step - loss: 1.6417\n16/16 [==============================] - 0s 627us/step - loss: 1.6417\n16/16 [==============================] - 0s 951us/step - loss: 1.6416\nEpoch 32 of 60\n\nTesting for epoch 32 index 1:\n16/16 [==============================] - 0s 886us/step - loss: 0.4775\n16/16 [==============================] - 0s 631us/step - loss: 1.5698\n16/16 [==============================] - 0s 1ms/step - loss: 1.6491\n16/16 [==============================] - 0s 1ms/step - loss: 1.6591\n16/16 [==============================] - 0s 642us/step - loss: 1.6617\n16/16 [==============================] - 0s 1ms/step - loss: 1.6620\n16/16 [==============================] - 0s 606us/step - loss: 1.6621\n16/16 [==============================] - 0s 649us/step - loss: 1.6620\n16/16 [==============================] - 0s 622us/step - loss: 1.6620\n16/16 [==============================] - 0s 621us/step - loss: 1.6619\n\nTesting for epoch 32 index 2:\n16/16 [==============================] - 0s 629us/step - loss: 0.4795\n16/16 [==============================] - 0s 638us/step - loss: 1.5898\n16/16 [==============================] - 0s 634us/step - loss: 1.6681\n16/16 [==============================] - 0s 1ms/step - loss: 1.6781\n16/16 [==============================] - 0s 677us/step - loss: 1.6809\n16/16 [==============================] - 0s 977us/step - loss: 1.6812\n16/16 [==============================] - 0s 1ms/step - loss: 1.6812\n16/16 [==============================] - 0s 1ms/step - loss: 1.6812\n16/16 [==============================] - 0s 1ms/step - loss: 1.6812\n16/16 [==============================] - 0s 1ms/step - loss: 1.6812\nEpoch 33 of 60\n\nTesting for epoch 33 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4773\n16/16 [==============================] - 0s 634us/step - loss: 1.5951\n16/16 [==============================] - 0s 1ms/step - loss: 1.6703\n16/16 [==============================] - 0s 599us/step - loss: 1.6803\n16/16 [==============================] - 0s 685us/step - loss: 1.6830\n16/16 [==============================] - 0s 617us/step - loss: 1.6833\n16/16 [==============================] - 0s 945us/step - loss: 1.6833\n16/16 [==============================] - 0s 1ms/step - loss: 1.6833\n16/16 [==============================] - 0s 1ms/step - loss: 1.6833\n16/16 [==============================] - 0s 602us/step - loss: 1.6832\n\nTesting for epoch 33 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4855\n16/16 [==============================] - 0s 1ms/step - loss: 1.6287\n16/16 [==============================] - 0s 1ms/step - loss: 1.7034\n16/16 [==============================] - 0s 1ms/step - loss: 1.7137\n16/16 [==============================] - 0s 639us/step - loss: 1.7163\n16/16 [==============================] - 0s 681us/step - loss: 1.7166\n16/16 [==============================] - 0s 620us/step - loss: 1.7167\n16/16 [==============================] - 0s 583us/step - loss: 1.7167\n16/16 [==============================] - 0s 1ms/step - loss: 1.7167\n16/16 [==============================] - 0s 1ms/step - loss: 1.7166\nEpoch 34 of 60\n\nTesting for epoch 34 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4900\n16/16 [==============================] - 0s 773us/step - loss: 1.6487\n16/16 [==============================] - 0s 624us/step - loss: 1.7219\n16/16 [==============================] - 0s 1ms/step - loss: 1.7322\n16/16 [==============================] - 0s 1ms/step - loss: 1.7348\n16/16 [==============================] - 0s 1ms/step - loss: 1.7352\n16/16 [==============================] - 0s 1ms/step - loss: 1.7352\n16/16 [==============================] - 0s 1ms/step - loss: 1.7352\n16/16 [==============================] - 0s 1ms/step - loss: 1.7352\n16/16 [==============================] - 0s 1ms/step - loss: 1.7351\n\nTesting for epoch 34 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.5048\n16/16 [==============================] - 0s 1ms/step - loss: 1.6447\n16/16 [==============================] - 0s 1ms/step - loss: 1.7129\n16/16 [==============================] - 0s 1ms/step - loss: 1.7226\n16/16 [==============================] - 0s 1ms/step - loss: 1.7250\n16/16 [==============================] - 0s 1ms/step - loss: 1.7253\n16/16 [==============================] - 0s 717us/step - loss: 1.7253\n16/16 [==============================] - 0s 830us/step - loss: 1.7253\n16/16 [==============================] - 0s 624us/step - loss: 1.7253\n16/16 [==============================] - 0s 682us/step - loss: 1.7252\nEpoch 35 of 60\n\nTesting for epoch 35 index 1:\n16/16 [==============================] - 0s 648us/step - loss: 0.5151\n16/16 [==============================] - 0s 1ms/step - loss: 1.6720\n16/16 [==============================] - 0s 838us/step - loss: 1.7389\n16/16 [==============================] - 0s 1ms/step - loss: 1.7487\n16/16 [==============================] - 0s 1ms/step - loss: 1.7510\n16/16 [==============================] - 0s 832us/step - loss: 1.7513\n16/16 [==============================] - 0s 1ms/step - loss: 1.7514\n16/16 [==============================] - 0s 595us/step - loss: 1.7514\n16/16 [==============================] - 0s 1ms/step - loss: 1.7514\n16/16 [==============================] - 0s 639us/step - loss: 1.7513\n\nTesting for epoch 35 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.5348\n16/16 [==============================] - 0s 870us/step - loss: 1.6661\n16/16 [==============================] - 0s 1ms/step - loss: 1.7277\n16/16 [==============================] - 0s 640us/step - loss: 1.7367\n16/16 [==============================] - 0s 897us/step - loss: 1.7389\n16/16 [==============================] - 0s 1ms/step - loss: 1.7391\n16/16 [==============================] - 0s 1ms/step - loss: 1.7392\n16/16 [==============================] - 0s 1ms/step - loss: 1.7392\n16/16 [==============================] - 0s 745us/step - loss: 1.7392\n16/16 [==============================] - 0s 1ms/step - loss: 1.7391\nEpoch 36 of 60\n\nTesting for epoch 36 index 1:\n16/16 [==============================] - 0s 688us/step - loss: 0.5491\n16/16 [==============================] - 0s 635us/step - loss: 1.6837\n16/16 [==============================] - 0s 601us/step - loss: 1.7423\n16/16 [==============================] - 0s 601us/step - loss: 1.7511\n16/16 [==============================] - 0s 1ms/step - loss: 1.7531\n16/16 [==============================] - 0s 656us/step - loss: 1.7534\n16/16 [==============================] - 0s 1ms/step - loss: 1.7534\n16/16 [==============================] - 0s 1ms/step - loss: 1.7534\n16/16 [==============================] - 0s 1ms/step - loss: 1.7534\n16/16 [==============================] - 0s 1ms/step - loss: 1.7534\n\nTesting for epoch 36 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.5738\n16/16 [==============================] - 0s 1ms/step - loss: 1.6942\n16/16 [==============================] - 0s 1ms/step - loss: 1.7482\n16/16 [==============================] - 0s 1ms/step - loss: 1.7566\n16/16 [==============================] - 0s 623us/step - loss: 1.7585\n16/16 [==============================] - 0s 741us/step - loss: 1.7588\n16/16 [==============================] - 0s 774us/step - loss: 1.7588\n16/16 [==============================] - 0s 1ms/step - loss: 1.7588\n16/16 [==============================] - 0s 1ms/step - loss: 1.7588\n16/16 [==============================] - 0s 1ms/step - loss: 1.7587\nEpoch 37 of 60\n\nTesting for epoch 37 index 1:\n16/16 [==============================] - 0s 604us/step - loss: 0.5910\n16/16 [==============================] - 0s 1ms/step - loss: 1.7056\n16/16 [==============================] - 0s 1ms/step - loss: 1.7570\n16/16 [==============================] - 0s 1ms/step - loss: 1.7652\n16/16 [==============================] - 0s 596us/step - loss: 1.7670\n16/16 [==============================] - 0s 944us/step - loss: 1.7673\n16/16 [==============================] - 0s 1ms/step - loss: 1.7673\n16/16 [==============================] - 0s 619us/step - loss: 1.7673\n16/16 [==============================] - 0s 617us/step - loss: 1.7673\n16/16 [==============================] - 0s 1ms/step - loss: 1.7673\n\nTesting for epoch 37 index 2:\n16/16 [==============================] - 0s 648us/step - loss: 0.6175\n16/16 [==============================] - 0s 628us/step - loss: 1.7136\n16/16 [==============================] - 0s 666us/step - loss: 1.7615\n16/16 [==============================] - 0s 612us/step - loss: 1.7696\n16/16 [==============================] - 0s 1ms/step - loss: 1.7713\n16/16 [==============================] - 0s 742us/step - loss: 1.7715\n16/16 [==============================] - 0s 626us/step - loss: 1.7716\n16/16 [==============================] - 0s 614us/step - loss: 1.7716\n16/16 [==============================] - 0s 623us/step - loss: 1.7715\n16/16 [==============================] - 0s 932us/step - loss: 1.7715\nEpoch 38 of 60\n\nTesting for epoch 38 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.6365\n16/16 [==============================] - 0s 1ms/step - loss: 1.7285\n16/16 [==============================] - 0s 1ms/step - loss: 1.7737\n16/16 [==============================] - 0s 1ms/step - loss: 1.7815\n16/16 [==============================] - 0s 1ms/step - loss: 1.7831\n16/16 [==============================] - 0s 617us/step - loss: 1.7834\n16/16 [==============================] - 0s 1ms/step - loss: 1.7834\n16/16 [==============================] - 0s 1ms/step - loss: 1.7834\n16/16 [==============================] - 0s 601us/step - loss: 1.7834\n16/16 [==============================] - 0s 929us/step - loss: 1.7834\n\nTesting for epoch 38 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.6645\n16/16 [==============================] - 0s 1ms/step - loss: 1.7425\n16/16 [==============================] - 0s 1ms/step - loss: 1.7848\n16/16 [==============================] - 0s 641us/step - loss: 1.7924\n16/16 [==============================] - 0s 1ms/step - loss: 1.7938\n16/16 [==============================] - 0s 1ms/step - loss: 1.7941\n16/16 [==============================] - 0s 821us/step - loss: 1.7941\n16/16 [==============================] - 0s 1ms/step - loss: 1.7941\n16/16 [==============================] - 0s 626us/step - loss: 1.7941\n16/16 [==============================] - 0s 636us/step - loss: 1.7941\nEpoch 39 of 60\n\nTesting for epoch 39 index 1:\n16/16 [==============================] - 0s 634us/step - loss: 0.6840\n16/16 [==============================] - 0s 759us/step - loss: 1.7590\n16/16 [==============================] - 0s 1ms/step - loss: 1.7997\n16/16 [==============================] - 0s 1ms/step - loss: 1.8070\n16/16 [==============================] - 0s 1ms/step - loss: 1.8084\n16/16 [==============================] - 0s 1ms/step - loss: 1.8086\n16/16 [==============================] - 0s 639us/step - loss: 1.8086\n16/16 [==============================] - 0s 621us/step - loss: 1.8086\n16/16 [==============================] - 0s 670us/step - loss: 1.8086\n16/16 [==============================] - 0s 1ms/step - loss: 1.8086\n\nTesting for epoch 39 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.7124\n16/16 [==============================] - 0s 999us/step - loss: 1.7783\n16/16 [==============================] - 0s 1ms/step - loss: 1.8175\n16/16 [==============================] - 0s 688us/step - loss: 1.8245\n16/16 [==============================] - 0s 1ms/step - loss: 1.8258\n16/16 [==============================] - 0s 1ms/step - loss: 1.8261\n16/16 [==============================] - 0s 716us/step - loss: 1.8261\n16/16 [==============================] - 0s 658us/step - loss: 1.8261\n16/16 [==============================] - 0s 632us/step - loss: 1.8261\n16/16 [==============================] - 0s 635us/step - loss: 1.8261\nEpoch 40 of 60\n\nTesting for epoch 40 index 1:\n16/16 [==============================] - 0s 921us/step - loss: 0.7273\n16/16 [==============================] - 0s 863us/step - loss: 1.7767\n16/16 [==============================] - 0s 995us/step - loss: 1.8143\n16/16 [==============================] - 0s 593us/step - loss: 1.8210\n16/16 [==============================] - 0s 1ms/step - loss: 1.8223\n16/16 [==============================] - 0s 877us/step - loss: 1.8225\n16/16 [==============================] - 0s 1ms/step - loss: 1.8226\n16/16 [==============================] - 0s 820us/step - loss: 1.8226\n16/16 [==============================] - 0s 1ms/step - loss: 1.8225\n16/16 [==============================] - 0s 1ms/step - loss: 1.8225\n\nTesting for epoch 40 index 2:\n16/16 [==============================] - 0s 688us/step - loss: 0.7514\n16/16 [==============================] - 0s 1ms/step - loss: 1.7804\n16/16 [==============================] - 0s 1ms/step - loss: 1.8152\n16/16 [==============================] - 0s 585us/step - loss: 1.8214\n16/16 [==============================] - 0s 1ms/step - loss: 1.8225\n16/16 [==============================] - 0s 619us/step - loss: 1.8227\n16/16 [==============================] - 0s 1ms/step - loss: 1.8228\n16/16 [==============================] - 0s 1ms/step - loss: 1.8228\n16/16 [==============================] - 0s 1ms/step - loss: 1.8227\n16/16 [==============================] - 0s 852us/step - loss: 1.8227\nEpoch 41 of 60\n\nTesting for epoch 41 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.7649\n16/16 [==============================] - 0s 1ms/step - loss: 1.7944\n16/16 [==============================] - 0s 1ms/step - loss: 1.8299\n16/16 [==============================] - 0s 804us/step - loss: 1.8361\n16/16 [==============================] - 0s 1ms/step - loss: 1.8372\n16/16 [==============================] - 0s 612us/step - loss: 1.8375\n16/16 [==============================] - 0s 1ms/step - loss: 1.8375\n16/16 [==============================] - 0s 1ms/step - loss: 1.8375\n16/16 [==============================] - 0s 1ms/step - loss: 1.8375\n16/16 [==============================] - 0s 641us/step - loss: 1.8375\n\nTesting for epoch 41 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.7882\n16/16 [==============================] - 0s 853us/step - loss: 1.8148\n16/16 [==============================] - 0s 659us/step - loss: 1.8491\n16/16 [==============================] - 0s 615us/step - loss: 1.8553\n16/16 [==============================] - 0s 931us/step - loss: 1.8563\n16/16 [==============================] - 0s 1ms/step - loss: 1.8566\n16/16 [==============================] - 0s 634us/step - loss: 1.8566\n16/16 [==============================] - 0s 861us/step - loss: 1.8566\n16/16 [==============================] - 0s 960us/step - loss: 1.8566\n16/16 [==============================] - 0s 1ms/step - loss: 1.8566\nEpoch 42 of 60\n\nTesting for epoch 42 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.7975\n16/16 [==============================] - 0s 646us/step - loss: 1.8225\n16/16 [==============================] - 0s 1ms/step - loss: 1.8555\n16/16 [==============================] - 0s 588us/step - loss: 1.8616\n16/16 [==============================] - 0s 898us/step - loss: 1.8626\n16/16 [==============================] - 0s 1ms/step - loss: 1.8628\n16/16 [==============================] - 0s 1ms/step - loss: 1.8628\n16/16 [==============================] - 0s 1ms/step - loss: 1.8628\n16/16 [==============================] - 0s 661us/step - loss: 1.8628\n16/16 [==============================] - 0s 636us/step - loss: 1.8628\n\nTesting for epoch 42 index 2:\n16/16 [==============================] - 0s 590us/step - loss: 0.8169\n16/16 [==============================] - 0s 1ms/step - loss: 1.8391\n16/16 [==============================] - 0s 633us/step - loss: 1.8715\n16/16 [==============================] - 0s 585us/step - loss: 1.8774\n16/16 [==============================] - 0s 615us/step - loss: 1.8784\n16/16 [==============================] - 0s 596us/step - loss: 1.8786\n16/16 [==============================] - 0s 1ms/step - loss: 1.8787\n16/16 [==============================] - 0s 1ms/step - loss: 1.8787\n16/16 [==============================] - 0s 631us/step - loss: 1.8787\n16/16 [==============================] - 0s 671us/step - loss: 1.8787\nEpoch 43 of 60\n\nTesting for epoch 43 index 1:\n16/16 [==============================] - 0s 865us/step - loss: 0.8231\n16/16 [==============================] - 0s 1ms/step - loss: 1.8496\n16/16 [==============================] - 0s 620us/step - loss: 1.8823\n16/16 [==============================] - 0s 664us/step - loss: 1.8883\n16/16 [==============================] - 0s 600us/step - loss: 1.8893\n16/16 [==============================] - 0s 1ms/step - loss: 1.8895\n16/16 [==============================] - 0s 1ms/step - loss: 1.8896\n16/16 [==============================] - 0s 1ms/step - loss: 1.8896\n16/16 [==============================] - 0s 628us/step - loss: 1.8896\n16/16 [==============================] - 0s 1ms/step - loss: 1.8895\n\nTesting for epoch 43 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.8391\n16/16 [==============================] - 0s 609us/step - loss: 1.8681\n16/16 [==============================] - 0s 871us/step - loss: 1.9009\n16/16 [==============================] - 0s 565us/step - loss: 1.9070\n16/16 [==============================] - 0s 1ms/step - loss: 1.9079\n16/16 [==============================] - 0s 610us/step - loss: 1.9081\n16/16 [==============================] - 0s 637us/step - loss: 1.9082\n16/16 [==============================] - 0s 1ms/step - loss: 1.9082\n16/16 [==============================] - 0s 1ms/step - loss: 1.9082\n16/16 [==============================] - 0s 880us/step - loss: 1.9082\nEpoch 44 of 60\n\nTesting for epoch 44 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.8437\n16/16 [==============================] - 0s 1ms/step - loss: 1.8798\n16/16 [==============================] - 0s 923us/step - loss: 1.9120\n16/16 [==============================] - 0s 1ms/step - loss: 1.9179\n16/16 [==============================] - 0s 612us/step - loss: 1.9189\n16/16 [==============================] - 0s 1ms/step - loss: 1.9191\n16/16 [==============================] - 0s 681us/step - loss: 1.9191\n16/16 [==============================] - 0s 1ms/step - loss: 1.9191\n16/16 [==============================] - 0s 1ms/step - loss: 1.9191\n16/16 [==============================] - 0s 1ms/step - loss: 1.9191\n\nTesting for epoch 44 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.8544\n16/16 [==============================] - 0s 645us/step - loss: 1.8871\n16/16 [==============================] - 0s 632us/step - loss: 1.9189\n16/16 [==============================] - 0s 1ms/step - loss: 1.9248\n16/16 [==============================] - 0s 1ms/step - loss: 1.9257\n16/16 [==============================] - 0s 1ms/step - loss: 1.9259\n16/16 [==============================] - 0s 836us/step - loss: 1.9259\n16/16 [==============================] - 0s 695us/step - loss: 1.9259\n16/16 [==============================] - 0s 1ms/step - loss: 1.9259\n16/16 [==============================] - 0s 808us/step - loss: 1.9259\nEpoch 45 of 60\n\nTesting for epoch 45 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.8583\n16/16 [==============================] - 0s 1ms/step - loss: 1.9099\n16/16 [==============================] - 0s 785us/step - loss: 1.9431\n16/16 [==============================] - 0s 782us/step - loss: 1.9491\n16/16 [==============================] - 0s 1ms/step - loss: 1.9501\n16/16 [==============================] - 0s 711us/step - loss: 1.9503\n16/16 [==============================] - 0s 617us/step - loss: 1.9503\n16/16 [==============================] - 0s 601us/step - loss: 1.9503\n16/16 [==============================] - 0s 671us/step - loss: 1.9503\n16/16 [==============================] - 0s 602us/step - loss: 1.9504\n\nTesting for epoch 45 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.8705\n16/16 [==============================] - 0s 1ms/step - loss: 1.9270\n16/16 [==============================] - 0s 928us/step - loss: 1.9599\n16/16 [==============================] - 0s 758us/step - loss: 1.9658\n16/16 [==============================] - 0s 2ms/step - loss: 1.9668\n16/16 [==============================] - 0s 1ms/step - loss: 1.9670\n16/16 [==============================] - 0s 594us/step - loss: 1.9670\n16/16 [==============================] - 0s 600us/step - loss: 1.9670\n16/16 [==============================] - 0s 599us/step - loss: 1.9670\n16/16 [==============================] - 0s 924us/step - loss: 1.9670\nEpoch 46 of 60\n\nTesting for epoch 46 index 1:\n16/16 [==============================] - 0s 643us/step - loss: 0.8683\n16/16 [==============================] - 0s 1ms/step - loss: 1.9287\n16/16 [==============================] - 0s 665us/step - loss: 1.9614\n16/16 [==============================] - 0s 1ms/step - loss: 1.9673\n16/16 [==============================] - 0s 1ms/step - loss: 1.9683\n16/16 [==============================] - 0s 1ms/step - loss: 1.9685\n16/16 [==============================] - 0s 654us/step - loss: 1.9685\n16/16 [==============================] - 0s 762us/step - loss: 1.9685\n16/16 [==============================] - 0s 628us/step - loss: 1.9685\n16/16 [==============================] - 0s 654us/step - loss: 1.9685\n\nTesting for epoch 46 index 2:\n16/16 [==============================] - 0s 659us/step - loss: 0.8815\n16/16 [==============================] - 0s 1ms/step - loss: 1.9514\n16/16 [==============================] - 0s 1ms/step - loss: 1.9846\n16/16 [==============================] - 0s 1ms/step - loss: 1.9905\n16/16 [==============================] - 0s 1ms/step - loss: 1.9915\n16/16 [==============================] - 0s 786us/step - loss: 1.9916\n16/16 [==============================] - 0s 655us/step - loss: 1.9917\n16/16 [==============================] - 0s 1ms/step - loss: 1.9917\n16/16 [==============================] - 0s 973us/step - loss: 1.9917\n16/16 [==============================] - 0s 1ms/step - loss: 1.9917\nEpoch 47 of 60\n\nTesting for epoch 47 index 1:\n16/16 [==============================] - 0s 741us/step - loss: 0.8773\n16/16 [==============================] - 0s 672us/step - loss: 1.9488\n16/16 [==============================] - 0s 660us/step - loss: 1.9816\n16/16 [==============================] - 0s 1ms/step - loss: 1.9875\n16/16 [==============================] - 0s 899us/step - loss: 1.9885\n16/16 [==============================] - 0s 1ms/step - loss: 1.9887\n16/16 [==============================] - 0s 885us/step - loss: 1.9887\n16/16 [==============================] - 0s 661us/step - loss: 1.9887\n16/16 [==============================] - 0s 1ms/step - loss: 1.9887\n16/16 [==============================] - 0s 972us/step - loss: 1.9887\n\nTesting for epoch 47 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 0.8923\n16/16 [==============================] - 0s 802us/step - loss: 1.9764\n16/16 [==============================] - 0s 733us/step - loss: 2.0094\n16/16 [==============================] - 0s 878us/step - loss: 2.0152\n16/16 [==============================] - 0s 667us/step - loss: 2.0162\n16/16 [==============================] - 0s 1ms/step - loss: 2.0164\n16/16 [==============================] - 0s 831us/step - loss: 2.0164\n16/16 [==============================] - 0s 1ms/step - loss: 2.0164\n16/16 [==============================] - 0s 1ms/step - loss: 2.0164\n16/16 [==============================] - 0s 1ms/step - loss: 2.0165\nEpoch 48 of 60\n\nTesting for epoch 48 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.8980\n16/16 [==============================] - 0s 829us/step - loss: 2.0059\n16/16 [==============================] - 0s 1ms/step - loss: 2.0403\n16/16 [==============================] - 0s 1ms/step - loss: 2.0464\n16/16 [==============================] - 0s 991us/step - loss: 2.0474\n16/16 [==============================] - 0s 657us/step - loss: 2.0476\n16/16 [==============================] - 0s 1ms/step - loss: 2.0476\n16/16 [==============================] - 0s 1ms/step - loss: 2.0476\n16/16 [==============================] - 0s 625us/step - loss: 2.0476\n16/16 [==============================] - 0s 1ms/step - loss: 2.0476\n\nTesting for epoch 48 index 2:\n16/16 [==============================] - 0s 640us/step - loss: 0.9077\n16/16 [==============================] - 0s 1ms/step - loss: 2.0130\n16/16 [==============================] - 0s 1ms/step - loss: 2.0470\n16/16 [==============================] - 0s 953us/step - loss: 2.0530\n16/16 [==============================] - 0s 1ms/step - loss: 2.0539\n16/16 [==============================] - 0s 1ms/step - loss: 2.0541\n16/16 [==============================] - 0s 1ms/step - loss: 2.0542\n16/16 [==============================] - 0s 1ms/step - loss: 2.0542\n16/16 [==============================] - 0s 1ms/step - loss: 2.0542\n16/16 [==============================] - 0s 1ms/step - loss: 2.0542\nEpoch 49 of 60\n\nTesting for epoch 49 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.9081\n16/16 [==============================] - 0s 1ms/step - loss: 2.0204\n16/16 [==============================] - 0s 1ms/step - loss: 2.0543\n16/16 [==============================] - 0s 694us/step - loss: 2.0602\n16/16 [==============================] - 0s 1ms/step - loss: 2.0611\n16/16 [==============================] - 0s 1ms/step - loss: 2.0613\n16/16 [==============================] - 0s 815us/step - loss: 2.0614\n16/16 [==============================] - 0s 1ms/step - loss: 2.0614\n16/16 [==============================] - 0s 804us/step - loss: 2.0614\n16/16 [==============================] - 0s 1ms/step - loss: 2.0614\n\nTesting for epoch 49 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.9192\n16/16 [==============================] - 0s 2ms/step - loss: 2.0292\n16/16 [==============================] - 0s 1ms/step - loss: 2.0625\n16/16 [==============================] - 0s 1ms/step - loss: 2.0683\n16/16 [==============================] - 0s 1ms/step - loss: 2.0692\n16/16 [==============================] - 0s 1ms/step - loss: 2.0693\n16/16 [==============================] - 0s 1ms/step - loss: 2.0694\n16/16 [==============================] - 0s 1ms/step - loss: 2.0694\n16/16 [==============================] - 0s 1ms/step - loss: 2.0694\n16/16 [==============================] - 0s 1ms/step - loss: 2.0694\nEpoch 50 of 60\n\nTesting for epoch 50 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.9220\n16/16 [==============================] - 0s 1ms/step - loss: 2.0413\n16/16 [==============================] - 0s 1ms/step - loss: 2.0749\n16/16 [==============================] - 0s 1ms/step - loss: 2.0807\n16/16 [==============================] - 0s 1ms/step - loss: 2.0816\n16/16 [==============================] - 0s 1ms/step - loss: 2.0818\n16/16 [==============================] - 0s 1ms/step - loss: 2.0819\n16/16 [==============================] - 0s 1ms/step - loss: 2.0819\n16/16 [==============================] - 0s 1ms/step - loss: 2.0819\n16/16 [==============================] - 0s 1ms/step - loss: 2.0819\n\nTesting for epoch 50 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 0.9344\n16/16 [==============================] - 0s 2ms/step - loss: 2.0501\n16/16 [==============================] - 0s 2ms/step - loss: 2.0831\n16/16 [==============================] - 0s 2ms/step - loss: 2.0889\n16/16 [==============================] - 0s 2ms/step - loss: 2.0898\n16/16 [==============================] - 0s 2ms/step - loss: 2.0900\n16/16 [==============================] - 0s 2ms/step - loss: 2.0900\n16/16 [==============================] - 0s 2ms/step - loss: 2.0900\n16/16 [==============================] - 0s 2ms/step - loss: 2.0900\n16/16 [==============================] - 0s 2ms/step - loss: 2.0900\nEpoch 51 of 60\n\nTesting for epoch 51 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 0.9430\n16/16 [==============================] - 0s 2ms/step - loss: 2.0784\n16/16 [==============================] - 0s 1ms/step - loss: 2.1121\n16/16 [==============================] - 0s 1ms/step - loss: 2.1180\n16/16 [==============================] - 0s 1ms/step - loss: 2.1189\n16/16 [==============================] - 0s 670us/step - loss: 2.1191\n16/16 [==============================] - 0s 707us/step - loss: 2.1191\n16/16 [==============================] - 0s 720us/step - loss: 2.1191\n16/16 [==============================] - 0s 685us/step - loss: 2.1191\n16/16 [==============================] - 0s 600us/step - loss: 2.1191\n\nTesting for epoch 51 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.9590\n16/16 [==============================] - 0s 1ms/step - loss: 2.0944\n16/16 [==============================] - 0s 2ms/step - loss: 2.1283\n16/16 [==============================] - 0s 1ms/step - loss: 2.1342\n16/16 [==============================] - 0s 2ms/step - loss: 2.1351\n16/16 [==============================] - 0s 2ms/step - loss: 2.1353\n16/16 [==============================] - 0s 2ms/step - loss: 2.1353\n16/16 [==============================] - 0s 1ms/step - loss: 2.1353\n16/16 [==============================] - 0s 1ms/step - loss: 2.1353\n16/16 [==============================] - 0s 1ms/step - loss: 2.1354\nEpoch 52 of 60\n\nTesting for epoch 52 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.9632\n16/16 [==============================] - 0s 1ms/step - loss: 2.1018\n16/16 [==============================] - 0s 1ms/step - loss: 2.1355\n16/16 [==============================] - 0s 1ms/step - loss: 2.1415\n16/16 [==============================] - 0s 1ms/step - loss: 2.1424\n16/16 [==============================] - 0s 1ms/step - loss: 2.1426\n16/16 [==============================] - 0s 1ms/step - loss: 2.1426\n16/16 [==============================] - 0s 2ms/step - loss: 2.1426\n16/16 [==============================] - 0s 1ms/step - loss: 2.1426\n16/16 [==============================] - 0s 1ms/step - loss: 2.1427\n\nTesting for epoch 52 index 2:\n16/16 [==============================] - 0s 593us/step - loss: 0.9811\n16/16 [==============================] - 0s 1ms/step - loss: 2.1158\n16/16 [==============================] - 0s 601us/step - loss: 2.1494\n16/16 [==============================] - 0s 604us/step - loss: 2.1553\n16/16 [==============================] - 0s 1ms/step - loss: 2.1562\n16/16 [==============================] - 0s 580us/step - loss: 2.1564\n16/16 [==============================] - 0s 606us/step - loss: 2.1564\n16/16 [==============================] - 0s 669us/step - loss: 2.1565\n16/16 [==============================] - 0s 933us/step - loss: 2.1565\n16/16 [==============================] - 0s 604us/step - loss: 2.1565\nEpoch 53 of 60\n\nTesting for epoch 53 index 1:\n16/16 [==============================] - 0s 617us/step - loss: 0.9849\n16/16 [==============================] - 0s 874us/step - loss: 2.1127\n16/16 [==============================] - 0s 601us/step - loss: 2.1443\n16/16 [==============================] - 0s 721us/step - loss: 2.1500\n16/16 [==============================] - 0s 709us/step - loss: 2.1508\n16/16 [==============================] - 0s 636us/step - loss: 2.1510\n16/16 [==============================] - 0s 641us/step - loss: 2.1510\n16/16 [==============================] - 0s 3ms/step - loss: 2.1510\n16/16 [==============================] - 0s 832us/step - loss: 2.1510\n16/16 [==============================] - 0s 943us/step - loss: 2.1511\n\nTesting for epoch 53 index 2:\n16/16 [==============================] - 0s 960us/step - loss: 1.0029\n16/16 [==============================] - 0s 847us/step - loss: 2.1223\n16/16 [==============================] - 0s 1ms/step - loss: 2.1535\n16/16 [==============================] - 0s 663us/step - loss: 2.1592\n16/16 [==============================] - 0s 1ms/step - loss: 2.1600\n16/16 [==============================] - 0s 1ms/step - loss: 2.1601\n16/16 [==============================] - 0s 1ms/step - loss: 2.1602\n16/16 [==============================] - 0s 1ms/step - loss: 2.1602\n16/16 [==============================] - 0s 1ms/step - loss: 2.1602\n16/16 [==============================] - 0s 1ms/step - loss: 2.1602\nEpoch 54 of 60\n\nTesting for epoch 54 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.0201\n16/16 [==============================] - 0s 2ms/step - loss: 2.1555\n16/16 [==============================] - 0s 698us/step - loss: 2.1869\n16/16 [==============================] - 0s 738us/step - loss: 2.1925\n16/16 [==============================] - 0s 974us/step - loss: 2.1933\n16/16 [==============================] - 0s 1ms/step - loss: 2.1935\n16/16 [==============================] - 0s 811us/step - loss: 2.1935\n16/16 [==============================] - 0s 1ms/step - loss: 2.1935\n16/16 [==============================] - 0s 1ms/step - loss: 2.1935\n16/16 [==============================] - 0s 685us/step - loss: 2.1935\n\nTesting for epoch 54 index 2:\n16/16 [==============================] - 0s 644us/step - loss: 1.0391\n16/16 [==============================] - 0s 1ms/step - loss: 2.1625\n16/16 [==============================] - 0s 1ms/step - loss: 2.1931\n16/16 [==============================] - 0s 894us/step - loss: 2.1987\n16/16 [==============================] - 0s 2ms/step - loss: 2.1995\n16/16 [==============================] - 0s 700us/step - loss: 2.1996\n16/16 [==============================] - 0s 1ms/step - loss: 2.1997\n16/16 [==============================] - 0s 851us/step - loss: 2.1997\n16/16 [==============================] - 0s 925us/step - loss: 2.1997\n16/16 [==============================] - 0s 868us/step - loss: 2.1997\nEpoch 55 of 60\n\nTesting for epoch 55 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 1.0520\n16/16 [==============================] - 0s 627us/step - loss: 2.1801\n16/16 [==============================] - 0s 1ms/step - loss: 2.2107\n16/16 [==============================] - 0s 633us/step - loss: 2.2163\n16/16 [==============================] - 0s 899us/step - loss: 2.2171\n16/16 [==============================] - 0s 711us/step - loss: 2.2172\n16/16 [==============================] - 0s 1ms/step - loss: 2.2173\n16/16 [==============================] - 0s 664us/step - loss: 2.2173\n16/16 [==============================] - 0s 1ms/step - loss: 2.2173\n16/16 [==============================] - 0s 1ms/step - loss: 2.2173\n\nTesting for epoch 55 index 2:\n16/16 [==============================] - 0s 589us/step - loss: 1.0727\n16/16 [==============================] - 0s 603us/step - loss: 2.1879\n16/16 [==============================] - 0s 581us/step - loss: 2.2176\n16/16 [==============================] - 0s 580us/step - loss: 2.2229\n16/16 [==============================] - 0s 582us/step - loss: 2.2236\n16/16 [==============================] - 0s 571us/step - loss: 2.2238\n16/16 [==============================] - 0s 574us/step - loss: 2.2238\n16/16 [==============================] - 0s 561us/step - loss: 2.2238\n16/16 [==============================] - 0s 506us/step - loss: 2.2238\n16/16 [==============================] - 0s 526us/step - loss: 2.2239\nEpoch 56 of 60\n\nTesting for epoch 56 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 1.0790\n16/16 [==============================] - 0s 1ms/step - loss: 2.1883\n16/16 [==============================] - 0s 2ms/step - loss: 2.2177\n16/16 [==============================] - 0s 2ms/step - loss: 2.2230\n16/16 [==============================] - 0s 2ms/step - loss: 2.2237\n16/16 [==============================] - 0s 2ms/step - loss: 2.2239\n16/16 [==============================] - 0s 2ms/step - loss: 2.2239\n16/16 [==============================] - 0s 2ms/step - loss: 2.2239\n16/16 [==============================] - 0s 2ms/step - loss: 2.2240\n16/16 [==============================] - 0s 2ms/step - loss: 2.2240\n\nTesting for epoch 56 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 1.1019\n16/16 [==============================] - 0s 2ms/step - loss: 2.2025\n16/16 [==============================] - 0s 2ms/step - loss: 2.2310\n16/16 [==============================] - 0s 2ms/step - loss: 2.2362\n16/16 [==============================] - 0s 2ms/step - loss: 2.2369\n16/16 [==============================] - 0s 2ms/step - loss: 2.2371\n16/16 [==============================] - 0s 2ms/step - loss: 2.2371\n16/16 [==============================] - 0s 2ms/step - loss: 2.2371\n16/16 [==============================] - 0s 2ms/step - loss: 2.2371\n16/16 [==============================] - 0s 2ms/step - loss: 2.2372\nEpoch 57 of 60\n\nTesting for epoch 57 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 1.1130\n16/16 [==============================] - 0s 2ms/step - loss: 2.2126\n16/16 [==============================] - 0s 2ms/step - loss: 2.2408\n16/16 [==============================] - 0s 2ms/step - loss: 2.2460\n16/16 [==============================] - 0s 2ms/step - loss: 2.2466\n16/16 [==============================] - 0s 2ms/step - loss: 2.2468\n16/16 [==============================] - 0s 2ms/step - loss: 2.2468\n16/16 [==============================] - 0s 2ms/step - loss: 2.2469\n16/16 [==============================] - 0s 2ms/step - loss: 2.2469\n16/16 [==============================] - 0s 2ms/step - loss: 2.2469\n\nTesting for epoch 57 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 1.1360\n16/16 [==============================] - 0s 2ms/step - loss: 2.2253\n16/16 [==============================] - 0s 2ms/step - loss: 2.2519\n16/16 [==============================] - 0s 2ms/step - loss: 2.2569\n16/16 [==============================] - 0s 2ms/step - loss: 2.2576\n16/16 [==============================] - 0s 2ms/step - loss: 2.2577\n16/16 [==============================] - 0s 2ms/step - loss: 2.2577\n16/16 [==============================] - 0s 2ms/step - loss: 2.2577\n16/16 [==============================] - 0s 2ms/step - loss: 2.2578\n16/16 [==============================] - 0s 1ms/step - loss: 2.2578\nEpoch 58 of 60\n\nTesting for epoch 58 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.1524\n16/16 [==============================] - 0s 2ms/step - loss: 2.2520\n16/16 [==============================] - 0s 1ms/step - loss: 2.2798\n16/16 [==============================] - 0s 2ms/step - loss: 2.2850\n16/16 [==============================] - 0s 2ms/step - loss: 2.2857\n16/16 [==============================] - 0s 1ms/step - loss: 2.2858\n16/16 [==============================] - 0s 2ms/step - loss: 2.2859\n16/16 [==============================] - 0s 966us/step - loss: 2.2859\n16/16 [==============================] - 0s 2ms/step - loss: 2.2859\n16/16 [==============================] - 0s 2ms/step - loss: 2.2859\n\nTesting for epoch 58 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 1.1690\n16/16 [==============================] - 0s 984us/step - loss: 2.2519\n16/16 [==============================] - 0s 2ms/step - loss: 2.2784\n16/16 [==============================] - 0s 2ms/step - loss: 2.2834\n16/16 [==============================] - 0s 925us/step - loss: 2.2840\n16/16 [==============================] - 0s 2ms/step - loss: 2.2842\n16/16 [==============================] - 0s 2ms/step - loss: 2.2842\n16/16 [==============================] - 0s 2ms/step - loss: 2.2842\n16/16 [==============================] - 0s 767us/step - loss: 2.2842\n16/16 [==============================] - 0s 1ms/step - loss: 2.2843\nEpoch 59 of 60\n\nTesting for epoch 59 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 1.1811\n16/16 [==============================] - 0s 2ms/step - loss: 2.2676\n16/16 [==============================] - 0s 2ms/step - loss: 2.2948\n16/16 [==============================] - 0s 2ms/step - loss: 2.2999\n16/16 [==============================] - 0s 2ms/step - loss: 2.3005\n16/16 [==============================] - 0s 2ms/step - loss: 2.3007\n16/16 [==============================] - 0s 2ms/step - loss: 2.3008\n16/16 [==============================] - 0s 1ms/step - loss: 2.3008\n16/16 [==============================] - 0s 657us/step - loss: 2.3008\n16/16 [==============================] - 0s 654us/step - loss: 2.3008\n\nTesting for epoch 59 index 2:\n16/16 [==============================] - 0s 3ms/step - loss: 1.1986\n16/16 [==============================] - 0s 600us/step - loss: 2.2691\n16/16 [==============================] - 0s 579us/step - loss: 2.2946\n16/16 [==============================] - 0s 2ms/step - loss: 2.2995\n16/16 [==============================] - 0s 2ms/step - loss: 2.3001\n16/16 [==============================] - 0s 2ms/step - loss: 2.3003\n16/16 [==============================] - 0s 2ms/step - loss: 2.3003\n16/16 [==============================] - 0s 2ms/step - loss: 2.3003\n16/16 [==============================] - 0s 2ms/step - loss: 2.3004\n16/16 [==============================] - 0s 2ms/step - loss: 2.3004\nEpoch 60 of 60\n\nTesting for epoch 60 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 1.2123\n16/16 [==============================] - 0s 2ms/step - loss: 2.2903\n16/16 [==============================] - 0s 2ms/step - loss: 2.3163\n16/16 [==============================] - 0s 2ms/step - loss: 2.3213\n16/16 [==============================] - 0s 2ms/step - loss: 2.3219\n16/16 [==============================] - 0s 2ms/step - loss: 2.3221\n16/16 [==============================] - 0s 2ms/step - loss: 2.3221\n16/16 [==============================] - 0s 2ms/step - loss: 2.3221\n16/16 [==============================] - 0s 2ms/step - loss: 2.3221\n16/16 [==============================] - 0s 2ms/step - loss: 2.3222\n\nTesting for epoch 60 index 2:\n16/16 [==============================] - 0s 795us/step - loss: 1.2301\n16/16 [==============================] - 0s 1ms/step - loss: 2.2941\n16/16 [==============================] - 0s 597us/step - loss: 2.3188\n16/16 [==============================] - 0s 524us/step - loss: 2.3236\n16/16 [==============================] - 0s 889us/step - loss: 2.3242\n16/16 [==============================] - 0s 675us/step - loss: 2.3243\n16/16 [==============================] - 0s 523us/step - loss: 2.3243\n16/16 [==============================] - 0s 577us/step - loss: 2.3243\n16/16 [==============================] - 0s 531us/step - loss: 2.3243\n16/16 [==============================] - 0s 646us/step - loss: 2.3244\n\n\n\noutlier_MO_GAAL_one = list(clf.labels_)\n\n\noutlier_MO_GAAL_one = list(map(lambda x: 1 if x==0  else -1,outlier_MO_GAAL_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_MO_GAAL_one,tab_orbit)\n\n\n_conf.conf(\"MO-GAAL (Liu et al., 2019)\")\n\n\n\n\nAccuracy: 0.950\nPrecision: 0.950\nRecall: 1.000\nF1 Score: 0.974\n\n\n\nthirteen = twelve.append(_conf.tab)\n\n\n\nLSCP\n\ndetectors = [KNN(), LOF(), OCSVM()]\nclf = LSCP(detectors)\nclf.fit(_df[['x', 'y','f']])\n_df['LSCP_clf'] = clf.labels_\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/pyod/models/lscp.py:382: UserWarning: The number of histogram bins is greater than the number of classifiers, reducing n_bins to n_clf.\n  warnings.warn(\n\n\n\noutlier_LSCP_one = list(clf.labels_)\n\n\noutlier_LSCP_one = list(map(lambda x: 1 if x==0  else -1,outlier_LSCP_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_LSCP_one,tab_orbit)\n\n\n_conf.conf(\"LSCP (Zhao et al., 2019)\")\n\n\n\n\nAccuracy: 0.948\nPrecision: 0.999\nRecall: 0.946\nF1 Score: 0.972\n\n\n\nfourteen_orbit = thirteen.append(_conf.tab)"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#orbit-result",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#orbit-result",
    "title": "Class code for Comparison Study",
    "section": "Orbit Result",
    "text": "Orbit Result\n\nround(fourteen_orbit,4)\n\n\n\n\n\n  \n    \n      \n      Accuracy\n      Precision\n      Recall\n      F1\n    \n  \n  \n    \n      GODE\n      0.997\n      0.9969\n      1.0000\n      0.9984\n    \n    \n      LOF (Breunig et al., 2000)\n      0.886\n      0.9872\n      0.8916\n      0.9369\n    \n    \n      kNN (Ramaswamy et al., 2000)\n      0.948\n      0.9989\n      0.9463\n      0.9719\n    \n    \n      CBLOF (He et al., 2003)\n      0.918\n      0.9568\n      0.9568\n      0.9568\n    \n    \n      OCSVM (Sch ̈olkopf et al., 2001)\n      0.923\n      0.9877\n      0.9305\n      0.9583\n    \n    \n      MCD (Hardin and Rocke, 2004)\n      0.866\n      0.9533\n      0.9032\n      0.9276\n    \n    \n      Feature Bagging (Lazarevic and Kumar, 2005)\n      0.912\n      0.9789\n      0.9274\n      0.9524\n    \n    \n      ABOD (Kriegel et al., 2008)\n      0.988\n      0.9937\n      0.9937\n      0.9937\n    \n    \n      Isolation Forest (Liu et al., 2008)\n      0.378\n      0.9970\n      0.3463\n      0.5141\n    \n    \n      HBOS (Goldstein and Dengel, 2012)\n      0.881\n      0.9612\n      0.9116\n      0.9357\n    \n    \n      SOS (Janssens et al., 2012)\n      0.881\n      0.9612\n      0.9116\n      0.9357\n    \n    \n      SO-GAAL (Liu et al., 2019)\n      0.876\n      0.9589\n      0.9084\n      0.9330\n    \n    \n      MO-GAAL (Liu et al., 2019)\n      0.950\n      0.9500\n      1.0000\n      0.9744\n    \n    \n      LSCP (Zhao et al., 2019)\n      0.948\n      0.9989\n      0.9463\n      0.9719"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#bunny",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#bunny",
    "title": "Class code for Comparison Study",
    "section": "Bunny",
    "text": "Bunny\n\nG = graphs.Bunny()\nn = G.N\n\n\ng = filters.Heat(G, tau=75) \n\n\nnormal = np.random.randn(n)\nunif = np.concatenate([np.random.uniform(low=3,high=7,size=60), np.random.uniform(low=-7,high=-3,size=60),np.zeros(n-120)]); np.random.shuffle(unif)\nnoise = normal + unif\nindex_of_trueoutlier2 = np.where(unif!=0)\n\n\nf = np.zeros(n)\nf[1000] = -3234\nf = g.filter(f, method='chebyshev') \n\n2022-11-26 07:54:05,353:[WARNING](pygsp.graphs.graph.lmax): The largest eigenvalue G.lmax is not available, we need to estimate it. Explicitly call G.estimate_lmax() or G.compute_fourier_basis() once beforehand to suppress the warning.\n\n\n\nG.coords.shape\n\n(2503, 3)\n\n\n\n_W = G.W.toarray()\n_x = G.coords[:,0]\n_y = G.coords[:,1]\n_z = -G.coords[:,2]\n\n\n_df = pd.DataFrame({'x' : _x, 'y' : _y, 'z' : _z, 'fnoise':f+noise,'f' : f, 'noise': noise})\n\n\noutlier_true_one_2 = unif.copy()\n\n\noutlier_true_one_2 = list(map(lambda x: -1 if x !=0  else 1,outlier_true_one_2))\n\n\nX = np.array(_df)[:,:4]\n\n\nGODE\n\n_BUNNY = BUNNY(_df)\n\n\n_BUNNY.fit(sd=20,ref=10)\n\n\noutlier_simul_one = (_BUNNY.df['Residual']**2).tolist()\n\n\noutlier_simul_one = list(map(lambda x: -1 if x > 10 else 1,outlier_simul_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_simul_one,tab_bunny)\n\n\n_conf.conf(\"GODE\")\n\n\n\n\nAccuracy: 0.995\nPrecision: 0.995\nRecall: 0.999\nF1 Score: 0.997\n\n\n\none = _conf.tab\n\n\n\nLOF\n\nclf = LocalOutlierFactor(n_neighbors=2)\n\n\n_conf = Conf_matrx(outlier_true_one_2,clf.fit_predict(X),tab_bunny)\n\n\n_conf.conf(\"LOF (Breunig et al., 2000)\")\n\n\n\n\nAccuracy: 0.928\nPrecision: 0.957\nRecall: 0.969\nF1 Score: 0.963\n\n\n\ntwo = one.append(_conf.tab)\n\n\n\nKNN\n\nclf = KNN()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['knn_Clf'] = clf.labels_\n\n\noutlier_KNN_one = list(clf.labels_)\n\n\noutlier_KNN_one = list(map(lambda x: 1 if x==0  else -1,outlier_KNN_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_KNN_one,tab_bunny)\n\n\n_conf.conf(\"kNN (Ramaswamy et al., 2000)\")\n\n\n\n\nAccuracy: 0.940\nPrecision: 0.996\nRecall: 0.941\nF1 Score: 0.968\n\n\n\nthree = two.append(_conf.tab)\n\n\n\nCBLOF\n\nclf = CBLOF(contamination=0.05,check_estimator=False, random_state=77)\nclf.fit(_df[['x', 'y','fnoise']])\n_df['CBLOF_Clf'] = clf.labels_\n\n\noutlier_CBLOF_one = list(clf.labels_)\n\n\noutlier_CBLOF_one = list(map(lambda x: 1 if x==0  else -1,outlier_CBLOF_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_CBLOF_one,tab_bunny)\n\n\n_conf.conf(\"CBLOF (He et al., 2003)\")\n\n\n\n\nAccuracy: 0.978\nPrecision: 0.989\nRecall: 0.987\nF1 Score: 0.988\n\n\n\nfour = three.append(_conf.tab)\n\n\n\nOCSVM\n\nclf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n\n\nclf.fit(X)\n\nOneClassSVM(gamma=0.1, nu=0.1)\n\n\n\noutlier_OSVM_one = list(clf.predict(X))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_OSVM_one,tab_bunny)\n\n\n_conf.conf(\"OCSVM (Sch ̈olkopf et al., 2001)\")\n\n\n\n\nAccuracy: 0.932\nPrecision: 0.991\nRecall: 0.937\nF1 Score: 0.963\n\n\n\nfive = four.append(_conf.tab)\n\n\n\nMCD\n\nclf = MCD()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['MCD_clf'] = clf.labels_\n\n\noutlier_MCD_one = list(clf.labels_)\n\n\noutlier_MCD_one = list(map(lambda x: 1 if x==0  else -1,outlier_MCD_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_MCD_one,tab_bunny)\n\n\n_conf.conf(\"MCD (Hardin and Rocke, 2004)\")\n\n\n\n\nAccuracy: 0.935\nPrecision: 0.993\nRecall: 0.938\nF1 Score: 0.965\n\n\n\nsix = five.append(_conf.tab)\n\n\n\nFeature Bagging\n\nclf = FeatureBagging()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['FeatureBagging_clf'] = clf.labels_\n\n\noutlier_FeatureBagging_one = list(clf.labels_)\n\n\noutlier_FeatureBagging_one = list(map(lambda x: 1 if x==0  else -1,outlier_FeatureBagging_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_FeatureBagging_one,tab_bunny)\n\n\n_conf.conf(\"Feature Bagging (Lazarevic and Kumar, 2005)\")\n\n\n\n\nAccuracy: 0.915\nPrecision: 0.982\nRecall: 0.928\nF1 Score: 0.954\n\n\n\nseven = six.append(_conf.tab)\n\n\n\nABOD\n\nclf = ABOD(contamination=0.05)\nclf.fit(_df[['x', 'y','fnoise']])\n_df['ABOD_Clf'] = clf.labels_\n\n\noutlier_ABOD_one = list(clf.labels_)\n\n\noutlier_ABOD_one = list(map(lambda x: 1 if x==0  else -1,outlier_ABOD_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_ABOD_one,tab_bunny)\n\n\n_conf.conf(\"ABOD (Kriegel et al., 2008)\")\n\n\n\n\nAccuracy: 0.977\nPrecision: 0.989\nRecall: 0.987\nF1 Score: 0.988\n\n\n\neight = seven.append(_conf.tab)\n\n\n\nIForest\n\nod = IForest(\n    threshold=0.,\n    n_estimators=100\n)\n\n\nod.fit(_df[['x', 'y','fnoise']])\n\n\npreds = od.predict(\n    _df[['x', 'y','fnoise']],\n    return_instance_score=True\n)\n\n\n_df['IF_alibi'] = preds['data']['is_outlier']\n\n\noutlier_alibi_one = _df['IF_alibi']\n\n\noutlier_alibi_one = list(map(lambda x: 1 if x==0  else -1,outlier_alibi_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_alibi_one,tab_bunny)\n\n\n_conf.conf(\"Isolation Forest (Liu et al., 2008)\")\n\n\n\n\nAccuracy: 0.794\nPrecision: 0.995\nRecall: 0.788\nF1 Score: 0.879\n\n\n\nnine = eight.append(_conf.tab)\n\n\n\nHBOS\n\nclf = HBOS()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['HBOS_clf'] = clf.labels_\n\n\noutlier_HBOS_one = list(clf.labels_)\n\n\noutlier_HBOS_one = list(map(lambda x: 1 if x==0  else -1,outlier_HBOS_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_HBOS_one,tab_bunny)\n\n\n_conf.conf(\"HBOS (Goldstein and Dengel, 2012)\")\n\n\n\n\nAccuracy: 0.895\nPrecision: 0.969\nRecall: 0.919\nF1 Score: 0.944\n\n\n\nten = nine.append(_conf.tab)\n\n\n\nSOS\n\noutlier_SOS_one = list(clf.labels_)\n\n\noutlier_SOS_one = list(map(lambda x: 1 if x==0  else -1,outlier_SOS_one))\n\n\nclf = SOS()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['SOS_clf'] = clf.labels_\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_SOS_one,tab_bunny)\n\n\n_conf.conf(\"SOS (Janssens et al., 2012)\")\n\n\n\n\nAccuracy: 0.895\nPrecision: 0.969\nRecall: 0.919\nF1 Score: 0.944\n\n\n\neleven = ten.append(_conf.tab)\n\n\n\nSO_GAAL\n\nclf = SO_GAAL()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['SO_GAAL_clf'] = clf.labels_\n\nEpoch 1 of 60\n\nTesting for epoch 1 index 1:\n\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n\n\n\nTesting for epoch 1 index 2:\n\nTesting for epoch 1 index 3:\n\nTesting for epoch 1 index 4:\n\nTesting for epoch 1 index 5:\nEpoch 2 of 60\n\nTesting for epoch 2 index 1:\n\nTesting for epoch 2 index 2:\n\nTesting for epoch 2 index 3:\n\nTesting for epoch 2 index 4:\n\nTesting for epoch 2 index 5:\nEpoch 3 of 60\n\nTesting for epoch 3 index 1:\n\nTesting for epoch 3 index 2:\n\nTesting for epoch 3 index 3:\n\nTesting for epoch 3 index 4:\n\nTesting for epoch 3 index 5:\nEpoch 4 of 60\n\nTesting for epoch 4 index 1:\n\nTesting for epoch 4 index 2:\n\nTesting for epoch 4 index 3:\n\nTesting for epoch 4 index 4:\n\nTesting for epoch 4 index 5:\nEpoch 5 of 60\n\nTesting for epoch 5 index 1:\n\nTesting for epoch 5 index 2:\n\nTesting for epoch 5 index 3:\n\nTesting for epoch 5 index 4:\n\nTesting for epoch 5 index 5:\nEpoch 6 of 60\n\nTesting for epoch 6 index 1:\n\nTesting for epoch 6 index 2:\n\nTesting for epoch 6 index 3:\n\nTesting for epoch 6 index 4:\n\nTesting for epoch 6 index 5:\nEpoch 7 of 60\n\nTesting for epoch 7 index 1:\n\nTesting for epoch 7 index 2:\n\nTesting for epoch 7 index 3:\n\nTesting for epoch 7 index 4:\n\nTesting for epoch 7 index 5:\nEpoch 8 of 60\n\nTesting for epoch 8 index 1:\n\nTesting for epoch 8 index 2:\n\nTesting for epoch 8 index 3:\n\nTesting for epoch 8 index 4:\n\nTesting for epoch 8 index 5:\nEpoch 9 of 60\n\nTesting for epoch 9 index 1:\n\nTesting for epoch 9 index 2:\n\nTesting for epoch 9 index 3:\n\nTesting for epoch 9 index 4:\n\nTesting for epoch 9 index 5:\nEpoch 10 of 60\n\nTesting for epoch 10 index 1:\n\nTesting for epoch 10 index 2:\n\nTesting for epoch 10 index 3:\n\nTesting for epoch 10 index 4:\n\nTesting for epoch 10 index 5:\nEpoch 11 of 60\n\nTesting for epoch 11 index 1:\n\nTesting for epoch 11 index 2:\n\nTesting for epoch 11 index 3:\n\nTesting for epoch 11 index 4:\n\nTesting for epoch 11 index 5:\nEpoch 12 of 60\n\nTesting for epoch 12 index 1:\n\nTesting for epoch 12 index 2:\n\nTesting for epoch 12 index 3:\n\nTesting for epoch 12 index 4:\n\nTesting for epoch 12 index 5:\nEpoch 13 of 60\n\nTesting for epoch 13 index 1:\n\nTesting for epoch 13 index 2:\n\nTesting for epoch 13 index 3:\n\nTesting for epoch 13 index 4:\n\nTesting for epoch 13 index 5:\nEpoch 14 of 60\n\nTesting for epoch 14 index 1:\n\nTesting for epoch 14 index 2:\n\nTesting for epoch 14 index 3:\n\nTesting for epoch 14 index 4:\n\nTesting for epoch 14 index 5:\nEpoch 15 of 60\n\nTesting for epoch 15 index 1:\n\nTesting for epoch 15 index 2:\n\nTesting for epoch 15 index 3:\n\nTesting for epoch 15 index 4:\n\nTesting for epoch 15 index 5:\nEpoch 16 of 60\n\nTesting for epoch 16 index 1:\n\nTesting for epoch 16 index 2:\n\nTesting for epoch 16 index 3:\n\nTesting for epoch 16 index 4:\n\nTesting for epoch 16 index 5:\nEpoch 17 of 60\n\nTesting for epoch 17 index 1:\n\nTesting for epoch 17 index 2:\n\nTesting for epoch 17 index 3:\n\nTesting for epoch 17 index 4:\n\nTesting for epoch 17 index 5:\nEpoch 18 of 60\n\nTesting for epoch 18 index 1:\n\nTesting for epoch 18 index 2:\n\nTesting for epoch 18 index 3:\n\nTesting for epoch 18 index 4:\n\nTesting for epoch 18 index 5:\nEpoch 19 of 60\n\nTesting for epoch 19 index 1:\n\nTesting for epoch 19 index 2:\n\nTesting for epoch 19 index 3:\n\nTesting for epoch 19 index 4:\n\nTesting for epoch 19 index 5:\nEpoch 20 of 60\n\nTesting for epoch 20 index 1:\n\nTesting for epoch 20 index 2:\n\nTesting for epoch 20 index 3:\n\nTesting for epoch 20 index 4:\n\nTesting for epoch 20 index 5:\nEpoch 21 of 60\n\nTesting for epoch 21 index 1:\n\nTesting for epoch 21 index 2:\n\nTesting for epoch 21 index 3:\n\nTesting for epoch 21 index 4:\n\nTesting for epoch 21 index 5:\nEpoch 22 of 60\n\nTesting for epoch 22 index 1:\n16/16 [==============================] - 0s 894us/step - loss: 1.8529\n\nTesting for epoch 22 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8921\n\nTesting for epoch 22 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9309\n\nTesting for epoch 22 index 4:\n16/16 [==============================] - 0s 690us/step - loss: 1.8584\n\nTesting for epoch 22 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8820\nEpoch 23 of 60\n\nTesting for epoch 23 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9128\n\nTesting for epoch 23 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9055\n\nTesting for epoch 23 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9463\n\nTesting for epoch 23 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9150\n\nTesting for epoch 23 index 5:\n16/16 [==============================] - 0s 755us/step - loss: 1.9138\nEpoch 24 of 60\n\nTesting for epoch 24 index 1:\n16/16 [==============================] - 0s 636us/step - loss: 2.0252\n\nTesting for epoch 24 index 2:\n16/16 [==============================] - 0s 640us/step - loss: 1.9456\n\nTesting for epoch 24 index 3:\n16/16 [==============================] - 0s 701us/step - loss: 1.9662\n\nTesting for epoch 24 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9841\n\nTesting for epoch 24 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0037\nEpoch 25 of 60\n\nTesting for epoch 25 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9889\n\nTesting for epoch 25 index 2:\n16/16 [==============================] - 0s 871us/step - loss: 1.9856\n\nTesting for epoch 25 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0014\n\nTesting for epoch 25 index 4:\n16/16 [==============================] - 0s 778us/step - loss: 2.0162\n\nTesting for epoch 25 index 5:\n16/16 [==============================] - 0s 664us/step - loss: 2.0739\nEpoch 26 of 60\n\nTesting for epoch 26 index 1:\n16/16 [==============================] - 0s 637us/step - loss: 2.0179\n\nTesting for epoch 26 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0133\n\nTesting for epoch 26 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0655\n\nTesting for epoch 26 index 4:\n16/16 [==============================] - 0s 637us/step - loss: 2.0657\n\nTesting for epoch 26 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0669\nEpoch 27 of 60\n\nTesting for epoch 27 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0880\n\nTesting for epoch 27 index 2:\n16/16 [==============================] - 0s 800us/step - loss: 2.0889\n\nTesting for epoch 27 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1112\n\nTesting for epoch 27 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0641\n\nTesting for epoch 27 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0520\nEpoch 28 of 60\n\nTesting for epoch 28 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0533\n\nTesting for epoch 28 index 2:\n16/16 [==============================] - 0s 601us/step - loss: 2.1067\n\nTesting for epoch 28 index 3:\n16/16 [==============================] - 0s 645us/step - loss: 2.1065\n\nTesting for epoch 28 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0956\n\nTesting for epoch 28 index 5:\n16/16 [==============================] - 0s 634us/step - loss: 2.0811\nEpoch 29 of 60\n\nTesting for epoch 29 index 1:\n16/16 [==============================] - 0s 633us/step - loss: 2.0727\n\nTesting for epoch 29 index 2:\n16/16 [==============================] - 0s 687us/step - loss: 2.1834\n\nTesting for epoch 29 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0984\n\nTesting for epoch 29 index 4:\n16/16 [==============================] - 0s 599us/step - loss: 2.1578\n\nTesting for epoch 29 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1489\nEpoch 30 of 60\n\nTesting for epoch 30 index 1:\n16/16 [==============================] - 0s 671us/step - loss: 2.1636\n\nTesting for epoch 30 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1516\n\nTesting for epoch 30 index 3:\n16/16 [==============================] - 0s 636us/step - loss: 2.1534\n\nTesting for epoch 30 index 4:\n16/16 [==============================] - 0s 776us/step - loss: 2.1465\n\nTesting for epoch 30 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1006\nEpoch 31 of 60\n\nTesting for epoch 31 index 1:\n16/16 [==============================] - 0s 768us/step - loss: 2.1580\n\nTesting for epoch 31 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1679\n\nTesting for epoch 31 index 3:\n16/16 [==============================] - 0s 932us/step - loss: 2.1854\n\nTesting for epoch 31 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1869\n\nTesting for epoch 31 index 5:\n16/16 [==============================] - 0s 600us/step - loss: 2.1570\nEpoch 32 of 60\n\nTesting for epoch 32 index 1:\n16/16 [==============================] - 0s 701us/step - loss: 2.2004\n\nTesting for epoch 32 index 2:\n16/16 [==============================] - 0s 664us/step - loss: 2.2094\n\nTesting for epoch 32 index 3:\n16/16 [==============================] - 0s 665us/step - loss: 2.2316\n\nTesting for epoch 32 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1808\n\nTesting for epoch 32 index 5:\n16/16 [==============================] - 0s 606us/step - loss: 2.2633\nEpoch 33 of 60\n\nTesting for epoch 33 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2481\n\nTesting for epoch 33 index 2:\n16/16 [==============================] - 0s 629us/step - loss: 2.2154\n\nTesting for epoch 33 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2065\n\nTesting for epoch 33 index 4:\n16/16 [==============================] - 0s 632us/step - loss: 2.2313\n\nTesting for epoch 33 index 5:\n16/16 [==============================] - 0s 728us/step - loss: 2.2298\nEpoch 34 of 60\n\nTesting for epoch 34 index 1:\n16/16 [==============================] - 0s 651us/step - loss: 2.2541\n\nTesting for epoch 34 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2413\n\nTesting for epoch 34 index 3:\n16/16 [==============================] - 0s 665us/step - loss: 2.1930\n\nTesting for epoch 34 index 4:\n16/16 [==============================] - 0s 607us/step - loss: 2.2856\n\nTesting for epoch 34 index 5:\n16/16 [==============================] - 0s 650us/step - loss: 2.2537\nEpoch 35 of 60\n\nTesting for epoch 35 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2461\n\nTesting for epoch 35 index 2:\n16/16 [==============================] - 0s 654us/step - loss: 2.3097\n\nTesting for epoch 35 index 3:\n16/16 [==============================] - 0s 831us/step - loss: 2.3159\n\nTesting for epoch 35 index 4:\n16/16 [==============================] - 0s 934us/step - loss: 2.2306\n\nTesting for epoch 35 index 5:\n16/16 [==============================] - 0s 654us/step - loss: 2.2956\nEpoch 36 of 60\n\nTesting for epoch 36 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2296\n\nTesting for epoch 36 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2378\n\nTesting for epoch 36 index 3:\n16/16 [==============================] - 0s 926us/step - loss: 2.2114\n\nTesting for epoch 36 index 4:\n16/16 [==============================] - 0s 716us/step - loss: 2.2166\n\nTesting for epoch 36 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2483\nEpoch 37 of 60\n\nTesting for epoch 37 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2669\n\nTesting for epoch 37 index 2:\n16/16 [==============================] - 0s 718us/step - loss: 2.2966\n\nTesting for epoch 37 index 3:\n16/16 [==============================] - 0s 776us/step - loss: 2.2346\n\nTesting for epoch 37 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3040\n\nTesting for epoch 37 index 5:\n16/16 [==============================] - 0s 780us/step - loss: 2.3003\nEpoch 38 of 60\n\nTesting for epoch 38 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2809\n\nTesting for epoch 38 index 2:\n16/16 [==============================] - 0s 789us/step - loss: 2.2804\n\nTesting for epoch 38 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2915\n\nTesting for epoch 38 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2829\n\nTesting for epoch 38 index 5:\n16/16 [==============================] - 0s 923us/step - loss: 2.3199\nEpoch 39 of 60\n\nTesting for epoch 39 index 1:\n16/16 [==============================] - 0s 980us/step - loss: 2.2642\n\nTesting for epoch 39 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3208\n\nTesting for epoch 39 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3127\n\nTesting for epoch 39 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3514\n\nTesting for epoch 39 index 5:\n16/16 [==============================] - 0s 829us/step - loss: 2.3363\nEpoch 40 of 60\n\nTesting for epoch 40 index 1:\n16/16 [==============================] - 0s 629us/step - loss: 2.3203\n\nTesting for epoch 40 index 2:\n16/16 [==============================] - 0s 658us/step - loss: 2.3100\n\nTesting for epoch 40 index 3:\n16/16 [==============================] - 0s 625us/step - loss: 2.2837\n\nTesting for epoch 40 index 4:\n16/16 [==============================] - 0s 640us/step - loss: 2.2877\n\nTesting for epoch 40 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3374\nEpoch 41 of 60\n\nTesting for epoch 41 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3149\n\nTesting for epoch 41 index 2:\n16/16 [==============================] - 0s 658us/step - loss: 2.3535\n\nTesting for epoch 41 index 3:\n16/16 [==============================] - 0s 652us/step - loss: 2.3861\n\nTesting for epoch 41 index 4:\n16/16 [==============================] - 0s 723us/step - loss: 2.3328\n\nTesting for epoch 41 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3450\nEpoch 42 of 60\n\nTesting for epoch 42 index 1:\n16/16 [==============================] - 0s 641us/step - loss: 2.3578\n\nTesting for epoch 42 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3235\n\nTesting for epoch 42 index 3:\n16/16 [==============================] - 0s 958us/step - loss: 2.3421\n\nTesting for epoch 42 index 4:\n16/16 [==============================] - 0s 593us/step - loss: 2.3656\n\nTesting for epoch 42 index 5:\n16/16 [==============================] - 0s 623us/step - loss: 2.3044\nEpoch 43 of 60\n\nTesting for epoch 43 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3273\n\nTesting for epoch 43 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3797\n\nTesting for epoch 43 index 3:\n16/16 [==============================] - 0s 654us/step - loss: 2.3372\n\nTesting for epoch 43 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3387\n\nTesting for epoch 43 index 5:\n16/16 [==============================] - 0s 608us/step - loss: 2.4377\nEpoch 44 of 60\n\nTesting for epoch 44 index 1:\n16/16 [==============================] - 0s 965us/step - loss: 2.4568\n\nTesting for epoch 44 index 2:\n16/16 [==============================] - 0s 640us/step - loss: 2.4050\n\nTesting for epoch 44 index 3:\n16/16 [==============================] - 0s 943us/step - loss: 2.3936\n\nTesting for epoch 44 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3910\n\nTesting for epoch 44 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4026\nEpoch 45 of 60\n\nTesting for epoch 45 index 1:\n16/16 [==============================] - 0s 647us/step - loss: 2.4177\n\nTesting for epoch 45 index 2:\n16/16 [==============================] - 0s 941us/step - loss: 2.4015\n\nTesting for epoch 45 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3971\n\nTesting for epoch 45 index 4:\n16/16 [==============================] - 0s 678us/step - loss: 2.3933\n\nTesting for epoch 45 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4488\nEpoch 46 of 60\n\nTesting for epoch 46 index 1:\n16/16 [==============================] - 0s 594us/step - loss: 2.3598\n\nTesting for epoch 46 index 2:\n16/16 [==============================] - 0s 655us/step - loss: 2.4883\n\nTesting for epoch 46 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4234\n\nTesting for epoch 46 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3641\n\nTesting for epoch 46 index 5:\n16/16 [==============================] - 0s 649us/step - loss: 2.4212\nEpoch 47 of 60\n\nTesting for epoch 47 index 1:\n16/16 [==============================] - 0s 828us/step - loss: 2.5119\n\nTesting for epoch 47 index 2:\n16/16 [==============================] - 0s 625us/step - loss: 2.4255\n\nTesting for epoch 47 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4828\n\nTesting for epoch 47 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4336\n\nTesting for epoch 47 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3916\nEpoch 48 of 60\n\nTesting for epoch 48 index 1:\n16/16 [==============================] - 0s 630us/step - loss: 2.4157\n\nTesting for epoch 48 index 2:\n16/16 [==============================] - 0s 621us/step - loss: 2.4543\n\nTesting for epoch 48 index 3:\n16/16 [==============================] - 0s 672us/step - loss: 2.3956\n\nTesting for epoch 48 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4783\n\nTesting for epoch 48 index 5:\n16/16 [==============================] - 0s 630us/step - loss: 2.4045\nEpoch 49 of 60\n\nTesting for epoch 49 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4787\n\nTesting for epoch 49 index 2:\n16/16 [==============================] - 0s 880us/step - loss: 2.4557\n\nTesting for epoch 49 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4497\n\nTesting for epoch 49 index 4:\n16/16 [==============================] - 0s 635us/step - loss: 2.4115\n\nTesting for epoch 49 index 5:\n16/16 [==============================] - 0s 613us/step - loss: 2.4469\nEpoch 50 of 60\n\nTesting for epoch 50 index 1:\n16/16 [==============================] - 0s 764us/step - loss: 2.4250\n\nTesting for epoch 50 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4706\n\nTesting for epoch 50 index 3:\n16/16 [==============================] - 0s 620us/step - loss: 2.3919\n\nTesting for epoch 50 index 4:\n16/16 [==============================] - 0s 698us/step - loss: 2.4463\n\nTesting for epoch 50 index 5:\n16/16 [==============================] - 0s 958us/step - loss: 2.4810\nEpoch 51 of 60\n\nTesting for epoch 51 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4359\n\nTesting for epoch 51 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4080\n\nTesting for epoch 51 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4634\n\nTesting for epoch 51 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5226\n\nTesting for epoch 51 index 5:\n16/16 [==============================] - 0s 894us/step - loss: 2.4385\nEpoch 52 of 60\n\nTesting for epoch 52 index 1:\n16/16 [==============================] - 0s 856us/step - loss: 2.5063\n\nTesting for epoch 52 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4672\n\nTesting for epoch 52 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5011\n\nTesting for epoch 52 index 4:\n16/16 [==============================] - 0s 618us/step - loss: 2.5610\n\nTesting for epoch 52 index 5:\n16/16 [==============================] - 0s 679us/step - loss: 2.5239\nEpoch 53 of 60\n\nTesting for epoch 53 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5248\n\nTesting for epoch 53 index 2:\n16/16 [==============================] - 0s 990us/step - loss: 2.5142\n\nTesting for epoch 53 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5164\n\nTesting for epoch 53 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3996\n\nTesting for epoch 53 index 5:\n16/16 [==============================] - 0s 894us/step - loss: 2.4939\nEpoch 54 of 60\n\nTesting for epoch 54 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4897\n\nTesting for epoch 54 index 2:\n16/16 [==============================] - 0s 617us/step - loss: 2.5320\n\nTesting for epoch 54 index 3:\n16/16 [==============================] - 0s 619us/step - loss: 2.5544\n\nTesting for epoch 54 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4986\n\nTesting for epoch 54 index 5:\n16/16 [==============================] - 0s 648us/step - loss: 2.5618\nEpoch 55 of 60\n\nTesting for epoch 55 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5605\n\nTesting for epoch 55 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4780\n\nTesting for epoch 55 index 3:\n16/16 [==============================] - 0s 665us/step - loss: 2.4659\n\nTesting for epoch 55 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4696\n\nTesting for epoch 55 index 5:\n16/16 [==============================] - 0s 643us/step - loss: 2.5610\nEpoch 56 of 60\n\nTesting for epoch 56 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4586\n\nTesting for epoch 56 index 2:\n16/16 [==============================] - 0s 665us/step - loss: 2.4735\n\nTesting for epoch 56 index 3:\n16/16 [==============================] - 0s 964us/step - loss: 2.5013\n\nTesting for epoch 56 index 4:\n16/16 [==============================] - 0s 840us/step - loss: 2.4765\n\nTesting for epoch 56 index 5:\n16/16 [==============================] - 0s 908us/step - loss: 2.5925\nEpoch 57 of 60\n\nTesting for epoch 57 index 1:\n16/16 [==============================] - 0s 644us/step - loss: 2.5213\n\nTesting for epoch 57 index 2:\n16/16 [==============================] - 0s 624us/step - loss: 2.5540\n\nTesting for epoch 57 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5273\n\nTesting for epoch 57 index 4:\n16/16 [==============================] - 0s 665us/step - loss: 2.5155\n\nTesting for epoch 57 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5001\nEpoch 58 of 60\n\nTesting for epoch 58 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5154\n\nTesting for epoch 58 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5593\n\nTesting for epoch 58 index 3:\n16/16 [==============================] - 0s 653us/step - loss: 2.4897\n\nTesting for epoch 58 index 4:\n16/16 [==============================] - 0s 621us/step - loss: 2.5391\n\nTesting for epoch 58 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5966\nEpoch 59 of 60\n\nTesting for epoch 59 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5325\n\nTesting for epoch 59 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5563\n\nTesting for epoch 59 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4993\n\nTesting for epoch 59 index 4:\n16/16 [==============================] - 0s 625us/step - loss: 2.5589\n\nTesting for epoch 59 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5403\nEpoch 60 of 60\n\nTesting for epoch 60 index 1:\n16/16 [==============================] - 0s 833us/step - loss: 2.5143\n\nTesting for epoch 60 index 2:\n16/16 [==============================] - 0s 808us/step - loss: 2.5618\n\nTesting for epoch 60 index 3:\n16/16 [==============================] - 0s 796us/step - loss: 2.5960\n\nTesting for epoch 60 index 4:\n16/16 [==============================] - 0s 599us/step - loss: 2.5405\n\nTesting for epoch 60 index 5:\n16/16 [==============================] - 0s 650us/step - loss: 2.5440\n\n\n\noutlier_SO_GAAL_one = list(clf.labels_)\n\n\noutlier_SO_GAAL_one = list(map(lambda x: 1 if x==0  else -1,outlier_SO_GAAL_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_SO_GAAL_one,tab_bunny)\n\n\n_conf.conf(\"SO-GAAL (Liu et al., 2019)\")\n\n\n\n\nAccuracy: 0.952\nPrecision: 0.952\nRecall: 1.000\nF1 Score: 0.975\n\n\n\ntwelve = eleven.append(_conf.tab)\n\n\n\nMO_GAAL\n\nclf = MO_GAAL()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['MO_GAAL_clf'] = clf.labels_\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n\n\nEpoch 1 of 60\n\nTesting for epoch 1 index 1:\n\nTesting for epoch 1 index 2:\n\nTesting for epoch 1 index 3:\n\nTesting for epoch 1 index 4:\n\nTesting for epoch 1 index 5:\nEpoch 2 of 60\n\nTesting for epoch 2 index 1:\n\nTesting for epoch 2 index 2:\n\nTesting for epoch 2 index 3:\n\nTesting for epoch 2 index 4:\n\nTesting for epoch 2 index 5:\nEpoch 3 of 60\n\nTesting for epoch 3 index 1:\n\nTesting for epoch 3 index 2:\n\nTesting for epoch 3 index 3:\n\nTesting for epoch 3 index 4:\n\nTesting for epoch 3 index 5:\nEpoch 4 of 60\n\nTesting for epoch 4 index 1:\n\nTesting for epoch 4 index 2:\n\nTesting for epoch 4 index 3:\n\nTesting for epoch 4 index 4:\n\nTesting for epoch 4 index 5:\nEpoch 5 of 60\n\nTesting for epoch 5 index 1:\n\nTesting for epoch 5 index 2:\n\nTesting for epoch 5 index 3:\n\nTesting for epoch 5 index 4:\n\nTesting for epoch 5 index 5:\nEpoch 6 of 60\n\nTesting for epoch 6 index 1:\n\nTesting for epoch 6 index 2:\n\nTesting for epoch 6 index 3:\n\nTesting for epoch 6 index 4:\n\nTesting for epoch 6 index 5:\nEpoch 7 of 60\n\nTesting for epoch 7 index 1:\n\nTesting for epoch 7 index 2:\n\nTesting for epoch 7 index 3:\n\nTesting for epoch 7 index 4:\n\nTesting for epoch 7 index 5:\nEpoch 8 of 60\n\nTesting for epoch 8 index 1:\n\nTesting for epoch 8 index 2:\n\nTesting for epoch 8 index 3:\n\nTesting for epoch 8 index 4:\n\nTesting for epoch 8 index 5:\nEpoch 9 of 60\n\nTesting for epoch 9 index 1:\n\nTesting for epoch 9 index 2:\n\nTesting for epoch 9 index 3:\n\nTesting for epoch 9 index 4:\n\nTesting for epoch 9 index 5:\nEpoch 10 of 60\n\nTesting for epoch 10 index 1:\n\nTesting for epoch 10 index 2:\n\nTesting for epoch 10 index 3:\n\nTesting for epoch 10 index 4:\n\nTesting for epoch 10 index 5:\nEpoch 11 of 60\n\nTesting for epoch 11 index 1:\n\nTesting for epoch 11 index 2:\n\nTesting for epoch 11 index 3:\n\nTesting for epoch 11 index 4:\n\nTesting for epoch 11 index 5:\nEpoch 12 of 60\n\nTesting for epoch 12 index 1:\n\nTesting for epoch 12 index 2:\n\nTesting for epoch 12 index 3:\n\nTesting for epoch 12 index 4:\n\nTesting for epoch 12 index 5:\nEpoch 13 of 60\n\nTesting for epoch 13 index 1:\n\nTesting for epoch 13 index 2:\n\nTesting for epoch 13 index 3:\n\nTesting for epoch 13 index 4:\n\nTesting for epoch 13 index 5:\nEpoch 14 of 60\n\nTesting for epoch 14 index 1:\n\nTesting for epoch 14 index 2:\n\nTesting for epoch 14 index 3:\n\nTesting for epoch 14 index 4:\n\nTesting for epoch 14 index 5:\nEpoch 15 of 60\n\nTesting for epoch 15 index 1:\n\nTesting for epoch 15 index 2:\n\nTesting for epoch 15 index 3:\n\nTesting for epoch 15 index 4:\n\nTesting for epoch 15 index 5:\nEpoch 16 of 60\n\nTesting for epoch 16 index 1:\n\nTesting for epoch 16 index 2:\n\nTesting for epoch 16 index 3:\n\nTesting for epoch 16 index 4:\n\nTesting for epoch 16 index 5:\nEpoch 17 of 60\n\nTesting for epoch 17 index 1:\n\nTesting for epoch 17 index 2:\n\nTesting for epoch 17 index 3:\n\nTesting for epoch 17 index 4:\n\nTesting for epoch 17 index 5:\nEpoch 18 of 60\n\nTesting for epoch 18 index 1:\n\nTesting for epoch 18 index 2:\n\nTesting for epoch 18 index 3:\n\nTesting for epoch 18 index 4:\n\nTesting for epoch 18 index 5:\nEpoch 19 of 60\n\nTesting for epoch 19 index 1:\n\nTesting for epoch 19 index 2:\n\nTesting for epoch 19 index 3:\n\nTesting for epoch 19 index 4:\n\nTesting for epoch 19 index 5:\nEpoch 20 of 60\n\nTesting for epoch 20 index 1:\n\nTesting for epoch 20 index 2:\n\nTesting for epoch 20 index 3:\n\nTesting for epoch 20 index 4:\n\nTesting for epoch 20 index 5:\nEpoch 21 of 60\n\nTesting for epoch 21 index 1:\n\nTesting for epoch 21 index 2:\n16/16 [==============================] - 0s 839us/step - loss: 0.2862\n16/16 [==============================] - 0s 1ms/step - loss: 1.3562\n16/16 [==============================] - 0s 879us/step - loss: 1.6391\n16/16 [==============================] - 0s 676us/step - loss: 1.7457\n16/16 [==============================] - 0s 668us/step - loss: 1.7800\n16/16 [==============================] - 0s 797us/step - loss: 1.7893\n16/16 [==============================] - 0s 1ms/step - loss: 1.7882\n16/16 [==============================] - 0s 750us/step - loss: 1.7810\n16/16 [==============================] - 0s 661us/step - loss: 1.7768\n16/16 [==============================] - 0s 1ms/step - loss: 1.7746\n\nTesting for epoch 21 index 3:\n16/16 [==============================] - 0s 709us/step - loss: 0.2829\n16/16 [==============================] - 0s 1ms/step - loss: 1.3627\n16/16 [==============================] - 0s 1ms/step - loss: 1.6520\n16/16 [==============================] - 0s 1ms/step - loss: 1.7617\n16/16 [==============================] - 0s 648us/step - loss: 1.7969\n16/16 [==============================] - 0s 655us/step - loss: 1.8064\n16/16 [==============================] - 0s 1ms/step - loss: 1.8050\n16/16 [==============================] - 0s 1ms/step - loss: 1.7975\n16/16 [==============================] - 0s 1ms/step - loss: 1.7932\n16/16 [==============================] - 0s 1ms/step - loss: 1.7909\n\nTesting for epoch 21 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2819\n16/16 [==============================] - 0s 662us/step - loss: 1.3750\n16/16 [==============================] - 0s 649us/step - loss: 1.6692\n16/16 [==============================] - 0s 664us/step - loss: 1.7821\n16/16 [==============================] - 0s 644us/step - loss: 1.8194\n16/16 [==============================] - 0s 671us/step - loss: 1.8316\n16/16 [==============================] - 0s 651us/step - loss: 1.8318\n16/16 [==============================] - 0s 661us/step - loss: 1.8249\n16/16 [==============================] - 0s 990us/step - loss: 1.8208\n16/16 [==============================] - 0s 1ms/step - loss: 1.8185\n\nTesting for epoch 21 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2784\n16/16 [==============================] - 0s 1ms/step - loss: 1.3590\n16/16 [==============================] - 0s 645us/step - loss: 1.6490\n16/16 [==============================] - 0s 1ms/step - loss: 1.7586\n16/16 [==============================] - 0s 655us/step - loss: 1.7914\n16/16 [==============================] - 0s 1ms/step - loss: 1.7998\n16/16 [==============================] - 0s 1ms/step - loss: 1.7975\n16/16 [==============================] - 0s 653us/step - loss: 1.7896\n16/16 [==============================] - 0s 675us/step - loss: 1.7852\n16/16 [==============================] - 0s 1ms/step - loss: 1.7829\nEpoch 22 of 60\n\nTesting for epoch 22 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2787\n16/16 [==============================] - 0s 1ms/step - loss: 1.3475\n16/16 [==============================] - 0s 646us/step - loss: 1.6341\n16/16 [==============================] - 0s 690us/step - loss: 1.7422\n16/16 [==============================] - 0s 1ms/step - loss: 1.7757\n16/16 [==============================] - 0s 1ms/step - loss: 1.7855\n16/16 [==============================] - 0s 1ms/step - loss: 1.7843\n16/16 [==============================] - 0s 1ms/step - loss: 1.7771\n16/16 [==============================] - 0s 1ms/step - loss: 1.7729\n16/16 [==============================] - 0s 1ms/step - loss: 1.7708\n\nTesting for epoch 22 index 2:\n16/16 [==============================] - 0s 680us/step - loss: 0.2756\n16/16 [==============================] - 0s 1ms/step - loss: 1.3616\n16/16 [==============================] - 0s 678us/step - loss: 1.6485\n16/16 [==============================] - 0s 946us/step - loss: 1.7539\n16/16 [==============================] - 0s 673us/step - loss: 1.7847\n16/16 [==============================] - 0s 656us/step - loss: 1.7921\n16/16 [==============================] - 0s 1ms/step - loss: 1.7895\n16/16 [==============================] - 0s 660us/step - loss: 1.7812\n16/16 [==============================] - 0s 1ms/step - loss: 1.7768\n16/16 [==============================] - 0s 730us/step - loss: 1.7745\n\nTesting for epoch 22 index 3:\n16/16 [==============================] - 0s 660us/step - loss: 0.2723\n16/16 [==============================] - 0s 1ms/step - loss: 1.3959\n16/16 [==============================] - 0s 642us/step - loss: 1.7002\n16/16 [==============================] - 0s 874us/step - loss: 1.8105\n16/16 [==============================] - 0s 1ms/step - loss: 1.8423\n16/16 [==============================] - 0s 1ms/step - loss: 1.8494\n16/16 [==============================] - 0s 657us/step - loss: 1.8460\n16/16 [==============================] - 0s 940us/step - loss: 1.8371\n16/16 [==============================] - 0s 634us/step - loss: 1.8324\n16/16 [==============================] - 0s 905us/step - loss: 1.8299\n\nTesting for epoch 22 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2702\n16/16 [==============================] - 0s 649us/step - loss: 1.3792\n16/16 [==============================] - 0s 867us/step - loss: 1.6818\n16/16 [==============================] - 0s 619us/step - loss: 1.7923\n16/16 [==============================] - 0s 859us/step - loss: 1.8248\n16/16 [==============================] - 0s 609us/step - loss: 1.8327\n16/16 [==============================] - 0s 1ms/step - loss: 1.8298\n16/16 [==============================] - 0s 584us/step - loss: 1.8209\n16/16 [==============================] - 0s 590us/step - loss: 1.8163\n16/16 [==============================] - 0s 602us/step - loss: 1.8139\n\nTesting for epoch 22 index 5:\n16/16 [==============================] - 0s 683us/step - loss: 0.2694\n16/16 [==============================] - 0s 794us/step - loss: 1.3853\n16/16 [==============================] - 0s 1ms/step - loss: 1.6907\n16/16 [==============================] - 0s 1ms/step - loss: 1.8014\n16/16 [==============================] - 0s 634us/step - loss: 1.8329\n16/16 [==============================] - 0s 1ms/step - loss: 1.8400\n16/16 [==============================] - 0s 694us/step - loss: 1.8367\n16/16 [==============================] - 0s 1ms/step - loss: 1.8275\n16/16 [==============================] - 0s 589us/step - loss: 1.8228\n16/16 [==============================] - 0s 1ms/step - loss: 1.8204\nEpoch 23 of 60\n\nTesting for epoch 23 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2679\n16/16 [==============================] - 0s 1ms/step - loss: 1.4280\n16/16 [==============================] - 0s 1ms/step - loss: 1.7530\n16/16 [==============================] - 0s 1ms/step - loss: 1.8709\n16/16 [==============================] - 0s 1ms/step - loss: 1.9036\n16/16 [==============================] - 0s 643us/step - loss: 1.9107\n16/16 [==============================] - 0s 1ms/step - loss: 1.9074\n16/16 [==============================] - 0s 1ms/step - loss: 1.8981\n16/16 [==============================] - 0s 1ms/step - loss: 1.8933\n16/16 [==============================] - 0s 1ms/step - loss: 1.8908\n\nTesting for epoch 23 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2673\n16/16 [==============================] - 0s 974us/step - loss: 1.4127\n16/16 [==============================] - 0s 820us/step - loss: 1.7343\n16/16 [==============================] - 0s 630us/step - loss: 1.8524\n16/16 [==============================] - 0s 606us/step - loss: 1.8844\n16/16 [==============================] - 0s 719us/step - loss: 1.8917\n16/16 [==============================] - 0s 1ms/step - loss: 1.8882\n16/16 [==============================] - 0s 611us/step - loss: 1.8784\n16/16 [==============================] - 0s 666us/step - loss: 1.8735\n16/16 [==============================] - 0s 1ms/step - loss: 1.8709\n\nTesting for epoch 23 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2645\n16/16 [==============================] - 0s 1ms/step - loss: 1.4021\n16/16 [==============================] - 0s 594us/step - loss: 1.7169\n16/16 [==============================] - 0s 587us/step - loss: 1.8300\n16/16 [==============================] - 0s 613us/step - loss: 1.8582\n16/16 [==============================] - 0s 1ms/step - loss: 1.8634\n16/16 [==============================] - 0s 1ms/step - loss: 1.8590\n16/16 [==============================] - 0s 644us/step - loss: 1.8494\n16/16 [==============================] - 0s 622us/step - loss: 1.8445\n16/16 [==============================] - 0s 617us/step - loss: 1.8420\n\nTesting for epoch 23 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2641\n16/16 [==============================] - 0s 1ms/step - loss: 1.4346\n16/16 [==============================] - 0s 1ms/step - loss: 1.7641\n16/16 [==============================] - 0s 860us/step - loss: 1.8848\n16/16 [==============================] - 0s 1ms/step - loss: 1.9154\n16/16 [==============================] - 0s 962us/step - loss: 1.9222\n16/16 [==============================] - 0s 634us/step - loss: 1.9176\n16/16 [==============================] - 0s 1ms/step - loss: 1.9075\n16/16 [==============================] - 0s 640us/step - loss: 1.9024\n16/16 [==============================] - 0s 1ms/step - loss: 1.8998\n\nTesting for epoch 23 index 5:\n16/16 [==============================] - 0s 935us/step - loss: 0.2571\n16/16 [==============================] - 0s 600us/step - loss: 1.4423\n16/16 [==============================] - 0s 1ms/step - loss: 1.7744\n16/16 [==============================] - 0s 1ms/step - loss: 1.8942\n16/16 [==============================] - 0s 1ms/step - loss: 1.9222\n16/16 [==============================] - 0s 1ms/step - loss: 1.9268\n16/16 [==============================] - 0s 702us/step - loss: 1.9205\n16/16 [==============================] - 0s 637us/step - loss: 1.9092\n16/16 [==============================] - 0s 1ms/step - loss: 1.9038\n16/16 [==============================] - 0s 638us/step - loss: 1.9011\nEpoch 24 of 60\n\nTesting for epoch 24 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2576\n16/16 [==============================] - 0s 1ms/step - loss: 1.4150\n16/16 [==============================] - 0s 586us/step - loss: 1.7381\n16/16 [==============================] - 0s 825us/step - loss: 1.8529\n16/16 [==============================] - 0s 848us/step - loss: 1.8794\n16/16 [==============================] - 0s 716us/step - loss: 1.8834\n16/16 [==============================] - 0s 1ms/step - loss: 1.8775\n16/16 [==============================] - 0s 598us/step - loss: 1.8670\n16/16 [==============================] - 0s 614us/step - loss: 1.8618\n16/16 [==============================] - 0s 1ms/step - loss: 1.8593\n\nTesting for epoch 24 index 2:\n16/16 [==============================] - 0s 604us/step - loss: 0.2602\n16/16 [==============================] - 0s 1ms/step - loss: 1.4321\n16/16 [==============================] - 0s 1ms/step - loss: 1.7581\n16/16 [==============================] - 0s 1ms/step - loss: 1.8731\n16/16 [==============================] - 0s 636us/step - loss: 1.8998\n16/16 [==============================] - 0s 1ms/step - loss: 1.9035\n16/16 [==============================] - 0s 607us/step - loss: 1.8975\n16/16 [==============================] - 0s 1ms/step - loss: 1.8867\n16/16 [==============================] - 0s 646us/step - loss: 1.8815\n16/16 [==============================] - 0s 1ms/step - loss: 1.8790\n\nTesting for epoch 24 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2559\n16/16 [==============================] - 0s 1ms/step - loss: 1.4384\n16/16 [==============================] - 0s 610us/step - loss: 1.7673\n16/16 [==============================] - 0s 1ms/step - loss: 1.8808\n16/16 [==============================] - 0s 663us/step - loss: 1.9062\n16/16 [==============================] - 0s 844us/step - loss: 1.9082\n16/16 [==============================] - 0s 780us/step - loss: 1.9007\n16/16 [==============================] - 0s 602us/step - loss: 1.8887\n16/16 [==============================] - 0s 735us/step - loss: 1.8831\n16/16 [==============================] - 0s 1ms/step - loss: 1.8805\n\nTesting for epoch 24 index 4:\n16/16 [==============================] - 0s 828us/step - loss: 0.2595\n16/16 [==============================] - 0s 1ms/step - loss: 1.4660\n16/16 [==============================] - 0s 1ms/step - loss: 1.8046\n16/16 [==============================] - 0s 1ms/step - loss: 1.9238\n16/16 [==============================] - 0s 613us/step - loss: 1.9510\n16/16 [==============================] - 0s 613us/step - loss: 1.9550\n16/16 [==============================] - 0s 607us/step - loss: 1.9486\n16/16 [==============================] - 0s 600us/step - loss: 1.9375\n16/16 [==============================] - 0s 1ms/step - loss: 1.9321\n16/16 [==============================] - 0s 1ms/step - loss: 1.9295\n\nTesting for epoch 24 index 5:\n16/16 [==============================] - 0s 602us/step - loss: 0.2490\n16/16 [==============================] - 0s 783us/step - loss: 1.4405\n16/16 [==============================] - 0s 962us/step - loss: 1.7687\n16/16 [==============================] - 0s 1ms/step - loss: 1.8795\n16/16 [==============================] - 0s 1ms/step - loss: 1.9005\n16/16 [==============================] - 0s 1ms/step - loss: 1.8995\n16/16 [==============================] - 0s 1ms/step - loss: 1.8901\n16/16 [==============================] - 0s 1ms/step - loss: 1.8774\n16/16 [==============================] - 0s 1ms/step - loss: 1.8717\n16/16 [==============================] - 0s 1ms/step - loss: 1.8690\nEpoch 25 of 60\n\nTesting for epoch 25 index 1:\n16/16 [==============================] - 0s 951us/step - loss: 0.2496\n16/16 [==============================] - 0s 610us/step - loss: 1.4539\n16/16 [==============================] - 0s 1ms/step - loss: 1.7891\n16/16 [==============================] - 0s 1ms/step - loss: 1.9046\n16/16 [==============================] - 0s 628us/step - loss: 1.9285\n16/16 [==============================] - 0s 645us/step - loss: 1.9295\n16/16 [==============================] - 0s 1ms/step - loss: 1.9219\n16/16 [==============================] - 0s 602us/step - loss: 1.9101\n16/16 [==============================] - 0s 1ms/step - loss: 1.9046\n16/16 [==============================] - 0s 888us/step - loss: 1.9020\n\nTesting for epoch 25 index 2:\n16/16 [==============================] - 0s 647us/step - loss: 0.2496\n16/16 [==============================] - 0s 600us/step - loss: 1.4771\n16/16 [==============================] - 0s 677us/step - loss: 1.8228\n16/16 [==============================] - 0s 1ms/step - loss: 1.9419\n16/16 [==============================] - 0s 663us/step - loss: 1.9656\n16/16 [==============================] - 0s 1ms/step - loss: 1.9663\n16/16 [==============================] - 0s 1ms/step - loss: 1.9579\n16/16 [==============================] - 0s 629us/step - loss: 1.9450\n16/16 [==============================] - 0s 638us/step - loss: 1.9393\n16/16 [==============================] - 0s 1ms/step - loss: 1.9365\n\nTesting for epoch 25 index 3:\n16/16 [==============================] - 0s 652us/step - loss: 0.2531\n16/16 [==============================] - 0s 591us/step - loss: 1.4810\n16/16 [==============================] - 0s 613us/step - loss: 1.8268\n16/16 [==============================] - 0s 1ms/step - loss: 1.9468\n16/16 [==============================] - 0s 992us/step - loss: 1.9711\n16/16 [==============================] - 0s 1ms/step - loss: 1.9715\n16/16 [==============================] - 0s 595us/step - loss: 1.9633\n16/16 [==============================] - 0s 1ms/step - loss: 1.9507\n16/16 [==============================] - 0s 638us/step - loss: 1.9451\n16/16 [==============================] - 0s 1ms/step - loss: 1.9424\n\nTesting for epoch 25 index 4:\n16/16 [==============================] - 0s 642us/step - loss: 0.2477\n16/16 [==============================] - 0s 608us/step - loss: 1.4965\n16/16 [==============================] - 0s 1ms/step - loss: 1.8427\n16/16 [==============================] - 0s 1ms/step - loss: 1.9619\n16/16 [==============================] - 0s 707us/step - loss: 1.9847\n16/16 [==============================] - 0s 1ms/step - loss: 1.9844\n16/16 [==============================] - 0s 1ms/step - loss: 1.9761\n16/16 [==============================] - 0s 627us/step - loss: 1.9633\n16/16 [==============================] - 0s 604us/step - loss: 1.9575\n16/16 [==============================] - 0s 1ms/step - loss: 1.9548\n\nTesting for epoch 25 index 5:\n16/16 [==============================] - 0s 666us/step - loss: 0.2442\n16/16 [==============================] - 0s 1ms/step - loss: 1.5123\n16/16 [==============================] - 0s 663us/step - loss: 1.8671\n16/16 [==============================] - 0s 868us/step - loss: 1.9886\n16/16 [==============================] - 0s 1ms/step - loss: 2.0106\n16/16 [==============================] - 0s 1ms/step - loss: 2.0090\n16/16 [==============================] - 0s 1ms/step - loss: 1.9998\n16/16 [==============================] - 0s 1ms/step - loss: 1.9861\n16/16 [==============================] - 0s 1ms/step - loss: 1.9801\n16/16 [==============================] - 0s 897us/step - loss: 1.9772\nEpoch 26 of 60\n\nTesting for epoch 26 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2455\n16/16 [==============================] - 0s 641us/step - loss: 1.4842\n16/16 [==============================] - 0s 1ms/step - loss: 1.8252\n16/16 [==============================] - 0s 604us/step - loss: 1.9429\n16/16 [==============================] - 0s 853us/step - loss: 1.9640\n16/16 [==============================] - 0s 1ms/step - loss: 1.9625\n16/16 [==============================] - 0s 616us/step - loss: 1.9538\n16/16 [==============================] - 0s 861us/step - loss: 1.9412\n16/16 [==============================] - 0s 1ms/step - loss: 1.9356\n16/16 [==============================] - 0s 1ms/step - loss: 1.9330\n\nTesting for epoch 26 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2433\n16/16 [==============================] - 0s 1ms/step - loss: 1.4858\n16/16 [==============================] - 0s 837us/step - loss: 1.8253\n16/16 [==============================] - 0s 1ms/step - loss: 1.9440\n16/16 [==============================] - 0s 1ms/step - loss: 1.9633\n16/16 [==============================] - 0s 1ms/step - loss: 1.9608\n16/16 [==============================] - 0s 1ms/step - loss: 1.9512\n16/16 [==============================] - 0s 595us/step - loss: 1.9375\n16/16 [==============================] - 0s 623us/step - loss: 1.9316\n16/16 [==============================] - 0s 632us/step - loss: 1.9289\n\nTesting for epoch 26 index 3:\n16/16 [==============================] - 0s 947us/step - loss: 0.2431\n16/16 [==============================] - 0s 727us/step - loss: 1.5060\n16/16 [==============================] - 0s 614us/step - loss: 1.8472\n16/16 [==============================] - 0s 619us/step - loss: 1.9665\n16/16 [==============================] - 0s 1ms/step - loss: 1.9845\n16/16 [==============================] - 0s 1ms/step - loss: 1.9803\n16/16 [==============================] - 0s 1ms/step - loss: 1.9697\n16/16 [==============================] - 0s 640us/step - loss: 1.9554\n16/16 [==============================] - 0s 1ms/step - loss: 1.9494\n16/16 [==============================] - 0s 1ms/step - loss: 1.9467\n\nTesting for epoch 26 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2399\n16/16 [==============================] - 0s 1ms/step - loss: 1.4785\n16/16 [==============================] - 0s 1ms/step - loss: 1.8107\n16/16 [==============================] - 0s 571us/step - loss: 1.9298\n16/16 [==============================] - 0s 1ms/step - loss: 1.9481\n16/16 [==============================] - 0s 1ms/step - loss: 1.9445\n16/16 [==============================] - 0s 625us/step - loss: 1.9342\n16/16 [==============================] - 0s 631us/step - loss: 1.9201\n16/16 [==============================] - 0s 646us/step - loss: 1.9141\n16/16 [==============================] - 0s 626us/step - loss: 1.9114\n\nTesting for epoch 26 index 5:\n16/16 [==============================] - 0s 880us/step - loss: 0.2394\n16/16 [==============================] - 0s 1ms/step - loss: 1.5075\n16/16 [==============================] - 0s 632us/step - loss: 1.8517\n16/16 [==============================] - 0s 687us/step - loss: 1.9741\n16/16 [==============================] - 0s 644us/step - loss: 1.9911\n16/16 [==============================] - 0s 671us/step - loss: 1.9874\n16/16 [==============================] - 0s 621us/step - loss: 1.9765\n16/16 [==============================] - 0s 604us/step - loss: 1.9621\n16/16 [==============================] - 0s 610us/step - loss: 1.9561\n16/16 [==============================] - 0s 1ms/step - loss: 1.9533\nEpoch 27 of 60\n\nTesting for epoch 27 index 1:\n16/16 [==============================] - 0s 626us/step - loss: 0.2399\n16/16 [==============================] - 0s 1ms/step - loss: 1.5196\n16/16 [==============================] - 0s 1ms/step - loss: 1.8658\n16/16 [==============================] - 0s 1ms/step - loss: 1.9878\n16/16 [==============================] - 0s 937us/step - loss: 2.0043\n16/16 [==============================] - 0s 1ms/step - loss: 2.0001\n16/16 [==============================] - 0s 755us/step - loss: 1.9881\n16/16 [==============================] - 0s 607us/step - loss: 1.9734\n16/16 [==============================] - 0s 1ms/step - loss: 1.9673\n16/16 [==============================] - 0s 675us/step - loss: 1.9644\n\nTesting for epoch 27 index 2:\n16/16 [==============================] - 0s 584us/step - loss: 0.2383\n16/16 [==============================] - 0s 1ms/step - loss: 1.5491\n16/16 [==============================] - 0s 1ms/step - loss: 1.9024\n16/16 [==============================] - 0s 587us/step - loss: 2.0252\n16/16 [==============================] - 0s 793us/step - loss: 2.0399\n16/16 [==============================] - 0s 1ms/step - loss: 2.0364\n16/16 [==============================] - 0s 589us/step - loss: 2.0246\n16/16 [==============================] - 0s 625us/step - loss: 2.0102\n16/16 [==============================] - 0s 604us/step - loss: 2.0042\n16/16 [==============================] - 0s 600us/step - loss: 2.0014\n\nTesting for epoch 27 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2373\n16/16 [==============================] - 0s 1ms/step - loss: 1.5548\n16/16 [==============================] - 0s 1ms/step - loss: 1.9083\n16/16 [==============================] - 0s 1ms/step - loss: 2.0331\n16/16 [==============================] - 0s 594us/step - loss: 2.0485\n16/16 [==============================] - 0s 609us/step - loss: 2.0460\n16/16 [==============================] - 0s 917us/step - loss: 2.0331\n16/16 [==============================] - 0s 1ms/step - loss: 2.0181\n16/16 [==============================] - 0s 1ms/step - loss: 2.0119\n16/16 [==============================] - 0s 1ms/step - loss: 2.0090\n\nTesting for epoch 27 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2391\n16/16 [==============================] - 0s 713us/step - loss: 1.5529\n16/16 [==============================] - 0s 1ms/step - loss: 1.9056\n16/16 [==============================] - 0s 666us/step - loss: 2.0294\n16/16 [==============================] - 0s 1ms/step - loss: 2.0429\n16/16 [==============================] - 0s 1ms/step - loss: 2.0388\n16/16 [==============================] - 0s 1ms/step - loss: 2.0248\n16/16 [==============================] - 0s 1ms/step - loss: 2.0092\n16/16 [==============================] - 0s 1ms/step - loss: 2.0028\n16/16 [==============================] - 0s 1ms/step - loss: 1.9998\n\nTesting for epoch 27 index 5:\n16/16 [==============================] - 0s 643us/step - loss: 0.2391\n16/16 [==============================] - 0s 1ms/step - loss: 1.5363\n16/16 [==============================] - 0s 896us/step - loss: 1.8850\n16/16 [==============================] - 0s 1ms/step - loss: 2.0052\n16/16 [==============================] - 0s 610us/step - loss: 2.0195\n16/16 [==============================] - 0s 1ms/step - loss: 2.0152\n16/16 [==============================] - 0s 1ms/step - loss: 2.0012\n16/16 [==============================] - 0s 1ms/step - loss: 1.9857\n16/16 [==============================] - 0s 619us/step - loss: 1.9794\n16/16 [==============================] - 0s 1ms/step - loss: 1.9765\nEpoch 28 of 60\n\nTesting for epoch 28 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2338\n16/16 [==============================] - 0s 1ms/step - loss: 1.5579\n16/16 [==============================] - 0s 624us/step - loss: 1.9117\n16/16 [==============================] - 0s 1ms/step - loss: 2.0320\n16/16 [==============================] - 0s 612us/step - loss: 2.0450\n16/16 [==============================] - 0s 645us/step - loss: 2.0389\n16/16 [==============================] - 0s 721us/step - loss: 2.0244\n16/16 [==============================] - 0s 696us/step - loss: 2.0088\n16/16 [==============================] - 0s 608us/step - loss: 2.0024\n16/16 [==============================] - 0s 826us/step - loss: 1.9995\n\nTesting for epoch 28 index 2:\n16/16 [==============================] - 0s 748us/step - loss: 0.2338\n16/16 [==============================] - 0s 1ms/step - loss: 1.5385\n16/16 [==============================] - 0s 1ms/step - loss: 1.8839\n16/16 [==============================] - 0s 670us/step - loss: 2.0016\n16/16 [==============================] - 0s 626us/step - loss: 2.0138\n16/16 [==============================] - 0s 625us/step - loss: 2.0076\n16/16 [==============================] - 0s 1ms/step - loss: 1.9939\n16/16 [==============================] - 0s 579us/step - loss: 1.9788\n16/16 [==============================] - 0s 898us/step - loss: 1.9727\n16/16 [==============================] - 0s 1ms/step - loss: 1.9699\n\nTesting for epoch 28 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2316\n16/16 [==============================] - 0s 590us/step - loss: 1.5536\n16/16 [==============================] - 0s 627us/step - loss: 1.9020\n16/16 [==============================] - 0s 1ms/step - loss: 2.0198\n16/16 [==============================] - 0s 1ms/step - loss: 2.0311\n16/16 [==============================] - 0s 1ms/step - loss: 2.0229\n16/16 [==============================] - 0s 646us/step - loss: 2.0074\n16/16 [==============================] - 0s 606us/step - loss: 1.9907\n16/16 [==============================] - 0s 591us/step - loss: 1.9841\n16/16 [==============================] - 0s 1ms/step - loss: 1.9812\n\nTesting for epoch 28 index 4:\n16/16 [==============================] - 0s 618us/step - loss: 0.2325\n16/16 [==============================] - 0s 1ms/step - loss: 1.5661\n16/16 [==============================] - 0s 654us/step - loss: 1.9179\n16/16 [==============================] - 0s 1ms/step - loss: 2.0361\n16/16 [==============================] - 0s 1ms/step - loss: 2.0486\n16/16 [==============================] - 0s 589us/step - loss: 2.0418\n16/16 [==============================] - 0s 676us/step - loss: 2.0272\n16/16 [==============================] - 0s 686us/step - loss: 2.0113\n16/16 [==============================] - 0s 664us/step - loss: 2.0049\n16/16 [==============================] - 0s 1ms/step - loss: 2.0020\n\nTesting for epoch 28 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2280\n16/16 [==============================] - 0s 1ms/step - loss: 1.5553\n16/16 [==============================] - 0s 646us/step - loss: 1.9013\n16/16 [==============================] - 0s 1ms/step - loss: 2.0131\n16/16 [==============================] - 0s 929us/step - loss: 2.0218\n16/16 [==============================] - 0s 838us/step - loss: 2.0122\n16/16 [==============================] - 0s 882us/step - loss: 1.9958\n16/16 [==============================] - 0s 602us/step - loss: 1.9791\n16/16 [==============================] - 0s 608us/step - loss: 1.9725\n16/16 [==============================] - 0s 657us/step - loss: 1.9696\nEpoch 29 of 60\n\nTesting for epoch 29 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2275\n16/16 [==============================] - 0s 767us/step - loss: 1.6042\n16/16 [==============================] - 0s 629us/step - loss: 1.9618\n16/16 [==============================] - 0s 1ms/step - loss: 2.0777\n16/16 [==============================] - 0s 1ms/step - loss: 2.0859\n16/16 [==============================] - 0s 645us/step - loss: 2.0750\n16/16 [==============================] - 0s 646us/step - loss: 2.0571\n16/16 [==============================] - 0s 900us/step - loss: 2.0390\n16/16 [==============================] - 0s 1ms/step - loss: 2.0319\n16/16 [==============================] - 0s 1ms/step - loss: 2.0288\n\nTesting for epoch 29 index 2:\n16/16 [==============================] - 0s 600us/step - loss: 0.2278\n16/16 [==============================] - 0s 625us/step - loss: 1.6003\n16/16 [==============================] - 0s 627us/step - loss: 1.9580\n16/16 [==============================] - 0s 599us/step - loss: 2.0748\n16/16 [==============================] - 0s 608us/step - loss: 2.0844\n16/16 [==============================] - 0s 1ms/step - loss: 2.0744\n16/16 [==============================] - 0s 911us/step - loss: 2.0568\n16/16 [==============================] - 0s 1ms/step - loss: 2.0391\n16/16 [==============================] - 0s 1ms/step - loss: 2.0323\n16/16 [==============================] - 0s 632us/step - loss: 2.0293\n\nTesting for epoch 29 index 3:\n16/16 [==============================] - 0s 606us/step - loss: 0.2275\n16/16 [==============================] - 0s 640us/step - loss: 1.5908\n16/16 [==============================] - 0s 787us/step - loss: 1.9416\n16/16 [==============================] - 0s 614us/step - loss: 2.0550\n16/16 [==============================] - 0s 655us/step - loss: 2.0633\n16/16 [==============================] - 0s 1ms/step - loss: 2.0544\n16/16 [==============================] - 0s 624us/step - loss: 2.0373\n16/16 [==============================] - 0s 590us/step - loss: 2.0202\n16/16 [==============================] - 0s 665us/step - loss: 2.0135\n16/16 [==============================] - 0s 1ms/step - loss: 2.0105\n\nTesting for epoch 29 index 4:\n16/16 [==============================] - 0s 637us/step - loss: 0.2262\n16/16 [==============================] - 0s 1ms/step - loss: 1.6456\n16/16 [==============================] - 0s 861us/step - loss: 2.0122\n16/16 [==============================] - 0s 1ms/step - loss: 2.1306\n16/16 [==============================] - 0s 1ms/step - loss: 2.1391\n16/16 [==============================] - 0s 1ms/step - loss: 2.1297\n16/16 [==============================] - 0s 621us/step - loss: 2.1118\n16/16 [==============================] - 0s 608us/step - loss: 2.0941\n16/16 [==============================] - 0s 1ms/step - loss: 2.0872\n16/16 [==============================] - 0s 652us/step - loss: 2.0842\n\nTesting for epoch 29 index 5:\n16/16 [==============================] - 0s 876us/step - loss: 0.2207\n16/16 [==============================] - 0s 656us/step - loss: 1.5952\n16/16 [==============================] - 0s 647us/step - loss: 1.9409\n16/16 [==============================] - 0s 611us/step - loss: 2.0496\n16/16 [==============================] - 0s 616us/step - loss: 2.0545\n16/16 [==============================] - 0s 1ms/step - loss: 2.0428\n16/16 [==============================] - 0s 599us/step - loss: 2.0236\n16/16 [==============================] - 0s 625us/step - loss: 2.0050\n16/16 [==============================] - 0s 897us/step - loss: 1.9980\n16/16 [==============================] - 0s 1ms/step - loss: 1.9949\nEpoch 30 of 60\n\nTesting for epoch 30 index 1:\n16/16 [==============================] - 0s 595us/step - loss: 0.2217\n16/16 [==============================] - 0s 1ms/step - loss: 1.6089\n16/16 [==============================] - 0s 1ms/step - loss: 1.9546\n16/16 [==============================] - 0s 1ms/step - loss: 2.0641\n16/16 [==============================] - 0s 638us/step - loss: 2.0694\n16/16 [==============================] - 0s 636us/step - loss: 2.0580\n16/16 [==============================] - 0s 1ms/step - loss: 2.0394\n16/16 [==============================] - 0s 1ms/step - loss: 2.0217\n16/16 [==============================] - 0s 673us/step - loss: 2.0150\n16/16 [==============================] - 0s 1ms/step - loss: 2.0120\n\nTesting for epoch 30 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2210\n16/16 [==============================] - 0s 828us/step - loss: 1.6202\n16/16 [==============================] - 0s 1ms/step - loss: 1.9660\n16/16 [==============================] - 0s 1ms/step - loss: 2.0750\n16/16 [==============================] - 0s 1ms/step - loss: 2.0805\n16/16 [==============================] - 0s 655us/step - loss: 2.0695\n16/16 [==============================] - 0s 1ms/step - loss: 2.0510\n16/16 [==============================] - 0s 1ms/step - loss: 2.0329\n16/16 [==============================] - 0s 681us/step - loss: 2.0260\n16/16 [==============================] - 0s 660us/step - loss: 2.0231\n\nTesting for epoch 30 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2210\n16/16 [==============================] - 0s 1ms/step - loss: 1.6647\n16/16 [==============================] - 0s 1ms/step - loss: 2.0279\n16/16 [==============================] - 0s 1ms/step - loss: 2.1422\n16/16 [==============================] - 0s 638us/step - loss: 2.1469\n16/16 [==============================] - 0s 1ms/step - loss: 2.1338\n16/16 [==============================] - 0s 787us/step - loss: 2.1132\n16/16 [==============================] - 0s 1ms/step - loss: 2.0938\n16/16 [==============================] - 0s 1ms/step - loss: 2.0865\n16/16 [==============================] - 0s 1ms/step - loss: 2.0833\n\nTesting for epoch 30 index 4:\n16/16 [==============================] - 0s 636us/step - loss: 0.2203\n16/16 [==============================] - 0s 640us/step - loss: 1.6660\n16/16 [==============================] - 0s 629us/step - loss: 2.0216\n16/16 [==============================] - 0s 1ms/step - loss: 2.1328\n16/16 [==============================] - 0s 607us/step - loss: 2.1365\n16/16 [==============================] - 0s 1ms/step - loss: 2.1239\n16/16 [==============================] - 0s 1ms/step - loss: 2.1042\n16/16 [==============================] - 0s 1ms/step - loss: 2.0851\n16/16 [==============================] - 0s 637us/step - loss: 2.0778\n16/16 [==============================] - 0s 1ms/step - loss: 2.0747\n\nTesting for epoch 30 index 5:\n16/16 [==============================] - 0s 975us/step - loss: 0.2161\n16/16 [==============================] - 0s 1ms/step - loss: 1.6263\n16/16 [==============================] - 0s 1ms/step - loss: 1.9708\n16/16 [==============================] - 0s 601us/step - loss: 2.0786\n16/16 [==============================] - 0s 806us/step - loss: 2.0799\n16/16 [==============================] - 0s 787us/step - loss: 2.0658\n16/16 [==============================] - 0s 870us/step - loss: 2.0449\n16/16 [==============================] - 0s 1ms/step - loss: 2.0250\n16/16 [==============================] - 0s 1ms/step - loss: 2.0177\n16/16 [==============================] - 0s 826us/step - loss: 2.0146\nEpoch 31 of 60\n\nTesting for epoch 31 index 1:\n16/16 [==============================] - 0s 611us/step - loss: 0.2170\n16/16 [==============================] - 0s 1ms/step - loss: 1.6675\n16/16 [==============================] - 0s 814us/step - loss: 2.0171\n16/16 [==============================] - 0s 610us/step - loss: 2.1244\n16/16 [==============================] - 0s 619us/step - loss: 2.1233\n16/16 [==============================] - 0s 1ms/step - loss: 2.1079\n16/16 [==============================] - 0s 856us/step - loss: 2.0871\n16/16 [==============================] - 0s 1ms/step - loss: 2.0677\n16/16 [==============================] - 0s 632us/step - loss: 2.0605\n16/16 [==============================] - 0s 602us/step - loss: 2.0574\n\nTesting for epoch 31 index 2:\n16/16 [==============================] - 0s 636us/step - loss: 0.2144\n16/16 [==============================] - 0s 709us/step - loss: 1.6430\n16/16 [==============================] - 0s 667us/step - loss: 1.9817\n16/16 [==============================] - 0s 682us/step - loss: 2.0881\n16/16 [==============================] - 0s 1ms/step - loss: 2.0878\n16/16 [==============================] - 0s 1ms/step - loss: 2.0724\n16/16 [==============================] - 0s 629us/step - loss: 2.0513\n16/16 [==============================] - 0s 1ms/step - loss: 2.0314\n16/16 [==============================] - 0s 884us/step - loss: 2.0241\n16/16 [==============================] - 0s 585us/step - loss: 2.0210\n\nTesting for epoch 31 index 3:\n16/16 [==============================] - 0s 726us/step - loss: 0.2157\n16/16 [==============================] - 0s 1ms/step - loss: 1.6566\n16/16 [==============================] - 0s 1ms/step - loss: 2.0048\n16/16 [==============================] - 0s 708us/step - loss: 2.1166\n16/16 [==============================] - 0s 1ms/step - loss: 2.1189\n16/16 [==============================] - 0s 1ms/step - loss: 2.1054\n16/16 [==============================] - 0s 684us/step - loss: 2.0858\n16/16 [==============================] - 0s 922us/step - loss: 2.0670\n16/16 [==============================] - 0s 1ms/step - loss: 2.0600\n16/16 [==============================] - 0s 602us/step - loss: 2.0569\n\nTesting for epoch 31 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2162\n16/16 [==============================] - 0s 1ms/step - loss: 1.6785\n16/16 [==============================] - 0s 1ms/step - loss: 2.0249\n16/16 [==============================] - 0s 1ms/step - loss: 2.1348\n16/16 [==============================] - 0s 630us/step - loss: 2.1339\n16/16 [==============================] - 0s 1ms/step - loss: 2.1192\n16/16 [==============================] - 0s 604us/step - loss: 2.0985\n16/16 [==============================] - 0s 660us/step - loss: 2.0787\n16/16 [==============================] - 0s 741us/step - loss: 2.0714\n16/16 [==============================] - 0s 1ms/step - loss: 2.0683\n\nTesting for epoch 31 index 5:\n16/16 [==============================] - 0s 636us/step - loss: 0.2121\n16/16 [==============================] - 0s 1ms/step - loss: 1.7297\n16/16 [==============================] - 0s 1ms/step - loss: 2.0914\n16/16 [==============================] - 0s 1ms/step - loss: 2.2077\n16/16 [==============================] - 0s 615us/step - loss: 2.2066\n16/16 [==============================] - 0s 615us/step - loss: 2.1895\n16/16 [==============================] - 0s 1ms/step - loss: 2.1665\n16/16 [==============================] - 0s 659us/step - loss: 2.1450\n16/16 [==============================] - 0s 1ms/step - loss: 2.1372\n16/16 [==============================] - 0s 911us/step - loss: 2.1339\nEpoch 32 of 60\n\nTesting for epoch 32 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2120\n16/16 [==============================] - 0s 787us/step - loss: 1.7164\n16/16 [==============================] - 0s 644us/step - loss: 2.0674\n16/16 [==============================] - 0s 629us/step - loss: 2.1800\n16/16 [==============================] - 0s 1ms/step - loss: 2.1770\n16/16 [==============================] - 0s 652us/step - loss: 2.1600\n16/16 [==============================] - 0s 620us/step - loss: 2.1373\n16/16 [==============================] - 0s 784us/step - loss: 2.1164\n16/16 [==============================] - 0s 644us/step - loss: 2.1088\n16/16 [==============================] - 0s 606us/step - loss: 2.1055\n\nTesting for epoch 32 index 2:\n16/16 [==============================] - 0s 685us/step - loss: 0.2108\n16/16 [==============================] - 0s 789us/step - loss: 1.7226\n16/16 [==============================] - 0s 653us/step - loss: 2.0726\n16/16 [==============================] - 0s 1ms/step - loss: 2.1850\n16/16 [==============================] - 0s 737us/step - loss: 2.1834\n16/16 [==============================] - 0s 659us/step - loss: 2.1657\n16/16 [==============================] - 0s 633us/step - loss: 2.1428\n16/16 [==============================] - 0s 851us/step - loss: 2.1217\n16/16 [==============================] - 0s 636us/step - loss: 2.1141\n16/16 [==============================] - 0s 599us/step - loss: 2.1108\n\nTesting for epoch 32 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2095\n16/16 [==============================] - 0s 894us/step - loss: 1.7267\n16/16 [==============================] - 0s 693us/step - loss: 2.0755\n16/16 [==============================] - 0s 1ms/step - loss: 2.1840\n16/16 [==============================] - 0s 1ms/step - loss: 2.1795\n16/16 [==============================] - 0s 932us/step - loss: 2.1586\n16/16 [==============================] - 0s 617us/step - loss: 2.1342\n16/16 [==============================] - 0s 1ms/step - loss: 2.1125\n16/16 [==============================] - 0s 672us/step - loss: 2.1046\n16/16 [==============================] - 0s 1ms/step - loss: 2.1013\n\nTesting for epoch 32 index 4:\n16/16 [==============================] - 0s 629us/step - loss: 0.2097\n16/16 [==============================] - 0s 656us/step - loss: 1.7201\n16/16 [==============================] - 0s 725us/step - loss: 2.0713\n16/16 [==============================] - 0s 610us/step - loss: 2.1837\n16/16 [==============================] - 0s 857us/step - loss: 2.1826\n16/16 [==============================] - 0s 1ms/step - loss: 2.1660\n16/16 [==============================] - 0s 619us/step - loss: 2.1441\n16/16 [==============================] - 0s 1ms/step - loss: 2.1235\n16/16 [==============================] - 0s 759us/step - loss: 2.1159\n16/16 [==============================] - 0s 1ms/step - loss: 2.1127\n\nTesting for epoch 32 index 5:\n16/16 [==============================] - 0s 617us/step - loss: 0.2078\n16/16 [==============================] - 0s 1ms/step - loss: 1.7352\n16/16 [==============================] - 0s 633us/step - loss: 2.0834\n16/16 [==============================] - 0s 617us/step - loss: 2.1909\n16/16 [==============================] - 0s 661us/step - loss: 2.1858\n16/16 [==============================] - 0s 635us/step - loss: 2.1659\n16/16 [==============================] - 0s 675us/step - loss: 2.1419\n16/16 [==============================] - 0s 1ms/step - loss: 2.1201\n16/16 [==============================] - 0s 638us/step - loss: 2.1122\n16/16 [==============================] - 0s 924us/step - loss: 2.1089\nEpoch 33 of 60\n\nTesting for epoch 33 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2075\n16/16 [==============================] - 0s 1ms/step - loss: 1.7201\n16/16 [==============================] - 0s 808us/step - loss: 2.0682\n16/16 [==============================] - 0s 1ms/step - loss: 2.1789\n16/16 [==============================] - 0s 1ms/step - loss: 2.1772\n16/16 [==============================] - 0s 832us/step - loss: 2.1596\n16/16 [==============================] - 0s 1ms/step - loss: 2.1366\n16/16 [==============================] - 0s 631us/step - loss: 2.1155\n16/16 [==============================] - 0s 1ms/step - loss: 2.1079\n16/16 [==============================] - 0s 717us/step - loss: 2.1048\n\nTesting for epoch 33 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2033\n16/16 [==============================] - 0s 880us/step - loss: 1.7117\n16/16 [==============================] - 0s 689us/step - loss: 2.0529\n16/16 [==============================] - 0s 646us/step - loss: 2.1588\n16/16 [==============================] - 0s 629us/step - loss: 2.1535\n16/16 [==============================] - 0s 977us/step - loss: 2.1332\n16/16 [==============================] - 0s 564us/step - loss: 2.1092\n16/16 [==============================] - 0s 647us/step - loss: 2.0876\n16/16 [==============================] - 0s 1ms/step - loss: 2.0798\n16/16 [==============================] - 0s 591us/step - loss: 2.0766\n\nTesting for epoch 33 index 3:\n16/16 [==============================] - 0s 990us/step - loss: 0.2022\n16/16 [==============================] - 0s 1ms/step - loss: 1.7204\n16/16 [==============================] - 0s 612us/step - loss: 2.0604\n16/16 [==============================] - 0s 631us/step - loss: 2.1673\n16/16 [==============================] - 0s 1ms/step - loss: 2.1620\n16/16 [==============================] - 0s 629us/step - loss: 2.1416\n16/16 [==============================] - 0s 929us/step - loss: 2.1173\n16/16 [==============================] - 0s 1ms/step - loss: 2.0956\n16/16 [==============================] - 0s 1ms/step - loss: 2.0879\n16/16 [==============================] - 0s 617us/step - loss: 2.0846\n\nTesting for epoch 33 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2030\n16/16 [==============================] - 0s 1ms/step - loss: 1.7209\n16/16 [==============================] - 0s 605us/step - loss: 2.0625\n16/16 [==============================] - 0s 1ms/step - loss: 2.1710\n16/16 [==============================] - 0s 700us/step - loss: 2.1667\n16/16 [==============================] - 0s 1ms/step - loss: 2.1474\n16/16 [==============================] - 0s 902us/step - loss: 2.1242\n16/16 [==============================] - 0s 667us/step - loss: 2.1033\n16/16 [==============================] - 0s 1ms/step - loss: 2.0957\n16/16 [==============================] - 0s 1ms/step - loss: 2.0925\n\nTesting for epoch 33 index 5:\n16/16 [==============================] - 0s 621us/step - loss: 0.2029\n16/16 [==============================] - 0s 610us/step - loss: 1.7547\n16/16 [==============================] - 0s 611us/step - loss: 2.1118\n16/16 [==============================] - 0s 597us/step - loss: 2.2254\n16/16 [==============================] - 0s 576us/step - loss: 2.2233\n16/16 [==============================] - 0s 580us/step - loss: 2.2050\n16/16 [==============================] - 0s 612us/step - loss: 2.1821\n16/16 [==============================] - 0s 605us/step - loss: 2.1610\n16/16 [==============================] - 0s 807us/step - loss: 2.1534\n16/16 [==============================] - 0s 1ms/step - loss: 2.1502\nEpoch 34 of 60\n\nTesting for epoch 34 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1969\n16/16 [==============================] - 0s 955us/step - loss: 1.7401\n16/16 [==============================] - 0s 642us/step - loss: 2.0891\n16/16 [==============================] - 0s 691us/step - loss: 2.1926\n16/16 [==============================] - 0s 1ms/step - loss: 2.1827\n16/16 [==============================] - 0s 706us/step - loss: 2.1592\n16/16 [==============================] - 0s 1ms/step - loss: 2.1331\n16/16 [==============================] - 0s 835us/step - loss: 2.1100\n16/16 [==============================] - 0s 1ms/step - loss: 2.1018\n16/16 [==============================] - 0s 1ms/step - loss: 2.0983\n\nTesting for epoch 34 index 2:\n16/16 [==============================] - 0s 659us/step - loss: 0.1983\n16/16 [==============================] - 0s 664us/step - loss: 1.7511\n16/16 [==============================] - 0s 1ms/step - loss: 2.1021\n16/16 [==============================] - 0s 632us/step - loss: 2.2065\n16/16 [==============================] - 0s 1ms/step - loss: 2.1983\n16/16 [==============================] - 0s 1ms/step - loss: 2.1764\n16/16 [==============================] - 0s 1ms/step - loss: 2.1510\n16/16 [==============================] - 0s 639us/step - loss: 2.1285\n16/16 [==============================] - 0s 641us/step - loss: 2.1205\n16/16 [==============================] - 0s 1ms/step - loss: 2.1171\n\nTesting for epoch 34 index 3:\n16/16 [==============================] - 0s 774us/step - loss: 0.1992\n16/16 [==============================] - 0s 643us/step - loss: 1.7271\n16/16 [==============================] - 0s 1ms/step - loss: 2.0661\n16/16 [==============================] - 0s 1ms/step - loss: 2.1656\n16/16 [==============================] - 0s 1ms/step - loss: 2.1575\n16/16 [==============================] - 0s 1ms/step - loss: 2.1343\n16/16 [==============================] - 0s 620us/step - loss: 2.1083\n16/16 [==============================] - 0s 675us/step - loss: 2.0856\n16/16 [==============================] - 0s 1ms/step - loss: 2.0776\n16/16 [==============================] - 0s 641us/step - loss: 2.0743\n\nTesting for epoch 34 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1973\n16/16 [==============================] - 0s 634us/step - loss: 1.7673\n16/16 [==============================] - 0s 641us/step - loss: 2.1133\n16/16 [==============================] - 0s 807us/step - loss: 2.2143\n16/16 [==============================] - 0s 1ms/step - loss: 2.2060\n16/16 [==============================] - 0s 680us/step - loss: 2.1823\n16/16 [==============================] - 0s 915us/step - loss: 2.1568\n16/16 [==============================] - 0s 1ms/step - loss: 2.1343\n16/16 [==============================] - 0s 637us/step - loss: 2.1262\n16/16 [==============================] - 0s 1ms/step - loss: 2.1229\n\nTesting for epoch 34 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1957\n16/16 [==============================] - 0s 1ms/step - loss: 1.7630\n16/16 [==============================] - 0s 1ms/step - loss: 2.1149\n16/16 [==============================] - 0s 1ms/step - loss: 2.2172\n16/16 [==============================] - 0s 1ms/step - loss: 2.2101\n16/16 [==============================] - 0s 785us/step - loss: 2.1867\n16/16 [==============================] - 0s 767us/step - loss: 2.1611\n16/16 [==============================] - 0s 1ms/step - loss: 2.1384\n16/16 [==============================] - 0s 1ms/step - loss: 2.1303\n16/16 [==============================] - 0s 835us/step - loss: 2.1270\nEpoch 35 of 60\n\nTesting for epoch 35 index 1:\n16/16 [==============================] - 0s 659us/step - loss: 0.1958\n16/16 [==============================] - 0s 636us/step - loss: 1.7773\n16/16 [==============================] - 0s 580us/step - loss: 2.1316\n16/16 [==============================] - 0s 632us/step - loss: 2.2359\n16/16 [==============================] - 0s 699us/step - loss: 2.2285\n16/16 [==============================] - 0s 642us/step - loss: 2.2029\n16/16 [==============================] - 0s 953us/step - loss: 2.1759\n16/16 [==============================] - 0s 1ms/step - loss: 2.1522\n16/16 [==============================] - 0s 1ms/step - loss: 2.1437\n16/16 [==============================] - 0s 843us/step - loss: 2.1402\n\nTesting for epoch 35 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1914\n16/16 [==============================] - 0s 908us/step - loss: 1.7573\n16/16 [==============================] - 0s 718us/step - loss: 2.1000\n16/16 [==============================] - 0s 1ms/step - loss: 2.1991\n16/16 [==============================] - 0s 626us/step - loss: 2.1890\n16/16 [==============================] - 0s 746us/step - loss: 2.1621\n16/16 [==============================] - 0s 649us/step - loss: 2.1343\n16/16 [==============================] - 0s 653us/step - loss: 2.1108\n16/16 [==============================] - 0s 1ms/step - loss: 2.1026\n16/16 [==============================] - 0s 1ms/step - loss: 2.0992\n\nTesting for epoch 35 index 3:\n16/16 [==============================] - 0s 615us/step - loss: 0.1948\n16/16 [==============================] - 0s 637us/step - loss: 1.8010\n16/16 [==============================] - 0s 1ms/step - loss: 2.1603\n16/16 [==============================] - 0s 628us/step - loss: 2.2660\n16/16 [==============================] - 0s 1ms/step - loss: 2.2587\n16/16 [==============================] - 0s 706us/step - loss: 2.2337\n16/16 [==============================] - 0s 637us/step - loss: 2.2068\n16/16 [==============================] - 0s 631us/step - loss: 2.1834\n16/16 [==============================] - 0s 1ms/step - loss: 2.1750\n16/16 [==============================] - 0s 1ms/step - loss: 2.1716\n\nTesting for epoch 35 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1911\n16/16 [==============================] - 0s 641us/step - loss: 1.7696\n16/16 [==============================] - 0s 841us/step - loss: 2.1178\n16/16 [==============================] - 0s 778us/step - loss: 2.2186\n16/16 [==============================] - 0s 1ms/step - loss: 2.2089\n16/16 [==============================] - 0s 1ms/step - loss: 2.1826\n16/16 [==============================] - 0s 1ms/step - loss: 2.1555\n16/16 [==============================] - 0s 1ms/step - loss: 2.1326\n16/16 [==============================] - 0s 1ms/step - loss: 2.1245\n16/16 [==============================] - 0s 1ms/step - loss: 2.1212\n\nTesting for epoch 35 index 5:\n16/16 [==============================] - 0s 728us/step - loss: 0.1921\n16/16 [==============================] - 0s 635us/step - loss: 1.8098\n16/16 [==============================] - 0s 1ms/step - loss: 2.1669\n16/16 [==============================] - 0s 661us/step - loss: 2.2703\n16/16 [==============================] - 0s 1ms/step - loss: 2.2623\n16/16 [==============================] - 0s 664us/step - loss: 2.2363\n16/16 [==============================] - 0s 650us/step - loss: 2.2087\n16/16 [==============================] - 0s 648us/step - loss: 2.1849\n16/16 [==============================] - 0s 642us/step - loss: 2.1766\n16/16 [==============================] - 0s 662us/step - loss: 2.1732\nEpoch 36 of 60\n\nTesting for epoch 36 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1943\n16/16 [==============================] - 0s 873us/step - loss: 1.7990\n16/16 [==============================] - 0s 1ms/step - loss: 2.1590\n16/16 [==============================] - 0s 1ms/step - loss: 2.2620\n16/16 [==============================] - 0s 1ms/step - loss: 2.2528\n16/16 [==============================] - 0s 600us/step - loss: 2.2259\n16/16 [==============================] - 0s 584us/step - loss: 2.1981\n16/16 [==============================] - 0s 1ms/step - loss: 2.1742\n16/16 [==============================] - 0s 1ms/step - loss: 2.1658\n16/16 [==============================] - 0s 1ms/step - loss: 2.1623\n\nTesting for epoch 36 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1929\n16/16 [==============================] - 0s 1ms/step - loss: 1.7656\n16/16 [==============================] - 0s 784us/step - loss: 2.1149\n16/16 [==============================] - 0s 619us/step - loss: 2.2138\n16/16 [==============================] - 0s 587us/step - loss: 2.2056\n16/16 [==============================] - 0s 590us/step - loss: 2.1795\n16/16 [==============================] - 0s 567us/step - loss: 2.1526\n16/16 [==============================] - 0s 616us/step - loss: 2.1299\n16/16 [==============================] - 0s 1ms/step - loss: 2.1219\n16/16 [==============================] - 0s 594us/step - loss: 2.1186\n\nTesting for epoch 36 index 3:\n16/16 [==============================] - 0s 761us/step - loss: 0.1915\n16/16 [==============================] - 0s 1ms/step - loss: 1.7777\n16/16 [==============================] - 0s 1ms/step - loss: 2.1276\n16/16 [==============================] - 0s 1ms/step - loss: 2.2233\n16/16 [==============================] - 0s 1ms/step - loss: 2.2119\n16/16 [==============================] - 0s 1ms/step - loss: 2.1831\n16/16 [==============================] - 0s 1ms/step - loss: 2.1549\n16/16 [==============================] - 0s 1ms/step - loss: 2.1315\n16/16 [==============================] - 0s 1ms/step - loss: 2.1235\n16/16 [==============================] - 0s 1ms/step - loss: 2.1202\n\nTesting for epoch 36 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1890\n16/16 [==============================] - 0s 947us/step - loss: 1.7926\n16/16 [==============================] - 0s 1ms/step - loss: 2.1436\n16/16 [==============================] - 0s 1ms/step - loss: 2.2395\n16/16 [==============================] - 0s 625us/step - loss: 2.2279\n16/16 [==============================] - 0s 1ms/step - loss: 2.1978\n16/16 [==============================] - 0s 1ms/step - loss: 2.1692\n16/16 [==============================] - 0s 1ms/step - loss: 2.1451\n16/16 [==============================] - 0s 1ms/step - loss: 2.1368\n16/16 [==============================] - 0s 1ms/step - loss: 2.1334\n\nTesting for epoch 36 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1867\n16/16 [==============================] - 0s 1ms/step - loss: 1.8739\n16/16 [==============================] - 0s 1ms/step - loss: 2.2491\n16/16 [==============================] - 0s 992us/step - loss: 2.3538\n16/16 [==============================] - 0s 1ms/step - loss: 2.3422\n16/16 [==============================] - 0s 870us/step - loss: 2.3119\n16/16 [==============================] - 0s 1ms/step - loss: 2.2828\n16/16 [==============================] - 0s 627us/step - loss: 2.2576\n16/16 [==============================] - 0s 615us/step - loss: 2.2488\n16/16 [==============================] - 0s 748us/step - loss: 2.2452\nEpoch 37 of 60\n\nTesting for epoch 37 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1852\n16/16 [==============================] - 0s 725us/step - loss: 1.8347\n16/16 [==============================] - 0s 646us/step - loss: 2.1942\n16/16 [==============================] - 0s 856us/step - loss: 2.2953\n16/16 [==============================] - 0s 661us/step - loss: 2.2842\n16/16 [==============================] - 0s 604us/step - loss: 2.2557\n16/16 [==============================] - 0s 602us/step - loss: 2.2280\n16/16 [==============================] - 0s 908us/step - loss: 2.2039\n16/16 [==============================] - 0s 1ms/step - loss: 2.1955\n16/16 [==============================] - 0s 1ms/step - loss: 2.1921\n\nTesting for epoch 37 index 2:\n16/16 [==============================] - 0s 896us/step - loss: 0.1867\n16/16 [==============================] - 0s 668us/step - loss: 1.7861\n16/16 [==============================] - 0s 1ms/step - loss: 2.1329\n16/16 [==============================] - 0s 777us/step - loss: 2.2272\n16/16 [==============================] - 0s 1ms/step - loss: 2.2136\n16/16 [==============================] - 0s 736us/step - loss: 2.1838\n16/16 [==============================] - 0s 606us/step - loss: 2.1556\n16/16 [==============================] - 0s 1ms/step - loss: 2.1315\n16/16 [==============================] - 0s 1ms/step - loss: 2.1232\n16/16 [==============================] - 0s 626us/step - loss: 2.1198\n\nTesting for epoch 37 index 3:\n16/16 [==============================] - 0s 579us/step - loss: 0.1835\n16/16 [==============================] - 0s 1ms/step - loss: 1.8104\n16/16 [==============================] - 0s 1ms/step - loss: 2.1585\n16/16 [==============================] - 0s 634us/step - loss: 2.2523\n16/16 [==============================] - 0s 702us/step - loss: 2.2377\n16/16 [==============================] - 0s 635us/step - loss: 2.2062\n16/16 [==============================] - 0s 604us/step - loss: 2.1763\n16/16 [==============================] - 0s 1ms/step - loss: 2.1511\n16/16 [==============================] - 0s 1ms/step - loss: 2.1427\n16/16 [==============================] - 0s 625us/step - loss: 2.1392\n\nTesting for epoch 37 index 4:\n16/16 [==============================] - 0s 613us/step - loss: 0.1853\n16/16 [==============================] - 0s 700us/step - loss: 1.8215\n16/16 [==============================] - 0s 1ms/step - loss: 2.1727\n16/16 [==============================] - 0s 723us/step - loss: 2.2672\n16/16 [==============================] - 0s 714us/step - loss: 2.2522\n16/16 [==============================] - 0s 621us/step - loss: 2.2214\n16/16 [==============================] - 0s 669us/step - loss: 2.1932\n16/16 [==============================] - 0s 607us/step - loss: 2.1690\n16/16 [==============================] - 0s 700us/step - loss: 2.1607\n16/16 [==============================] - 0s 651us/step - loss: 2.1573\n\nTesting for epoch 37 index 5:\n16/16 [==============================] - 0s 628us/step - loss: 0.1840\n16/16 [==============================] - 0s 600us/step - loss: 1.8633\n16/16 [==============================] - 0s 622us/step - loss: 2.2307\n16/16 [==============================] - 0s 620us/step - loss: 2.3265\n16/16 [==============================] - 0s 622us/step - loss: 2.3089\n16/16 [==============================] - 0s 652us/step - loss: 2.2739\n16/16 [==============================] - 0s 1ms/step - loss: 2.2421\n16/16 [==============================] - 0s 968us/step - loss: 2.2152\n16/16 [==============================] - 0s 1ms/step - loss: 2.2061\n16/16 [==============================] - 0s 851us/step - loss: 2.2024\nEpoch 38 of 60\n\nTesting for epoch 38 index 1:\n16/16 [==============================] - 0s 639us/step - loss: 0.1830\n16/16 [==============================] - 0s 1ms/step - loss: 1.8733\n16/16 [==============================] - 0s 621us/step - loss: 2.2434\n16/16 [==============================] - 0s 966us/step - loss: 2.3381\n16/16 [==============================] - 0s 1ms/step - loss: 2.3205\n16/16 [==============================] - 0s 1ms/step - loss: 2.2861\n16/16 [==============================] - 0s 1ms/step - loss: 2.2544\n16/16 [==============================] - 0s 1ms/step - loss: 2.2276\n16/16 [==============================] - 0s 936us/step - loss: 2.2186\n16/16 [==============================] - 0s 617us/step - loss: 2.2149\n\nTesting for epoch 38 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1816\n16/16 [==============================] - 0s 618us/step - loss: 1.8904\n16/16 [==============================] - 0s 1ms/step - loss: 2.2667\n16/16 [==============================] - 0s 707us/step - loss: 2.3649\n16/16 [==============================] - 0s 1ms/step - loss: 2.3499\n16/16 [==============================] - 0s 1ms/step - loss: 2.3184\n16/16 [==============================] - 0s 1ms/step - loss: 2.2894\n16/16 [==============================] - 0s 1ms/step - loss: 2.2641\n16/16 [==============================] - 0s 1ms/step - loss: 2.2555\n16/16 [==============================] - 0s 632us/step - loss: 2.2520\n\nTesting for epoch 38 index 3:\n16/16 [==============================] - 0s 607us/step - loss: 0.1833\n16/16 [==============================] - 0s 641us/step - loss: 1.8758\n16/16 [==============================] - 0s 1ms/step - loss: 2.2495\n16/16 [==============================] - 0s 617us/step - loss: 2.3460\n16/16 [==============================] - 0s 639us/step - loss: 2.3310\n16/16 [==============================] - 0s 930us/step - loss: 2.2983\n16/16 [==============================] - 0s 622us/step - loss: 2.2679\n16/16 [==============================] - 0s 1ms/step - loss: 2.2418\n16/16 [==============================] - 0s 624us/step - loss: 2.2328\n16/16 [==============================] - 0s 1ms/step - loss: 2.2291\n\nTesting for epoch 38 index 4:\n16/16 [==============================] - 0s 969us/step - loss: 0.1794\n16/16 [==============================] - 0s 1ms/step - loss: 1.9049\n16/16 [==============================] - 0s 1ms/step - loss: 2.2851\n16/16 [==============================] - 0s 615us/step - loss: 2.3832\n16/16 [==============================] - 0s 712us/step - loss: 2.3669\n16/16 [==============================] - 0s 616us/step - loss: 2.3326\n16/16 [==============================] - 0s 1ms/step - loss: 2.3017\n16/16 [==============================] - 0s 630us/step - loss: 2.2754\n16/16 [==============================] - 0s 837us/step - loss: 2.2665\n16/16 [==============================] - 0s 1ms/step - loss: 2.2628\n\nTesting for epoch 38 index 5:\n16/16 [==============================] - 0s 632us/step - loss: 0.1800\n16/16 [==============================] - 0s 626us/step - loss: 1.8637\n16/16 [==============================] - 0s 904us/step - loss: 2.2352\n16/16 [==============================] - 0s 1ms/step - loss: 2.3308\n16/16 [==============================] - 0s 1ms/step - loss: 2.3141\n16/16 [==============================] - 0s 1ms/step - loss: 2.2801\n16/16 [==============================] - 0s 1ms/step - loss: 2.2488\n16/16 [==============================] - 0s 1ms/step - loss: 2.2227\n16/16 [==============================] - 0s 919us/step - loss: 2.2139\n16/16 [==============================] - 0s 966us/step - loss: 2.2103\nEpoch 39 of 60\n\nTesting for epoch 39 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1775\n16/16 [==============================] - 0s 644us/step - loss: 1.9059\n16/16 [==============================] - 0s 669us/step - loss: 2.2860\n16/16 [==============================] - 0s 1ms/step - loss: 2.3835\n16/16 [==============================] - 0s 664us/step - loss: 2.3656\n16/16 [==============================] - 0s 1ms/step - loss: 2.3309\n16/16 [==============================] - 0s 689us/step - loss: 2.2992\n16/16 [==============================] - 0s 1ms/step - loss: 2.2725\n16/16 [==============================] - 0s 1ms/step - loss: 2.2635\n16/16 [==============================] - 0s 1ms/step - loss: 2.2598\n\nTesting for epoch 39 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1770\n16/16 [==============================] - 0s 882us/step - loss: 1.8485\n16/16 [==============================] - 0s 1ms/step - loss: 2.2087\n16/16 [==============================] - 0s 843us/step - loss: 2.3001\n16/16 [==============================] - 0s 1ms/step - loss: 2.2820\n16/16 [==============================] - 0s 1ms/step - loss: 2.2470\n16/16 [==============================] - 0s 808us/step - loss: 2.2158\n16/16 [==============================] - 0s 674us/step - loss: 2.1898\n16/16 [==============================] - 0s 665us/step - loss: 2.1810\n16/16 [==============================] - 0s 666us/step - loss: 2.1774\n\nTesting for epoch 39 index 3:\n16/16 [==============================] - 0s 702us/step - loss: 0.1778\n16/16 [==============================] - 0s 1ms/step - loss: 1.8604\n16/16 [==============================] - 0s 1ms/step - loss: 2.2199\n16/16 [==============================] - 0s 1ms/step - loss: 2.3081\n16/16 [==============================] - 0s 1ms/step - loss: 2.2886\n16/16 [==============================] - 0s 1ms/step - loss: 2.2537\n16/16 [==============================] - 0s 899us/step - loss: 2.2230\n16/16 [==============================] - 0s 847us/step - loss: 2.1975\n16/16 [==============================] - 0s 674us/step - loss: 2.1890\n16/16 [==============================] - 0s 648us/step - loss: 2.1856\n\nTesting for epoch 39 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1772\n16/16 [==============================] - 0s 1ms/step - loss: 1.9029\n16/16 [==============================] - 0s 1ms/step - loss: 2.2795\n16/16 [==============================] - 0s 929us/step - loss: 2.3721\n16/16 [==============================] - 0s 656us/step - loss: 2.3526\n16/16 [==============================] - 0s 685us/step - loss: 2.3171\n16/16 [==============================] - 0s 697us/step - loss: 2.2856\n16/16 [==============================] - 0s 686us/step - loss: 2.2594\n16/16 [==============================] - 0s 1ms/step - loss: 2.2505\n16/16 [==============================] - 0s 938us/step - loss: 2.2469\n\nTesting for epoch 39 index 5:\n16/16 [==============================] - 0s 736us/step - loss: 0.1773\n16/16 [==============================] - 0s 1ms/step - loss: 1.8690\n16/16 [==============================] - 0s 1ms/step - loss: 2.2337\n16/16 [==============================] - 0s 997us/step - loss: 2.3199\n16/16 [==============================] - 0s 1ms/step - loss: 2.2990\n16/16 [==============================] - 0s 965us/step - loss: 2.2626\n16/16 [==============================] - 0s 1ms/step - loss: 2.2305\n16/16 [==============================] - 0s 681us/step - loss: 2.2046\n16/16 [==============================] - 0s 701us/step - loss: 2.1959\n16/16 [==============================] - 0s 684us/step - loss: 2.1924\nEpoch 40 of 60\n\nTesting for epoch 40 index 1:\n16/16 [==============================] - 0s 857us/step - loss: 0.1749\n16/16 [==============================] - 0s 647us/step - loss: 1.8422\n16/16 [==============================] - 0s 695us/step - loss: 2.2055\n16/16 [==============================] - 0s 1ms/step - loss: 2.2933\n16/16 [==============================] - 0s 858us/step - loss: 2.2752\n16/16 [==============================] - 0s 1ms/step - loss: 2.2414\n16/16 [==============================] - 0s 1ms/step - loss: 2.2106\n16/16 [==============================] - 0s 702us/step - loss: 2.1853\n16/16 [==============================] - 0s 1ms/step - loss: 2.1768\n16/16 [==============================] - 0s 653us/step - loss: 2.1733\n\nTesting for epoch 40 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 0.1735\n16/16 [==============================] - 0s 2ms/step - loss: 1.8843\n16/16 [==============================] - 0s 2ms/step - loss: 2.2440\n16/16 [==============================] - 0s 2ms/step - loss: 2.3291\n16/16 [==============================] - 0s 2ms/step - loss: 2.3069\n16/16 [==============================] - 0s 2ms/step - loss: 2.2710\n16/16 [==============================] - 0s 2ms/step - loss: 2.2398\n16/16 [==============================] - 0s 2ms/step - loss: 2.2144\n16/16 [==============================] - 0s 2ms/step - loss: 2.2058\n16/16 [==============================] - 0s 1ms/step - loss: 2.2023\n\nTesting for epoch 40 index 3:\n16/16 [==============================] - 0s 776us/step - loss: 0.1737\n16/16 [==============================] - 0s 2ms/step - loss: 1.8765\n16/16 [==============================] - 0s 2ms/step - loss: 2.2332\n16/16 [==============================] - 0s 2ms/step - loss: 2.3172\n16/16 [==============================] - 0s 2ms/step - loss: 2.2945\n16/16 [==============================] - 0s 2ms/step - loss: 2.2569\n16/16 [==============================] - 0s 3ms/step - loss: 2.2234\n16/16 [==============================] - 0s 2ms/step - loss: 2.1967\n16/16 [==============================] - 0s 2ms/step - loss: 2.1880\n16/16 [==============================] - 0s 2ms/step - loss: 2.1844\n\nTesting for epoch 40 index 4:\n16/16 [==============================] - 0s 2ms/step - loss: 0.1761\n16/16 [==============================] - 0s 716us/step - loss: 1.8934\n16/16 [==============================] - 0s 703us/step - loss: 2.2560\n16/16 [==============================] - 0s 675us/step - loss: 2.3469\n16/16 [==============================] - 0s 683us/step - loss: 2.3263\n16/16 [==============================] - 0s 735us/step - loss: 2.2914\n16/16 [==============================] - 0s 2ms/step - loss: 2.2595\n16/16 [==============================] - 0s 2ms/step - loss: 2.2333\n16/16 [==============================] - 0s 2ms/step - loss: 2.2245\n16/16 [==============================] - 0s 1ms/step - loss: 2.2209\n\nTesting for epoch 40 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1722\n16/16 [==============================] - 0s 2ms/step - loss: 1.9280\n16/16 [==============================] - 0s 1ms/step - loss: 2.2976\n16/16 [==============================] - 0s 1ms/step - loss: 2.3871\n16/16 [==============================] - 0s 1ms/step - loss: 2.3629\n16/16 [==============================] - 0s 1ms/step - loss: 2.3248\n16/16 [==============================] - 0s 2ms/step - loss: 2.2908\n16/16 [==============================] - 0s 1ms/step - loss: 2.2634\n16/16 [==============================] - 0s 2ms/step - loss: 2.2542\n16/16 [==============================] - 0s 2ms/step - loss: 2.2505\nEpoch 41 of 60\n\nTesting for epoch 41 index 1:\n16/16 [==============================] - 0s 665us/step - loss: 0.1699\n16/16 [==============================] - 0s 649us/step - loss: 1.9410\n16/16 [==============================] - 0s 642us/step - loss: 2.3137\n16/16 [==============================] - 0s 647us/step - loss: 2.4031\n16/16 [==============================] - 0s 634us/step - loss: 2.3771\n16/16 [==============================] - 0s 655us/step - loss: 2.3376\n16/16 [==============================] - 0s 2ms/step - loss: 2.3020\n16/16 [==============================] - 0s 1ms/step - loss: 2.2737\n16/16 [==============================] - 0s 670us/step - loss: 2.2643\n16/16 [==============================] - 0s 643us/step - loss: 2.2604\n\nTesting for epoch 41 index 2:\n16/16 [==============================] - 0s 629us/step - loss: 0.1723\n16/16 [==============================] - 0s 2ms/step - loss: 1.8916\n16/16 [==============================] - 0s 807us/step - loss: 2.2597\n16/16 [==============================] - 0s 933us/step - loss: 2.3476\n16/16 [==============================] - 0s 933us/step - loss: 2.3238\n16/16 [==============================] - 0s 923us/step - loss: 2.2886\n16/16 [==============================] - 0s 952us/step - loss: 2.2566\n16/16 [==============================] - 0s 1ms/step - loss: 2.2308\n16/16 [==============================] - 0s 995us/step - loss: 2.2222\n16/16 [==============================] - 0s 911us/step - loss: 2.2188\n\nTesting for epoch 41 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1698\n16/16 [==============================] - 0s 2ms/step - loss: 1.9182\n16/16 [==============================] - 0s 1ms/step - loss: 2.2868\n16/16 [==============================] - 0s 755us/step - loss: 2.3684\n16/16 [==============================] - 0s 652us/step - loss: 2.3398\n16/16 [==============================] - 0s 650us/step - loss: 2.3013\n16/16 [==============================] - 0s 648us/step - loss: 2.2666\n16/16 [==============================] - 0s 660us/step - loss: 2.2391\n16/16 [==============================] - 0s 1ms/step - loss: 2.2300\n16/16 [==============================] - 0s 2ms/step - loss: 2.2264\n\nTesting for epoch 41 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1686\n16/16 [==============================] - 0s 648us/step - loss: 1.9577\n16/16 [==============================] - 0s 887us/step - loss: 2.3428\n16/16 [==============================] - 0s 1ms/step - loss: 2.4313\n16/16 [==============================] - 0s 645us/step - loss: 2.4052\n16/16 [==============================] - 0s 1ms/step - loss: 2.3674\n16/16 [==============================] - 0s 2ms/step - loss: 2.3327\n16/16 [==============================] - 0s 653us/step - loss: 2.3051\n16/16 [==============================] - 0s 2ms/step - loss: 2.2958\n16/16 [==============================] - 0s 2ms/step - loss: 2.2921\n\nTesting for epoch 41 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1707\n16/16 [==============================] - 0s 1ms/step - loss: 1.9381\n16/16 [==============================] - 0s 626us/step - loss: 2.3141\n16/16 [==============================] - 0s 2ms/step - loss: 2.3976\n16/16 [==============================] - 0s 1ms/step - loss: 2.3711\n16/16 [==============================] - 0s 2ms/step - loss: 2.3333\n16/16 [==============================] - 0s 2ms/step - loss: 2.2988\n16/16 [==============================] - 0s 2ms/step - loss: 2.2715\n16/16 [==============================] - 0s 1ms/step - loss: 2.2624\n16/16 [==============================] - 0s 2ms/step - loss: 2.2588\nEpoch 42 of 60\n\nTesting for epoch 42 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 0.1687\n16/16 [==============================] - 0s 2ms/step - loss: 1.9747\n16/16 [==============================] - 0s 2ms/step - loss: 2.3635\n16/16 [==============================] - 0s 2ms/step - loss: 2.4468\n16/16 [==============================] - 0s 2ms/step - loss: 2.4175\n16/16 [==============================] - 0s 2ms/step - loss: 2.3761\n16/16 [==============================] - 0s 1ms/step - loss: 2.3383\n16/16 [==============================] - 0s 2ms/step - loss: 2.3088\n16/16 [==============================] - 0s 1ms/step - loss: 2.2990\n16/16 [==============================] - 0s 2ms/step - loss: 2.2950\n\nTesting for epoch 42 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 0.1671\n16/16 [==============================] - 0s 1ms/step - loss: 1.9391\n16/16 [==============================] - 0s 2ms/step - loss: 2.3156\n16/16 [==============================] - 0s 1ms/step - loss: 2.3979\n16/16 [==============================] - 0s 2ms/step - loss: 2.3719\n16/16 [==============================] - 0s 2ms/step - loss: 2.3335\n16/16 [==============================] - 0s 2ms/step - loss: 2.2984\n16/16 [==============================] - 0s 2ms/step - loss: 2.2707\n16/16 [==============================] - 0s 2ms/step - loss: 2.2614\n16/16 [==============================] - 0s 2ms/step - loss: 2.2577\n\nTesting for epoch 42 index 3:\n16/16 [==============================] - 0s 2ms/step - loss: 0.1662\n16/16 [==============================] - 0s 1ms/step - loss: 1.9564\n16/16 [==============================] - 0s 1ms/step - loss: 2.3366\n16/16 [==============================] - 0s 2ms/step - loss: 2.4170\n16/16 [==============================] - 0s 2ms/step - loss: 2.3906\n16/16 [==============================] - 0s 2ms/step - loss: 2.3517\n16/16 [==============================] - 0s 1ms/step - loss: 2.3168\n16/16 [==============================] - 0s 1ms/step - loss: 2.2891\n16/16 [==============================] - 0s 2ms/step - loss: 2.2799\n16/16 [==============================] - 0s 3ms/step - loss: 2.2762\n\nTesting for epoch 42 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1664\n16/16 [==============================] - 0s 836us/step - loss: 1.9228\n16/16 [==============================] - 0s 857us/step - loss: 2.2967\n16/16 [==============================] - 0s 827us/step - loss: 2.3769\n16/16 [==============================] - 0s 2ms/step - loss: 2.3522\n16/16 [==============================] - 0s 2ms/step - loss: 2.3146\n16/16 [==============================] - 0s 2ms/step - loss: 2.2804\n16/16 [==============================] - 0s 2ms/step - loss: 2.2535\n16/16 [==============================] - 0s 2ms/step - loss: 2.2446\n16/16 [==============================] - 0s 2ms/step - loss: 2.2410\n\nTesting for epoch 42 index 5:\n16/16 [==============================] - 0s 968us/step - loss: 0.1647\n16/16 [==============================] - 0s 836us/step - loss: 1.9825\n16/16 [==============================] - 0s 1ms/step - loss: 2.3655\n16/16 [==============================] - 0s 991us/step - loss: 2.4400\n16/16 [==============================] - 0s 708us/step - loss: 2.4065\n16/16 [==============================] - 0s 988us/step - loss: 2.3618\n16/16 [==============================] - 0s 1ms/step - loss: 2.3236\n16/16 [==============================] - 0s 769us/step - loss: 2.2942\n16/16 [==============================] - 0s 630us/step - loss: 2.2846\n16/16 [==============================] - 0s 832us/step - loss: 2.2808\nEpoch 43 of 60\n\nTesting for epoch 43 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 0.1616\n16/16 [==============================] - 0s 2ms/step - loss: 1.9799\n16/16 [==============================] - 0s 2ms/step - loss: 2.3682\n16/16 [==============================] - 0s 2ms/step - loss: 2.4457\n16/16 [==============================] - 0s 2ms/step - loss: 2.4156\n16/16 [==============================] - 0s 2ms/step - loss: 2.3727\n16/16 [==============================] - 0s 2ms/step - loss: 2.3349\n16/16 [==============================] - 0s 2ms/step - loss: 2.3056\n16/16 [==============================] - 0s 2ms/step - loss: 2.2959\n16/16 [==============================] - 0s 2ms/step - loss: 2.2921\n\nTesting for epoch 43 index 2:\n16/16 [==============================] - 0s 594us/step - loss: 0.1670\n16/16 [==============================] - 0s 542us/step - loss: 1.9252\n16/16 [==============================] - 0s 603us/step - loss: 2.3035\n16/16 [==============================] - 0s 638us/step - loss: 2.3810\n16/16 [==============================] - 0s 646us/step - loss: 2.3532\n16/16 [==============================] - 0s 490us/step - loss: 2.3132\n16/16 [==============================] - 0s 517us/step - loss: 2.2779\n16/16 [==============================] - 0s 1ms/step - loss: 2.2503\n16/16 [==============================] - 0s 895us/step - loss: 2.2412\n16/16 [==============================] - 0s 570us/step - loss: 2.2376\n\nTesting for epoch 43 index 3:\n16/16 [==============================] - 0s 626us/step - loss: 0.1649\n16/16 [==============================] - 0s 921us/step - loss: 1.9655\n16/16 [==============================] - 0s 1ms/step - loss: 2.3495\n16/16 [==============================] - 0s 708us/step - loss: 2.4250\n16/16 [==============================] - 0s 1ms/step - loss: 2.3950\n16/16 [==============================] - 0s 1ms/step - loss: 2.3532\n16/16 [==============================] - 0s 681us/step - loss: 2.3167\n16/16 [==============================] - 0s 632us/step - loss: 2.2886\n16/16 [==============================] - 0s 1ms/step - loss: 2.2794\n16/16 [==============================] - 0s 644us/step - loss: 2.2756\n\nTesting for epoch 43 index 4:\n16/16 [==============================] - 0s 716us/step - loss: 0.1628\n16/16 [==============================] - 0s 631us/step - loss: 1.9172\n16/16 [==============================] - 0s 634us/step - loss: 2.2852\n16/16 [==============================] - 0s 601us/step - loss: 2.3543\n16/16 [==============================] - 0s 1ms/step - loss: 2.3241\n16/16 [==============================] - 0s 1ms/step - loss: 2.2817\n16/16 [==============================] - 0s 1ms/step - loss: 2.2446\n16/16 [==============================] - 0s 637us/step - loss: 2.2162\n16/16 [==============================] - 0s 1ms/step - loss: 2.2068\n16/16 [==============================] - 0s 622us/step - loss: 2.2031\n\nTesting for epoch 43 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1632\n16/16 [==============================] - 0s 614us/step - loss: 1.9494\n16/16 [==============================] - 0s 585us/step - loss: 2.3248\n16/16 [==============================] - 0s 1ms/step - loss: 2.3958\n16/16 [==============================] - 0s 605us/step - loss: 2.3639\n16/16 [==============================] - 0s 1ms/step - loss: 2.3212\n16/16 [==============================] - 0s 659us/step - loss: 2.2846\n16/16 [==============================] - 0s 920us/step - loss: 2.2564\n16/16 [==============================] - 0s 642us/step - loss: 2.2472\n16/16 [==============================] - 0s 1ms/step - loss: 2.2436\nEpoch 44 of 60\n\nTesting for epoch 44 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1623\n16/16 [==============================] - 0s 632us/step - loss: 1.9453\n16/16 [==============================] - 0s 701us/step - loss: 2.3227\n16/16 [==============================] - 0s 1ms/step - loss: 2.3937\n16/16 [==============================] - 0s 598us/step - loss: 2.3616\n16/16 [==============================] - 0s 623us/step - loss: 2.3172\n16/16 [==============================] - 0s 1ms/step - loss: 2.2785\n16/16 [==============================] - 0s 802us/step - loss: 2.2493\n16/16 [==============================] - 0s 1ms/step - loss: 2.2398\n16/16 [==============================] - 0s 900us/step - loss: 2.2360\n\nTesting for epoch 44 index 2:\n16/16 [==============================] - 0s 639us/step - loss: 0.1612\n16/16 [==============================] - 0s 709us/step - loss: 1.9442\n16/16 [==============================] - 0s 617us/step - loss: 2.3257\n16/16 [==============================] - 0s 598us/step - loss: 2.4020\n16/16 [==============================] - 0s 1ms/step - loss: 2.3753\n16/16 [==============================] - 0s 625us/step - loss: 2.3369\n16/16 [==============================] - 0s 611us/step - loss: 2.3024\n16/16 [==============================] - 0s 601us/step - loss: 2.2753\n16/16 [==============================] - 0s 764us/step - loss: 2.2663\n16/16 [==============================] - 0s 1ms/step - loss: 2.2626\n\nTesting for epoch 44 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1585\n16/16 [==============================] - 0s 1ms/step - loss: 2.0528\n16/16 [==============================] - 0s 609us/step - loss: 2.4585\n16/16 [==============================] - 0s 789us/step - loss: 2.5358\n16/16 [==============================] - 0s 587us/step - loss: 2.5023\n16/16 [==============================] - 0s 672us/step - loss: 2.4563\n16/16 [==============================] - 0s 805us/step - loss: 2.4162\n16/16 [==============================] - 0s 1ms/step - loss: 2.3857\n16/16 [==============================] - 0s 1ms/step - loss: 2.3757\n16/16 [==============================] - 0s 1ms/step - loss: 2.3718\n\nTesting for epoch 44 index 4:\n16/16 [==============================] - 0s 735us/step - loss: 0.1614\n16/16 [==============================] - 0s 1ms/step - loss: 2.0477\n16/16 [==============================] - 0s 646us/step - loss: 2.4531\n16/16 [==============================] - 0s 1ms/step - loss: 2.5273\n16/16 [==============================] - 0s 560us/step - loss: 2.4941\n16/16 [==============================] - 0s 604us/step - loss: 2.4496\n16/16 [==============================] - 0s 1ms/step - loss: 2.4104\n16/16 [==============================] - 0s 599us/step - loss: 2.3801\n16/16 [==============================] - 0s 1ms/step - loss: 2.3702\n16/16 [==============================] - 0s 1ms/step - loss: 2.3661\n\nTesting for epoch 44 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1584\n16/16 [==============================] - 0s 1ms/step - loss: 2.0297\n16/16 [==============================] - 0s 1ms/step - loss: 2.4309\n16/16 [==============================] - 0s 1ms/step - loss: 2.5028\n16/16 [==============================] - 0s 844us/step - loss: 2.4704\n16/16 [==============================] - 0s 721us/step - loss: 2.4258\n16/16 [==============================] - 0s 1ms/step - loss: 2.3870\n16/16 [==============================] - 0s 724us/step - loss: 2.3575\n16/16 [==============================] - 0s 1ms/step - loss: 2.3479\n16/16 [==============================] - 0s 1ms/step - loss: 2.3440\nEpoch 45 of 60\n\nTesting for epoch 45 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1607\n16/16 [==============================] - 0s 1ms/step - loss: 1.9936\n16/16 [==============================] - 0s 1ms/step - loss: 2.3817\n16/16 [==============================] - 0s 618us/step - loss: 2.4495\n16/16 [==============================] - 0s 1ms/step - loss: 2.4171\n16/16 [==============================] - 0s 1ms/step - loss: 2.3735\n16/16 [==============================] - 0s 929us/step - loss: 2.3355\n16/16 [==============================] - 0s 1ms/step - loss: 2.3067\n16/16 [==============================] - 0s 1ms/step - loss: 2.2974\n16/16 [==============================] - 0s 967us/step - loss: 2.2937\n\nTesting for epoch 45 index 2:\n16/16 [==============================] - 0s 625us/step - loss: 0.1592\n16/16 [==============================] - 0s 748us/step - loss: 2.0080\n16/16 [==============================] - 0s 948us/step - loss: 2.3934\n16/16 [==============================] - 0s 626us/step - loss: 2.4562\n16/16 [==============================] - 0s 1ms/step - loss: 2.4202\n16/16 [==============================] - 0s 1ms/step - loss: 2.3733\n16/16 [==============================] - 0s 600us/step - loss: 2.3338\n16/16 [==============================] - 0s 1ms/step - loss: 2.3041\n16/16 [==============================] - 0s 1ms/step - loss: 2.2945\n16/16 [==============================] - 0s 579us/step - loss: 2.2907\n\nTesting for epoch 45 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1563\n16/16 [==============================] - 0s 1ms/step - loss: 1.9989\n16/16 [==============================] - 0s 791us/step - loss: 2.3823\n16/16 [==============================] - 0s 592us/step - loss: 2.4454\n16/16 [==============================] - 0s 657us/step - loss: 2.4112\n16/16 [==============================] - 0s 731us/step - loss: 2.3670\n16/16 [==============================] - 0s 1ms/step - loss: 2.3288\n16/16 [==============================] - 0s 745us/step - loss: 2.2995\n16/16 [==============================] - 0s 595us/step - loss: 2.2900\n16/16 [==============================] - 0s 629us/step - loss: 2.2862\n\nTesting for epoch 45 index 4:\n16/16 [==============================] - 0s 948us/step - loss: 0.1570\n16/16 [==============================] - 0s 860us/step - loss: 1.9843\n16/16 [==============================] - 0s 836us/step - loss: 2.3656\n16/16 [==============================] - 0s 1ms/step - loss: 2.4254\n16/16 [==============================] - 0s 1ms/step - loss: 2.3894\n16/16 [==============================] - 0s 1ms/step - loss: 2.3424\n16/16 [==============================] - 0s 644us/step - loss: 2.3025\n16/16 [==============================] - 0s 606us/step - loss: 2.2727\n16/16 [==============================] - 0s 1ms/step - loss: 2.2631\n16/16 [==============================] - 0s 810us/step - loss: 2.2593\n\nTesting for epoch 45 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1540\n16/16 [==============================] - 0s 621us/step - loss: 2.0578\n16/16 [==============================] - 0s 754us/step - loss: 2.4644\n16/16 [==============================] - 0s 1ms/step - loss: 2.5294\n16/16 [==============================] - 0s 1ms/step - loss: 2.4946\n16/16 [==============================] - 0s 1ms/step - loss: 2.4481\n16/16 [==============================] - 0s 1ms/step - loss: 2.4079\n16/16 [==============================] - 0s 759us/step - loss: 2.3773\n16/16 [==============================] - 0s 1ms/step - loss: 2.3674\n16/16 [==============================] - 0s 634us/step - loss: 2.3635\nEpoch 46 of 60\n\nTesting for epoch 46 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1569\n16/16 [==============================] - 0s 624us/step - loss: 2.0009\n16/16 [==============================] - 0s 644us/step - loss: 2.3983\n16/16 [==============================] - 0s 620us/step - loss: 2.4650\n16/16 [==============================] - 0s 1ms/step - loss: 2.4337\n16/16 [==============================] - 0s 657us/step - loss: 2.3895\n16/16 [==============================] - 0s 662us/step - loss: 2.3506\n16/16 [==============================] - 0s 1ms/step - loss: 2.3207\n16/16 [==============================] - 0s 826us/step - loss: 2.3109\n16/16 [==============================] - 0s 1ms/step - loss: 2.3070\n\nTesting for epoch 46 index 2:\n16/16 [==============================] - 0s 648us/step - loss: 0.1564\n16/16 [==============================] - 0s 906us/step - loss: 1.9920\n16/16 [==============================] - 0s 1ms/step - loss: 2.3850\n16/16 [==============================] - 0s 632us/step - loss: 2.4473\n16/16 [==============================] - 0s 914us/step - loss: 2.4129\n16/16 [==============================] - 0s 650us/step - loss: 2.3665\n16/16 [==============================] - 0s 1ms/step - loss: 2.3262\n16/16 [==============================] - 0s 1ms/step - loss: 2.2963\n16/16 [==============================] - 0s 631us/step - loss: 2.2867\n16/16 [==============================] - 0s 1ms/step - loss: 2.2828\n\nTesting for epoch 46 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1523\n16/16 [==============================] - 0s 610us/step - loss: 1.9810\n16/16 [==============================] - 0s 1ms/step - loss: 2.3695\n16/16 [==============================] - 0s 599us/step - loss: 2.4291\n16/16 [==============================] - 0s 1ms/step - loss: 2.3939\n16/16 [==============================] - 0s 596us/step - loss: 2.3485\n16/16 [==============================] - 0s 651us/step - loss: 2.3100\n16/16 [==============================] - 0s 1ms/step - loss: 2.2810\n16/16 [==============================] - 0s 1ms/step - loss: 2.2716\n16/16 [==============================] - 0s 789us/step - loss: 2.2678\n\nTesting for epoch 46 index 4:\n16/16 [==============================] - 0s 605us/step - loss: 0.1539\n16/16 [==============================] - 0s 946us/step - loss: 2.0070\n16/16 [==============================] - 0s 1ms/step - loss: 2.3983\n16/16 [==============================] - 0s 562us/step - loss: 2.4547\n16/16 [==============================] - 0s 1ms/step - loss: 2.4185\n16/16 [==============================] - 0s 646us/step - loss: 2.3714\n16/16 [==============================] - 0s 1ms/step - loss: 2.3316\n16/16 [==============================] - 0s 1ms/step - loss: 2.3019\n16/16 [==============================] - 0s 1ms/step - loss: 2.2923\n16/16 [==============================] - 0s 629us/step - loss: 2.2885\n\nTesting for epoch 46 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1531\n16/16 [==============================] - 0s 1ms/step - loss: 2.0956\n16/16 [==============================] - 0s 1ms/step - loss: 2.5234\n16/16 [==============================] - 0s 649us/step - loss: 2.5872\n16/16 [==============================] - 0s 1ms/step - loss: 2.5491\n16/16 [==============================] - 0s 1ms/step - loss: 2.4983\n16/16 [==============================] - 0s 1ms/step - loss: 2.4550\n16/16 [==============================] - 0s 1ms/step - loss: 2.4226\n16/16 [==============================] - 0s 1ms/step - loss: 2.4122\n16/16 [==============================] - 0s 1ms/step - loss: 2.4080\nEpoch 47 of 60\n\nTesting for epoch 47 index 1:\n16/16 [==============================] - 0s 631us/step - loss: 0.1539\n16/16 [==============================] - 0s 877us/step - loss: 2.0259\n16/16 [==============================] - 0s 1ms/step - loss: 2.4331\n16/16 [==============================] - 0s 724us/step - loss: 2.4907\n16/16 [==============================] - 0s 631us/step - loss: 2.4546\n16/16 [==============================] - 0s 631us/step - loss: 2.4070\n16/16 [==============================] - 0s 1ms/step - loss: 2.3665\n16/16 [==============================] - 0s 1ms/step - loss: 2.3359\n16/16 [==============================] - 0s 995us/step - loss: 2.3261\n16/16 [==============================] - 0s 1ms/step - loss: 2.3222\n\nTesting for epoch 47 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1519\n16/16 [==============================] - 0s 639us/step - loss: 2.0542\n16/16 [==============================] - 0s 934us/step - loss: 2.4779\n16/16 [==============================] - 0s 1ms/step - loss: 2.5430\n16/16 [==============================] - 0s 666us/step - loss: 2.5111\n16/16 [==============================] - 0s 645us/step - loss: 2.4667\n16/16 [==============================] - 0s 606us/step - loss: 2.4283\n16/16 [==============================] - 0s 842us/step - loss: 2.3987\n16/16 [==============================] - 0s 617us/step - loss: 2.3890\n16/16 [==============================] - 0s 1ms/step - loss: 2.3851\n\nTesting for epoch 47 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1528\n16/16 [==============================] - 0s 957us/step - loss: 2.0201\n16/16 [==============================] - 0s 1ms/step - loss: 2.4226\n16/16 [==============================] - 0s 610us/step - loss: 2.4747\n16/16 [==============================] - 0s 626us/step - loss: 2.4384\n16/16 [==============================] - 0s 1ms/step - loss: 2.3899\n16/16 [==============================] - 0s 1ms/step - loss: 2.3487\n16/16 [==============================] - 0s 1ms/step - loss: 2.3181\n16/16 [==============================] - 0s 1ms/step - loss: 2.3083\n16/16 [==============================] - 0s 635us/step - loss: 2.3044\n\nTesting for epoch 47 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1513\n16/16 [==============================] - 0s 1ms/step - loss: 2.0221\n16/16 [==============================] - 0s 953us/step - loss: 2.4286\n16/16 [==============================] - 0s 584us/step - loss: 2.4816\n16/16 [==============================] - 0s 794us/step - loss: 2.4463\n16/16 [==============================] - 0s 1ms/step - loss: 2.3995\n16/16 [==============================] - 0s 1ms/step - loss: 2.3596\n16/16 [==============================] - 0s 1ms/step - loss: 2.3298\n16/16 [==============================] - 0s 635us/step - loss: 2.3202\n16/16 [==============================] - 0s 1ms/step - loss: 2.3163\n\nTesting for epoch 47 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1505\n16/16 [==============================] - 0s 1ms/step - loss: 2.0238\n16/16 [==============================] - 0s 1ms/step - loss: 2.4319\n16/16 [==============================] - 0s 1ms/step - loss: 2.4849\n16/16 [==============================] - 0s 628us/step - loss: 2.4502\n16/16 [==============================] - 0s 1ms/step - loss: 2.4034\n16/16 [==============================] - 0s 660us/step - loss: 2.3643\n16/16 [==============================] - 0s 614us/step - loss: 2.3350\n16/16 [==============================] - 0s 628us/step - loss: 2.3256\n16/16 [==============================] - 0s 1ms/step - loss: 2.3218\nEpoch 48 of 60\n\nTesting for epoch 48 index 1:\n16/16 [==============================] - 0s 637us/step - loss: 0.1506\n16/16 [==============================] - 0s 929us/step - loss: 2.0292\n16/16 [==============================] - 0s 635us/step - loss: 2.4452\n16/16 [==============================] - 0s 625us/step - loss: 2.5006\n16/16 [==============================] - 0s 639us/step - loss: 2.4658\n16/16 [==============================] - 0s 1ms/step - loss: 2.4185\n16/16 [==============================] - 0s 640us/step - loss: 2.3780\n16/16 [==============================] - 0s 1ms/step - loss: 2.3475\n16/16 [==============================] - 0s 593us/step - loss: 2.3377\n16/16 [==============================] - 0s 947us/step - loss: 2.3338\n\nTesting for epoch 48 index 2:\n16/16 [==============================] - 0s 642us/step - loss: 0.1520\n16/16 [==============================] - 0s 927us/step - loss: 2.0468\n16/16 [==============================] - 0s 907us/step - loss: 2.4645\n16/16 [==============================] - 0s 1ms/step - loss: 2.5202\n16/16 [==============================] - 0s 721us/step - loss: 2.4857\n16/16 [==============================] - 0s 1ms/step - loss: 2.4381\n16/16 [==============================] - 0s 1ms/step - loss: 2.3972\n16/16 [==============================] - 0s 1ms/step - loss: 2.3664\n16/16 [==============================] - 0s 755us/step - loss: 2.3565\n16/16 [==============================] - 0s 1ms/step - loss: 2.3525\n\nTesting for epoch 48 index 3:\n16/16 [==============================] - 0s 836us/step - loss: 0.1459\n16/16 [==============================] - 0s 1ms/step - loss: 2.0895\n16/16 [==============================] - 0s 1ms/step - loss: 2.5139\n16/16 [==============================] - 0s 1ms/step - loss: 2.5640\n16/16 [==============================] - 0s 773us/step - loss: 2.5243\n16/16 [==============================] - 0s 754us/step - loss: 2.4719\n16/16 [==============================] - 0s 617us/step - loss: 2.4279\n16/16 [==============================] - 0s 1ms/step - loss: 2.3952\n16/16 [==============================] - 0s 935us/step - loss: 2.3847\n16/16 [==============================] - 0s 1ms/step - loss: 2.3804\n\nTesting for epoch 48 index 4:\n16/16 [==============================] - 0s 635us/step - loss: 0.1465\n16/16 [==============================] - 0s 640us/step - loss: 2.0292\n16/16 [==============================] - 0s 610us/step - loss: 2.4415\n16/16 [==============================] - 0s 635us/step - loss: 2.4915\n16/16 [==============================] - 0s 634us/step - loss: 2.4547\n16/16 [==============================] - 0s 621us/step - loss: 2.4048\n16/16 [==============================] - 0s 644us/step - loss: 2.3627\n16/16 [==============================] - 0s 750us/step - loss: 2.3313\n16/16 [==============================] - 0s 1ms/step - loss: 2.3213\n16/16 [==============================] - 0s 705us/step - loss: 2.3173\n\nTesting for epoch 48 index 5:\n16/16 [==============================] - 0s 624us/step - loss: 0.1449\n16/16 [==============================] - 0s 1ms/step - loss: 2.0603\n16/16 [==============================] - 0s 1ms/step - loss: 2.4760\n16/16 [==============================] - 0s 1ms/step - loss: 2.5243\n16/16 [==============================] - 0s 1ms/step - loss: 2.4877\n16/16 [==============================] - 0s 1ms/step - loss: 2.4388\n16/16 [==============================] - 0s 1ms/step - loss: 2.3979\n16/16 [==============================] - 0s 615us/step - loss: 2.3674\n16/16 [==============================] - 0s 1ms/step - loss: 2.3576\n16/16 [==============================] - 0s 1ms/step - loss: 2.3537\nEpoch 49 of 60\n\nTesting for epoch 49 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1456\n16/16 [==============================] - 0s 1ms/step - loss: 2.1177\n16/16 [==============================] - 0s 1ms/step - loss: 2.5539\n16/16 [==============================] - 0s 1ms/step - loss: 2.6038\n16/16 [==============================] - 0s 641us/step - loss: 2.5627\n16/16 [==============================] - 0s 1ms/step - loss: 2.5085\n16/16 [==============================] - 0s 1ms/step - loss: 2.4635\n16/16 [==============================] - 0s 1ms/step - loss: 2.4301\n16/16 [==============================] - 0s 1ms/step - loss: 2.4196\n16/16 [==============================] - 0s 1ms/step - loss: 2.4155\n\nTesting for epoch 49 index 2:\n16/16 [==============================] - 0s 633us/step - loss: 0.1446\n16/16 [==============================] - 0s 650us/step - loss: 2.0868\n16/16 [==============================] - 0s 634us/step - loss: 2.5135\n16/16 [==============================] - 0s 605us/step - loss: 2.5636\n16/16 [==============================] - 0s 1ms/step - loss: 2.5272\n16/16 [==============================] - 0s 640us/step - loss: 2.4769\n16/16 [==============================] - 0s 1ms/step - loss: 2.4353\n16/16 [==============================] - 0s 608us/step - loss: 2.4045\n16/16 [==============================] - 0s 1ms/step - loss: 2.3946\n16/16 [==============================] - 0s 1ms/step - loss: 2.3907\n\nTesting for epoch 49 index 3:\n16/16 [==============================] - 0s 649us/step - loss: 0.1457\n16/16 [==============================] - 0s 624us/step - loss: 2.0862\n16/16 [==============================] - 0s 875us/step - loss: 2.5107\n16/16 [==============================] - 0s 590us/step - loss: 2.5578\n16/16 [==============================] - 0s 573us/step - loss: 2.5165\n16/16 [==============================] - 0s 1ms/step - loss: 2.4632\n16/16 [==============================] - 0s 1ms/step - loss: 2.4191\n16/16 [==============================] - 0s 1ms/step - loss: 2.3864\n16/16 [==============================] - 0s 1ms/step - loss: 2.3760\n16/16 [==============================] - 0s 611us/step - loss: 2.3718\n\nTesting for epoch 49 index 4:\n16/16 [==============================] - 0s 664us/step - loss: 0.1438\n16/16 [==============================] - 0s 610us/step - loss: 2.0347\n16/16 [==============================] - 0s 601us/step - loss: 2.4431\n16/16 [==============================] - 0s 1ms/step - loss: 2.4871\n16/16 [==============================] - 0s 636us/step - loss: 2.4479\n16/16 [==============================] - 0s 606us/step - loss: 2.3982\n16/16 [==============================] - 0s 618us/step - loss: 2.3571\n16/16 [==============================] - 0s 639us/step - loss: 2.3267\n16/16 [==============================] - 0s 657us/step - loss: 2.3170\n16/16 [==============================] - 0s 629us/step - loss: 2.3131\n\nTesting for epoch 49 index 5:\n16/16 [==============================] - 0s 641us/step - loss: 0.1474\n16/16 [==============================] - 0s 988us/step - loss: 2.0366\n16/16 [==============================] - 0s 1ms/step - loss: 2.4480\n16/16 [==============================] - 0s 903us/step - loss: 2.4936\n16/16 [==============================] - 0s 1ms/step - loss: 2.4559\n16/16 [==============================] - 0s 1ms/step - loss: 2.4066\n16/16 [==============================] - 0s 635us/step - loss: 2.3658\n16/16 [==============================] - 0s 1ms/step - loss: 2.3355\n16/16 [==============================] - 0s 797us/step - loss: 2.3259\n16/16 [==============================] - 0s 735us/step - loss: 2.3222\nEpoch 50 of 60\n\nTesting for epoch 50 index 1:\n16/16 [==============================] - 0s 696us/step - loss: 0.1436\n16/16 [==============================] - 0s 905us/step - loss: 2.1021\n16/16 [==============================] - 0s 841us/step - loss: 2.5288\n16/16 [==============================] - 0s 1ms/step - loss: 2.5741\n16/16 [==============================] - 0s 623us/step - loss: 2.5332\n16/16 [==============================] - 0s 637us/step - loss: 2.4820\n16/16 [==============================] - 0s 644us/step - loss: 2.4395\n16/16 [==============================] - 0s 949us/step - loss: 2.4076\n16/16 [==============================] - 0s 640us/step - loss: 2.3974\n16/16 [==============================] - 0s 1ms/step - loss: 2.3933\n\nTesting for epoch 50 index 2:\n16/16 [==============================] - 0s 745us/step - loss: 0.1413\n16/16 [==============================] - 0s 1ms/step - loss: 2.0536\n16/16 [==============================] - 0s 1ms/step - loss: 2.4615\n16/16 [==============================] - 0s 1ms/step - loss: 2.4976\n16/16 [==============================] - 0s 1ms/step - loss: 2.4550\n16/16 [==============================] - 0s 1ms/step - loss: 2.4020\n16/16 [==============================] - 0s 1ms/step - loss: 2.3590\n16/16 [==============================] - 0s 1ms/step - loss: 2.3276\n16/16 [==============================] - 0s 1ms/step - loss: 2.3176\n16/16 [==============================] - 0s 1ms/step - loss: 2.3137\n\nTesting for epoch 50 index 3:\n16/16 [==============================] - 0s 624us/step - loss: 0.1436\n16/16 [==============================] - 0s 1ms/step - loss: 2.0689\n16/16 [==============================] - 0s 1ms/step - loss: 2.4842\n16/16 [==============================] - 0s 1ms/step - loss: 2.5212\n16/16 [==============================] - 0s 1ms/step - loss: 2.4800\n16/16 [==============================] - 0s 613us/step - loss: 2.4299\n16/16 [==============================] - 0s 614us/step - loss: 2.3885\n16/16 [==============================] - 0s 622us/step - loss: 2.3575\n16/16 [==============================] - 0s 1ms/step - loss: 2.3477\n16/16 [==============================] - 0s 1ms/step - loss: 2.3438\n\nTesting for epoch 50 index 4:\n16/16 [==============================] - 0s 636us/step - loss: 0.1412\n16/16 [==============================] - 0s 622us/step - loss: 2.0655\n16/16 [==============================] - 0s 584us/step - loss: 2.4782\n16/16 [==============================] - 0s 999us/step - loss: 2.5137\n16/16 [==============================] - 0s 648us/step - loss: 2.4722\n16/16 [==============================] - 0s 843us/step - loss: 2.4209\n16/16 [==============================] - 0s 889us/step - loss: 2.3791\n16/16 [==============================] - 0s 1ms/step - loss: 2.3482\n16/16 [==============================] - 0s 1ms/step - loss: 2.3383\n16/16 [==============================] - 0s 1ms/step - loss: 2.3344\n\nTesting for epoch 50 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1411\n16/16 [==============================] - 0s 608us/step - loss: 2.0600\n16/16 [==============================] - 0s 591us/step - loss: 2.4682\n16/16 [==============================] - 0s 1ms/step - loss: 2.5009\n16/16 [==============================] - 0s 885us/step - loss: 2.4578\n16/16 [==============================] - 0s 1ms/step - loss: 2.4045\n16/16 [==============================] - 0s 918us/step - loss: 2.3612\n16/16 [==============================] - 0s 1ms/step - loss: 2.3293\n16/16 [==============================] - 0s 1ms/step - loss: 2.3193\n16/16 [==============================] - 0s 648us/step - loss: 2.3153\nEpoch 51 of 60\n\nTesting for epoch 51 index 1:\n16/16 [==============================] - 0s 658us/step - loss: 0.1459\n16/16 [==============================] - 0s 1ms/step - loss: 2.0524\n16/16 [==============================] - 0s 593us/step - loss: 2.4624\n16/16 [==============================] - 0s 1ms/step - loss: 2.4994\n16/16 [==============================] - 0s 1ms/step - loss: 2.4614\n16/16 [==============================] - 0s 647us/step - loss: 2.4144\n16/16 [==============================] - 0s 956us/step - loss: 2.3751\n16/16 [==============================] - 0s 619us/step - loss: 2.3456\n16/16 [==============================] - 0s 1ms/step - loss: 2.3363\n16/16 [==============================] - 0s 1ms/step - loss: 2.3326\n\nTesting for epoch 51 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1414\n16/16 [==============================] - 0s 691us/step - loss: 2.1374\n16/16 [==============================] - 0s 586us/step - loss: 2.5654\n16/16 [==============================] - 0s 1ms/step - loss: 2.5987\n16/16 [==============================] - 0s 601us/step - loss: 2.5514\n16/16 [==============================] - 0s 615us/step - loss: 2.4948\n16/16 [==============================] - 0s 952us/step - loss: 2.4484\n16/16 [==============================] - 0s 996us/step - loss: 2.4143\n16/16 [==============================] - 0s 1ms/step - loss: 2.4035\n16/16 [==============================] - 0s 1ms/step - loss: 2.3993\n\nTesting for epoch 51 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1408\n16/16 [==============================] - 0s 1ms/step - loss: 2.1433\n16/16 [==============================] - 0s 1ms/step - loss: 2.5741\n16/16 [==============================] - 0s 1ms/step - loss: 2.6115\n16/16 [==============================] - 0s 684us/step - loss: 2.5677\n16/16 [==============================] - 0s 566us/step - loss: 2.5132\n16/16 [==============================] - 0s 617us/step - loss: 2.4685\n16/16 [==============================] - 0s 1ms/step - loss: 2.4356\n16/16 [==============================] - 0s 1ms/step - loss: 2.4250\n16/16 [==============================] - 0s 977us/step - loss: 2.4208\n\nTesting for epoch 51 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1386\n16/16 [==============================] - 0s 626us/step - loss: 2.2159\n16/16 [==============================] - 0s 607us/step - loss: 2.6627\n16/16 [==============================] - 0s 1ms/step - loss: 2.7016\n16/16 [==============================] - 0s 616us/step - loss: 2.6579\n16/16 [==============================] - 0s 624us/step - loss: 2.6043\n16/16 [==============================] - 0s 593us/step - loss: 2.5597\n16/16 [==============================] - 0s 1ms/step - loss: 2.5264\n16/16 [==============================] - 0s 596us/step - loss: 2.5158\n16/16 [==============================] - 0s 1ms/step - loss: 2.5116\n\nTesting for epoch 51 index 5:\n16/16 [==============================] - 0s 988us/step - loss: 0.1406\n16/16 [==============================] - 0s 819us/step - loss: 2.1877\n16/16 [==============================] - 0s 1ms/step - loss: 2.6298\n16/16 [==============================] - 0s 1ms/step - loss: 2.6666\n16/16 [==============================] - 0s 1ms/step - loss: 2.6212\n16/16 [==============================] - 0s 611us/step - loss: 2.5652\n16/16 [==============================] - 0s 1ms/step - loss: 2.5194\n16/16 [==============================] - 0s 1ms/step - loss: 2.4854\n16/16 [==============================] - 0s 787us/step - loss: 2.4747\n16/16 [==============================] - 0s 1ms/step - loss: 2.4704\nEpoch 52 of 60\n\nTesting for epoch 52 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1391\n16/16 [==============================] - 0s 1ms/step - loss: 2.1726\n16/16 [==============================] - 0s 1ms/step - loss: 2.6126\n16/16 [==============================] - 0s 1ms/step - loss: 2.6499\n16/16 [==============================] - 0s 622us/step - loss: 2.6046\n16/16 [==============================] - 0s 1ms/step - loss: 2.5491\n16/16 [==============================] - 0s 602us/step - loss: 2.5038\n16/16 [==============================] - 0s 1ms/step - loss: 2.4703\n16/16 [==============================] - 0s 1ms/step - loss: 2.4596\n16/16 [==============================] - 0s 625us/step - loss: 2.4553\n\nTesting for epoch 52 index 2:\n16/16 [==============================] - 0s 722us/step - loss: 0.1402\n16/16 [==============================] - 0s 1ms/step - loss: 2.1077\n16/16 [==============================] - 0s 1ms/step - loss: 2.5214\n16/16 [==============================] - 0s 1ms/step - loss: 2.5507\n16/16 [==============================] - 0s 1ms/step - loss: 2.5038\n16/16 [==============================] - 0s 638us/step - loss: 2.4486\n16/16 [==============================] - 0s 1ms/step - loss: 2.4039\n16/16 [==============================] - 0s 1ms/step - loss: 2.3711\n16/16 [==============================] - 0s 1ms/step - loss: 2.3608\n16/16 [==============================] - 0s 806us/step - loss: 2.3567\n\nTesting for epoch 52 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1413\n16/16 [==============================] - 0s 1ms/step - loss: 2.0789\n16/16 [==============================] - 0s 635us/step - loss: 2.4879\n16/16 [==============================] - 0s 827us/step - loss: 2.5226\n16/16 [==============================] - 0s 1ms/step - loss: 2.4818\n16/16 [==============================] - 0s 1ms/step - loss: 2.4316\n16/16 [==============================] - 0s 1ms/step - loss: 2.3905\n16/16 [==============================] - 0s 1ms/step - loss: 2.3598\n16/16 [==============================] - 0s 702us/step - loss: 2.3501\n16/16 [==============================] - 0s 672us/step - loss: 2.3462\n\nTesting for epoch 52 index 4:\n16/16 [==============================] - 0s 645us/step - loss: 0.1397\n16/16 [==============================] - 0s 615us/step - loss: 2.2173\n16/16 [==============================] - 0s 828us/step - loss: 2.6506\n16/16 [==============================] - 0s 1ms/step - loss: 2.6828\n16/16 [==============================] - 0s 1ms/step - loss: 2.6351\n16/16 [==============================] - 0s 1ms/step - loss: 2.5777\n16/16 [==============================] - 0s 603us/step - loss: 2.5312\n16/16 [==============================] - 0s 586us/step - loss: 2.4972\n16/16 [==============================] - 0s 874us/step - loss: 2.4864\n16/16 [==============================] - 0s 806us/step - loss: 2.4822\n\nTesting for epoch 52 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1400\n16/16 [==============================] - 0s 1ms/step - loss: 2.1024\n16/16 [==============================] - 0s 1ms/step - loss: 2.5108\n16/16 [==============================] - 0s 1ms/step - loss: 2.5405\n16/16 [==============================] - 0s 656us/step - loss: 2.4959\n16/16 [==============================] - 0s 1ms/step - loss: 2.4414\n16/16 [==============================] - 0s 1ms/step - loss: 2.3972\n16/16 [==============================] - 0s 928us/step - loss: 2.3648\n16/16 [==============================] - 0s 1ms/step - loss: 2.3547\n16/16 [==============================] - 0s 1ms/step - loss: 2.3508\nEpoch 53 of 60\n\nTesting for epoch 53 index 1:\n16/16 [==============================] - 0s 638us/step - loss: 0.1365\n16/16 [==============================] - 0s 618us/step - loss: 2.0904\n16/16 [==============================] - 0s 1ms/step - loss: 2.4896\n16/16 [==============================] - 0s 894us/step - loss: 2.5170\n16/16 [==============================] - 0s 1ms/step - loss: 2.4731\n16/16 [==============================] - 0s 995us/step - loss: 2.4215\n16/16 [==============================] - 0s 1ms/step - loss: 2.3790\n16/16 [==============================] - 0s 625us/step - loss: 2.3476\n16/16 [==============================] - 0s 634us/step - loss: 2.3377\n16/16 [==============================] - 0s 645us/step - loss: 2.3337\n\nTesting for epoch 53 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1351\n16/16 [==============================] - 0s 1ms/step - loss: 2.1565\n16/16 [==============================] - 0s 618us/step - loss: 2.5687\n16/16 [==============================] - 0s 762us/step - loss: 2.5966\n16/16 [==============================] - 0s 1ms/step - loss: 2.5494\n16/16 [==============================] - 0s 1ms/step - loss: 2.4930\n16/16 [==============================] - 0s 648us/step - loss: 2.4471\n16/16 [==============================] - 0s 608us/step - loss: 2.4134\n16/16 [==============================] - 0s 959us/step - loss: 2.4028\n16/16 [==============================] - 0s 1ms/step - loss: 2.3986\n\nTesting for epoch 53 index 3:\n16/16 [==============================] - 0s 809us/step - loss: 0.1389\n16/16 [==============================] - 0s 1ms/step - loss: 2.1419\n16/16 [==============================] - 0s 1ms/step - loss: 2.5512\n16/16 [==============================] - 0s 677us/step - loss: 2.5797\n16/16 [==============================] - 0s 668us/step - loss: 2.5344\n16/16 [==============================] - 0s 671us/step - loss: 2.4806\n16/16 [==============================] - 0s 657us/step - loss: 2.4371\n16/16 [==============================] - 0s 1ms/step - loss: 2.4050\n16/16 [==============================] - 0s 652us/step - loss: 2.3948\n16/16 [==============================] - 0s 1ms/step - loss: 2.3908\n\nTesting for epoch 53 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1349\n16/16 [==============================] - 0s 1ms/step - loss: 2.1901\n16/16 [==============================] - 0s 716us/step - loss: 2.6044\n16/16 [==============================] - 0s 1ms/step - loss: 2.6295\n16/16 [==============================] - 0s 1ms/step - loss: 2.5800\n16/16 [==============================] - 0s 632us/step - loss: 2.5219\n16/16 [==============================] - 0s 1ms/step - loss: 2.4752\n16/16 [==============================] - 0s 1ms/step - loss: 2.4411\n16/16 [==============================] - 0s 1ms/step - loss: 2.4303\n16/16 [==============================] - 0s 608us/step - loss: 2.4260\n\nTesting for epoch 53 index 5:\n16/16 [==============================] - 0s 626us/step - loss: 0.1361\n16/16 [==============================] - 0s 1ms/step - loss: 2.1862\n16/16 [==============================] - 0s 634us/step - loss: 2.6026\n16/16 [==============================] - 0s 1ms/step - loss: 2.6289\n16/16 [==============================] - 0s 602us/step - loss: 2.5794\n16/16 [==============================] - 0s 608us/step - loss: 2.5210\n16/16 [==============================] - 0s 945us/step - loss: 2.4747\n16/16 [==============================] - 0s 1ms/step - loss: 2.4412\n16/16 [==============================] - 0s 1ms/step - loss: 2.4307\n16/16 [==============================] - 0s 1ms/step - loss: 2.4265\nEpoch 54 of 60\n\nTesting for epoch 54 index 1:\n16/16 [==============================] - 0s 591us/step - loss: 0.1354\n16/16 [==============================] - 0s 716us/step - loss: 2.1140\n16/16 [==============================] - 0s 645us/step - loss: 2.5078\n16/16 [==============================] - 0s 1ms/step - loss: 2.5307\n16/16 [==============================] - 0s 576us/step - loss: 2.4835\n16/16 [==============================] - 0s 1ms/step - loss: 2.4278\n16/16 [==============================] - 0s 1ms/step - loss: 2.3835\n16/16 [==============================] - 0s 721us/step - loss: 2.3512\n16/16 [==============================] - 0s 683us/step - loss: 2.3411\n16/16 [==============================] - 0s 973us/step - loss: 2.3371\n\nTesting for epoch 54 index 2:\n16/16 [==============================] - 0s 746us/step - loss: 0.1375\n16/16 [==============================] - 0s 1ms/step - loss: 2.1227\n16/16 [==============================] - 0s 1ms/step - loss: 2.5193\n16/16 [==============================] - 0s 1ms/step - loss: 2.5434\n16/16 [==============================] - 0s 1ms/step - loss: 2.4969\n16/16 [==============================] - 0s 928us/step - loss: 2.4432\n16/16 [==============================] - 0s 851us/step - loss: 2.4006\n16/16 [==============================] - 0s 661us/step - loss: 2.3696\n16/16 [==============================] - 0s 980us/step - loss: 2.3598\n16/16 [==============================] - 0s 1ms/step - loss: 2.3560\n\nTesting for epoch 54 index 3:\n16/16 [==============================] - 0s 939us/step - loss: 0.1385\n16/16 [==============================] - 0s 1ms/step - loss: 2.1281\n16/16 [==============================] - 0s 629us/step - loss: 2.5285\n16/16 [==============================] - 0s 1ms/step - loss: 2.5562\n16/16 [==============================] - 0s 598us/step - loss: 2.5119\n16/16 [==============================] - 0s 633us/step - loss: 2.4583\n16/16 [==============================] - 0s 638us/step - loss: 2.4151\n16/16 [==============================] - 0s 639us/step - loss: 2.3836\n16/16 [==============================] - 0s 900us/step - loss: 2.3737\n16/16 [==============================] - 0s 1ms/step - loss: 2.3698\n\nTesting for epoch 54 index 4:\n16/16 [==============================] - 0s 650us/step - loss: 0.1350\n16/16 [==============================] - 0s 636us/step - loss: 2.1239\n16/16 [==============================] - 0s 1ms/step - loss: 2.5171\n16/16 [==============================] - 0s 853us/step - loss: 2.5400\n16/16 [==============================] - 0s 596us/step - loss: 2.4914\n16/16 [==============================] - 0s 619us/step - loss: 2.4353\n16/16 [==============================] - 0s 1ms/step - loss: 2.3913\n16/16 [==============================] - 0s 1ms/step - loss: 2.3594\n16/16 [==============================] - 0s 1ms/step - loss: 2.3494\n16/16 [==============================] - 0s 665us/step - loss: 2.3455\n\nTesting for epoch 54 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1328\n16/16 [==============================] - 0s 1ms/step - loss: 2.1880\n16/16 [==============================] - 0s 1ms/step - loss: 2.5967\n16/16 [==============================] - 0s 619us/step - loss: 2.6189\n16/16 [==============================] - 0s 603us/step - loss: 2.5676\n16/16 [==============================] - 0s 1ms/step - loss: 2.5074\n16/16 [==============================] - 0s 1ms/step - loss: 2.4597\n16/16 [==============================] - 0s 1ms/step - loss: 2.4251\n16/16 [==============================] - 0s 1ms/step - loss: 2.4143\n16/16 [==============================] - 0s 1ms/step - loss: 2.4100\nEpoch 55 of 60\n\nTesting for epoch 55 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1320\n16/16 [==============================] - 0s 624us/step - loss: 2.2799\n16/16 [==============================] - 0s 824us/step - loss: 2.7184\n16/16 [==============================] - 0s 644us/step - loss: 2.7484\n16/16 [==============================] - 0s 655us/step - loss: 2.6979\n16/16 [==============================] - 0s 690us/step - loss: 2.6378\n16/16 [==============================] - 0s 1ms/step - loss: 2.5895\n16/16 [==============================] - 0s 637us/step - loss: 2.5547\n16/16 [==============================] - 0s 806us/step - loss: 2.5438\n16/16 [==============================] - 0s 757us/step - loss: 2.5395\n\nTesting for epoch 55 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1319\n16/16 [==============================] - 0s 1ms/step - loss: 2.2685\n16/16 [==============================] - 0s 1ms/step - loss: 2.6951\n16/16 [==============================] - 0s 1ms/step - loss: 2.7210\n16/16 [==============================] - 0s 1ms/step - loss: 2.6693\n16/16 [==============================] - 0s 905us/step - loss: 2.6086\n16/16 [==============================] - 0s 985us/step - loss: 2.5598\n16/16 [==============================] - 0s 897us/step - loss: 2.5247\n16/16 [==============================] - 0s 1ms/step - loss: 2.5138\n16/16 [==============================] - 0s 726us/step - loss: 2.5094\n\nTesting for epoch 55 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1306\n16/16 [==============================] - 0s 628us/step - loss: 2.1294\n16/16 [==============================] - 0s 571us/step - loss: 2.5162\n16/16 [==============================] - 0s 651us/step - loss: 2.5364\n16/16 [==============================] - 0s 657us/step - loss: 2.4872\n16/16 [==============================] - 0s 620us/step - loss: 2.4317\n16/16 [==============================] - 0s 627us/step - loss: 2.3874\n16/16 [==============================] - 0s 631us/step - loss: 2.3554\n16/16 [==============================] - 0s 911us/step - loss: 2.3455\n16/16 [==============================] - 0s 1ms/step - loss: 2.3416\n\nTesting for epoch 55 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1298\n16/16 [==============================] - 0s 633us/step - loss: 2.2952\n16/16 [==============================] - 0s 1ms/step - loss: 2.7216\n16/16 [==============================] - 0s 1ms/step - loss: 2.7446\n16/16 [==============================] - 0s 1ms/step - loss: 2.6897\n16/16 [==============================] - 0s 611us/step - loss: 2.6254\n16/16 [==============================] - 0s 1ms/step - loss: 2.5745\n16/16 [==============================] - 0s 1ms/step - loss: 2.5375\n16/16 [==============================] - 0s 1ms/step - loss: 2.5260\n16/16 [==============================] - 0s 935us/step - loss: 2.5215\n\nTesting for epoch 55 index 5:\n16/16 [==============================] - 0s 604us/step - loss: 0.1304\n16/16 [==============================] - 0s 1ms/step - loss: 2.1990\n16/16 [==============================] - 0s 598us/step - loss: 2.6016\n16/16 [==============================] - 0s 1ms/step - loss: 2.6241\n16/16 [==============================] - 0s 1ms/step - loss: 2.5736\n16/16 [==============================] - 0s 753us/step - loss: 2.5147\n16/16 [==============================] - 0s 1ms/step - loss: 2.4677\n16/16 [==============================] - 0s 626us/step - loss: 2.4338\n16/16 [==============================] - 0s 718us/step - loss: 2.4233\n16/16 [==============================] - 0s 852us/step - loss: 2.4191\nEpoch 56 of 60\n\nTesting for epoch 56 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1279\n16/16 [==============================] - 0s 629us/step - loss: 2.1722\n16/16 [==============================] - 0s 1ms/step - loss: 2.5696\n16/16 [==============================] - 0s 1ms/step - loss: 2.5888\n16/16 [==============================] - 0s 687us/step - loss: 2.5353\n16/16 [==============================] - 0s 644us/step - loss: 2.4735\n16/16 [==============================] - 0s 1ms/step - loss: 2.4255\n16/16 [==============================] - 0s 1ms/step - loss: 2.3910\n16/16 [==============================] - 0s 1ms/step - loss: 2.3803\n16/16 [==============================] - 0s 1ms/step - loss: 2.3761\n\nTesting for epoch 56 index 2:\n16/16 [==============================] - 0s 725us/step - loss: 0.1313\n16/16 [==============================] - 0s 1ms/step - loss: 2.1862\n16/16 [==============================] - 0s 644us/step - loss: 2.5858\n16/16 [==============================] - 0s 1ms/step - loss: 2.6081\n16/16 [==============================] - 0s 1ms/step - loss: 2.5577\n16/16 [==============================] - 0s 1ms/step - loss: 2.4986\n16/16 [==============================] - 0s 1ms/step - loss: 2.4523\n16/16 [==============================] - 0s 1ms/step - loss: 2.4191\n16/16 [==============================] - 0s 708us/step - loss: 2.4087\n16/16 [==============================] - 0s 632us/step - loss: 2.4046\n\nTesting for epoch 56 index 3:\n16/16 [==============================] - 0s 620us/step - loss: 0.1281\n16/16 [==============================] - 0s 632us/step - loss: 2.2017\n16/16 [==============================] - 0s 607us/step - loss: 2.5988\n16/16 [==============================] - 0s 573us/step - loss: 2.6178\n16/16 [==============================] - 0s 653us/step - loss: 2.5652\n16/16 [==============================] - 0s 1ms/step - loss: 2.5044\n16/16 [==============================] - 0s 808us/step - loss: 2.4563\n16/16 [==============================] - 0s 648us/step - loss: 2.4216\n16/16 [==============================] - 0s 724us/step - loss: 2.4108\n16/16 [==============================] - 0s 727us/step - loss: 2.4066\n\nTesting for epoch 56 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1283\n16/16 [==============================] - 0s 643us/step - loss: 2.2432\n16/16 [==============================] - 0s 1ms/step - loss: 2.6497\n16/16 [==============================] - 0s 612us/step - loss: 2.6688\n16/16 [==============================] - 0s 1ms/step - loss: 2.6142\n16/16 [==============================] - 0s 837us/step - loss: 2.5514\n16/16 [==============================] - 0s 836us/step - loss: 2.5023\n16/16 [==============================] - 0s 1ms/step - loss: 2.4675\n16/16 [==============================] - 0s 641us/step - loss: 2.4567\n16/16 [==============================] - 0s 1ms/step - loss: 2.4524\n\nTesting for epoch 56 index 5:\n16/16 [==============================] - 0s 876us/step - loss: 0.1307\n16/16 [==============================] - 0s 1ms/step - loss: 2.2389\n16/16 [==============================] - 0s 1ms/step - loss: 2.6476\n16/16 [==============================] - 0s 1ms/step - loss: 2.6694\n16/16 [==============================] - 0s 646us/step - loss: 2.6174\n16/16 [==============================] - 0s 651us/step - loss: 2.5567\n16/16 [==============================] - 0s 630us/step - loss: 2.5086\n16/16 [==============================] - 0s 645us/step - loss: 2.4744\n16/16 [==============================] - 0s 1ms/step - loss: 2.4637\n16/16 [==============================] - 0s 634us/step - loss: 2.4595\nEpoch 57 of 60\n\nTesting for epoch 57 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1294\n16/16 [==============================] - 0s 1ms/step - loss: 2.2039\n16/16 [==============================] - 0s 1ms/step - loss: 2.6016\n16/16 [==============================] - 0s 1ms/step - loss: 2.6199\n16/16 [==============================] - 0s 618us/step - loss: 2.5666\n16/16 [==============================] - 0s 1ms/step - loss: 2.5066\n16/16 [==============================] - 0s 1ms/step - loss: 2.4594\n16/16 [==============================] - 0s 1ms/step - loss: 2.4255\n16/16 [==============================] - 0s 967us/step - loss: 2.4150\n16/16 [==============================] - 0s 594us/step - loss: 2.4109\n\nTesting for epoch 57 index 2:\n16/16 [==============================] - 0s 638us/step - loss: 0.1284\n16/16 [==============================] - 0s 633us/step - loss: 2.2483\n16/16 [==============================] - 0s 996us/step - loss: 2.6576\n16/16 [==============================] - 0s 1ms/step - loss: 2.6801\n16/16 [==============================] - 0s 610us/step - loss: 2.6274\n16/16 [==============================] - 0s 1ms/step - loss: 2.5661\n16/16 [==============================] - 0s 1ms/step - loss: 2.5171\n16/16 [==============================] - 0s 905us/step - loss: 2.4817\n16/16 [==============================] - 0s 1ms/step - loss: 2.4707\n16/16 [==============================] - 0s 1ms/step - loss: 2.4664\n\nTesting for epoch 57 index 3:\n16/16 [==============================] - 0s 629us/step - loss: 0.1308\n16/16 [==============================] - 0s 1ms/step - loss: 2.2241\n16/16 [==============================] - 0s 776us/step - loss: 2.6243\n16/16 [==============================] - 0s 1ms/step - loss: 2.6430\n16/16 [==============================] - 0s 1ms/step - loss: 2.5906\n16/16 [==============================] - 0s 647us/step - loss: 2.5309\n16/16 [==============================] - 0s 632us/step - loss: 2.4834\n16/16 [==============================] - 0s 617us/step - loss: 2.4493\n16/16 [==============================] - 0s 622us/step - loss: 2.4387\n16/16 [==============================] - 0s 619us/step - loss: 2.4346\n\nTesting for epoch 57 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1249\n16/16 [==============================] - 0s 1ms/step - loss: 2.2824\n16/16 [==============================] - 0s 886us/step - loss: 2.6944\n16/16 [==============================] - 0s 1ms/step - loss: 2.7119\n16/16 [==============================] - 0s 840us/step - loss: 2.6561\n16/16 [==============================] - 0s 1ms/step - loss: 2.5941\n16/16 [==============================] - 0s 749us/step - loss: 2.5456\n16/16 [==============================] - 0s 633us/step - loss: 2.5112\n16/16 [==============================] - 0s 631us/step - loss: 2.5004\n16/16 [==============================] - 0s 598us/step - loss: 2.4962\n\nTesting for epoch 57 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1268\n16/16 [==============================] - 0s 641us/step - loss: 2.2303\n16/16 [==============================] - 0s 987us/step - loss: 2.6315\n16/16 [==============================] - 0s 755us/step - loss: 2.6520\n16/16 [==============================] - 0s 1ms/step - loss: 2.6023\n16/16 [==============================] - 0s 1ms/step - loss: 2.5447\n16/16 [==============================] - 0s 1ms/step - loss: 2.4980\n16/16 [==============================] - 0s 1ms/step - loss: 2.4644\n16/16 [==============================] - 0s 1ms/step - loss: 2.4540\n16/16 [==============================] - 0s 1ms/step - loss: 2.4499\nEpoch 58 of 60\n\nTesting for epoch 58 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1253\n16/16 [==============================] - 0s 1ms/step - loss: 2.2804\n16/16 [==============================] - 0s 1ms/step - loss: 2.6909\n16/16 [==============================] - 0s 1ms/step - loss: 2.7047\n16/16 [==============================] - 0s 1ms/step - loss: 2.6461\n16/16 [==============================] - 0s 890us/step - loss: 2.5810\n16/16 [==============================] - 0s 1ms/step - loss: 2.5304\n16/16 [==============================] - 0s 1ms/step - loss: 2.4944\n16/16 [==============================] - 0s 1ms/step - loss: 2.4833\n16/16 [==============================] - 0s 1ms/step - loss: 2.4788\n\nTesting for epoch 58 index 2:\n16/16 [==============================] - 0s 749us/step - loss: 0.1268\n16/16 [==============================] - 0s 1ms/step - loss: 2.2590\n16/16 [==============================] - 0s 613us/step - loss: 2.6587\n16/16 [==============================] - 0s 617us/step - loss: 2.6734\n16/16 [==============================] - 0s 1ms/step - loss: 2.6186\n16/16 [==============================] - 0s 1ms/step - loss: 2.5570\n16/16 [==============================] - 0s 950us/step - loss: 2.5081\n16/16 [==============================] - 0s 1ms/step - loss: 2.4730\n16/16 [==============================] - 0s 661us/step - loss: 2.4621\n16/16 [==============================] - 0s 623us/step - loss: 2.4579\n\nTesting for epoch 58 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1254\n16/16 [==============================] - 0s 854us/step - loss: 2.2600\n16/16 [==============================] - 0s 1ms/step - loss: 2.6602\n16/16 [==============================] - 0s 655us/step - loss: 2.6753\n16/16 [==============================] - 0s 1ms/step - loss: 2.6206\n16/16 [==============================] - 0s 1ms/step - loss: 2.5580\n16/16 [==============================] - 0s 706us/step - loss: 2.5089\n16/16 [==============================] - 0s 1ms/step - loss: 2.4739\n16/16 [==============================] - 0s 1ms/step - loss: 2.4630\n16/16 [==============================] - 0s 1ms/step - loss: 2.4588\n\nTesting for epoch 58 index 4:\n16/16 [==============================] - 0s 681us/step - loss: 0.1267\n16/16 [==============================] - 0s 775us/step - loss: 2.2358\n16/16 [==============================] - 0s 608us/step - loss: 2.6324\n16/16 [==============================] - 0s 628us/step - loss: 2.6479\n16/16 [==============================] - 0s 610us/step - loss: 2.5940\n16/16 [==============================] - 0s 608us/step - loss: 2.5341\n16/16 [==============================] - 0s 637us/step - loss: 2.4871\n16/16 [==============================] - 0s 611us/step - loss: 2.4534\n16/16 [==============================] - 0s 641us/step - loss: 2.4430\n16/16 [==============================] - 0s 763us/step - loss: 2.4389\n\nTesting for epoch 58 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1232\n16/16 [==============================] - 0s 1ms/step - loss: 2.3007\n16/16 [==============================] - 0s 1ms/step - loss: 2.7090\n16/16 [==============================] - 0s 631us/step - loss: 2.7214\n16/16 [==============================] - 0s 973us/step - loss: 2.6639\n16/16 [==============================] - 0s 667us/step - loss: 2.5993\n16/16 [==============================] - 0s 1ms/step - loss: 2.5482\n16/16 [==============================] - 0s 1ms/step - loss: 2.5115\n16/16 [==============================] - 0s 1ms/step - loss: 2.5002\n16/16 [==============================] - 0s 1ms/step - loss: 2.4957\nEpoch 59 of 60\n\nTesting for epoch 59 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1249\n16/16 [==============================] - 0s 703us/step - loss: 2.2634\n16/16 [==============================] - 0s 1ms/step - loss: 2.6562\n16/16 [==============================] - 0s 1ms/step - loss: 2.6629\n16/16 [==============================] - 0s 1ms/step - loss: 2.6041\n16/16 [==============================] - 0s 872us/step - loss: 2.5390\n16/16 [==============================] - 0s 617us/step - loss: 2.4883\n16/16 [==============================] - 0s 687us/step - loss: 2.4527\n16/16 [==============================] - 0s 615us/step - loss: 2.4419\n16/16 [==============================] - 0s 783us/step - loss: 2.4376\n\nTesting for epoch 59 index 2:\n16/16 [==============================] - 0s 700us/step - loss: 0.1249\n16/16 [==============================] - 0s 601us/step - loss: 2.2865\n16/16 [==============================] - 0s 625us/step - loss: 2.6880\n16/16 [==============================] - 0s 581us/step - loss: 2.7004\n16/16 [==============================] - 0s 1ms/step - loss: 2.6446\n16/16 [==============================] - 0s 714us/step - loss: 2.5815\n16/16 [==============================] - 0s 1ms/step - loss: 2.5315\n16/16 [==============================] - 0s 1ms/step - loss: 2.4957\n16/16 [==============================] - 0s 740us/step - loss: 2.4847\n16/16 [==============================] - 0s 1ms/step - loss: 2.4803\n\nTesting for epoch 59 index 3:\n16/16 [==============================] - 0s 584us/step - loss: 0.1245\n16/16 [==============================] - 0s 787us/step - loss: 2.2729\n16/16 [==============================] - 0s 1ms/step - loss: 2.6637\n16/16 [==============================] - 0s 1ms/step - loss: 2.6712\n16/16 [==============================] - 0s 608us/step - loss: 2.6147\n16/16 [==============================] - 0s 654us/step - loss: 2.5526\n16/16 [==============================] - 0s 1ms/step - loss: 2.5033\n16/16 [==============================] - 0s 891us/step - loss: 2.4684\n16/16 [==============================] - 0s 697us/step - loss: 2.4576\n16/16 [==============================] - 0s 639us/step - loss: 2.4534\n\nTesting for epoch 59 index 4:\n16/16 [==============================] - 0s 637us/step - loss: 0.1250\n16/16 [==============================] - 0s 941us/step - loss: 2.2686\n16/16 [==============================] - 0s 599us/step - loss: 2.6612\n16/16 [==============================] - 0s 1ms/step - loss: 2.6698\n16/16 [==============================] - 0s 778us/step - loss: 2.6129\n16/16 [==============================] - 0s 691us/step - loss: 2.5509\n16/16 [==============================] - 0s 1ms/step - loss: 2.5026\n16/16 [==============================] - 0s 623us/step - loss: 2.4683\n16/16 [==============================] - 0s 885us/step - loss: 2.4578\n16/16 [==============================] - 0s 701us/step - loss: 2.4537\n\nTesting for epoch 59 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1234\n16/16 [==============================] - 0s 1ms/step - loss: 2.3238\n16/16 [==============================] - 0s 803us/step - loss: 2.7242\n16/16 [==============================] - 0s 601us/step - loss: 2.7327\n16/16 [==============================] - 0s 647us/step - loss: 2.6743\n16/16 [==============================] - 0s 970us/step - loss: 2.6098\n16/16 [==============================] - 0s 926us/step - loss: 2.5591\n16/16 [==============================] - 0s 1ms/step - loss: 2.5229\n16/16 [==============================] - 0s 1ms/step - loss: 2.5118\n16/16 [==============================] - 0s 678us/step - loss: 2.5074\nEpoch 60 of 60\n\nTesting for epoch 60 index 1:\n16/16 [==============================] - 0s 642us/step - loss: 0.1248\n16/16 [==============================] - 0s 597us/step - loss: 2.2835\n16/16 [==============================] - 0s 629us/step - loss: 2.6774\n16/16 [==============================] - 0s 1ms/step - loss: 2.6863\n16/16 [==============================] - 0s 644us/step - loss: 2.6273\n16/16 [==============================] - 0s 1ms/step - loss: 2.5622\n16/16 [==============================] - 0s 956us/step - loss: 2.5114\n16/16 [==============================] - 0s 664us/step - loss: 2.4755\n16/16 [==============================] - 0s 875us/step - loss: 2.4643\n16/16 [==============================] - 0s 593us/step - loss: 2.4600\n\nTesting for epoch 60 index 2:\n16/16 [==============================] - 0s 716us/step - loss: 0.1237\n16/16 [==============================] - 0s 626us/step - loss: 2.2727\n16/16 [==============================] - 0s 1ms/step - loss: 2.6608\n16/16 [==============================] - 0s 1ms/step - loss: 2.6688\n16/16 [==============================] - 0s 1ms/step - loss: 2.6110\n16/16 [==============================] - 0s 919us/step - loss: 2.5473\n16/16 [==============================] - 0s 1ms/step - loss: 2.4975\n16/16 [==============================] - 0s 780us/step - loss: 2.4622\n16/16 [==============================] - 0s 716us/step - loss: 2.4513\n16/16 [==============================] - 0s 638us/step - loss: 2.4470\n\nTesting for epoch 60 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1213\n16/16 [==============================] - 0s 616us/step - loss: 2.3241\n16/16 [==============================] - 0s 618us/step - loss: 2.7152\n16/16 [==============================] - 0s 1ms/step - loss: 2.7211\n16/16 [==============================] - 0s 1ms/step - loss: 2.6613\n16/16 [==============================] - 0s 631us/step - loss: 2.5967\n16/16 [==============================] - 0s 741us/step - loss: 2.5461\n16/16 [==============================] - 0s 633us/step - loss: 2.5105\n16/16 [==============================] - 0s 2ms/step - loss: 2.4996\n16/16 [==============================] - 0s 599us/step - loss: 2.4952\n\nTesting for epoch 60 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1217\n16/16 [==============================] - 0s 1ms/step - loss: 2.3310\n16/16 [==============================] - 0s 1ms/step - loss: 2.7333\n16/16 [==============================] - 0s 1ms/step - loss: 2.7466\n16/16 [==============================] - 0s 1ms/step - loss: 2.6903\n16/16 [==============================] - 0s 833us/step - loss: 2.6273\n16/16 [==============================] - 0s 608us/step - loss: 2.5776\n16/16 [==============================] - 0s 1ms/step - loss: 2.5424\n16/16 [==============================] - 0s 626us/step - loss: 2.5314\n16/16 [==============================] - 0s 709us/step - loss: 2.5270\n\nTesting for epoch 60 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1211\n16/16 [==============================] - 0s 1ms/step - loss: 2.2989\n16/16 [==============================] - 0s 1ms/step - loss: 2.6855\n16/16 [==============================] - 0s 1ms/step - loss: 2.6923\n16/16 [==============================] - 0s 1ms/step - loss: 2.6322\n16/16 [==============================] - 0s 650us/step - loss: 2.5665\n16/16 [==============================] - 0s 1ms/step - loss: 2.5155\n16/16 [==============================] - 0s 1ms/step - loss: 2.4800\n16/16 [==============================] - 0s 844us/step - loss: 2.4690\n16/16 [==============================] - 0s 593us/step - loss: 2.4647\n\n\n\noutlier_MO_GAAL_one = list(clf.labels_)\n\n\noutlier_MO_GAAL_one = list(map(lambda x: 1 if x==0  else -1,outlier_MO_GAAL_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_MO_GAAL_one,tab_bunny)\n\n\n_conf.conf(\"MO-GAAL (Liu et al., 2019)\")\n\n\n\n\nAccuracy: 0.952\nPrecision: 0.952\nRecall: 1.000\nF1 Score: 0.975\n\n\n\nthirteen = twelve.append(_conf.tab)\n\n\n\nLSCP\n\ndetectors = [KNN(), LOF(), OCSVM()]\nclf = LSCP(detectors)\nclf.fit(_df[['x', 'y','fnoise']])\n_df['LSCP_clf'] = clf.labels_\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/pyod/models/lscp.py:382: UserWarning: The number of histogram bins is greater than the number of classifiers, reducing n_bins to n_clf.\n  warnings.warn(\n\n\n\noutlier_LSCP_one = list(clf.labels_)\n\n\noutlier_LSCP_one = list(map(lambda x: 1 if x==0  else -1,outlier_LSCP_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_LSCP_one,tab_bunny)\n\n\n_conf.conf(\"LSCP (Zhao et al., 2019)\")\n\n\n\n\nAccuracy: 0.940\nPrecision: 0.996\nRecall: 0.941\nF1 Score: 0.967\n\n\n\nfourteen_bunny = thirteen.append(_conf.tab)"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#bunny-result",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#bunny-result",
    "title": "Class code for Comparison Study",
    "section": "Bunny Result",
    "text": "Bunny Result\n\nround(fourteen_bunny,4)\n\n\n\n\n\n  \n    \n      \n      Accuracy\n      Precision\n      Recall\n      F1\n    \n  \n  \n    \n      GODE\n      0.9948\n      0.9954\n      0.9992\n      0.9973\n    \n    \n      LOF (Breunig et al., 2000)\n      0.9285\n      0.9569\n      0.9685\n      0.9627\n    \n    \n      kNN (Ramaswamy et al., 2000)\n      0.9405\n      0.9960\n      0.9413\n      0.9679\n    \n    \n      CBLOF (He et al., 2003)\n      0.9776\n      0.9895\n      0.9870\n      0.9882\n    \n    \n      OCSVM (Sch ̈olkopf et al., 2001)\n      0.9321\n      0.9911\n      0.9371\n      0.9633\n    \n    \n      MCD (Hardin and Rocke, 2004)\n      0.9349\n      0.9929\n      0.9383\n      0.9648\n    \n    \n      Feature Bagging (Lazarevic and Kumar, 2005)\n      0.9149\n      0.9818\n      0.9278\n      0.9540\n    \n    \n      ABOD (Kriegel et al., 2008)\n      0.9768\n      0.9891\n      0.9866\n      0.9878\n    \n    \n      Isolation Forest (Liu et al., 2008)\n      0.7942\n      0.9947\n      0.7881\n      0.8794\n    \n    \n      HBOS (Goldstein and Dengel, 2012)\n      0.8953\n      0.9695\n      0.9190\n      0.9436\n    \n    \n      SOS (Janssens et al., 2012)\n      0.8953\n      0.9695\n      0.9190\n      0.9436\n    \n    \n      SO-GAAL (Liu et al., 2019)\n      0.9521\n      0.9521\n      1.0000\n      0.9754\n    \n    \n      MO-GAAL (Liu et al., 2019)\n      0.9521\n      0.9521\n      1.0000\n      0.9754\n    \n    \n      LSCP (Zhao et al., 2019)\n      0.9397\n      0.9956\n      0.9408\n      0.9674"
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html",
    "title": "Graph code",
    "section": "",
    "text": "Poster"
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#linear1",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#linear1",
    "title": "Graph code",
    "section": "Linear(1)",
    "text": "Linear(1)\n\n_x = np.linspace(0,2,1000)\n_y1 = 5*_x\n_y = _y1 + x # x is epsilon\n\n\ndf1=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\nw=np.zeros((1000,1000))\n\n\nfor i in range(1000):\n    for j in range(1000):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nclass SIMUL:\n    def __init__(self,df):\n        self.df = df\n        self.y = df.y.to_numpy()\n        self.y1 = df.y1.to_numpy()\n        self.x = df.x.to_numpy()\n        self.n = len(self.y)\n        self.W = w\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)      \n    def fit(self,sd=5,ref=30,ymin=-5,ymax=20,cuts=0,cutf=995): # fit with ebayesthresh\n        self._eigen()\n        self.ybar = self.Psi.T @ self.y # fbar := graph fourier transform of f\n        self.power = self.ybar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.ybar**2),sd=sd))\n        self.ybar_threshed = np.where(self.power_threshed>0,self.ybar,0)\n        self.yhat = self.Psi@self.ybar_threshed\n        self.df = self.df.assign(yHat = self.yhat)\n        self.df = self.df.assign(Residual = self.df.y- self.df.yHat)\n        self.differ=(np.abs(self.y-self.yhat)-np.min(np.abs(self.y-self.yhat)))/(np.max(np.abs(self.y-self.yhat))-np.min(np.abs(self.y-self.yhat))) #color 표현은 위핸 표준화\n        self.df = self.df.assign(differ = self.differ)\n        \n        fig,ax = plt.subplots(figsize=(10,10))\n        ax.scatter(self.x,self.y,color='gray',s=50,alpha=0.7)\n        ax.scatter(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],color='red',s=50)\n        ax.plot(self.x[cuts:cutf],self.yhat[cuts:cutf], '--k',lw=3)\n        ax.scatter(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],color='red',s=550,facecolors='none', edgecolors='r')\n        fig.tight_layout()\n        fig.savefig('fig1.eps',format='eps')\n\n\n_simul = SIMUL(df1)\n\n\n_simul.fit(sd=20,ref=25,ymin=-10,ymax=15)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque."
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#linear2",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#linear2",
    "title": "Graph code",
    "section": "Linear(2)",
    "text": "Linear(2)\n\n_x = np.linspace(0,2,1000)\n_y1 = 5*_x**2\n_y = _y1 + x # x is epsilon\n\n\ndf2=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul2 = SIMUL(df2)\n\n\n_simul2.fit(sd=20,ref=20,ymin=-10,ymax=15)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque."
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#cos",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#cos",
    "title": "Graph code",
    "section": "COS",
    "text": "COS\n\n_x = np.linspace(0,2,1000)\n_y1 = -2+ 3*np.cos(_x) + 1*np.cos(2*_x) + 5*np.cos(5*_x)\n_y = _y1 + x\n\n\ndf4=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul4 = SIMUL(df4)\n\n\n_simul4.fit(sd=20,ref=20,ymin=-10,ymax=15)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque."
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#sin",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#sin",
    "title": "Graph code",
    "section": "SIN",
    "text": "SIN\n\n_x = np.linspace(0,2,1000)\n_y1 =  3*np.sin(_x) + 1*np.sin(_x**2) + 5*np.sin(5*_x) \n_y = _y1 + x # x is epsilon\n\n\ndf5=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul5 = SIMUL(df5)\n\n\n_simul5.fit(ref=15,ymin=-10,ymax=15,cuts=5)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque."
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#d-manifold",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#d-manifold",
    "title": "Graph code",
    "section": "1D manifold",
    "text": "1D manifold\n\nnp.random.seed(777)\npi=np.pi\nn=1000\nang=np.linspace(-pi,pi-2*pi/n,n)\nr=5+np.cos(np.linspace(0,12*pi,n))\nvx=r*np.cos(ang)\nvy=r*np.sin(ang)\nf1=10*np.sin(np.linspace(0,6*pi,n))\nf = f1 + x\n\n\ndf = pd.DataFrame({'x' : vx, 'y' : vy, 'f' : f, 'f1' : f1})\n\n\nclass SIMUL:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.f.to_numpy()\n        self.f1 = df.f1.to_numpy()\n        self.x = df.x.to_numpy()\n        self.y = df.y.to_numpy()\n        self.n = len(self.f)\n        self.theta= None\n    def get_distance(self):\n        self.D = np.zeros([self.n,self.n])\n        locations = np.stack([self.x, self.y],axis=1)\n        for i in tqdm.tqdm(range(self.n)):\n            for j in range(i,self.n):\n                self.D[i,j]=np.linalg.norm(locations[i]-locations[j])\n        self.D = self.D + self.D.T\n    def get_weightmatrix(self,theta=1,beta=0.5,kappa=4000):\n        self.theta = theta\n        dist = np.where(self.D < kappa,self.D,0)\n        self.W = np.exp(-(dist/self.theta)**2)\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)       \n    def fit(self,sd=5,ref=60): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.f # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2),sd=sd))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(fHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.f- self.df.fHat)\n        self.dif=(np.abs(self.f-self.fhat)-np.min(np.abs(self.f-self.fhat)))/(np.max(np.abs(self.f-self.fhat))-np.min(np.abs(self.f-self.fhat)))\n        self.df = self.df.assign(dif = self.dif)\n        self.bottom = np.zeros_like(self.f)\n        self.width=0.05\n        self.depth=0.05\n#         fig = plt.figure(figsize=(10,10))\n        # ax = fig.add_subplot(1,1,1, projection='3d')\n        #\n        fig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(30,15),subplot_kw={\"projection\":\"3d\"})\n        ax1.grid(False)\n        ax1.scatter3D(self.x,self.y,self.f,zdir='z',s=50,marker='.',color='gray')\n        ax1.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.f[index_of_trueoutlier_bool],zdir='z',s=50,marker='.',color='red')\n        ax1.scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['f'],edgecolors='red',zdir='z',s=50,facecolors='none')\n        ax1.plot3D(self.x,self.y,self.f1,'--k',lw=3)\n        ax2.view_init(elev=30., azim=60)\n        \n        ax2.grid(False)\n        ax2.scatter3D(self.x,self.y,self.f,zdir='z',s=50,marker='.',color='gray')\n        ax2.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.f[index_of_trueoutlier_bool],zdir='z',s=50,marker='.',color='red') \n        ax2.scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['f'],edgecolors='red',zdir='z',s=50,facecolors='none')\n        ax2.plot3D(self.x,self.y,self.f1,'--k',lw=3)\n        ax2.view_init(elev=30., azim=40)\n        \n        ax3.grid(False)\n        ax3.scatter3D(self.x,self.y,self.f,zdir='z',s=50,marker='.',color='gray')\n        ax3.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.f[index_of_trueoutlier_bool],zdir='z',s=50,marker='.',color='red') \n        ax3.scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['f'],edgecolors='red',zdir='z',s=50,facecolors='none')\n        ax3.plot3D(self.x,self.y,self.f1,'--k',lw=3)\n        ax3.view_init(elev=30., azim=10)\n        \n        fig.savefig('fig2.eps',format='eps')\n\n\n_simul3d = SIMUL(df)\n\n\n_simul3d.get_distance()\n\n100%|██████████| 1000/1000 [00:01<00:00, 562.21it/s]\n\n\n\n_simul3d.get_weightmatrix(theta=(_simul3d.D[_simul3d.D>0].mean()),kappa=2500) \n\n\n%%capture --no-display\n_simul3d.fit(sd=15,ref=20)"
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#bunny",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#bunny",
    "title": "Graph code",
    "section": "Bunny",
    "text": "Bunny\n\nG = graphs.Bunny()\nn = G.N\n\n\ng = filters.Heat(G, tau=75) # 꼬리부분의 빨간신호를 퍼지게하는 정도\n\n\nnormal = np.random.randn(n)\nunif = np.concatenate([np.random.uniform(low=3,high=7,size=60), np.random.uniform(low=-7,high=-3,size=60),np.zeros(n-120)]); np.random.shuffle(unif)\nnoise = normal + unif\n\n\nindex_of_trueoutlier_bool = (unif!=0)\n\n\nf = np.zeros(n)\nf[1000] = -3234\nf = g.filter(f, method='chebyshev') \n\n2022-11-10 21:12:29,879:[WARNING](pygsp.graphs.graph.lmax): The largest eigenvalue G.lmax is not available, we need to estimate it. Explicitly call G.estimate_lmax() or G.compute_fourier_basis() once beforehand to suppress the warning.\n\n\n\nW = G.W.toarray()\nx = G.coords[:,0]\ny = G.coords[:,1]\nz = -G.coords[:,2]\n\n\ndf = pd.DataFrame({'x' : x, 'y' : y, 'z' : z, 'f' : f, 'noise' : noise})\n\n\nclass SIMUL:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.f.to_numpy()\n        self.z = df.z.to_numpy()\n        self.x = df.x.to_numpy()\n        self.y = df.y.to_numpy()\n        self.noise = df.noise.to_numpy()\n        self.fnoise = self.f + self.noise\n        self.W = W\n        self.n = len(self.f)\n        self.theta= None\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)       \n    def fit(self,sd=2.5,ref=6): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.fnoise # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2),sd=sd))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(fnoise = self.fnoise)\n        self.df = self.df.assign(fHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.f + self.df.noise - self.df.fHat)\n        self.bottom = np.zeros_like(self.f)\n        self.width=0.05\n        self.depth=0.05\n        \n        fig = plt.figure(figsize=(30,12),dpi=400)\n        ax1 = fig.add_subplot(251, projection='3d')\n        ax1.grid(False)\n        ax1.scatter3D(self.x,self.y,self.z,c='gray',zdir='z',alpha=0.5,marker='.')\n        ax1.view_init(elev=60., azim=-90)\n\n        ax2= fig.add_subplot(252, projection='3d')\n        ax2.grid(False)\n        ax2.scatter3D(self.x,self.y,self.z,c=self.f,cmap='hsv',zdir='z',marker='.',alpha=0.5,vmin=-12,vmax=10)\n        ax2.view_init(elev=60., azim=-90)\n\n        ax3= fig.add_subplot(253, projection='3d')\n        ax3.grid(False)\n        ax3.scatter3D(self.x,self.y,self.z,c=self.fnoise,cmap='hsv',zdir='z',marker='.',alpha=0.5,vmin=-12,vmax=10)\n        ax3.view_init(elev=60., azim=-90)\n        \n        ax4= fig.add_subplot(254, projection='3d')\n        ax4.grid(False)\n        ax4.scatter3D(self.x,self.y,self.z,c=self.fnoise,cmap='hsv',zdir='z',marker='.',vmin=-12,vmax=10,s=1)\n        ax4.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.z[index_of_trueoutlier_bool],c=self.fnoise[index_of_trueoutlier_bool],cmap='hsv',zdir='z',marker='.',s=50)\n        ax4.view_init(elev=60., azim=-90)\n\n        ax5= fig.add_subplot(255, projection='3d')\n        ax5.grid(False)\n        ax5.scatter3D(self.x,self.y,self.z,c=self.fnoise,cmap='hsv',zdir='z',marker='.',vmin=-12,vmax=10,s=1)\n        ax5.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.z[index_of_trueoutlier_bool],c=self.fnoise[index_of_trueoutlier_bool],cmap='hsv',zdir='z',marker='.',s=50)\n        ax5.scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['z'],zdir='z',s=550,marker='.',edgecolors='red',facecolors='none')\n        ax5.view_init(elev=60., azim=-90)\n        \n        ax6 = fig.add_subplot(256, projection='3d')\n        ax6.grid(False)\n        ax6.scatter3D(self.x,self.y,self.z,c='gray',zdir='z',alpha=0.5,marker='.')\n        ax6.view_init(elev=-60., azim=-90)\n\n        ax7= fig.add_subplot(257, projection='3d')\n        ax7.grid(False)\n        ax7.scatter3D(self.x,self.y,self.z,c=self.f,cmap='hsv',zdir='z',marker='.',alpha=0.5,vmin=-12,vmax=10)\n        ax7.view_init(elev=-60., azim=-90)\n\n        ax8= fig.add_subplot(258, projection='3d')\n        ax8.grid(False)\n        ax8.scatter3D(self.x,self.y,self.z,c=self.fnoise,cmap='hsv',zdir='z',marker='.',alpha=0.5,vmin=-12,vmax=10)\n        ax8.view_init(elev=-60., azim=-90)\n        \n        ax9= fig.add_subplot(259, projection='3d')\n        ax9.grid(False)\n        ax9.scatter3D(self.x,self.y,self.z,c=self.fnoise,cmap='hsv',zdir='z',marker='.',vmin=-12,vmax=10,s=1)\n        ax9.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.z[index_of_trueoutlier_bool],c=self.fnoise[index_of_trueoutlier_bool],cmap='hsv',zdir='z',marker='.',s=50)\n        ax9.view_init(elev=-60., azim=-90)\n\n        ax10= fig.add_subplot(2,5,10, projection='3d')\n        ax10.grid(False)\n        ax10.scatter3D(self.x,self.y,self.z,c=self.fnoise,cmap='hsv',zdir='z',marker='.',vmin=-12,vmax=10,s=1)\n        ax10.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.z[index_of_trueoutlier_bool],c=self.fnoise[index_of_trueoutlier_bool],cmap='hsv',zdir='z',marker='.',s=50)\n        ax10.scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['z'],zdir='z',s=550,marker='.',edgecolors='red',facecolors='none')\n        ax10.view_init(elev=-60., azim=-90)        \n        fig.savefig('fig_bunny.eps',format='eps')\n\n\n_simul = SIMUL(df)\n\n\nmax(_simul.f),max(_simul.fnoise)\n\n(-0.010827167666814895, 8.453057038638512)\n\n\n\nmin(_simul.f),min(_simul.fnoise)\n\n(-4.74620052476489, -11.196627043702925)\n\n\n\n%%capture --no-display\n_simul.fit(sd=20,ref=10)"
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#earthquake",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#earthquake",
    "title": "Graph code",
    "section": "Earthquake",
    "text": "Earthquake\n\ndf= pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv')\n\n\ndf_global= pd.concat([pd.read_csv('00_05.csv'),pd.read_csv('05_10.csv'),pd.read_csv('10_15.csv'),pd.read_csv('15_20.csv')]).iloc[:,[0,1,2,4]].rename(columns={'latitude':'Latitude','longitude':'Longitude','mag':'Magnitude'}).reset_index().iloc[:,1:]\n\n\ndf_global = df_global.assign(Year=list(map(lambda x: x.split('-')[0], df_global.time))).iloc[:,1:]\n\n\ndf_global.Year = df_global.Year.astype(np.float64)\n\n\nclass MooYaHo:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.Magnitude.to_numpy()\n        self.year = df.Year.to_numpy()\n        self.lat = df.Latitude.to_numpy()\n        self.long = df.Longitude.to_numpy()\n        self.n = len(self.f)\n        \n        self.theta= None\n    def get_distance(self):\n        self.D = np.zeros([self.n,self.n])\n        locations = np.stack([self.lat, self.long],axis=1)\n        for i in tqdm.tqdm(range(self.n)):\n            for j in range(i,self.n): \n                self.D[i,j]=haversine(locations[i],locations[j])\n        self.D = self.D+self.D.T\n    def get_weightmatrix(self,theta=1,beta=0.5,kappa=4000):\n        self.theta = theta\n        dist = np.where(self.D<kappa,self.D,0)\n        self.W = np.exp(-(dist/self.theta)**2)\n\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)        \n    def fit(self,m):\n        self._eigen()\n        self.fhat = self.Psi[:,0:m]@self.Psi[:,0:m].T@self.f\n        self.df = self.df.assign(MagnitudeHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.Magnitude- self.df.MagnitudeHat)\n        plt.plot(self.f,'.')\n        plt.plot(self.fhat,'x')\n\n\nclass MooYaHo2(MooYaHo): # ebayesthresh 기능추가\n    def fit2(self,ref=0.5): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.f # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2)))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(MagnitudeHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.Magnitude- self.df.MagnitudeHat)\n        self.con = np.where(self.df.Residual>0.7,1,0)\n\n\nclass eachlocation(MooYaHo2):\n    def haiti(self,MagThresh=7,ResThresh=1,adjzoom=5,adjmarkersize = 40):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=15,\n                        center=dict(lat=18.4430, lon=-72.5710), \n                        zoom= adjzoom,\n                        height=900,\n                        opacity = 0.8,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-3,3])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 5,\n                      marker_color= 'blue',\n                      opacity = 0.1\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= adjmarkersize,\n                      marker_color= 'red',\n                      opacity = 0.8\n                      )\n        fig.add_trace(go.Scattermapbox(\n                    lat=self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                    lon=self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                    mode='markers',\n                    marker=go.scattermapbox.Marker(\n                        size=20,\n                        color='rgb(255, 255, 255)',\n                        opacity=0.4\n                    )\n                ))\n        return fig \n    def lquique(self,MagThresh=7,ResThresh=1,adjzoom=5, adjmarkersize= 40):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=15,\n                        center=dict(lat=-32.6953, lon=-71.4416), \n                        zoom=adjzoom,\n                        height=900,\n                        opacity = 0.8,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 5,\n                      marker_color= 'blue',\n                      opacity = 0.1\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= adjmarkersize,\n                      marker_color= 'red',\n                      opacity = 0.8\n                      )\n        fig.add_trace(go.Scattermapbox(\n                    lat=self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                    lon=self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                    mode='markers',\n                    marker=go.scattermapbox.Marker(\n                        size=20,\n                        color='rgb(255, 255, 255)',\n                        opacity=0.8\n                    )\n                ))\n        return fig \n    def sichuan(self,MagThresh=7,ResThresh=1,adjzoom=5,adjmarkersize=40):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=15,\n                        center=dict(lat=30.3080, lon=102.8880), \n                        zoom=adjzoom,\n                        height=900,\n                        opacity = 0.6,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 5,\n                      marker_color= 'blue',\n                      opacity = 0.1\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= adjmarkersize,\n                      marker_color= 'red',\n                      opacity = 0.8\n                      )\n        fig.add_trace(go.Scattermapbox(\n                    lat=self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                    lon=self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                    mode='markers',\n                    marker=go.scattermapbox.Marker(\n                        size=20,\n                        color='rgb(255, 255, 255)',\n                        opacity=0.8\n                    )\n                ))\n        return fig \n\n\neach_location=eachlocation(df_global.query(\"2010 <= Year < 2015\"))\n\n- get distance\n\neach_location.get_distance()\n\n100%|██████████| 12498/12498 [03:24<00:00, 61.15it/s] \n\n\n\neach_location.D[each_location.D>0].mean()\n\n8810.865423093777\n\n\n\nplt.hist(each_location.D[each_location.D>0])\n\n(array([14176290., 16005894., 21186674., 22331128., 19394182., 17548252.,\n        16668048., 13316436., 12973260.,  2582550.]),\n array([8.97930163e-02, 2.00141141e+03, 4.00273303e+03, 6.00405465e+03,\n        8.00537626e+03, 1.00066979e+04, 1.20080195e+04, 1.40093411e+04,\n        1.60106627e+04, 1.80119844e+04, 2.00133060e+04]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n- weight matrix\n\neach_location.get_weightmatrix(theta=(8810.865423093777),kappa=2500) \n\n- fit\n\neach_location.fit2()\n\n\neach_location.haiti(MagThresh=6.9,ResThresh=0.5,adjzoom=5,adjmarkersize=40)\nfig = each_location.haiti(MagThresh=6.9,ResThresh=0.5,adjzoom=5,adjmarkersize=40)\nfig.write_image('fig_haiti.png',scale=3)\n\n\neach_location.lquique(MagThresh=6.4,ResThresh=0.4,adjzoom=5,adjmarkersize=40)\n# fig = each_location.lquique(MagThresh=6.4,ResThresh=0.4,adjzoom=5,adjmarkersize=20)\n# fig.write_image('fig_lquique.svg',scale=3)\n\n\neach_location.sichuan(MagThresh=6.5,ResThresh=0.4,adjzoom=5,adjmarkersize=40)\n# fig = each_location.sichuan(MagThresh=6.5,ResThresh=0.4,adjzoom=5,adjmarkersize=20)\n# fig.write_image('fig_sichuan.svg',scale=3)"
  },
  {
    "objectID": "posts/Quarto_tip/index.html",
    "href": "posts/Quarto_tip/index.html",
    "title": "Guarto tip",
    "section": "",
    "text": "About tips of quarto blog"
  },
  {
    "objectID": "posts/Quarto_tip/2023-01-02-quarto_tips.html",
    "href": "posts/Quarto_tip/2023-01-02-quarto_tips.html",
    "title": "quarto blog tips",
    "section": "",
    "text": "note, tip, warning, caution, and important.\n\n\ncallout\nThere are three types of callouts.\n:::{.callout-note}\nNote that there are five types of callouts, including:\n`note`, `tip`, `warning`, `caution`, and `important`.\n:::\n\n:::{.callout-tip}\n## Tip With Caption\n\nThis is an example of a callout with a caption.\n:::\n\n\ncollapse\n\n\n\n\n\n\n\ntype\ninfo\n\n\n\n\ndefault\nThe default appearance with colored header and an icon.\n\n\nsimple\nA lighter weight appearance that doesn’t include a colored header background.\n\n\nminimal\nA minimal treatment that applies borders to the callout, but doesn’t include a header background color or icon.\n\n\n\n::: {.callout-note appearance=\"simple\"}\n\n## Pay Attention\n\nUsing callouts is an effective way to highlight content that your reader give special consideration or attention.\n\n:::\n\n\nLists\nMarkdown Syntax Output\n* unordered list\n    + sub-item 1\n    + sub-item 2\n        - sub-sub-item 1\n1. ordered list\n2. item 2\n    i) sub-item 1\n         A.  sub-sub-item 1\n\n\nVideo\n\n\n\nSub figures\n::: {#fig-elephants layout-ncol=2}\n\n![Surus](surus.png){#fig-surus}\n\n![Hanno](hanno.png){#fig-hanno}\n\nFamous Elephants\n:::\n\n\nFigure Panels\n::: {layout-ncol=2}\n![Surus](surus.png)\n\n![Hanno](hanno.png)\n:::"
  },
  {
    "objectID": "posts/GCN/2022-12-07-torchgcn.html",
    "href": "posts/GCN/2022-12-07-torchgcn.html",
    "title": "TORCH_GEOMETRIC.NN",
    "section": "",
    "text": "221207\nhttps://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html\n\nimport torch\nfrom torch_geometric.data import Data\n\n\nedge_index = torch.tensor([[0, 1, 1, 2],\n                           [1, 0, 2, 1]], dtype=torch.long)\nx = torch.tensor([[-1], [0], [1]], dtype=torch.float)\ndata = Data(x=x, edge_index=edge_index)\n\n\ndata\n\nData(x=[3, 1], edge_index=[2, 4])\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n\nG=nx.Graph()\nG.add_node('0')\nG.add_node('1')\nG.add_node('2')\nG.add_edge('0','1')\nG.add_edge('1','2')\npos = {}\npos['0'] = (0,0)\npos['1'] = (1,1)\npos['2'] = (2,0)\nnx.draw(G,pos,with_labels=True)\nplt.show()\n\n\n\n\n\nfrom torch.nn import Linear, ReLU\nfrom torch_geometric.nn import Sequential, GCNConv\n\nex\nmodel = Sequential('x, edge_index', [\n    (GCNConv(in_channels, 64), 'x, edge_index -> x'),\n    ReLU(inplace=True),\n    (GCNConv(64, 64), 'x, edge_index -> x'),\n    ReLU(inplace=True),\n    Linear(64, out_channels),\n])\n\nmodel = Sequential('x, edge_index', [\n    (GCNConv(3, 64), 'x, edge_index -> x'),\n    ReLU(inplace=True),\n    (GCNConv(64, 64), 'x, edge_index -> x'),\n    ReLU(inplace=True),\n    Linear(64, 3),\n])\n\n\nmodel(x,edge_index)\n\n\nfrom torch.nn import Linear, ReLU, Dropout\nfrom torch_geometric.nn import Sequential, GCNConv, JumpingKnowledge\nfrom torch_geometric.nn import global_mean_pool\n\nmodel = Sequential('x, edge_index, batch', [\n    (Dropout(p=0.5), 'x -> x'),\n    (GCNConv(dataset.num_features, 64), 'x, edge_index -> x1'),\n    ReLU(inplace=True),\n    (GCNConv(64, 64), 'x1, edge_index -> x2'),\n    ReLU(inplace=True),\n    (lambda x1, x2: [x1, x2], 'x1, x2 -> xs'),\n    (JumpingKnowledge(\"cat\", 64, num_layers=2), 'xs -> x'),\n    (global_mean_pool, 'x, batch -> x'),\n    Linear(2 * 64, dataset.num_classes),\n])\n\nmodel = Sequential('x, edge_index, batch', [\n    (Dropout(p=0.5), 'x -> x'),\n    (GCNConv(dataset.num_features, 64), 'x, edge_index -> x1'),\n    ReLU(inplace=True),\n    (GCNConv(64, 64), 'x1, edge_index -> x2'),\n    ReLU(inplace=True),\n    (lambda x1, x2: [x1, x2], 'x1, x2 -> xs'),\n    (JumpingKnowledge(\"cat\", 64, num_layers=2), 'xs -> x'),\n    (global_mean_pool, 'x, batch -> x'),\n    Linear(2 * 64, dataset.num_classes),\n])\n\n\ntorch_geometric.nn.Linear()"
  },
  {
    "objectID": "posts/GCN/2022-12-28-gcn_simulation.html",
    "href": "posts/GCN/2022-12-28-gcn_simulation.html",
    "title": "Simulation of geometric-temporal",
    "section": "",
    "text": "Simulation\n\nhttps://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/dataset.html#module-torch_geometric_temporal.dataset.chickenpox\n\nimport\n\nimport networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\n\n\n\n공식 홈페이지 예제\n\ndata\n\nfrom torch_geometric_temporal.dataset import WikiMathsDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset(lags=14)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\n\nRecurrentGCN\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvGRU\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvGRU(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\n\nLearn\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=14, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [08:40<00:00, 10.42s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([1068, 14])\n\n\n\n1068개의 nodes\n한 개의 node에 mapping된 차원의 수\n\n\n_edge_index.shape\n\ntorch.Size([2, 27079])\n\n\n\n_edge_attr.shape\n\ntorch.Size([27079])\n\n\n\n_y.shape\n\ntorch.Size([1068])\n\n\n\n\n\n우리 예제\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\n\n\nT = 100\nN = 4 # number of Nodes\nE = np.array([[0,1],[1,2],[2,3],[3,0]]).T\nV = np.array([1,2,3,4])\nAMP = np.array([3,2,1,2.2])\nt = np.arange(0,T)\nnode_features = 1\n\n\nf = np.stack([a*np.sin(2*t**2/1000)+np.random.normal(loc=0,scale=0.2,size=T) for a in AMP],axis=1).reshape(T,N,node_features)\nf = torch.tensor(f).float()\n\n\nf.shape\n\ntorch.Size([100, 4, 1])\n\n\n\nX = f[:99,:,:]\ny = f[1:,:,:]\n\n\nplt.plot(y[:,0,0],label=\"v1\")\nplt.plot(y[:,1,0],label=\"v2\")\nplt.plot(y[:,2,0],label=\"v3\")\nplt.plot(y[:,3,0],label=\"v4\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efc48673490>\n\n\n\n\n\n\nedge_index = torch.tensor(E)\nedge_attr = torch.tensor(np.array([1,1,1,1]),dtype=torch.float32)\n\n\n_ee = enumerate(zip(X,y))\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:16<00:00,  3.01it/s]\n\n\n\nyhat = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nplt.plot(y[:,0,0],label=\"y in V1\")\nplt.plot(yhat[:,0,0],label=\"yhat in V1\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efc48524730>\n\n\n\n\n\n\nplt.plot(y[:,1,0],label=\"y in V2\")\nplt.plot(yhat[:,1,0],label=\"yhat in V2\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efc4849c730>\n\n\n\n\n\n\nplt.plot(y[:,2,0],label=\"y in V3\")\nplt.plot(yhat[:,2,0],label=\"yhat in V3\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efc484098e0>\n\n\n\n\n\n\nplt.plot(y[:,3,0],label=\"y in V4\")\nplt.plot(yhat[:,3,0],label=\"yhat in V4\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efc483f5880>\n\n\n\n\n\n\n\nGNAR\n\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr\n\n\n%load_ext rpy2.ipython\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython\n\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\n\n\n%%R\nsummary(fiveNet)\n\nGNARnet with 5 nodes and 10 edges\n of equal length  1\n\n\n\n%%R\nedges <- as.matrix(fiveNet)\nedges\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    1    1\n[2,]    0    0    1    1    0\n[3,]    0    1    0    1    0\n[4,]    1    1    1    0    0\n[5,]    1    0    0    0    0\n\n\n\n%%R\nprint(fiveNet)\n\nGNARnet with 5 nodes \nedges:1--4 1--5 2--3 2--4 3--2 3--4 4--1 4--2 4--3 5--1 \n     \n edges of each of length  1 \n\n\n\n%%R\ndata(\"fiveNode\")\nanswer <- GNARfit(vts = fiveVTS, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nanswer\n\nModel: \nGNAR(2,[1,1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\n dmatalpha1  dmatbeta1.1   dmatalpha2  dmatbeta2.1  \n    0.20624      0.50277      0.02124     -0.09523  \n\n\n\n\n%%R\nlayout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))\nplot(fiveVTS[, 1], ylab = \"Node A Time Series\")\nlines(fitted(answer)[, 1], col = 2)\nplot(fiveVTS[, 2], ylab = \"Node B Time Series\")\nlines(fitted(answer)[, 2], col = 2)\nplot(fiveVTS[, 3], ylab = \"Node C Time Series\")\nlines(fitted(answer)[, 3], col = 2)\nplot(fiveVTS[, 4], ylab = \"Node D Time Series\")\nlines(fitted(answer)[, 4], col = 2)\n\n\n\n\n\n%R -o fiveVTS\n%R -o edges\n\n\nnode: 5\ntime 200\n\n\nedges_tensor = torch.tensor(edges)\n\n\nnonzero_indices = edges_tensor.nonzero()\n\n\nfiveNet_edge = np.array(nonzero_indices).T\nfiveNet_edge\n\narray([[0, 0, 1, 1, 2, 2, 3, 3, 3, 4],\n       [3, 4, 2, 3, 1, 3, 0, 1, 2, 0]])\n\n\n\nfiveVTS.shape\n\n(200, 5)\n\n\n\nT = 200\nN = 5 # number of Nodes\nE = fiveNet_edge\nV = np.array([1,2,3,4,5])\nt = np.arange(0,T)\nnode_features = 1\n\n\nf = torch.tensor(fiveVTS).reshape(200,5,1).float()\n\n\nX = f[:199,:,:]\ny = f[1:,:,:]\n\n\nedge_index = torch.tensor(E)\nedge_attr = torch.tensor(np.array([1,1,1,1,1,1,1,1,1,1]),dtype=torch.float32)\n\n\n_ee = enumerate(zip(X,y))\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32<00:00,  1.54it/s]\n\n\n\nyhat = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nyhat.shape\n\n(199, 5, 1)\n\n\n\nplt.plot(y[:,1])\nplt.plot(yhat[:,1].data)\n\n\n\n\n\n\nWind network time series\nthe data suite vswind that contains a number of R objects pertaining to 721 wind speeds taken at each of 102 weather stations in England and Wales. The suite contains the vector time series vswindts, the associated network vswindnet, a character vector of the weather station location names in vswindnames and coordinates of the stations in the two column matrix vswindcoords. The data originate from the UK Met Office site http://wow.metoffice.gov.uk and full details can be found in the vswind help file in the GNAR package.\n\n%%R\noldpar <- par(cex = 0.75)\nwindnetplot()\npar(oldpar)\n\n\n\n\n\n%%R\nedges_wind <- as.matrix(vswindnet)\n\n\n%R -o vswindts\n%R -o edges_wind\n\n\nnodes : 102\ntime step : 721\n\n\nvswindts.shape\n\n(721, 102)\n\n\n\nedges_wind.shape\n\n(102, 102)\n\n\n\nedges_winds = torch.tensor(edges_wind)\n\n\nnonzero_indices_wind = edges_winds.nonzero()\n\n\nvswindnet_edge = np.array(nonzero_indices_wind).T\nvswindnet_edge.shape\n\n(2, 202)\n\n\n\nT = 721\nN = 102 # number of Nodes\nE = vswindnet_edge\nV = np.array(range(101))\nt = np.arange(0,T)\nnode_features = 1\n\n\nf = torch.tensor(vswindts).reshape(721,102,1).float()\n\n\nX = f[:720,:,:]\ny = f[1:,:,:]\n\n\nedge_index = torch.tensor(E)\nedge_attr = torch.tensor(np.array([1]*202),dtype=torch.float32)\n\n\n_ee = enumerate(zip(X,y))\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [02:16<00:00,  2.73s/it]\n\n\n\nyhat = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nyhat.shape\n\n(720, 102, 1)\n\n\n\nplt.plot(y[:,1])\nplt.plot(yhat[:,1].data)\n\n\n\n\n\n\nOECD GDP\n해당예제는 GNAR 패키지에서 네트워크(엣지)를 맞추는 예제로서 나옴, 그렇기에 네트워크 존재하지 않아 연구 예제로서 사용하지 않을 예정\n이 데이터는 네트워크를 추정하여 fit 및 predict함\nGOP growth rate time series\n\n35 countries from the OECD website\ntime series : 1961 - 2013\nT = 52\nNodes = 35\nIn this data set 20.8% (379 out of 1820) of the observations were missing due to some nodes not being included from the start.\n\n\n%%R\nlibrary(\"fields\")\n\n\n%R -o gdpVTS\n\n\ngdpVTS.shape\n\n(52, 35)\n\n\n\nplt.plot(gdpVTS[:,1])"
  },
  {
    "objectID": "posts/GCN/2023-01-15-Algorithm_EX_2.html",
    "href": "posts/GCN/2023-01-15-Algorithm_EX_2.html",
    "title": "GCN Algorithm Example 2",
    "section": "",
    "text": "Our method"
  },
  {
    "objectID": "posts/GCN/2023-01-15-Algorithm_EX_2.html#데이터-일부-missing-처리",
    "href": "posts/GCN/2023-01-15-Algorithm_EX_2.html#데이터-일부-missing-처리",
    "title": "GCN Algorithm Example 2",
    "section": "데이터 일부 missing 처리",
    "text": "데이터 일부 missing 처리\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ndata[0][1].x.shape,data[0][1].y.shape,data[0][1].edge_index.shape,data[0][1].edge_attr.shape\n\n(torch.Size([1068, 1]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n729\n\n\n\nT = time\nN = len(data[0][1].x)\n\n\nedge_index = data[0][1].edge_index\nedge_attr = data[0][1].edge_attr\n\n\nx = []\nfor i in range(time):\n    x.append(data[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx = data_tensor.reshape(time,1068,-1)\nx.shape\n\ntorch.Size([729, 1068, 1])\n\n\n\nplt.plot(x[0])\nplt.plot(x[1])\nplt.plot(x[2])\nplt.plot(x[3])\n\n\n\n\n\ny = []\nfor i in range(time):\n    y.append(data[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny = data_tensor.reshape(time,1068)\ny.shape\n\ntorch.Size([729, 1068])\n\n\n\nplt.plot(y[0])\n\n\n\n\n\n1) Block\n\nxblock = x.clone()\n\n\nxblock[5][500:1000] = float('nan')\n\n\nplt.plot(xblock[5])\n\n\n\n\n\nf = xblock\n\n\nX = f[:728,:,:]\ny = f[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [06:59<00:00,  8.38s/it]\n\n\n\nyhat = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nplt.plot(yhat[:,0].data)\nplt.plot(yhat[:,1].data)\nplt.plot(yhat[:,2].data)\nplt.plot(yhat[:,3].data)\n\n\n\n\n\n\n2) Random missing values\n\nxrandom = x.clone()\n\n\nnp.random.seed(1)\nseed_number = np.random.choice(len(xrandom[0]), 500, replace=False)\n\n\nxrandom[5][seed_number] = float('nan')\n\n\nplt.plot(xrandom[5])\n\n\n\n\n\n\n3) By 2\n\nxtwo = x.clone()\n\n\ntwo_number = np.arange(1, N+1, 2)\n\n\nxtwo[5][two_number] = float('nan')\n\n\nplt.plot(xtwo[5],'o')"
  },
  {
    "objectID": "posts/GCN/2023-01-15-Algorithm_EX_2.html#mean",
    "href": "posts/GCN/2023-01-15-Algorithm_EX_2.html#mean",
    "title": "GCN Algorithm Example 2",
    "section": "1.1. Mean",
    "text": "1.1. Mean\n\n1) Block\n\nxblock_mean = xblock.clone()\n\n\nxblock_mean[5][500:1000] = np.mean(xblock_mean[5][:499].tolist()+xblock_mean[5][1000:].tolist())\n\n\nplt.plot(xblock_mean[5])\n\n\n\n\n\n\n2) Random missing values\n\nxrandom_mean = xrandom.clone()\n\n\ndf = pd.DataFrame(xrandom_mean[5].tolist())\nmean_value = df.mean() # finds the mean value of the column A\ndf = df.fillna(mean_value) # replace missing values with the mean value\n\n\nxrandom_mean[5] = torch.Tensor(df.values)\n\n\nplt.plot(xrandom_mean[5])\n\n\n\n\n\n\n3) By 2\n\nxtwo_mean = xtwo.clone()\n\n\ndf = pd.DataFrame(xtwo_mean[5].tolist())\nmean_value = df.mean() # finds the mean value of the column A\ndf = df.fillna(mean_value) # replace missing values with the mean value\n\n\nxtwo_mean[5] = torch.Tensor(df.values)\n\n\nplt.plot(xtwo_mean[5])"
  },
  {
    "objectID": "posts/GCN/2023-01-15-Algorithm_EX_2.html#linear-interpolation",
    "href": "posts/GCN/2023-01-15-Algorithm_EX_2.html#linear-interpolation",
    "title": "GCN Algorithm Example 2",
    "section": "1.2. linear interpolation",
    "text": "1.2. linear interpolation\n\n1) Block\n\nxblock_linearinterpolation = xblock.clone()\n\n\n# Sample data points\nx = np.array([499,1000])\ny = np.array([xblock_linearinterpolation[5][499],xblock_linearinterpolation[5][1000]])\n\n# Create interpolating function\nf = interp1d(x, y, kind='linear')\n\n# Estimate y value for x = 2.5\ny_interp = f(range(500,1000))\n\n/tmp/ipykernel_2887955/2336840081.py:3: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n  y = np.array([xblock_linearinterpolation[5][499],xblock_linearinterpolation[5][1000]])\n/tmp/ipykernel_2887955/2336840081.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  y = np.array([xblock_linearinterpolation[5][499],xblock_linearinterpolation[5][1000]])\n\n\n\nxblock_linearinterpolation[5][500:1000] = torch.Tensor(y_interp).reshape(500,1)\n\n\nplt.plot(xblock_linearinterpolation[5])\n\n\n\n\n\n\n2) Random missing values\n\nxrandom_linearinterpolation = xrandom.clone()\n\n\ndf = pd.DataFrame(xrandom_linearinterpolation[5].tolist())\ndf.interpolate(method='linear', inplace=True)\ndf = df.fillna(0)\n\n\nxrandom_linearinterpolation[5] = torch.Tensor(df.values)\n\n\nplt.plot(xrandom_linearinterpolation[5])\n\n\n\n\n\n\n3) By 2\n\nxtwo_linearinterpolation = xtwo.clone()\n\n\ndf = pd.DataFrame(xtwo_linearinterpolation[5].tolist())\ndf.interpolate(method='linear', inplace=True)\ndf = df.fillna(0)\n\n\nxtwo_linearinterpolation[5] = torch.Tensor(df.values)\n\n\nplt.plot(xtwo_linearinterpolation[5])"
  },
  {
    "objectID": "posts/GCN/2023-01-15-Algorithm_EX_2.html#normal-distribution-random-values",
    "href": "posts/GCN/2023-01-15-Algorithm_EX_2.html#normal-distribution-random-values",
    "title": "GCN Algorithm Example 2",
    "section": "1.3. Normal distribution Random Values",
    "text": "1.3. Normal distribution Random Values\n\n1) Block\n\nxblock_normal = xblock.clone()\n\n\nxblock_normal[5][500:1000] = torch.tensor(np.random.normal(loc=0,scale=0.2,size=500)).reshape(500,1)\n\n\nplt.plot(xblock_normal[5])\n\n\n\n\n\n\n2) Random missing values\n\nxrandom_normal = xrandom.clone()\n\n\nxrandom_normal[5][seed_number] = torch.tensor(np.random.normal(loc=0,scale=0.2,size=500),dtype=torch.float32).reshape(500,1)\n\n\nplt.plot(xrandom_normal[5])\n\n\n\n\n\n\n3) By 2\n\nxtwo_normal = xtwo.clone()\n\n\nxtwo_normal[5][two_number] = torch.tensor(np.random.normal(loc=0,scale=0.2,size=534),dtype=torch.float32).reshape(534,1)\n\n\nplt.plot(xtwo_normal[5])"
  },
  {
    "objectID": "posts/GCN/2023-01-15-Algorithm_EX_2.html#mean-1",
    "href": "posts/GCN/2023-01-15-Algorithm_EX_2.html#mean-1",
    "title": "GCN Algorithm Example 2",
    "section": "2.1. Mean",
    "text": "2.1. Mean\n\n1) Block\n\nf_xblock_mean = xblock_mean.clone()\n\n\nX_f_xblock_mean = f_xblock_mean[:728,:,:]\ny_f_xblock_mean = f_xblock_mean[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_f_xblock_mean,y_f_xblock_mean)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [07:05<00:00,  8.52s/it]\n\n\n\nfhat_xblock_mean = torch.stack([model(xt, edge_index, edge_attr) for xt in X_f_xblock_mean]).detach().numpy()\n\n\nplt.plot(fhat_xblock_mean[5].data)\n\n\n\n\n\n\n2) Random missing values\n\nf_xrandom_mean = xrandom_mean.clone()\n\n\nX_f_xrandom_mean = f_xrandom_mean[:728,:,:]\ny_f_xrandom_mean = f_xrandom_mean[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_f_xrandom_mean,y_f_xrandom_mean)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [07:08<00:00,  8.57s/it]\n\n\n\nfhat_xrandom_mean = torch.stack([model(xt, edge_index, edge_attr) for xt in X_f_xrandom_mean]).detach().numpy()\n\n\nplt.plot(fhat_xrandom_mean[5].data)\n\n\n\n\n\n\n3) By 2\n\nf_xtwo_mean = xtwo_mean.clone()\n\n\nX_f_xtwo_mean = f_xtwo_mean[:728,:,:]\ny_f_xtwo_mean = f_xtwo_mean[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_f_xtwo_mean,y_f_xtwo_mean)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [07:08<00:00,  8.57s/it]\n\n\n\nfhat_xtwo_mean = torch.stack([model(xt, edge_index, edge_attr) for xt in X_f_xtwo_mean]).detach().numpy()\n\n\nplt.plot(fhat_xtwo_mean[5].data)"
  },
  {
    "objectID": "posts/GCN/2023-01-15-Algorithm_EX_2.html#linear-interpolation-1",
    "href": "posts/GCN/2023-01-15-Algorithm_EX_2.html#linear-interpolation-1",
    "title": "GCN Algorithm Example 2",
    "section": "2.2. linear interpolation",
    "text": "2.2. linear interpolation\n\n1) Block\n\nf_block_linearinterpolation = xblock_linearinterpolation.clone()\n\n\nX_f_block_linearinterpolation = f_block_linearinterpolation[:728,:,:]\ny_f_block_linearinterpolation = f_block_linearinterpolation[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_f_block_linearinterpolation,y_f_block_linearinterpolation)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [07:05<00:00,  8.50s/it]\n\n\n\nfhat_linearinterpolation = torch.stack([model(xt, edge_index, edge_attr) for xt in X_f_block_linearinterpolation]).detach().numpy()\n\n\nplt.plot(fhat_linearinterpolation[:,2].data)\n\n\n\n\n\n\n2) Random missing values\n\nf_random_linearinterpolation = xrandom_linearinterpolation.clone()\n\n\nX_f_random_linearinterpolation = f_random_linearinterpolation[:728,:,:]\ny_f_random_linearinterpolation = f_random_linearinterpolation[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_f_random_linearinterpolation,y_f_random_linearinterpolation)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [07:04<00:00,  8.49s/it]\n\n\n\nfhat_random_linearinterpolation = torch.stack([model(xt, edge_index, edge_attr) for xt in X_f_random_linearinterpolation]).detach().numpy()\n\n\nplt.plot(fhat_random_linearinterpolation[5].data)\n\n\n\n\n\n\n3) By 2\n\nf_two_linearinterpolation = xtwo_linearinterpolation.clone()\n\n\nX_f_two_linearinterpolation = f_two_linearinterpolation[:728,:,:]\ny_f_two_linearinterpolation = f_two_linearinterpolation[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_f_two_linearinterpolation,y_f_two_linearinterpolation)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [07:06<00:00,  8.52s/it]\n\n\n\nfhat_two_linearinterpolation = torch.stack([model(xt, edge_index, edge_attr) for xt in X_f_two_linearinterpolation]).detach().numpy()\n\n\nplt.plot(fhat_two_linearinterpolation[5].data)"
  },
  {
    "objectID": "posts/GCN/2023-01-15-Algorithm_EX_2.html#normal-distribution-random-values-1",
    "href": "posts/GCN/2023-01-15-Algorithm_EX_2.html#normal-distribution-random-values-1",
    "title": "GCN Algorithm Example 2",
    "section": "2.3. Normal distribution Random Values",
    "text": "2.3. Normal distribution Random Values\n\n1) Block\n\nf_xblock_normal = xblock_normal.clone()\n\n\nX_f_xblock_normal = f_xblock_normal[:728,:,:]\ny_f_xblock_normal = f_xblock_normal[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_f_xblock_normal,y_f_xblock_normal)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [07:06<00:00,  8.53s/it]\n\n\n\nfhat_xblock_normal = torch.stack([model(xt, edge_index, edge_attr) for xt in X_f_xblock_normal]).detach().numpy()\n\n\nplt.plot(fhat_xblock_normal[5].data)\n\n\n\n\n\n\n2) Random missing values\n\nf_xrandom_normal = xrandom_normal.clone()\n\n\nX_f_xrandom_normal = f_xrandom_normal[:728,:,:]\ny_f_xrandom_normal = f_xrandom_normal[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_f_xrandom_normal,y_f_xrandom_normal)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [07:06<00:00,  8.52s/it]\n\n\n\nfhat_xrandom_normal = torch.stack([model(xt, edge_index, edge_attr) for xt in X_f_xrandom_normal]).detach().numpy()\n\n\nplt.plot(f_xrandom_normal[5].data)\n\n\n\n\n\n\n3) By 2\n\nf_xtwo_normal = xtwo_normal.clone()\n\n\nX_f_xtwo_normal = f_xtwo_normal[:728,:,:]\ny_f_xtwo_normal = f_xtwo_normal[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_f_xtwo_normal,y_f_xtwo_normal)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [07:06<00:00,  8.53s/it]\n\n\n\nfhat_xtwo_normal = torch.stack([model(xt, edge_index, edge_attr) for xt in X_f_xtwo_normal]).detach().numpy()\n\n\nplt.plot(fhat_xtwo_normal[5].data)"
  },
  {
    "objectID": "posts/GCN/2023-01-15-Algorithm_EX_2.html#mean-2",
    "href": "posts/GCN/2023-01-15-Algorithm_EX_2.html#mean-2",
    "title": "GCN Algorithm Example 2",
    "section": "3.1. Mean",
    "text": "3.1. Mean\n\n3.1.1. Temporal\n\n1) Block\n\nw=np.zeros((1068,728,728))\n\n\nfor k in range(1068):\n    for i in range(728):\n        for j in range(728):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(1068)])\nD= np.array([np.diag(d[i]) for i in range(1068)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(1068)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(1068)])    \nfhatbar = np.hstack([Psi[i] @ fhat_xblock_mean[:,i] for i in range(1068)])\n_fhatbar = fhatbar.reshape(1068,728)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(1068)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(1068)])    \nfhatbarhat_block_mean_temporal = fhatbarhat.reshape(728,-1)\n\n\nplt.plot(fhatbarhat_block_mean_temporal[5])\n\n\n\n\n\n\n2) Random missing values\n\nd = np.array([w[i].sum(axis=1) for i in range(1068)])\nD= np.array([np.diag(d[i]) for i in range(1068)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(1068)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(1068)])    \nfhatbar = np.hstack([Psi[i] @ fhat_xrandom_mean[:,i] for i in range(1068)])\n_fhatbar = fhatbar.reshape(1068,728)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(1068)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(1068)])    \nfhatbarhat_random_mean_temporal = fhatbarhat.reshape(728,-1)\n\n\nplt.plot(fhatbarhat_random_mean_temporal[5])\n\n\n\n\n\n\n3) By 2\n\nd = np.array([w[i].sum(axis=1) for i in range(1068)])\nD= np.array([np.diag(d[i]) for i in range(1068)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(1068)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(1068)])    \nfhatbar = np.hstack([Psi[i] @ fhat_xtwo_mean[:,i] for i in range(1068)])\n_fhatbar = fhatbar.reshape(1068,728)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(1068)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(1068)])    \nfhatbarhat_two_mean_temporal = fhatbarhat.reshape(728,-1)\n\n\nplt.plot(fhatbarhat_two_mean_temporal[5])\n\n\n\n\n\n\n\n3.1.2. Spatio\n\n1) Block\n\nw=np.zeros((1068,1068))\n\n\nfor i in range(1068):\n    for j in range(1068):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_xblock_mean.reshape(1068,728)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(1068)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_xblock_mean_spatio = fhatbarhat.reshape(728,-1)\n\n\nplt.plot(fhatbarhat_xblock_mean_spatio[5])\n\n\n\n\n\n\n2) Random missing values\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_xrandom_mean.reshape(1068,728)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(1068)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_xrandom_mean_spatio = fhatbarhat.reshape(728,-1)\n\n\nplt.plot(fhatbarhat_xrandom_mean_spatio[5])\n\n\n\n\n\n\n3) By 2\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_xtwo_mean.reshape(1068,728)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(1068)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_xtwo_mean_spatio = fhatbarhat.reshape(728,-1)\n\n\nplt.plot(fhatbarhat_xtwo_mean_spatio[5])\n\n\n\n\n\n\n\n3.1.3. Spatio-Temporal\n\n1) Block\n\nw=np.zeros((777504,777504))\n\nMemoryError: Unable to allocate 4.40 TiB for an array with shape (777504, 777504) and data type float64\n\n\n\nfor i in range(777504):\n    for j in range(777504):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_xblock_mean.reshape(777504,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(777504)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_block_mean_spatio_temporal = fhatbarhat.reshape(728,1068,1)\n\n\nplt.plot(fhatbarhat_block_mean_spatio_temporal[5])\n\n\n\n2) Random missing values\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_xrandom_mean.reshape(777504,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(777504)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_random_mean_spatio_temporal = fhatbarhat.reshape(728,1068,1)\n\n\nplt.plot(fhatbarhat_random_mean_spatio_temporal[5])\n\n\n\n3) By 2\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_xtwo_mean.reshape(777504,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(777504)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_two_mean_spatio_temporal = fhatbarhat.reshape(728,1068,1)\n\n\nplt.plot(fhatbarhat_two_mean_spatio_temporal[5])"
  },
  {
    "objectID": "posts/GCN/2023-01-15-Algorithm_EX_2.html#linear-interpolation-2",
    "href": "posts/GCN/2023-01-15-Algorithm_EX_2.html#linear-interpolation-2",
    "title": "GCN Algorithm Example 2",
    "section": "3.2.linear interpolation",
    "text": "3.2.linear interpolation\n\n3.2.1. Temporal\n\n1) Block\n\nw=np.zeros((N,728,728))\n\n\nfor k in range(N):\n    for i in range(728):\n        for j in range(728):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(N)])\nD= np.array([np.diag(d[i]) for i in range(N)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(N)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(N)])\nfhatbar = np.hstack([Psi[i] @ fhat_linearinterpolation[:,i] for i in range(N)])\n_fhatbar = fhatbar.reshape(N,728)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(N)])    \nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(N)])    \nfhatbarhat_block_linearinterpolation_temporal = fhatbarhat.reshape(728,-1)\n\n\nplt.plot(fhatbarhat_block_linearinterpolation_temporal[5])\n\n\n\n\n\n\n2) Random missing values\n\nd = np.array([w[i].sum(axis=1) for i in range(N)])\nD= np.array([np.diag(d[i]) for i in range(N)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(N)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(N)])\nfhatbar = np.hstack([Psi[i] @ fhat_random_linearinterpolation[:,i] for i in range(N)])\n_fhatbar = fhatbar.reshape(N,728)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(N)])    \nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(N)])    \nfhatbarhat_random_linearinterpolation_temporal = fhatbarhat.reshape(728,-1)\n\n\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[5])\n\n\n\n\n\n\n3) By 2\n\nd = np.array([w[i].sum(axis=1) for i in range(N)])\nD= np.array([np.diag(d[i]) for i in range(N)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(N)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(N)])\nfhatbar = np.hstack([Psi[i] @ fhat_two_linearinterpolation[:,i] for i in range(N)])\n_fhatbar = fhatbar.reshape(N,728)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(N)])    \nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(N)])    \nfhatbarhat_two_linearinterpolation_temporal = fhatbarhat.reshape(728,-1)\n\n\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[5])\n\n\n\n\n\n\n\n3.2.2. Spatio\n\n1) Block\n\nw=np.zeros((N,N))\n\n\nfor i in range(N):\n    for j in range(N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_linearinterpolation.reshape(N,728)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(N)])    \nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_block_linearinterpolation_spatio = fhatbarhat.reshape(728,-1)\n\n\nplt.plot(fhatbarhat_block_linearinterpolation_spatio[5])\n\n\n\n\n\n\n2) Random missing values\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_random_linearinterpolation.reshape(N,728)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(N)])    \nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_random_linearinterpolation_spatio = fhatbarhat.reshape(728,-1)\n\n\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[5])\n\n\n\n\n\n\n3) By 2\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_two_linearinterpolation.reshape(N,728)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(N)])    \nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_two_linearinterpolation_spatio = fhatbarhat.reshape(728,-1)\n\n\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[5])\n\n\n\n\n\n\n\n3.2.3. Spatio-Temporal\n\n1) Block\n\nw=np.zeros((777504,777504))\n\nMemoryError: Unable to allocate 4.40 TiB for an array with shape (777504, 777504) and data type float64\n\n\n\nfor i in range(777504):\n    for j in range(777504):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_linearinterpolation.reshape(777504,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(777504)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhat_block_linearinterpolation_spatio_temporal = fhatbarhat.reshape(728,N,1)\n\n\nplt.plot(fhat_block_linearinterpolation_spatio_temporal[5])\n\n\n\n2) Random missing values\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_random_linearinterpolation.reshape(777504,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(777504)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhat_random_linearinterpolation_spatio_temporal = fhatbarhat.reshape(728,N,1)\n\n\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[5])\n\n\n\n3) By 2\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_two_linearinterpolation.reshape(777504,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(777504)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhat_two_linearinterpolation_spatio_temporal = fhatbarhat.reshape(728,N,1)\n\n\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[5])"
  },
  {
    "objectID": "posts/GCN/2023-01-15-Algorithm_EX_2.html#normal-distribution",
    "href": "posts/GCN/2023-01-15-Algorithm_EX_2.html#normal-distribution",
    "title": "GCN Algorithm Example 2",
    "section": "3.3. Normal distribution",
    "text": "3.3. Normal distribution\n\n3.3.1. Temporal\n\n1) Block\n\nw=np.zeros((N,728,728))\n\n\nfor k in range(N):\n    for i in range(728):\n        for j in range(728):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(N)])\nD= np.array([np.diag(d[i]) for i in range(N)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(N)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(N)])    \nfhatbar = np.hstack([Psi[i] @ fhat_xblock_normal[:,i] for i in range(N)])\n_fhatbar = fhatbar.reshape(N,728)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(N)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(N)])    \nfhatbarhat_block_normal_temporal = fhatbarhat.reshape(728,-1)\n\n\nplt.plot(fhatbarhat_block_normal_temporal[5])\n\n\n\n\n\n\n2) Random missing values\n\nd = np.array([w[i].sum(axis=1) for i in range(N)])\nD= np.array([np.diag(d[i]) for i in range(N)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(N)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(N)])    \nfhatbar = np.hstack([Psi[i] @ fhat_xrandom_normal[:,i] for i in range(N)])\n_fhatbar = fhatbar.reshape(N,728)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(N)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(N)])    \nfhatbarhat_random_normal_temporal = fhatbarhat.reshape(728,-1)\n\n\nplt.plot(fhatbarhat_block_normal_temporal[5])\n\n\n\n\n\n\n3) By 2\n\nd = np.array([w[i].sum(axis=1) for i in range(N)])\nD= np.array([np.diag(d[i]) for i in range(N)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(N)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(N)])    \nfhatbar = np.hstack([Psi[i] @ fhat_xtwo_normal[:,i] for i in range(N)])\n_fhatbar = fhatbar.reshape(N,728)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(N)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(N)])    \nfhatbarhat_two_normal_temporal = fhatbarhat.reshape(728,-1)\n\n\nplt.plot(fhatbarhat_two_normal_temporal[5])\n\n\n\n\n\n\n\n3.3.2. Spatio\n\n1) Block\n\nw=np.zeros((N,N))\n\n\nfor i in range(N):\n    for j in range(N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_xblock_normal.reshape(N,728)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(N)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_block_normal_spatio = fhatbarhat.reshape(728,-1)\n\n\nplt.plot(fhatbarhat_block_normal_spatio[5])\n\n\n\n\n\n\n2) Random missing values\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_xrandom_normal.reshape(N,728)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(N)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_random_normal_spatio = fhatbarhat.reshape(728,-1)\n\n\nplt.plot(fhatbarhat_random_normal_spatio[5])\n\n\n\n\n\n\n3) By 2\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_xrandom_normal.reshape(N,728)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(N)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_random_normal_spatio = fhatbarhat.reshape(728,-1)\n\n\nplt.plot(fhatbarhat_random_normal_spatio[5])\n\n\n\n\n\n\n\n3.3.3. Spatio-Temporal\n\n1) Block\n\nw=np.zeros((777504,777504))\n\nMemoryError: Unable to allocate 4.40 TiB for an array with shape (777504, 777504) and data type float64\n\n\n\nfor i in range(777504):\n    for j in range(777504):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_xblock_normal.reshape(777504,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(777504)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_block_normal_spatio_temporal = fhatbarhat.reshape(777504,N,1)\n\n\nplt.plot(fhatbarhat_normal_spatio_temporal[5])\n\n\n\n2) Random missing values\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_xrandom_normal.reshape(777504,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(777504)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_random_normal_spatio_temporal = fhatbarhat.reshape(777504,N,1)\n\n\nplt.plot(fhatbarhat_random_normal_spatio_temporal[5])\n\n\n\n3) By 2\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_xtwo_normal.reshape(777504,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(777504)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_two_normal_spatio_temporal = fhatbarhat.reshape(777504,N,1)\n\n\nplt.plot(fhatbarhat_two_normal_spatio_temporal[5])"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html",
    "title": "GCN Algorithm Example 1",
    "section": "",
    "text": "Our method"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#데이터-일부-missing-처리",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#데이터-일부-missing-처리",
    "title": "GCN Algorithm Example 1",
    "section": "데이터 일부 missing 처리",
    "text": "데이터 일부 missing 처리\n\n1) Block 처리\n\n[1] ST-GCN\n\n%%R\nfiveVTS0 <- fiveVTS\nfiveVTS0[50:150, 3] <- NA\n\n\n%R -o fiveVTS0\n\n\nplt.plot(fiveVTS0[:,2])\n\n\n\n\n\nT = 200\nN = 5 # number of Nodes\nE = fiveNet_edge\nV = np.array([1,2,3,4,5])\nt = np.arange(0,T)\nnode_features = 1\n\n\nf = torch.tensor(fiveVTS0).reshape(200,5,1).float()\n\n\nX = f[:199,:,:]\ny = f[1:,:,:]\n\n\nedge_index = torch.tensor(E)\nedge_attr = torch.tensor(np.array([1,1,1,1,1,1,1,1,1,1]),dtype=torch.float32)\n\n\n_ee = enumerate(zip(X,y))\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:33<00:00,  1.49it/s]\n\n\n\nyhat = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nplt.plot(yhat[:,0].data)\nplt.plot(yhat[:,1].data)\nplt.plot(yhat[:,2].data)\nplt.plot(yhat[:,3].data)"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#mean",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#mean",
    "title": "GCN Algorithm Example 1",
    "section": "1.1. Mean",
    "text": "1.1. Mean\n\n1) Block\n\nfiveVTS0_mean = fiveVTS0.copy()\n\n\nfiveVTS0_mean[49:150,2] = np.mean(fiveVTS0[:49,2].tolist()+fiveVTS0[150:,2].tolist())\n\n\nplt.plot(fiveVTS0_mean[:,2])\n\n\n\n\n\n\n2) Random missing values\n\nfiveVTSrandom_mean = fiveVTSrandom.copy()\n\n\ndf = pd.DataFrame(fiveVTSrandom[:,2])\nmean_value = df.mean() # finds the mean value of the column A\ndf = df.fillna(mean_value) # replace missing values with the mean value\n\n\nfiveVTSrandom_mean[:,2] = np.array(df).reshape(200,)\n\n\nplt.plot(fiveVTSrandom_mean[:,2])\n\n\n\n\n\n\n3) By 2\n\nfiveVTStwo_mean = fiveVTStwo.copy()\n\n\ndf = pd.DataFrame(fiveVTStwo[:,2])\nmean_value = df.mean() # finds the mean value of the column A\ndf = df.fillna(mean_value) # replace missing values with the mean value\n\n\nfiveVTStwo_mean[:,2] = np.array(df).reshape(200,)\n\n\nplt.plot(fiveVTStwo_mean[:,2])"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#linear-interpolation",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#linear-interpolation",
    "title": "GCN Algorithm Example 1",
    "section": "1.2. linear interpolation",
    "text": "1.2. linear interpolation\n\n1) Block\n\nfiveVTS0_linearinterpolation = fiveVTS0.copy()\n\n\n# Sample data points\nx = np.array([48,150])\ny = np.array([fiveVTS0_linearinterpolation[48,2],fiveVTS0_linearinterpolation[150,2]])\n\n# Create interpolating function\nf = interp1d(x, y, kind='linear')\n\n# Estimate y value for x = 2.5\ny_interp = f(range(49,150))\n\n\nfiveVTS0_linearinterpolation[49:150,2] = y_interp\n\n\nplt.plot(fiveVTS0_linearinterpolation[:,2])\n\n\n\n\n\n\n2) Random missing values\n\nfiveVTSrandom_linearinterpolation = fiveVTSrandom.copy()\n\n\n_df = pd.DataFrame(fiveVTSrandom_linearinterpolation[:,2])\n_df.interpolate(method='linear', inplace=True)\n_df = _df.fillna(0)\n\n\nfiveVTSrandom_linearinterpolation[:,2] = np.array(_df).reshape(200,)\n\n\nplt.plot(fiveVTSrandom_linearinterpolation[:,2])\n\n\n\n\n\n\n3) By 2\n\nfiveVTStwo_linearinterpolation = fiveVTStwo.copy()\n\n\n_df = pd.Series(fiveVTStwo_linearinterpolation[:,2])\n_df.interpolate(method='linear', inplace=True)\n_df = _df.fillna(0)\n\n\nfiveVTStwo_linearinterpolation[:,2] = _df\n\n\nplt.plot(fiveVTStwo_linearinterpolation[:,2])"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#normal-distribution-random-values",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#normal-distribution-random-values",
    "title": "GCN Algorithm Example 1",
    "section": "1.3. Normal distribution Random Values",
    "text": "1.3. Normal distribution Random Values\n\n1) Block\n\nfiveVTS0_normal = fiveVTS0.copy()\n\n\nfiveVTS0_normal[49:150,2] = np.random.normal(loc=0,scale=0.2,size=101)\n\n\nplt.plot(fiveVTS0_normal[:,2])\n\n\n\n\n\n\n2) Random missing values\n\nfiveVTSrandom_normal = fiveVTSrandom.copy()\n\n\nfiveVTSrandom_normal[sampleindex-1,2] = np.random.normal(loc=0,scale=0.2,size=100)\n\n\nplt.plot(fiveVTSrandom_normal[:,2])\n\n\n\n\n\n\n3) By 2\n\nfiveVTStwo_normal = fiveVTStwo.copy()\n\n\n_df = pd.Series(fiveVTStwo_normal[:,2])\n_df = _df.fillna(np.random.normal(loc=0,scale=0.2))\n\n\nfiveVTStwo_normal[:,2] = np.array(_df).reshape(200,)\n\n\nplt.plot(fiveVTStwo_normal[:,2])"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#mean-1",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#mean-1",
    "title": "GCN Algorithm Example 1",
    "section": "2.1. Mean",
    "text": "2.1. Mean\n\n1) Block\n\nf_mean = torch.tensor(fiveVTS0_mean).reshape(200,5,1).float()\n\n\nX_mean = f_mean[:199,:,:]\ny_mean = f_mean[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_mean,y_mean)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32<00:00,  1.56it/s]\n\n\n\nfhat_mean = torch.stack([model(xt, edge_index, edge_attr) for xt in X_mean]).detach().numpy()\n\n\nplt.plot(fhat_mean[:,2].data)\n\n\n\n\n\n\n2) Random missing values\n\nf_fiveVTSrandom_mean = torch.tensor(fiveVTSrandom_mean).reshape(200,5,1).float()\n\n\nX_fiveVTSrandom_mean = f_fiveVTSrandom_mean[:199,:,:]\ny_fiveVTSrandom_mean = f_fiveVTSrandom_mean[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_fiveVTSrandom_mean,y_fiveVTSrandom_mean)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32<00:00,  1.55it/s]\n\n\n\nfhat_fiveVTSrandom_mean = torch.stack([model(xt, edge_index, edge_attr) for xt in X_fiveVTSrandom_mean]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTSrandom_mean[:,2].data)\n\n\n\n\n\n\n3) By 2\n\nf_fiveVTStwo_mean = torch.tensor(fiveVTStwo_mean).reshape(200,5,1).float()\n\n\nX_fiveVTStwo_mean = f_fiveVTStwo_mean[:199,:,:]\ny_fiveVTStwo_mean = f_fiveVTStwo_mean[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_fiveVTStwo_mean,y_fiveVTStwo_mean)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32<00:00,  1.54it/s]\n\n\n\nfhat_fiveVTStwo_mean = torch.stack([model(xt, edge_index, edge_attr) for xt in X_fiveVTStwo_mean]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTStwo_mean[:,2].data)"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#linear-interpolation-1",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#linear-interpolation-1",
    "title": "GCN Algorithm Example 1",
    "section": "2.2. linear interpolation",
    "text": "2.2. linear interpolation\n\n1) Block\n\nf_linearinterpolation = torch.tensor(fiveVTS0_linearinterpolation).reshape(200,5,1).float()\n\n\nX_linearinterpolation = f_linearinterpolation[:199,:,:]\ny_linearinterpolation = f_linearinterpolation[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_linearinterpolation,y_linearinterpolation)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32<00:00,  1.55it/s]\n\n\n\nfhat_linearinterpolation = torch.stack([model(xt, edge_index, edge_attr) for xt in X_linearinterpolation]).detach().numpy()\n\n\nplt.plot(fhat_linearinterpolation[:,2].data)\n\n\n\n\n\n\n2) Random missing values\n\nf_fiveVTSrandom_linearinterpolation = torch.tensor(fiveVTSrandom_linearinterpolation).reshape(200,5,1).float()\n\n\nX_fiveVTSrandom_linearinterpolation = f_fiveVTSrandom_linearinterpolation[:199,:,:]\ny_fiveVTSrandom_linearinterpolation = f_fiveVTSrandom_linearinterpolation[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_fiveVTSrandom_linearinterpolation,y_fiveVTSrandom_linearinterpolation)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32<00:00,  1.55it/s]\n\n\n\nfhat_fiveVTSrandom_linearinterpolation = torch.stack([model(xt, edge_index, edge_attr) for xt in X_fiveVTSrandom_linearinterpolation]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTSrandom_linearinterpolation[:,2].data)\n\n\n\n\n\n\n3) By 2\n\nf_fiveVTStwo_linearinterpolation = torch.tensor(fiveVTStwo_linearinterpolation).reshape(200,5,1).float()\n\n\nX_fiveVTStwo_linearinterpolation = f_fiveVTSrandom_linearinterpolation[:199,:,:]\ny_fiveVTStwo_linearinterpolation = f_fiveVTSrandom_linearinterpolation[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_fiveVTStwo_linearinterpolation,y_fiveVTStwo_linearinterpolation)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32<00:00,  1.55it/s]\n\n\n\nfhat_fiveVTStwo_linearinterpolation = torch.stack([model(xt, edge_index, edge_attr) for xt in X_fiveVTStwo_linearinterpolation]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTStwo_linearinterpolation[:,2].data)"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#normal-distribution-random-values-1",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#normal-distribution-random-values-1",
    "title": "GCN Algorithm Example 1",
    "section": "2.3. Normal distribution Random Values",
    "text": "2.3. Normal distribution Random Values\n\n1) Block\n\nf_normal = torch.tensor(fiveVTS0_normal).reshape(200,5,1).float()\n\n\nX_normal = f_normal[:199,:,:]\ny_normal = f_normal[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_normal,y_normal)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32<00:00,  1.55it/s]\n\n\n\nfhat_normal = torch.stack([model(xt, edge_index, edge_attr) for xt in X_normal]).detach().numpy()\n\n\nplt.plot(fhat_normal[:,2].data)\n\n\n\n\n\n\n2) Random missing values\n\nf_fiveVTSrandom_normal = torch.tensor(fiveVTSrandom_normal).reshape(200,5,1).float()\n\n\nX_fiveVTSrandom_normal = f_fiveVTSrandom_normal[:199,:,:]\ny_fiveVTSrandom_normal = f_fiveVTSrandom_normal[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_fiveVTSrandom_normal,y_fiveVTSrandom_normal)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32<00:00,  1.54it/s]\n\n\n\nfhat_fiveVTSrandom_normal = torch.stack([model(xt, edge_index, edge_attr) for xt in X_fiveVTSrandom_normal]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTSrandom_normal[:,2].data)\n\n\n\n\n\n\n3) By 2\n\nf_fiveVTStwo_normal = torch.tensor(fiveVTStwo_normal).reshape(200,5,1).float()\n\n\nX_fiveVTStwo_normal = f_fiveVTStwo_normal[:199,:,:]\ny_fiveVTStwo_normal = f_fiveVTStwo_normal[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_fiveVTStwo_normal,y_fiveVTStwo_normal)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32<00:00,  1.54it/s]\n\n\n\nfhat_fiveVTStwo_normal = torch.stack([model(xt, edge_index, edge_attr) for xt in X_fiveVTStwo_normal]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTStwo_normal[:,2].data)"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#mean-2",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#mean-2",
    "title": "GCN Algorithm Example 1",
    "section": "3.1. Mean",
    "text": "3.1. Mean\n\n3.1.1. Temporal\n\n1) Block\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])    \nfhatbar = np.hstack([Psi[i] @ fhat_mean[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_mean_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_mean_temporal[:,0])\nplt.plot(fhatbarhat_mean_temporal[:,1])\nplt.plot(fhatbarhat_mean_temporal[:,2])\nplt.plot(fhatbarhat_mean_temporal[:,3])\nplt.plot(fhatbarhat_mean_temporal[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])    \nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTSrandom_mean[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_random_mean_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_random_mean_temporal[:,0])\nplt.plot(fhatbarhat_random_mean_temporal[:,1])\nplt.plot(fhatbarhat_random_mean_temporal[:,2])\nplt.plot(fhatbarhat_random_mean_temporal[:,3])\nplt.plot(fhatbarhat_random_mean_temporal[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])    \nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTStwo_mean[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_twomean_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_twomean_temporal[:,0])\nplt.plot(fhatbarhat_twomean_temporal[:,1])\nplt.plot(fhatbarhat_twomean_temporal[:,2])\nplt.plot(fhatbarhat_twomean_temporal[:,3])\nplt.plot(fhatbarhat_twomean_temporal[:,4])\n\n\n\n\n\n\n\n3.1.2. Spatio\n\n1) Block\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_mean.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_mean_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_mean_spatio[:,0])\nplt.plot(fhatbarhat_mean_spatio[:,1])\nplt.plot(fhatbarhat_mean_spatio[:,2])\nplt.plot(fhatbarhat_mean_spatio[:,3])\nplt.plot(fhatbarhat_mean_spatio[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTSrandom_mean.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_random_mean_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_random_mean_spatio[:,0])\nplt.plot(fhatbarhat_random_mean_spatio[:,1])\nplt.plot(fhatbarhat_random_mean_spatio[:,2])\nplt.plot(fhatbarhat_random_mean_spatio[:,3])\nplt.plot(fhatbarhat_random_mean_spatio[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTStwo_mean.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_two_mean_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_two_mean_spatio[:,0])\nplt.plot(fhatbarhat_two_mean_spatio[:,1])\nplt.plot(fhatbarhat_two_mean_spatio[:,2])\nplt.plot(fhatbarhat_two_mean_spatio[:,3])\nplt.plot(fhatbarhat_two_mean_spatio[:,4])\n\n\n\n\n\n\n\n3.1.3. Spatio-Temporal\n\n1) Block\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_mean.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_mean_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhatbarhat_mean_spatio_temporal[:,0])\nplt.plot(fhatbarhat_mean_spatio_temporal[:,1])\nplt.plot(fhatbarhat_mean_spatio_temporal[:,2])\nplt.plot(fhatbarhat_mean_spatio_temporal[:,3])\nplt.plot(fhatbarhat_mean_spatio_temporal[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTSrandom_mean.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_random_mean_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,0])\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,1])\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,2])\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,3])\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTStwo_mean.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_two_mean_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,0])\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,1])\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,2])\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,3])\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,4])"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#linear-interpolation-2",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#linear-interpolation-2",
    "title": "GCN Algorithm Example 1",
    "section": "3.2.linear interpolation",
    "text": "3.2.linear interpolation\n\n3.2.1. Temporal\n\n1) Block\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])\nfhatbar = np.hstack([Psi[i] @ fhat_linearinterpolation[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_linearinterpolation_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,0])\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,1])\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,2])\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,3])\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])\nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTSrandom_linearinterpolation[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_random_linearinterpolation_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,0])\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,1])\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,2])\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,3])\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])\nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTStwo_linearinterpolation[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_two_linearinterpolation_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,0])\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,1])\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,2])\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,3])\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,4])\n\n\n\n\n\n\n\n3.2.2. Spatio\n\n1) Block\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_linearinterpolation.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_linearinterpolation_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,0])\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,1])\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,2])\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,3])\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTSrandom_linearinterpolation.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_random_linearinterpolation_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,0])\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,1])\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,2])\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,3])\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTStwo_linearinterpolation.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_two_linearinterpolation_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,0])\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,1])\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,2])\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,3])\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,4])\n\n\n\n\n\n\n\n3.2.3. Spatio-Temporal\n\n1) Block\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_linearinterpolation.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhat_linearinterpolation_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,0])\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,1])\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,2])\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,3])\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTSrandom_linearinterpolation.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhat_random_linearinterpolation_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,0])\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,1])\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,2])\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,3])\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTStwo_linearinterpolation.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhat_two_linearinterpolation_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,0])\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,1])\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,2])\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,3])\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,4])"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#normal-distribution",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#normal-distribution",
    "title": "GCN Algorithm Example 1",
    "section": "3.3. Normal distribution",
    "text": "3.3. Normal distribution\n\n3.3.1. Temporal\n\n1) Block\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])    \nfhatbar = np.hstack([Psi[i] @ fhat_normal[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_normal_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_normal_temporal[:,0])\nplt.plot(fhatbarhat_normal_temporal[:,1])\nplt.plot(fhatbarhat_normal_temporal[:,2])\nplt.plot(fhatbarhat_normal_temporal[:,3])\nplt.plot(fhatbarhat_normal_temporal[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])    \nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTSrandom_normal[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_random_normal_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_random_normal_temporal[:,0])\nplt.plot(fhatbarhat_random_normal_temporal[:,1])\nplt.plot(fhatbarhat_random_normal_temporal[:,2])\nplt.plot(fhatbarhat_random_normal_temporal[:,3])\nplt.plot(fhatbarhat_random_normal_temporal[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])    \nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTStwo_normal[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_two_normal_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_two_normal_temporal[:,0])\nplt.plot(fhatbarhat_two_normal_temporal[:,1])\nplt.plot(fhatbarhat_two_normal_temporal[:,2])\nplt.plot(fhatbarhat_two_normal_temporal[:,3])\nplt.plot(fhatbarhat_two_normal_temporal[:,4])\n\n\n\n\n\n\n\n3.3.2. Spatio\n\n1) Block\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_normal.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_normal_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_normal_spatio[:,0])\nplt.plot(fhatbarhat_normal_spatio[:,1])\nplt.plot(fhatbarhat_normal_spatio[:,2])\nplt.plot(fhatbarhat_normal_spatio[:,3])\nplt.plot(fhatbarhat_normal_spatio[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTSrandom_normal.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_random_normal_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_random_normal_spatio[:,0])\nplt.plot(fhatbarhat_random_normal_spatio[:,1])\nplt.plot(fhatbarhat_random_normal_spatio[:,2])\nplt.plot(fhatbarhat_random_normal_spatio[:,3])\nplt.plot(fhatbarhat_random_normal_spatio[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTStwo_normal.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_two_normal_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_two_normal_spatio[:,0])\nplt.plot(fhatbarhat_two_normal_spatio[:,1])\nplt.plot(fhatbarhat_two_normal_spatio[:,2])\nplt.plot(fhatbarhat_two_normal_spatio[:,3])\nplt.plot(fhatbarhat_two_normal_spatio[:,4])\n\n\n\n\n\n\n\n3.3.3. Spatio-Temporal\n\n1) Block\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_normal.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_normal_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhatbarhat_normal_spatio_temporal[:,0])\nplt.plot(fhatbarhat_normal_spatio_temporal[:,1])\nplt.plot(fhatbarhat_normal_spatio_temporal[:,2])\nplt.plot(fhatbarhat_normal_spatio_temporal[:,3])\nplt.plot(fhatbarhat_normal_spatio_temporal[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTSrandom_normal.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_random_normal_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhatbarhat_random_normal_spatio_temporal[:,0])\nplt.plot(fhatbarhat_random_normal_spatio_temporal[:,1])\nplt.plot(fhatbarhat_random_normal_spatio_temporal[:,2])\nplt.plot(fhatbarhat_random_normal_spatio_temporal[:,3])\nplt.plot(fhatbarhat_random_normal_spatio_temporal[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTStwo_normal.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_two_normal_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhatbarhat_two_normal_spatio_temporal[:,0])\nplt.plot(fhatbarhat_two_normal_spatio_temporal[:,1])\nplt.plot(fhatbarhat_two_normal_spatio_temporal[:,2])\nplt.plot(fhatbarhat_two_normal_spatio_temporal[:,3])\nplt.plot(fhatbarhat_two_normal_spatio_temporal[:,4])"
  },
  {
    "objectID": "posts/GCN/index.html",
    "href": "posts/GCN/index.html",
    "title": "GCN",
    "section": "",
    "text": "About GCN Study"
  },
  {
    "objectID": "posts/GCN/2022-12-21-st-gcn Dataset.html",
    "href": "posts/GCN/2022-12-21-st-gcn Dataset.html",
    "title": "PyTorch Geometric Temporal Dataset",
    "section": "",
    "text": "PyTorch Geometric Temporal Dataset\nhttps://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/dataset.html#module-torch_geometric_temporal.dataset.chickenpox"
  },
  {
    "objectID": "posts/GCN/2022-12-21-st-gcn Dataset.html#chickenpoxdatasetloader",
    "href": "posts/GCN/2022-12-21-st-gcn Dataset.html#chickenpoxdatasetloader",
    "title": "PyTorch Geometric Temporal Dataset",
    "section": "ChickenpoxDatasetLoader",
    "text": "ChickenpoxDatasetLoader\nA dataset of county level chicken pox cases in Hungary between 2004 and 2014. We made it public during the development of PyTorch Geometric Temporal. The underlying graph is static - vertices are counties and edges are neighbourhoods. Vertex features are lagged weekly counts of the chickenpox cases (we included 4 lags). The target is the weekly number of cases for the upcoming week (signed integers). Our dataset consist of more than 500 snapshots (weeks).\n데이터정리\n\nT = 519\nN = 20 # number of nodes\nE = 102 # edges\n\\(f(v,t)\\)의 차원? (1,)\n시간에 따라서 Number of nodes가 변하는지? False\n시간에 따라서 Number of nodes가 변하는지? False\nX: (20,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (20,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 20\n\nvertices are counties\n\n-Edges : 102\n\nedges are neighbourhoods\n\n- Time : 519\n\nbetween 2004 and 2014\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import  ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset(lags=1)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n519\n\n\n\n(data[0][1]).x.type,(data[0][1]).edge_index.type,(data[0][1]).edge_attr.type,(data[0][1]).y.type\n\n(<function Tensor.type>,\n <function Tensor.type>,\n <function Tensor.type>,\n <function Tensor.type>)\n\n\n\nmax((data[4][1]).x[0])\n\ntensor(2.1339)\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(20)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[519, Data(x=[20, 1], edge_index=[2, 102], edge_attr=[102], y=[20])]\n\n\n\nlen(data[0][1].edge_index[0])\n\n102\n\n\n\nedge_list=[]\nfor i in range(519):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(20, 61)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import  ChickenpoxDatasetLoader\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.0011,  0.0286,  0.3547,  0.2954]), tensor(0.7106))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0]\n\ntensor([0.0286, 0.3547, 0.2954, 0.7106])\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0]\n\ntensor([ 0.3547,  0.2954,  0.7106, -0.6831])\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 20개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{20}\\}, t=1,2,\\dots,519\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:52<00:00,  1.05s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([20, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 102])\n\n\n\n_edge_attr.shape\n\ntorch.Size([102])\n\n\n\n_y.shape\n\ntorch.Size([20])\n\n\n\n_x.shape\n\ntorch.Size([20, 4])\n\n\nx\n\nVertex features are lagged weekly counts of the chickenpox cases (we included 4 lags). y\nThe target is the weekly number of cases for the upcoming week\n\n\n_x\n\ntensor([[-1.0814e-03,  2.8571e-02,  3.5474e-01,  2.9544e-01],\n        [-7.1114e-01, -5.9843e-01,  1.9051e-01,  1.0922e+00],\n        [-3.2281e+00, -2.2910e-01,  1.6103e+00, -1.5487e+00],\n        [ 6.4750e-01, -2.2117e+00, -9.6858e-01,  1.1862e+00],\n        [-1.7302e-01, -9.4717e-01,  1.0347e+00, -6.3751e-01],\n        [ 3.6345e-01, -7.5468e-01,  2.9768e-01, -1.6273e-01],\n        [-3.4174e+00,  1.7031e+00, -1.6434e+00,  1.7434e+00],\n        [-1.9641e+00,  5.5208e-01,  1.1811e+00,  6.7002e-01],\n        [-2.2133e+00,  3.0492e+00, -2.3839e+00,  1.8545e+00],\n        [-3.3141e-01,  9.5218e-01, -3.7281e-01, -8.2971e-02],\n        [-1.8380e+00, -5.8728e-01, -3.5514e-02, -7.2298e-02],\n        [-3.4669e-01, -1.9827e-01,  3.9540e-01, -2.4774e-01],\n        [ 1.4219e+00, -1.3266e+00,  5.2338e-01, -1.6374e-01],\n        [-7.7044e-01,  3.2872e-01, -1.0400e+00,  3.4945e-01],\n        [-7.8061e-01, -6.5022e-01,  1.4361e+00, -1.2864e-01],\n        [-1.0993e+00,  1.2732e-01,  5.3621e-01,  1.9023e-01],\n        [ 2.4583e+00, -1.7811e+00,  5.0732e-02, -9.4371e-01],\n        [ 1.0945e+00, -1.5922e+00,  1.3818e-01,  1.1855e+00],\n        [-7.0875e-01, -2.2460e-01, -7.0875e-01,  1.5630e+00],\n        [-1.8228e+00,  7.8633e-01, -5.6172e-01,  1.2647e+00]])\n\n\n\n_y\n\ntensor([ 0.7106, -0.0725,  2.6099,  1.7870,  0.8024, -0.2614, -0.8370,  1.9674,\n        -0.4212,  0.1655,  1.2519,  2.3743,  0.7877,  0.4531, -0.1721, -0.0614,\n         1.0452,  0.3203, -1.3791,  0.0036])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-st-gcn Dataset.html#pedalmedatasetloader",
    "href": "posts/GCN/2022-12-21-st-gcn Dataset.html#pedalmedatasetloader",
    "title": "PyTorch Geometric Temporal Dataset",
    "section": "PedalMeDatasetLoader",
    "text": "PedalMeDatasetLoader\nA dataset of PedalMe Bicycle deliver orders in London between 2020 and 2021. We made it public during the development of PyTorch Geometric Temporal. The underlying graph is static - vertices are localities and edges are spatial_connections. Vertex features are lagged weekly counts of the delivery demands (we included 4 lags). The target is the weekly number of deliveries the upcoming week. Our dataset consist of more than 30 snapshots (weeks).\n데이터정리\n\nT = 33\nV = 지역의 집합\nN = 15 # number of nodes\nE = 225 # edges\n\\(f(v,t)\\)의 차원? (1,) # number of deliveries\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (15,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (15,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 15\n\nvertices are localities\n\n-Edges : 225\n\nedges are spatial_connections\n\n- Time : 33\n\nbetween 2020 and 2021\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import  PedalMeDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = PedalMeDatasetLoader()\n\ndataset = loader.get_dataset(lags=1)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n33\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([15, 1]), torch.Size([2, 225]), torch.Size([225]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(15)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[33, Data(x=[15, 1], edge_index=[2, 225], edge_attr=[225], y=[15])]\n\n\n\nedge_list=[]\nfor i in range(33):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(15, 120)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import  PedalMeDatasetLoader\nloader = PedalMeDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([ 3.0574, -0.0477, -0.3076,  0.2437]), tensor(-0.2710))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0], (data[1][1]).y[0]\n\n(tensor([-0.0477, -0.3076,  0.2437, -0.2710]), tensor(0.2490))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0], (data[2][1]).y[0]\n\n(tensor([-0.3076,  0.2437, -0.2710,  0.2490]), tensor(-0.0357))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 15개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{15}\\}, t=1,2,\\dots,33\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:03<00:00, 16.04it/s]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([15, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 225])\n\n\n\n_edge_attr.shape\n\ntorch.Size([225])\n\n\n\n_y.shape\n\ntorch.Size([15])\n\n\n\n_x.shape\n\ntorch.Size([15, 4])\n\n\nx\n\nVertex features are lagged weekly counts of the delivery demands (we included 4 lags).\n주마다 배달 수요의 수가 얼마나 될지 percentage로, t-4시점까지?\n\ny\n\nThe target is the weekly number of deliveries the upcoming week. Our dataset consist of more than 30 snapshots (weeks).\n그 다음주에 배달의 수가 몇 퍼센트로 발생할지?\n\n\n_x[0:3]\n\ntensor([[ 3.0574, -0.0477, -0.3076,  0.2437],\n        [ 3.2126,  0.1240,  0.0764,  0.5582],\n        [ 1.9071, -0.8883,  1.5280, -0.7184]])\n\n\n\n_y\n\ntensor([-0.2710,  0.0888,  0.4733,  0.0907, -0.3129,  0.1184,  0.5886, -0.6571,\n         0.2647,  0.2338,  0.1720,  0.5720, -0.9568, -0.4138, -0.5271])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-st-gcn Dataset.html#wikimathsdatasetloader",
    "href": "posts/GCN/2022-12-21-st-gcn Dataset.html#wikimathsdatasetloader",
    "title": "PyTorch Geometric Temporal Dataset",
    "section": "WikiMathsDatasetLoader",
    "text": "WikiMathsDatasetLoader\nA dataset of vital mathematics articles from Wikipedia. We made it public during the development of PyTorch Geometric Temporal. The underlying graph is static - vertices are Wikipedia pages and edges are links between them. The graph is directed and weighted. Weights represent the number of links found at the source Wikipedia page linking to the target Wikipedia page. The target is the daily user visits to the Wikipedia pages between March 16th 2019 and March 15th 2021 which results in 731 periods.\n데이터정리\n\nT = 722\nV = 위키피디아 페이지\nN = 1068 # number of nodes\nE = 27079 # edges\n\\(f(v,t)\\)의 차원? (1,) # 해당페이지를 유저가 방문한 횟수\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (1068,8) (N,8), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3),f(v,t_4),f(v,t_5),f(v,t_6),f(v,t_7)\\)\ny: (1068,) (N,), \\(f(v,t_8)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 1068\n\nvertices are Wikipedia pages\n\n-Edges : 27079\n\nedges are links between them\n\n- Time : 722\n\nWikipedia pages between March 16th 2019 and March 15th 2021\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import  WikiMathsDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n722\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([1068, 8]), torch.Size([2, 27079]), torch.Size([27079]))\n\n\n\n(data[10][1]).x\n\ntensor([[ 0.4972,  0.6838,  0.7211,  ..., -0.8513,  0.1881,  1.3820],\n        [ 0.5457,  0.6016,  0.7071,  ..., -0.4599, -0.6089, -0.0626],\n        [ 0.6305,  1.1404,  0.8779,  ..., -0.5370,  0.7422,  0.3862],\n        ...,\n        [ 0.8699,  0.5451,  1.9254,  ..., -0.8351,  0.3828,  0.3828],\n        [ 0.2451,  0.9629,  1.0526,  ..., -0.9213,  0.8731, -0.1138],\n        [ 0.0200, -0.0871,  0.2342,  ..., -0.4712,  0.0717,  0.2859]])\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(1068)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(722):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(1068, 27079)\n\n\n\nnx.draw(G,node_color='green',node_size=100,width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\nnp.where(data[11][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\nnp.where(data[11][1].edge_index != data[20][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\nhttps://www.kaggle.com/code/mapologo/loading-wikipedia-math-essentials\n\nfrom torch_geometric_temporal.dataset import  WikiMathsDatasetLoader\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset(lags=8)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.4323, -0.4739,  0.2659,  0.4844,  0.5367,  0.6412,  0.2179, -0.7617]),\n tensor(-0.4067))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3,,x_4,x_5,x_6,x_7\\)\ny:= \\(x_9\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-0.4739,  0.2659,  0.4844,  0.5367,  0.6412,  0.2179, -0.7617, -0.4067]),\n tensor(0.3064))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8\\)\ny:= \\(x_9\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([ 0.2659,  0.4844,  0.5367,  0.6412,  0.2179, -0.7617, -0.4067,  0.3064]),\n tensor(0.4972))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9\\)\ny:=\\(x_{10}\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 1068개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7) \\to (x_8)\\)\n\\((x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8) \\to (x_9)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{1068}\\}, t=1,2,\\dots,722\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=8, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [09:28<00:00, 11.37s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([1068, 8])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 27079])\n\n\n어떤 페이지에 refer가 되었는지\n\n_edge_index[0][:5],_edge_index[1][:5]\n\n(tensor([0, 0, 0, 0, 0]), tensor([1, 2, 3, 4, 5]))\n\n\n\n_edge_attr.shape\n\ntorch.Size([27079])\n\n\n\n_edge_attr[:5]\n\ntensor([1., 4., 2., 2., 5.])\n\n\n\nWeights represent the number of links found at the source Wikipedia page linking to the target Wikipedia page.\n\n가중치는 엣지별 한 페이지에 refer되었는지, 몇 번 되었나 수 나옴\n\n_y.shape\n\ntorch.Size([1068])\n\n\n\n_x.shape\n\ntorch.Size([1068, 8])\n\n\nx\n\nlag 를 몇으로 지정하느냐에 따라 다르게 추출\n\ny\n\nThe target is the daily user visits to the Wikipedia pages between March 16th 2019 and March 15th 2021 which results in 731 periods.\n매일 위키피디아 해당 페이지에 몇 명의 유저가 방문하는지!\n음수가 왜 나오지..\n\n\n_x[0:3]\n\ntensor([[-0.4323, -0.4739,  0.2659,  0.4844,  0.5367,  0.6412,  0.2179, -0.7617],\n        [-0.4041, -0.4165, -0.0751,  0.1484,  0.4153,  0.4464, -0.3916, -0.8137],\n        [-0.3892,  0.0634,  0.5913,  0.5370,  0.4646,  0.2776, -0.0724, -0.8116]])\n\n\n\n_y[:3]\n\ntensor([-0.4067, -0.1620, -0.4043])\n\n\n\ny_hat[:3].data\n\ntensor([[-0.0648],\n        [ 0.0314],\n        [-1.0724]])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-st-gcn Dataset.html#windmilloutputlargedatasetloader",
    "href": "posts/GCN/2022-12-21-st-gcn Dataset.html#windmilloutputlargedatasetloader",
    "title": "PyTorch Geometric Temporal Dataset",
    "section": "WindmillOutputLargeDatasetLoader",
    "text": "WindmillOutputLargeDatasetLoader\nHourly energy output of windmills from a European country for more than 2 years. Vertices represent 319 windmills and weighted edges describe the strength of relationships. The target variable allows for regression tasks.\n데이터정리\n\nT = 17470\nV = 풍력발전소\nN = 319 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (319,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (319,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 319\n\nvertices represent 319 windmills\n\n-Edges : 101761\n\nweighted edges describe the strength of relationships.\n\n- Time : 17470\n\nmore than 2 years\n\n\nfrom torch_geometric_temporal.dataset import  WindmillOutputLargeDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WindmillOutputLargeDatasetLoader()\n\ndataset = loader.get_dataset(lags=1)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n17470\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([319, 1]), torch.Size([2, 101761]), torch.Size([101761]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(319)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[17470, Data(x=[319, 1], edge_index=[2, 101761], edge_attr=[101761], y=[319])]\n\n\ntime이 너무 많아서 일부만 시각화함!!\n\nedge_list=[]\nfor i in range(1000):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(319, 51040)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import  WindmillOutputLargeDatasetLoader\nloader = WindmillOutputLargeDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.5711, -0.7560,  2.6278, -0.8674]), tensor(-0.9877))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0], (data[1][1]).y[0]\n\n(tensor([-0.7560,  2.6278, -0.8674, -0.9877]), tensor(-0.8583))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([ 2.6278, -0.8674, -0.9877, -0.8583]), tensor(0.4282))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 319개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{319}\\}, t=1,2,\\dots,17470\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(5)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 5/5 [1:06:03<00:00, 792.70s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([319, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 101761])\n\n\n\n_edge_attr.shape\n\ntorch.Size([101761])\n\n\n\n_y.shape\n\ntorch.Size([319])\n\n\n\n_x.shape\n\ntorch.Size([319, 4])\n\n\nx\n\n\n\ny\n\nThe target variable allows for regression tasks.\n\n\n_x[0:3]\n\ntensor([[-0.5711, -0.7560,  2.6278, -0.8674],\n        [-0.6936, -0.7264,  2.4113, -0.6052],\n        [-0.8666, -0.7785,  2.2759, -0.6759]])\n\n\n\n_y[0]\n\ntensor(-0.9877)"
  },
  {
    "objectID": "posts/GCN/2022-12-21-st-gcn Dataset.html#windmilloutputmediumdatasetloader",
    "href": "posts/GCN/2022-12-21-st-gcn Dataset.html#windmilloutputmediumdatasetloader",
    "title": "PyTorch Geometric Temporal Dataset",
    "section": "WindmillOutputMediumDatasetLoader",
    "text": "WindmillOutputMediumDatasetLoader\nHourly energy output of windmills from a European country for more than 2 years. Vertices represent 26 windmills and weighted edges describe the strength of relationships. The target variable allows for regression tasks.\n데이터정리\n\nT = 17470\nV = 풍력발전소\nN = 319 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (319,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (319,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 26\n\nvertices represent 26 windmills\n\n-Edges : 225\n\nweighted edges describe the strength of relationships\n\n- Time : 676\n\nmore than 2 years\n\n\nfrom torch_geometric_temporal.dataset import  WindmillOutputMediumDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WindmillOutputMediumDatasetLoader()\n\ndataset = loader.get_dataset(lags=1)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n17470\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([26, 1]), torch.Size([2, 676]), torch.Size([676]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(26)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[17470, Data(x=[26, 1], edge_index=[2, 676], edge_attr=[676], y=[26])]\n\n\n\nedge_list=[]\nfor i in range(17463):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(26, 351)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import  WindmillOutputMediumDatasetLoader\nloader = WindmillOutputMediumDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.2170, -0.2055, -0.1587, -0.1930]), tensor(-0.2149))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-0.2055, -0.1587, -0.1930, -0.2149]), tensor(-0.2336))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([-0.1587, -0.1930, -0.2149, -0.2336]), tensor(-0.1785))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 26개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{26}\\}, t=1,2,\\dots,177470\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(5)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 5/5 [03:23<00:00, 40.73s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([26, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 676])\n\n\n\n_edge_attr.shape\n\ntorch.Size([676])\n\n\n\n_y.shape\n\ntorch.Size([26])\n\n\n\n_x.shape\n\ntorch.Size([26, 4])\n\n\nx\n\n\n\ny\n\nThe target variable allows for regression tasks.\n\n\n_x[0:3]\n\ntensor([[-0.2170, -0.2055, -0.1587, -0.1930],\n        [-0.1682, -0.2708, -0.1051,  1.1786],\n        [ 1.1540, -0.6707, -0.8291, -0.6823]])\n\n\n\n_y[0]\n\ntensor(-0.2149)"
  },
  {
    "objectID": "posts/GCN/2022-12-21-st-gcn Dataset.html#windmilloutputsmalldatasetloader",
    "href": "posts/GCN/2022-12-21-st-gcn Dataset.html#windmilloutputsmalldatasetloader",
    "title": "PyTorch Geometric Temporal Dataset",
    "section": "WindmillOutputSmallDatasetLoader",
    "text": "WindmillOutputSmallDatasetLoader\nHourly energy output of windmills from a European country for more than 2 years. Vertices represent 11 windmills and weighted edges describe the strength of relationships. The target variable allows for regression tasks.\n데이터정리\n\nT = 17470\nV = 풍력발전소\nN = 319 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (319,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (319,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 11\n\nvertices represent 11 windmills\n\n-Edges : 121\n\nweighted edges describe the strength of relationships\n\n- Time : 17470\n\nmore than 2 years\n\n\nfrom torch_geometric_temporal.dataset import WindmillOutputSmallDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WindmillOutputSmallDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n17463\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([11, 8]), torch.Size([2, 121]), torch.Size([121]))\n\n\n\ndata[-1]\n\n[17463, Data(x=[11, 8], edge_index=[2, 121], edge_attr=[121], y=[11])]\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(11)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(17463):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(11, 66)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import WindmillOutputSmallDatasetLoader\nloader = WindmillOutputSmallDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([ 0.8199, -0.4972,  0.4923, -0.8299]), tensor(-0.6885))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-0.4972,  0.4923, -0.8299, -0.6885]), tensor(0.7092))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([ 0.4923, -0.8299, -0.6885,  0.7092]), tensor(-0.9356))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 11개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{11}\\}, t=1,2,\\dots,17463\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(5)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 5/5 [02:55<00:00, 35.01s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([11, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 121])\n\n\n\n_edge_attr.shape\n\ntorch.Size([121])\n\n\n\n_y.shape\n\ntorch.Size([11])\n\n\n\n_x.shape\n\ntorch.Size([11, 4])\n\n\nx\n\n\n\ny\n\nThe target variable allows for regression tasks.\n\n\n_x[0:3]\n\ntensor([[ 0.8199, -0.4972,  0.4923, -0.8299],\n        [ 1.1377, -0.3742,  0.3668, -0.8333],\n        [ 0.9979, -0.5643,  0.4070, -0.8918]])\n\n\n\n_y\n\ntensor([-0.6885, -0.6594, -0.6303, -0.6983, -0.5416, -0.6186, -0.6031, -0.7580,\n        -0.6659, -0.5948, -0.5088])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-st-gcn Dataset.html#metrladatasetloader_real-world-traffic-dataset",
    "href": "posts/GCN/2022-12-21-st-gcn Dataset.html#metrladatasetloader_real-world-traffic-dataset",
    "title": "PyTorch Geometric Temporal Dataset",
    "section": "METRLADatasetLoader_real world traffic dataset",
    "text": "METRLADatasetLoader_real world traffic dataset\nA traffic forecasting dataset based on Los Angeles Metropolitan traffic conditions. The dataset contains traffic readings collected from 207 loop detectors on highways in Los Angeles County in aggregated 5 minute intervals for 4 months between March 2012 to June 2012.\n데이터정리\n\nT = 33\nV = 구역\nN = 207 # number of nodes\nE = 225\n\\(f(v,t)\\)의 차원? (3,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (207,4) (N,2,12), \\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11}\\)\ny: (207,) (N,), \\((x_{12})\\)\n예제코드적용가능여부: No\n\nhttps://arxiv.org/pdf/1707.01926.pdf\n- Nodes : 207\n\nvertices are localities\n\n-Edges : 225\n\nedges are spatial_connections\n\n- Time : 33\n\nbetween 2020 and 2021\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import METRLADatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = METRLADatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n34248\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([207, 2, 12]), torch.Size([2, 1722]), torch.Size([1722]))\n\n\n\ndata[-1]\n\n[34248,\n Data(x=[207, 2, 12], edge_index=[2, 1722], edge_attr=[1722], y=[207, 12])]\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(20)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(1000):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(207, 1520)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n논문 내용 중\n\n\n\nimage.png\n\n\n\nfrom torch_geometric_temporal.dataset import METRLADatasetLoader\nloader = METRLADatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\n\n\n\n\n\nNote\n\n\n\nlags option 없어서 error 뜸 : get_dataset() got an unexpected keyword argument ‘lags’\n\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([[ 0.5332,  0.4486,  0.5146, -2.6522, -2.6522,  0.1847,  0.6383,  0.4961,\n           0.7497,  0.4899,  0.5751,  0.4280],\n         [-1.7292, -1.7171, -1.7051, -1.6930, -1.6810, -1.6689, -1.6569, -1.6448,\n          -1.6328, -1.6207, -1.6087, -1.5966]]),\n tensor([0.3724, 0.2452, 0.4961, 0.6521, 0.1126, 0.5311, 0.5091, 0.4713, 0.4218,\n         0.3909, 0.4761, 0.5641]))\n\n\n\\(t=0\\)에서 \\(X,Z\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11}\\)\nZ:= \\(z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11}\\)\ny:= \\(x_{12}\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([[ 0.4486,  0.5146, -2.6522, -2.6522,  0.1847,  0.6383,  0.4961,  0.7497,\n           0.4899,  0.5751,  0.4280,  0.3724],\n         [-1.7171, -1.7051, -1.6930, -1.6810, -1.6689, -1.6569, -1.6448, -1.6328,\n          -1.6207, -1.6087, -1.5966, -1.5846]]),\n tensor([ 0.2452,  0.4961,  0.6521,  0.1126,  0.5311,  0.5091,  0.4713,  0.4218,\n          0.3909,  0.4761,  0.5641, -0.0022]))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12}\\)\nZ:= \\(z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12}\\)\ny:= \\(x_{13}\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([[ 0.5146, -2.6522, -2.6522,  0.1847,  0.6383,  0.4961,  0.7497,  0.4899,\n           0.5751,  0.4280,  0.3724,  0.2452],\n         [-1.7051, -1.6930, -1.6810, -1.6689, -1.6569, -1.6448, -1.6328, -1.6207,\n          -1.6087, -1.5966, -1.5846, -1.5725]]),\n tensor([ 0.4961,  0.6521,  0.1126,  0.5311,  0.5091,  0.4713,  0.4218,  0.3909,\n          0.4761,  0.5641, -0.0022,  0.4218]))\n\n\n\nX:= \\(x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12},x_{13}\\)\nZ:= \\(z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12},z_{13}\\)\ny:= \\(x_{14}\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 207개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11} \\to (x_{12})\\)\n\\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12},z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12} \\to (x_{13})\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{207}\\}, t=1,2,\\dots,34248\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=1, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([207, 2, 12])\n\n\n\nnode 207개, traffic sensor 2개\n\n\n_edge_index.shape\n\ntorch.Size([2, 1722])\n\n\n\n_edge_attr.shape\n\ntorch.Size([1722])\n\n\n\n_y.shape\n\ntorch.Size([207, 12])\n\n\n\n_x.shape\n\ntorch.Size([207, 2, 12])\n\n\ny\n\ntraffic speed\n\n\n_x[0]\n\ntensor([[ 0.5332,  0.4486,  0.5146, -2.6522, -2.6522,  0.1847,  0.6383,  0.4961,\n          0.7497,  0.4899,  0.5751,  0.4280],\n        [-1.7292, -1.7171, -1.7051, -1.6930, -1.6810, -1.6689, -1.6569, -1.6448,\n         -1.6328, -1.6207, -1.6087, -1.5966]])\n\n\n\n_y[0]\n\ntensor([0.3724, 0.2452, 0.4961, 0.6521, 0.1126, 0.5311, 0.5091, 0.4713, 0.4218,\n        0.3909, 0.4761, 0.5641])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-st-gcn Dataset.html#pemsbaydatasetloader",
    "href": "posts/GCN/2022-12-21-st-gcn Dataset.html#pemsbaydatasetloader",
    "title": "PyTorch Geometric Temporal Dataset",
    "section": "PemsBayDatasetLoader",
    "text": "PemsBayDatasetLoader\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1111/tgis.12644\nA traffic forecasting dataset as described in Diffusion Convolution Layer Paper.\nThis traffic dataset is collected by California Transportation Agencies (CalTrans) Performance Measurement System (PeMS). It is represented by a network of 325 traffic sensors in the Bay Area with 6 months of traffic readings ranging from Jan 1st 2017 to May 31th 2017 in 5 minute intervals.\n데이터정리\n\nT = 17470\nV = 풍력발전소\nN = 325 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (325,2,12) (N,2,12),\n\n\\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11}\\)\n\\(z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11}\\)\n\ny: (325,) (N,2,12),\n\n\\(x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23},x_{24}\\)\n\\(z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23},z_{24}\\)\n\n예제코드적용가능여부: No\n\n- Nodes : 325\n\nvertices are sensors\n\n-Edges : 2694\n\nweighted edges are between seonsor paris measured by the road nretwork distance\n\n- Time : 52081\n\n6 months of traffic readings ranging from Jan 1st 2017 to May 31th 2017 in 5 minute intervals\n\n\nfrom torch_geometric_temporal.dataset import PemsBayDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = PemsBayDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n52081\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([325, 2, 12]), torch.Size([2, 2694]), torch.Size([2694]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(325)).tolist()\n\n\ndata[-1]\n\n[52081,\n Data(x=[325, 2, 12], edge_index=[2, 2694], edge_attr=[2694], y=[325, 2, 12])]\n\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(1000):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(325, 2404)\n\n\n\nnx.draw(G,node_color='green',node_size=50,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import PemsBayDatasetLoader\nloader = PemsBayDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([[ 0.9821,  0.9928,  1.0251,  1.0574,  1.0466,  1.0681,  0.9821,  1.0251,\n           1.0143,  0.9928,  0.9498,  0.9821],\n         [-1.6127, -1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397, -1.5275,\n          -1.5153, -1.5032, -1.4910, -1.4788]]),\n tensor([[ 1.0143,  0.9821,  0.9821,  1.0036,  1.0143,  0.9605,  0.9498,  1.0251,\n           0.9928,  0.9928,  0.9498,  0.9928],\n         [-1.4667, -1.4545, -1.4423, -1.4302, -1.4180, -1.4058, -1.3937, -1.3815,\n          -1.3694, -1.3572, -1.3450, -1.3329]]))\n\n\n\\(t=0\\)에서 \\(X,Z\\)와 \\(y,s\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11}\\)\nZ:= \\(z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11}\\)\ny:= \\(x_{12},x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23}\\)\ns:= \\(z_{12},z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23}\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([[ 0.9928,  1.0251,  1.0574,  1.0466,  1.0681,  0.9821,  1.0251,  1.0143,\n           0.9928,  0.9498,  0.9821,  1.0143],\n         [-1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397, -1.5275, -1.5153,\n          -1.5032, -1.4910, -1.4788, -1.4667]]),\n tensor([[ 0.9821,  0.9821,  1.0036,  1.0143,  0.9605,  0.9498,  1.0251,  0.9928,\n           0.9928,  0.9498,  0.9928,  0.9821],\n         [-1.4545, -1.4423, -1.4302, -1.4180, -1.4058, -1.3937, -1.3815, -1.3694,\n          -1.3572, -1.3450, -1.3329, -1.3207]]))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12}\\)\nZ:= \\(z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12}\\)\ny:= \\(x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23},x_{24}\\)\ns:= \\(z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23},z_{24}\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([[ 1.0251,  1.0574,  1.0466,  1.0681,  0.9821,  1.0251,  1.0143,  0.9928,\n           0.9498,  0.9821,  1.0143,  0.9821],\n         [-1.5883, -1.5762, -1.5640, -1.5518, -1.5397, -1.5275, -1.5153, -1.5032,\n          -1.4910, -1.4788, -1.4667, -1.4545]]),\n tensor([[ 0.9821,  1.0036,  1.0143,  0.9605,  0.9498,  1.0251,  0.9928,  0.9928,\n           0.9498,  0.9928,  0.9821,  1.0143],\n         [-1.4423, -1.4302, -1.4180, -1.4058, -1.3937, -1.3815, -1.3694, -1.3572,\n          -1.3450, -1.3329, -1.3207, -1.3085]]))\n\n\n\nX:= \\(x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12},x_{13}\\)\nZ:= \\(z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12},z_{13}\\)\ny:= \\(x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23},x_{24},x_{25}\\)\ns:= \\(z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23},z_{24},z_{25}\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 325개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11} \\to x_{12},x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23}\\)\n\\(z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11} \\to z_{12},z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23}\\)\n\\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12} \\to x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23},x_{24}\\)\n\\(z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12} \\to z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23},z_{24}\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{325}\\}, t=1,2,\\dots,52081\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([325, 2, 12])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 2694])\n\n\n\n_edge_attr.shape\n\ntorch.Size([2694])\n\n\n\n_y.shape\n\ntorch.Size([325, 2, 12])\n\n\n\n_x.shape\n\ntorch.Size([325, 2, 12])\n\n\nx\n\n.!\n\ny\n\ncapturing temporal dependencies..?\n\nedges connect sensors\nFor instance, the traffic conditions on one road on Wednesday at 3:00 p.m. are similar to the traffic conditions on Thursday at the same time.\n\n_x[0:3]\n\ntensor([[[ 0.9821,  0.9928,  1.0251,  1.0574,  1.0466,  1.0681,  0.9821,\n           1.0251,  1.0143,  0.9928,  0.9498,  0.9821],\n         [-1.6127, -1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397,\n          -1.5275, -1.5153, -1.5032, -1.4910, -1.4788]],\n\n        [[ 0.6054,  0.5839,  0.6592,  0.6269,  0.6808,  0.6377,  0.6700,\n           0.6054,  0.6162,  0.6162,  0.5839,  0.5947],\n         [-1.6127, -1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397,\n          -1.5275, -1.5153, -1.5032, -1.4910, -1.4788]],\n\n        [[ 0.9390,  0.9175,  0.8960,  0.9175,  0.9067,  0.9175,  0.9175,\n           0.8852,  0.9283,  0.8960,  0.9067,  0.8960],\n         [-1.6127, -1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397,\n          -1.5275, -1.5153, -1.5032, -1.4910, -1.4788]]])\n\n\n\n_y[0]\n\ntensor([[ 1.0143,  0.9821,  0.9821,  1.0036,  1.0143,  0.9605,  0.9498,  1.0251,\n          0.9928,  0.9928,  0.9498,  0.9928],\n        [-1.4667, -1.4545, -1.4423, -1.4302, -1.4180, -1.4058, -1.3937, -1.3815,\n         -1.3694, -1.3572, -1.3450, -1.3329]])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-st-gcn Dataset.html#englandcoviddatasetloader",
    "href": "posts/GCN/2022-12-21-st-gcn Dataset.html#englandcoviddatasetloader",
    "title": "PyTorch Geometric Temporal Dataset",
    "section": "EnglandCovidDatasetLoader",
    "text": "EnglandCovidDatasetLoader\nA dataset of mobility and history of reported cases of COVID-19 in England NUTS3 regions, from 3 March to 12 of May. The dataset is segmented in days and the graph is directed and weighted. The graph indicates how many people moved from one region to the other each day, based on Facebook Data For Good disease prevention maps. The node features correspond to the number of COVID-19 cases in the region in the past window days. The task is to predict the number of cases in each node after 1 day.\nhttps://arxiv.org/pdf/2009.08388.pdf\n데이터정리\n\nT = 52\nV = 지역\nN = 129 # number of nodes\nE = 2158\n\\(f(v,t)\\)의 차원? (1,) # 코로나확진자수\n시간에 따라서 Number of nodes가 변하는지? False\n시간에 따라서 Number of nodes가 변하는지? False\nX: (20,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (20,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 129\n\nvertices are correspond to the number of COVID-19 cases in the region in the past window days.\n\n-Edges : 2158\n\nthe spatial edges capture county-to-county movement at a specific date, and a county is connected to a number of past instances of itself with temporal edges.\n\n- Time : 52\n\nfrom 3 March to 12 of May\n\n\nfrom torch_geometric_temporal.dataset import EnglandCovidDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = EnglandCovidDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n52\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([129, 8]), torch.Size([2, 2158]), torch.Size([2158]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(129)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[52, Data(x=[129, 8], edge_index=[2, 1424], edge_attr=[1424], y=[129])]\n\n\n\nlen(data[0][1].edge_index[0])\n\n2158\n\n\n\nedge_list=[]\nfor i in range(52):\n    for j in range(100):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(129, 1230)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[2][1].edge_index !=data[2][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import EnglandCovidDatasetLoader\nloader = EnglandCovidDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-1.4697, -1.9283, -1.6990, -1.8137]), tensor(-1.8137))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-1.9283, -1.6990, -1.8137, -1.8137]), tensor(-0.8965))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([-1.6990, -1.8137, -1.8137, -0.8965]), tensor(-1.1258))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 129개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{129}\\}, t=1,2,\\dots,52\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:07<00:00,  6.30it/s]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([129, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 2158])\n\n\n\n_edge_attr.shape\n\ntorch.Size([2158])\n\n\n\n_y.shape\n\ntorch.Size([129])\n\n\n\ny_hat.shape\n\ntorch.Size([129, 1])\n\n\n\n_x.shape\n\ntorch.Size([129, 4])\n\n\nx\n\n\n\ny\n\n\n\nThe node features correspond to the number of COVID-19 cases in the region in the past window days.\nThe task is to predict the number of cases in each node after 1 day\n\n_x[0:3]\n\ntensor([[-1.4697, -1.9283, -1.6990, -1.8137],\n        [-1.2510, -1.1812, -1.3208, -1.1812],\n        [-1.0934, -1.0934, -1.0934, -1.0934]])\n\n\n\n_y[:3]\n\ntensor([-1.8137, -1.3208, -1.0934])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-st-gcn Dataset.html#montevideobusdatasetloader",
    "href": "posts/GCN/2022-12-21-st-gcn Dataset.html#montevideobusdatasetloader",
    "title": "PyTorch Geometric Temporal Dataset",
    "section": "MontevideoBusDatasetLoader",
    "text": "MontevideoBusDatasetLoader\nhttps://www.fing.edu.uy/~renzom/msc/uploads/msc-thesis.pdf\nA dataset of inflow passenger at bus stop level from Montevideo city. This dataset comprises hourly inflow passenger data at bus stop level for 11 bus lines during October 2020 from Montevideo city (Uruguay). The bus lines selected are the ones that carry people to the center of the city and they load more than 25% of the total daily inflow traffic. Vertices are bus stops, edges are links between bus stops when a bus line connects them and the weight represent the road distance. The target is the passenger inflow. This is a curated dataset made from different data sources of the Metropolitan Transportation System (STM) of Montevideo.\n데이터정리\n\nT = 739\nV = 버스정류장\nN = 675 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # passenger inflow\n시간에 따라서 Number of nodes가 변하는지? False\n시간에 따라서 Number of nodes가 변하는지? False\nX: (675,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (675,,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 675\n\nvertices are bus stops\n\n-Edges : 690\n\nedges are links between bus stops when a bus line connects them and the weight represent the road distance\n\n- Time : 739\n\nhourly inflow passenger data at bus stop level for 11 bus lines during October 2020 from Montevideo city (Uruguay).\n\n\nfrom torch_geometric_temporal.dataset import MontevideoBusDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = MontevideoBusDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n739\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([675, 4]), torch.Size([2, 690]), torch.Size([690]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(675)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(739):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(675, 690)\n\n\n\nnx.draw(G,node_color='green',node_size=50,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import MontevideoBusDatasetLoader\nloader = MontevideoBusDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.4200, -0.4200, -0.4200, -0.4200]), tensor(-0.4200))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-0.4200, -0.4200, -0.4200, -0.4200]), tensor(-0.4200))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([-0.4200, -0.4200, -0.4200, -0.4200]), tensor(-0.4200))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 675개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{675}\\}, t=1,2,\\dots,739\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [01:51<00:00,  2.23s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([675, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 690])\n\n\n\n_edge_attr.shape\n\ntorch.Size([690])\n\n\n\n_y.shape\n\ntorch.Size([675])\n\n\n\n_x.shape\n\ntorch.Size([675, 4])\n\n\nx\n\n\n\ny\n\nThe target is the passenger inflow.\nThis is a curated dataset made from different data sources of the Metropolitan Transportation System (STM) of Montevide\n\n\n_x[0:3]\n\ntensor([[-0.4200, -0.4200, -0.4200, -0.4200],\n        [-0.0367, -0.0367, -0.0367, -0.0367],\n        [-0.2655, -0.2655, -0.2655, -0.2655]])\n\n\n\n_y[:3]\n\ntensor([-0.4200, -0.0367, -0.2655])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-st-gcn Dataset.html#twittertennisdatasetloader",
    "href": "posts/GCN/2022-12-21-st-gcn Dataset.html#twittertennisdatasetloader",
    "title": "PyTorch Geometric Temporal Dataset",
    "section": "TwitterTennisDatasetLoader",
    "text": "TwitterTennisDatasetLoader\nhttps://appliednetsci.springeropen.com/articles/10.1007/s41109-018-0080-5?ref=https://githubhelp.com\nTwitter mention graphs related to major tennis tournaments from 2017. Nodes are Twitter accounts and edges are mentions between them. Each snapshot contains the graph induced by the most popular nodes of the original dataset. Node labels encode the number of mentions received in the original dataset for the next snapshot. Read more on the original Twitter data in the ‘Temporal Walk Based Centrality Metric for Graph Streams’ paper.\n데이터정리\n\nT = 52081\nV = 트위터계정\n\nN = 1000 # number of nodes\nE = 119 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # passenger inflow\n시간에 따라서 N이 변하는지? ??\n시간에 따라서 E가 변하는지? True\nX: ?\ny: ?\n예제코드적용가능여부: No\n\n- Nodes : 1000\n\nvertices are Twitter accounts\n\n-Edges : 119\n\nedges are mentions between them\n\n- Time : 52081\n\nTwitter mention graphs related to major tennis tournaments from 2017\n\n\nfrom torch_geometric_temporal.dataset import TwitterTennisDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = TwitterTennisDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n119\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([1000, 16]), torch.Size([2, 89]), torch.Size([89]))\n\n\n\ndata[0][1].x[0]\n\ntensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\ndata[0][1].edge_index[0]\n\ntensor([ 42, 909, 909, 909, 233, 233, 450, 256, 256, 256, 256, 256, 434, 434,\n        434, 233, 233, 233, 233, 233, 233, 233,   9,   9, 355,  84,  84,  84,\n         84, 140, 140, 140, 140,   0, 140, 238, 238, 238, 649, 875, 875, 234,\n         73,  73, 341, 341, 341, 341, 341, 417, 293, 991,  74, 581, 282, 162,\n        144, 383, 383, 135, 135, 910, 910, 910, 910, 910,  87,  87,  87,  87,\n          9,   9, 934, 934, 162, 225,  42, 911, 911, 911, 911, 911, 911, 911,\n        911, 498, 498,  64, 435])\n\n\n\ndata[0][1].edge_attr\n\ntensor([2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 3., 2., 1., 1., 1., 1., 2., 2., 2., 1., 1., 1., 3.])\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(1000)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(119):\n    for j in range(40):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(1000, 2819)\n\n\n\nnx.draw(G,node_color='green',node_size=50,width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nlen(data[2][1].edge_index[0])\n\n67\n\n\n\nlen(data[0][1].edge_index[0])\n\n89\n\n\n다름..\n\n\nfrom torch_geometric_temporal.dataset import TwitterTennisDatasetLoader\nloader = TwitterTennisDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n tensor(4.8363))\n\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n tensor(4.9200))\n\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n tensor(6.5539))\n\n\n\n(data[3][1]).x[0],(data[3][1]).y[0]\n\n(tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n tensor(6.9651))\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([1000, 16])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 89])\n\n\n\n_edge_attr.shape\n\ntorch.Size([89])\n\n\n\n_y.shape\n\ntorch.Size([1000])\n\n\n\n_x.shape\n\ntorch.Size([1000, 16])\n\n\nx\n\n\n\ny\n\n\n\n\n_x[0:3]\n\ntensor([[0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n\n\n\n_y[0]\n\ntensor(4.8363)"
  },
  {
    "objectID": "posts/GCN/2022-12-21-st-gcn Dataset.html#mtmdatasetloader",
    "href": "posts/GCN/2022-12-21-st-gcn Dataset.html#mtmdatasetloader",
    "title": "PyTorch Geometric Temporal Dataset",
    "section": "MTMDatasetLoader",
    "text": "MTMDatasetLoader\nA dataset of Methods-Time Measurement-1 (MTM-1) motions, signalled as consecutive video frames of 21 3D hand keypoints, acquired via MediaPipe Hands from RGB-Video material. Vertices are the finger joints of the human hand and edges are the bones connecting them. The targets are manually labeled for each frame, according to one of the five MTM-1 motions (classes ): Grasp, Release, Move, Reach, Position plus a negative class for frames without graph signals (no hand present). This is a classification task where consecutive frames need to be assigned to the corresponding class . The data x is returned in shape (3, 21, T), the target is returned one-hot-encoded in shape (T, 6).\n데이터정리\n\nT = 14452\nV = 손의 shape에 대응하는 dot\n\nN = 325 # number of nodes\nE = 19 = N^2 # edges\n\\(f(v,t)\\)의 차원? (Grasp, Release, Move, Reach, Poision, -1)\n시간에 따라서 N이 변하는지? ??\n시간에 따라서 E가 변하는지? ??\nX: ?\ny: ?\n예제코드적용가능여부: No\n\n- Nodes : 325\n\nvertices are are the finger joints of the human hand\n\n-Edges : 19\n\nedges are the bones connecting them\n\n- Time : 14452\n\nfrom torch_geometric_temporal.dataset import MTMDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = MTMDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n14452\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([3, 21, 16]), torch.Size([2, 19]), torch.Size([19]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(21)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(14452):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(21, 19)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[12][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import MTMDatasetLoader\nloader = MTMDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n tensor([0., 0., 1., 0., 0., 0.]))\n\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n tensor([0., 0., 1., 0., 0., 0.]))\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([3, 21, 16])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 19])\n\n\n\n_edge_attr.shape\n\ntorch.Size([19])\n\n\n\n_y.shape\n\ntorch.Size([16, 6])\n\n\n\n_x.shape\n\ntorch.Size([3, 21, 16])\n\n\nx\n\nThe data x is returned in shape (3, 21, T),\n\ny\n\nThe targets are manually labeled for each frame, according to one of the five MTM-1 motions (classes ): Grasp, Release, Move, Reach, Position plus a negative class for frames without graph signals (no hand present).\nthe target is returned one-hot-encoded in shape (T, 6).\n\n\n_x[0]\n\ntensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n\n\n\n_y[0]\n\ntensor([0., 0., 1., 0., 0., 0.])"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html",
    "href": "posts/GCN/2023-01-05-GNAR.html",
    "title": "GNAR data",
    "section": "",
    "text": "GNAR"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#gnar-network-example",
    "href": "posts/GCN/2023-01-05-GNAR.html#gnar-network-example",
    "title": "GNAR data",
    "section": "2.3 GNAR network example",
    "text": "2.3 GNAR network example\n\nedge(list)\ndist(list)\n\n\n%%R\nplot(fiveNet, vertex.label = c(\"A\", \"B\", \"C\", \"D\", \"E\"))\n\n\n\n\n\n%%R\nsummary(\"fiveNet\")\n\n   Length     Class      Mode \n        1 character character \n\n\nother examples\n\nigraphtoGNAR or GNARtoigraph쓰는 예제\n\n\n%%R\nfiveNet2 <- GNARtoigraph(net = fiveNet)\nsummary(fiveNet2)\n\nIGRAPH eee756c U-W- 5 5 -- \n+ attr: weight (e/n)\n\n\n\n%%R\nfiveNet3 <- igraphtoGNAR(fiveNet2)\nall.equal(fiveNet, fiveNet3)\n\n[1] TRUE\n\n\n\n%%R\nprint(igraphtoGNAR(fiveNet2))\n\nGNARnet with 5 nodes \nedges:1--4 1--5 2--3 2--4 3--2 3--4 4--1 4--2 4--3 5--1 \n     \n edges of each of length  1 \n\n\nedge들 보고 싶을 떄\nwhereas the reverse conversion would be performed as\n\n%%R\ng <- make_ring(10)\nprint(igraphtoGNAR(g))\n\nGNARnet with 10 nodes \nedges:1--2 1--10 2--1 2--3 3--2 3--4 4--3 4--5 5--4 5--6 \n     6--5 6--7 7--6 7--8 8--7 8--9 9--8 9--10 10--1 10--9 \n     \n edges of each of length  1 \n\n\n\n%%R\nmake_ring(10)\n\nIGRAPH 9797a3f U--- 10 10 -- Ring graph\n+ attr: name (g/c), mutual (g/l), circular (g/l)\n+ edges from 9797a3f:\n [1] 1-- 2 2-- 3 3-- 4 4-- 5 5-- 6 6-- 7 7-- 8 8-- 9 9--10 1--10\n\n\n이어진 방향으로 각각의 edge를 만들어주는 게 igrapphtoGNAR이다\nGNARtoigraph function으로 높은 수준의 이웃 구조를 포함한 그래프를 추출할 수 있다.\n\nas.matrix or matrixtoGNAR로 인접 행렬 구할 수 있음\n\nwe can prosucean adjacency matrix for the fiveNet obeject with\n\n%%R\nas.matrix(fiveNet)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    1    1\n[2,]    0    0    1    1    0\n[3,]    0    1    0    1    0\n[4,]    1    1    1    0    0\n[5,]    1    0    0    0    0\n\n\nand an example converting a weighted adjacency matrix to a GNARnet object is\n\n%%R\nadj <- matrix(runif(9), ncol = 3, nrow = 3)\nadj[adj < 0.3] <- 0\nprint(matrixtoGNAR(adj))\n\nWARNING: diagonal entries present in original matrix, these will be removed\nGNARnet with 3 nodes \nedges:1--3 2--1 2--3 3--1 \n edges of unequal lengths"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#example-gnar-model-fitting",
    "href": "posts/GCN/2023-01-05-GNAR.html#example-gnar-model-fitting",
    "title": "GNAR data",
    "section": "2.4. Example: GNAR model fitting",
    "text": "2.4. Example: GNAR model fitting\n\nGNAR로 fit과 predict 가능\n\n\n%%R\ndata(\"fiveNode\")\nanswer <- GNARfit(vts = fiveVTS, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nanswer\n\nModel: \nGNAR(2,[1,1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\n dmatalpha1  dmatbeta1.1   dmatalpha2  dmatbeta2.1  \n    0.20624      0.50277      0.02124     -0.09523  \n\n\n\n\n파라메터 4개 가지고 있음\n\n\n%%R\nlayout(matrix(c(1, 2), 2, 1))\nplot(fiveVTS[, 1], ylab = \"Node A Time Series\")\nlines(fitted(answer)[, 1], col = 2)\nplot(fiveVTS[, 2], ylab = \"Node B Time Series\")\nlines(fitted(answer)[, 2], col = 2)\n\n\n\n\n\n%%R\nlayout(matrix(c(1, 2), 2, 1))\nplot(fiveVTS[, 3], ylab = \"Node C Time Series\")\nlines(fitted(answer)[, 3], col = 2)\nplot(fiveVTS[, 4], ylab = \"Node D Time Series\")\nlines(fitted(answer)[, 4], col = 2)\n\n\n\n\n\n각 노드의 time series(검정), fitted values from ‘answer’ model overlaid in red\n\n\n%%R\nmyresiduals <- residuals(answer)[, 1]\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(residuals(answer)[, 1]), ylab = \"`answer' model residuals\")\nhist(residuals(answer)[, 1], main = \"\", xlab = \"`answer' model residuals\")\n\n\n\n\n\n%%R\nmyresiduals <- residuals(answer)[, 2]\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(residuals(answer)[, 1]), ylab = \"`answer' model residuals\")\nhist(residuals(answer)[, 2], main = \"\", xlab = \"`answer' model residuals\")\n\n\n\n\n\n%%R\nmyresiduals <- residuals(answer)[, 3]\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(residuals(answer)[, 1]), ylab = \"`answer' model residuals\")\nhist(residuals(answer)[, 3], main = \"\", xlab = \"`answer' model residuals\")\n\n\n\n\n\n%%R\nmyresiduals <- residuals(answer)[, 4]\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(residuals(answer)[, 1]), ylab = \"`answer' model residuals\")\nhist(residuals(answer)[, 4], main = \"\", xlab = \"`answer' model residuals\")\n\n\n\n\n\nresidual plots from ‘answer’ model fit. Top: time sereies, Bottom: Histogram"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#example-gnar-data-simulation-on-a-given-network",
    "href": "posts/GCN/2023-01-05-GNAR.html#example-gnar-data-simulation-on-a-given-network",
    "title": "GNAR data",
    "section": "2.5. Example: GNAR data simulation on a given network",
    "text": "2.5. Example: GNAR data simulation on a given network\n\nfiveNet 네트워크를 사용하여 네트워크 시계열 시뮬레이션 진행\n두 시뮬레이션 모두 sigma argument를 사용하여 표준 편차가 제어되는 표준 정규 노이즈를 사용하여 생성된다.\n\n\n%%R\nset.seed(10)\nfiveVTS2 <- GNARsim(n = 200, net = fiveNet, alphaParams = list(c(0.4, 0, -0.6, 0, 0)), betaParams = list(c(0.3)))\n\n\nfiveVTS2 네트워크를 사용하여 시뮬레이션 된 것이다보니 파라메터 계수 비슷\n\n\n%%R\nprint(GNARfit(vts = fiveVTS2, net = fiveNet, alphaOrder = 1, betaOrder = 1, globalalpha = FALSE))\n\nModel: \nGNAR(1,[1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\ndmatalpha1node1  dmatalpha1node2  dmatalpha1node3  dmatalpha1node4  \n        0.45902          0.13133         -0.49166          0.03828  \ndmatalpha1node5      dmatbeta1.1  \n        0.02249          0.24848  \n\n\n\n\n%%R\nset.seed(10)\nfiveVTS3 <- GNARsim(n = 200, net = fiveNet, alphaParams = list(rep(0.2, 5), rep(0.3, 5)), betaParams = list(c(0.2, 0.3), c(0)))\nprint(GNARfit(vts = fiveVTS3, net = fiveNet, alphaOrder = 2, betaOrder = c(2,0)))\n\nModel: \nGNAR(2,[2,0]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\n dmatalpha1  dmatbeta1.1  dmatbeta1.2   dmatalpha2  \n     0.2537       0.1049       0.3146       0.2907  \n\n\n\n\n%%R\nfiveVTS4 <- simulate(GNARfit(vts = fiveVTS2, net = fiveNet, alphaOrder = 1, betaOrder = 1, globalalpha = FALSE), n = 200)\nprint(GNARfit(vts = fiveVTS4, net = fiveNet, alphaOrder = 1, betaOrder = 1, globalalpha = FALSE))\n\nModel: \nGNAR(1,[1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\ndmatalpha1node1  dmatalpha1node2  dmatalpha1node3  dmatalpha1node4  \n        0.48833          0.08260         -0.43978         -0.06986  \ndmatalpha1node5      dmatbeta1.1  \n       -0.05755          0.24028  \n\n\n\n\n위와 같이 GNAR 모델에 있는 시계열을 simulate하기 위해 GNARfit object에 대해 simulate S3 method 사용할 수 있다"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#missing-data-and-changing-connection-weights-with-gnar-models",
    "href": "posts/GCN/2023-01-05-GNAR.html#missing-data-and-changing-connection-weights-with-gnar-models",
    "title": "GNAR data",
    "section": "2.6 Missing data and changing connection weights with GNAR models",
    "text": "2.6 Missing data and changing connection weights with GNAR models\n\nThe flexibility of GNAR modelling이 의미하는 것은 연결 가중치를 바꾸지 않고 변하는 네트워크로 missing data 를 모델링 할 수 있다.\n한 노드가 missing data 구간이 생기면 그 구간에서만 네트워크를 변화하여 weight가 변경된다.\n\n\n%%R\nfiveVTS0 <- fiveVTS\nfiveVTS0[50:150, 3] <- NA\nnafit <- GNARfit(vts = fiveVTS0, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(fitted(nafit)[, 3]), ylab = \"Node C fitted values\")\nplot(ts(fitted(nafit)[, 4]), ylab = \"Node D fitted values\")\n\n\n\n\n\n%%R"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#stationary-conditions-for-a-gnar-process-with-fixed-network",
    "href": "posts/GCN/2023-01-05-GNAR.html#stationary-conditions-for-a-gnar-process-with-fixed-network",
    "title": "GNAR data",
    "section": "2.7. Stationary conditions for a GNAR process with fixed network",
    "text": "2.7. Stationary conditions for a GNAR process with fixed network\nTheorem 1\n\nGiven an unchanging network, \\(\\mathcal{G}\\) a sufficient condition for the GNAT model (1) to be stationary is\n\n\\[\\sum^p_{j=1}(|\\alpha_{i,j}| + \\sum^{C}_{c=1} \\sum^{s_j}_{r=1} |\\beta_{j,t,c}|)<1 , \\forall_i \\in 1,\\dots, N\\]\n위 조건을 GNARsim을 이용하여 확인 할 수 있다.\n\n%%R\nset.seed(10)\nfiveVTS4 <- GNARsim(n = 200, net = fiveNet, alphaParams = list(rep(0.2, 5)), betaParams = list(c(0.85)))\nc(mean(fiveVTS4[1:50, ]), mean(fiveVTS4[51:100, ]), mean(fiveVTS4[101:150, ]), mean(fiveVTS4[151:200, ]))\n\n[1]    -120.511   -1370.216  -15725.884 -180319.140\n\n\n\nThe mean increases rapidly indicating nonstationarity.\n평균이 빠르게 증가하는 것으로 보아 정상성을 띄고 있지 않다."
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#benefits-of-our-model-and-comparisons-to-others",
    "href": "posts/GCN/2023-01-05-GNAR.html#benefits-of-our-model-and-comparisons-to-others",
    "title": "GNAR data",
    "section": "2.8. Benefits of our model and comparisons to others",
    "text": "2.8. Benefits of our model and comparisons to others"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#order-selection",
    "href": "posts/GCN/2023-01-05-GNAR.html#order-selection",
    "title": "GNAR data",
    "section": "3.1. Order selection",
    "text": "3.1. Order selection\nBayesian information criterion\n\\[BIC(p,s) = ln|\\sum^{\\hat{}}_{p,s}| + T^{-1} M ln(T)\\]\n\n%%R\nBIC(GNARfit())\n\n[1] -0.003953124\n\n\n\n%%R\nBIC(GNARfit(betaOrder = c(2, 1)))\n\n[1] 0.02251406\n\n\nAkaike information criterion\n\\[AIC(p,s) = ln|\\sum^{\\hat{}}_{p,s}| + 2T^{-1} M\\]\n\n%%R\nAIC(GNARfit())\n\n[1] -0.06991947\n\n\n\n%%R\nAIC(GNARfit(betaOrder = c(2, 1)))\n\n[1] -0.05994387"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#model-selection-on-a-wind-network-time-series",
    "href": "posts/GCN/2023-01-05-GNAR.html#model-selection-on-a-wind-network-time-series",
    "title": "GNAR data",
    "section": "3.2. Model selection on a wind network time series",
    "text": "3.2. Model selection on a wind network time series\nthe data suite vswind that contains a number of R objects pertaining to 721 wind speeds taken at each of 102 weather stations in England and Wales. The suite contains the vector time series vswindts, the associated network vswindnet, a character vector of the weather station location names in vswindnames and coordinates of the stations in the two column matrix vswindcoords. The data originate from the UK Met Office site http://wow.metoffice.gov.uk and full details can be found in the vswind help file in the GNAR package.\n\nnodes : 102\ntime step : 721\n\n\n%%R\noldpar <- par(cex = 0.75)\nwindnetplot()\npar(oldpar)\n\n\n\n\nPlot of the wind speed network\n\nblue numbers are relative distance between sites\nlabels are the site name\n\n\n%%R\nBIC(GNARfit(vts = vswindts, net = vswindnet, alphaOrder = 1, betaOrder = 0))\n\n[1] -233.3848\n\n\n\n%%R\nBIC(GNARfit(vts = vswindts, net = vswindnet, alphaOrder = 1, betaOrder = 0, globalalpha = FALSE))\n\n[1] -233.1697\n\n\n\n%%R\nBIC.Alpha2.Beta <- matrix(0, ncol = 15, nrow = 15)\nfor(b1 in 0:14)\n    for(b2 in 0:14)\n        BIC.Alpha2.Beta[b1 + 1, b2 + 1] <- BIC(GNARfit(vts = vswindts,\n                    net = vswindnet, alphaOrder = 2, betaOrder = c(b1, b2)))\ncontour(0:14, 0:14, log(251 + BIC.Alpha2.Beta), xlab = \"Lag 1 Neighbour Order\", ylab = \"Lag 2 Neighbour Order\")\n\n\n\n\n\na set of GNAR(2,[b1,b2]) models with b1, b2 ranging from zero to 14\nContour plot of BIC values for the two-lag autoregressive model incorporating b1-stage and b2-stage neighbours at time lags one and two. Values shown are log(251 + BIC) to display clearer contours.\n\n이해 덜 됨..\n\nincreasing the lag two neighbour sets beyond first stage neighbours would appear to increase the BIC for those lag one neighbour stages greater than five\n\nchatGPT\n이 문장을 조금 더 자세히 설명하면, BIC(Bayesian Information Criterion)는 모델을 선택할 때 사용하는 지표로서, 우리가 선택한 모델이 얼마나 적합한지를 측정합니다. 이 문장에서는, 이웃 집합의 대기 시간이 증가할수록 BIC 값이 증가할 것이라고 언급하고 있습니다. 이는 우리가 선택한 모델이 적합하지 않을 가능성이 있다는 의미입니다. 그래프를 보고 있을 때, 수평 윤곽선은 BIC 값이 0인 스테이지를 의미합니다. 이는 우리가 선택한 모델이 완벽하게 적합한다는 의미입니다.\n\n%%R\ngoodmod <- GNARfit(vts = vswindts, net = vswindnet, alphaOrder = 2, betaOrder = c(5, 1))\ngoodmod\n\nModel: \nGNAR(2,[5,1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\n dmatalpha1  dmatbeta1.1  dmatbeta1.2  dmatbeta1.3  dmatbeta1.4  dmatbeta1.5  \n    0.56911      0.10932      0.03680      0.02332      0.02937      0.04709  \n dmatalpha2  dmatbeta2.1  \n    0.23424     -0.04872"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#constructing-a-network-to-aid-prediction",
    "href": "posts/GCN/2023-01-05-GNAR.html#constructing-a-network-to-aid-prediction",
    "title": "GNAR data",
    "section": "3.3. Constructing a network to aid prediction",
    "text": "3.3. Constructing a network to aid prediction\nWe propose a network construction method that uses prediction error, but note here that our scope is not to estimate an underlying network, but merely to find a structure that is useful in the task of prediction.\nwe use a prediction error measure, understood as the sum of squared differences between the observations and the estimates:\n\\[\\sum^N_{i=1} (X_{i,t} - \\hat{X}_{i,t})^2\\]\n\n%%R\nprediction <- predict(GNARfit(vts = fiveVTS[1:199,], net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1)))\nprediction\n\nTime Series:\nStart = 1 \nEnd = 1 \nFrequency = 1 \n    Series 1  Series 2  Series 3  Series 4   Series 5\n1 -0.6427718 0.2060671 0.2525534 0.1228404 -0.8231921"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#oecd-gdp-network-structure-aids-prediction",
    "href": "posts/GCN/2023-01-05-GNAR.html#oecd-gdp-network-structure-aids-prediction",
    "title": "GNAR data",
    "section": "4. OECD GDP: Network structure aids prediction",
    "text": "4. OECD GDP: Network structure aids prediction\nGOP growth rate time series\n\n35 countries from the OECD website\ntime series : 1961 - 2013\nT = 52\nNodes = 35\nIn this data set 20.8% (379 out of 1820) of the observations were missing due to some nodes not being included from the start.\nwe do not uese covariate information, so C=1\n\n\n%%R\nlibrary(\"fields\")\nlayout(matrix(c(1, 2), nrow = 1, ncol = 2), widths = c(4.5, 1))\nimage(t(apply(gdpVTS, 1, rev)), xaxt = \"n\", yaxt = \"n\", col = gray.colors(14), xlab = \"Year\", ylab = \"Country\")\naxis(side = 1, at = seq(from = 0, to = 1, length = 52), labels = FALSE, col.ticks = \"grey\")\naxis(side = 1, at = seq(from = 0, to = 1, length = 52)[5*(1:11)], labels = (1:52)[5*(1:11)])\naxis(side = 2, at = seq(from = 1, to = 0, length = 35), labels = colnames(gdpVTS), las = 1, cex = 0.8)\nlayout(matrix(1))\nimage.plot(zlim = range(gdpVTS, na.rm = TRUE), legend.only = TRUE, col = gray.colors(14))\n\n\n\n\nHeat plot(grey scale) of the differenced time series,\n\nwhite space indicates missing time series observations"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#finding-a-network-to-aid-prediction",
    "href": "posts/GCN/2023-01-05-GNAR.html#finding-a-network-to-aid-prediction",
    "title": "GNAR data",
    "section": "4.1. Finding a network to aid prediction",
    "text": "4.1. Finding a network to aid prediction\n\n%%R\nnet1 <- seedToNet(seed.no = seed.nos[1], nnodes = 35, graph.prob = 0.15)\nnet2 <- seedToNet(seed.no = seed.nos[2], nnodes = 35, graph.prob = 0.15)\n\n\n%%R\nlayout(matrix(c(2, 1), 1, 2))\npar(mar=c(0,1,0,1))\nplot(net1, vertex.label = colnames(gdpVTS), vertex.size = 0)\nplot(net2, vertex.label = colnames(gdpVTS), vertex.size = 0)\n\n\n\n\n\nErdos-Renyi random graphs xonstructed from the first two elements of the seed.nos variable with 35 nodes and connection probability 0.15.\n자기회귀 모델인 GNAR 모델을 예측에 사용할 때, 어떤 네트워크가 가장 적합한지 조사해야 함.\n이때 각 노드의 자기 상관 함수를 이용한 초기 분석 결과, 2차 자기회귀 구성 요소가 충분할 것으로 예상되어 p = 2까지의 GNAR 모델을 시험함.\n각 시간 지연에서 최대 2개의 이웃 집합을 포함함.\n이에 따라 아래와 같은 GNAR 모델이 시험됨.\n\nGNAR(1, [0]), GNAR(1, [1]), GNAR(2, [0, 0]), GNAR(2, [1, 0]), GNAR(2, [1, 1]), GNAR(2, [2, 0]), GNAR(2, [2, 1]), 그리고 GNAR(2, [2, 2])가 시험되며, 각각 individual-\\(\\alpha\\)와 global-\\(\\alpha\\) GNAR 모델로 적합함.\n총 16개의 모델이 생성됨.\n이 중에서 전체 GDP 예측에 사용할 GNAR 모델을 선택할 것.\n연결 확률이 0.15인 10,000개의 임의의 양방향 네트워크를 생성하고, 위에서 언급한 GNAR 모델을 이용해 예측할 것.\n그래서 이 예제는 상당한 계산 시간이 필요(데스크탑 PC에서 약 90분).\n이를 위해 아래 코드에는 일부 분석만 포함.\n계산 상의 이유로, 우선 각 노드에서 표준 편차로 나눠서 잔차가 각 노드에서 동일한 분산을 가지게 함.\nseedSim 함수는 예측값과 원래 값의 제곱 차이의 합을 출력하고, 이를 예측 정확도의 측정 기준으로 사용\n\n\n\n%%R\ngdpVTSn <- apply(gdpVTS, 2, function(x){x / sd(x[1:50], na.rm = TRUE)})\nalphas <- c(rep(1, 2), rep(2, 6))\nbetas <- list(c(0), c(1), c(0, 0), c(1, 0), c(1, 1), c(2, 0), c(2, 1), c(2, 2))\nseedSim <- function(seedNo, modelNo, globalalpha){\n    net1 <- seedToNet(seed.no = seedNo, nnodes = 35, graph.prob = 0.15)\n    gdpPred <- predict(GNARfit(vts = gdpVTSn[1:50, ], net = net1,\n                               alphaOrder = alphas[modelNo], betaOrder = betas[[modelNo]],\n                               globalalpha = globalalpha))\n    return(sum((gdpPred - gdpVTSn[51, ])^2))\n    }\n\n\n%%R\nseedSim(seedNo = seed.nos[1], modelNo = 1, globalalpha = TRUE)\n\n[1] 23.36913\n\n\n\n%%R\nseedSim(seed.nos[1], modelNo = 3, globalalpha = TRUE)\n\n[1] 11.50739\n\n\n\n%%R\nseedSim(seed.nos[1], modelNo = 3, globalalpha = FALSE)\n\n[1] 18.96766\n\n\n\n\n\nimage.png\n\n\n\n10,000개의 임의의 네트워크와 16개의 모델로부터 시뮬레이션한 예측 오류의 박스 그래프\n(계산 시간이 길어(90분) 코드는 생략).\n일반적으로 global-α 모델은 더 낮은 예측 오류를 일으킴.\n그래서 이 버전의 GNAR 모델을 사용할 것.\n그림 9에서 첫 번째 모델인 GNAR(1, [0])과 세 번째 모델인 GNAR(2, [0, 0])의 경우, “박스 그래프”는 인접한 매개변수가 적합되지 않아 결과가 전부 동일해 짧은 수평선으로 표시됨.\n다른 global-α 모델들은 이 안에 포함되어 있기 때문에, global-α GNAR(2, [2, 2])의 예측 오류가 최소가 되는 임의의 그래프를 선택할 것.\n이는 seed.nos[921]에서 생성된 네트워크가 선택되게 됩니다.\n\n\n%%R\nnet921 <- seedToNet(seed.no = seed.nos[921], nnodes = 35, graph.prob = 0.15)\nlayout(matrix(c(1), 1, 1))\nplot(net921, vertex.label = colnames(gdpVTS), vertex.size = 0)\n\n\n\n\nRandomly generated un-weighted and un-directed graph over the OECD ountries that minimises the prediction error at t = 51 using GNAR(2, [2, 2]).\n\nseed.nos[921]에서 생성된 네트워크\n네트워크에는 전부 2개 이상의 이웃을 가지고 있는 countries들이 있고, 총 97개의 edges이 있음.\n이 “921” 네트워크는 GDP 예측을 위해 생성되었기 때문에, 찾은 네트워크에 인식 가능한 구조가 있지 않을 것이라고 예상할 수 있음\n그러나 미국, 멕시코, 캐나다는 각각 8개, 8개, 6개의 edge을 가지고 있어 매우 잘 연결되어 있음.\n스웨덴과 칠레도 잘 연결되어 있으며, 각각 8개와 7개의 edge을 가지고 있습니다.\n예측 성능이 유사한 적은 개수의 edge를 가진 네트워크를 찾기 위해 테스트 될 수 있지만, 여기서는 전체 선택된 네트워크를 그대로 사용.\n이 네트워크를 사용하면 BIC를 이용해 최적의 GNAR 순서를 선택할 수 있음.\n\n\n%%R\nres <- rep(NA, 8)\nfor(i in 1:8){\n    res[i] <- BIC(GNARfit(gdpVTSn[1:50, ],\n                          net = seedToNet(seed.nos[921], nnodes = 35, graph.prob = 0.15),\n                          alphaOrder = alphas[i], betaOrder = betas[[i]]))}\norder(res)\n\n[1] 6 3 4 7 8 5 1 2\n\n\n\n%%R\nsort(res)\n\n[1] -64.44811 -64.32155 -64.18751 -64.12683 -64.09656 -63.86919 -60.67858\n[8] -60.54207"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#results-and-comparisons",
    "href": "posts/GCN/2023-01-05-GNAR.html#results-and-comparisons",
    "title": "GNAR data",
    "section": "4.2. Results and comparisons",
    "text": "4.2. Results and comparisons\n\n이전 섹션의 모델을 사용해 t=52일 때의 값을 예측\n이 예측 오류를 표준 AR과 VAR 모델을 사용해 찾은 예측 오류와 비교\nGNAR 예측은 선택된 네트워크(seed.nos[921]에 해당)를 가진 GNAR(2, [2, 0]) 모델을 t=51까지의 데이터에 적합시키고, t=52일 때의 값을 예측\n우선 series를 정규화한 다음, 모델 적합으로부터 SSE를 계산합니다.\n\n\n%%R\ngdpVTSn2 <- apply(gdpVTS, 2, function(x){x / sd(x[1:51], na.rm = TRUE)})\ngdpFit <- GNARfit(gdpVTSn2[1:51,], net = net921, alphaOrder = 2, betaOrder = c(2, 0))\nsummary(gdpFit)\n\n\nCall:\nlm(formula = yvec2 ~ dmat2 + 0)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4806 -0.5491 -0.0121  0.5013  3.1208 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \ndmat2alpha1  -0.41693    0.03154 -13.221  < 2e-16 ***\ndmat2beta1.1 -0.12662    0.05464  -2.317   0.0206 *  \ndmat2beta1.2  0.28044    0.06233   4.500  7.4e-06 ***\ndmat2alpha2  -0.33282    0.02548 -13.064  < 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.8926 on 1332 degrees of freedom\n  (23 observations deleted due to missingness)\nMultiple R-squared:  0.1859,    Adjusted R-squared:  0.1834 \nF-statistic: 76.02 on 4 and 1332 DF,  p-value: < 2.2e-16\n\nGNAR BIC: -62.86003\n\n\n\n%%R\nsum((predict(gdpFit) - gdpVTSn2[52, ])^2)\n\n[1] 5.737203\n\n\n이 GNAR 모델의 적합된 매개변수는 \\(\\alpha^1 = - 0.42, \\beta^1,1 = - 0.13, \\beta^1,2 = 0.28\\), 그리고 \\(\\alpha^2 = - 0.33\\)입니다.\n\n\n\nModel\nparameters\nprediction error\n\n\n\n\nGNAR(2,[2,0])\n4\n5.7\n\n\nIndividual AR(2)\n38\n8.1\n\n\nVAR(1)\n199\n26.2\n\n\n\nEstimated prediction error of differenced real GDP change at t = 52 for all 35 countries.\n우리의 방법과 CRAN forecast 패키지의 버전 8.0에서의 forecast.ar()과 auto.arima() 함수를 사용해 각 노드별로 AR 모델을 적합한 결과를 비교\n\n섹션 4.1의 자기상관 분석을 고려해 각각 35개의 개별 모델의 최대 AR 순서를 p=2로 설정\n\n\n%%R\nlibrary(\"forecast\")\narforecast <- apply(gdpVTSn2[1:51, ], 2, function(x){\n            forecast(auto.arima(x[!is.na(x)], d= ,D=0,max.p = 2,max.q=0,\n                                max.P=0,max.Q = 0,stationary = TRUE, seasonal = FALSE), ic = \"bic\",\n                     allowmean = FALSE, allowdraft = FALSE, trace = FALSE, h=1)$mean\n})\nsum((arforecast - gdpVTSn2[52, ])^2)\n\n[1] 7.8974\n\n\nWe fit the model using the VAR function and then use the restrict function to reduce dimensionality further, by setting to zero any coefficient whose associated absolute t-statistic value is less than two.\n\n%%R\nlibrary(\"vars\")\ngdpVTSn2.0 <- gdpVTSn2\ngdpVTSn2.0[is.na(gdpVTSn2.0)] <- 0\nvarforecast <- predict(restrict(VAR(gdpVTSn2.0[1:51, ], p = 1, type = \"none\")), n.ahead = 1)\n\ncompute the prediction error\n\n%%R\ngetfcst <- function(x){return(x[1])}\nvarforecastpt <- unlist(lapply(varforecast$fcst, getfcst))\nsum((varforecastpt - gdpVTSn2.0[52, ])^2)\n\n[1] 26.19805\n\n\nGNAR 모델은 AR과 VAR 결과보다 적은 예측 오류를 제공합니다. 이는 AR과 비교했을 때 29%가 줄어들고, VAR과 비교했을 때 78%가 줄어듭니다.\n위 절차를 반복해 2단계 앞으로의 예측을 기반으로 분석을 수행합니다.\n이 경우 다른 네트워크가 GNAR(2,[2,2]) 모델의 예측 오류를 최소화합니다.\n그러나 BIC 단계에서 GNAR(2,[0,0]) 모델이 최적으로 적합된 것을 식별하였고, 이는 네트워크 회귀 매개변수를 포함하지 않는 모델입니다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": ":)",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2023\n\n\nGCN Algorithm Example 2\n\n\nSEOYEON CHOI\n\n\n\n\nJan 11, 2023\n\n\nGCN Algorithm Example 1\n\n\nSEOYEON CHOI\n\n\n\n\nJan 5, 2023\n\n\nGNAR data\n\n\nSEOYEON CHOI\n\n\n\n\nJan 2, 2023\n\n\nquarto blog tips\n\n\nSEOYEON CHOI\n\n\n\n\nDec 31, 2022\n\n\nStudy for Spaces\n\n\nSEOYEON CHOI\n\n\n\n\nDec 28, 2022\n\n\nSimulation of geometric-temporal\n\n\nSEOYEON CHOI\n\n\n\n\nDec 27, 2022\n\n\nDiscrete Fourier Transform\n\n\nSEOYEON CHOI\n\n\n\n\nDec 21, 2022\n\n\nPyTorch Geometric Temporal Dataset\n\n\nSEOYEON CHOI\n\n\n\n\nDec 5, 2022\n\n\nTORCH_GEOMETRIC.NN\n\n\nSEOYEON CHOI\n\n\n\n\nDec 5, 2022\n\n\nGCN\n\n\nSEOYEON CHOI\n\n\n\n\nDec 1, 2022\n\n\nStudy\n\n\nSEOYEON CHOI\n\n\n\n\nDec 1, 2022\n\n\nGraph code\n\n\nSEOYEON CHOI\n\n\n\n\nNov 24, 2022\n\n\nEarthquake\n\n\nSEOYEON CHOI\n\n\n\n\nNov 24, 2022\n\n\nClass code for Comparison Study\n\n\nSEOYEON CHOI\n\n\n\n\nSep 2, 2022\n\n\nSimulation\n\n\nSEOYEON CHOI\n\n\n\n\nSep 1, 2022\n\n\nGODE\n\n\nSEOYEON CHOI\n\n\n\n\nSep 1, 2022\n\n\nGuarto tip\n\n\nSEOYEON CHOI\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, there ;)"
  }
]