[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": ":)",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\nSimulation Tables\n\n\nSEOYEON CHOI\n\n\n\n\nMay 15, 2023\n\n\nSTGCN on Keras\n\n\nSEOYEON CHOI\n\n\n\n\nMay 11, 2023\n\n\nPyG Geometric Temporal CPU vs GPU\n\n\nSEOYEON CHOI\n\n\n\n\nMay 11, 2023\n\n\nPyG Geometric Temporal Examples\n\n\nSEOYEON CHOI\n\n\n\n\nMay 10, 2023\n\n\nEbayesThresh: R Programs for Empirical Bayes Thresholding, Review\n\n\nSEOYEON CHOI\n\n\n\n\nMay 6, 2023\n\n\nITSTGCN Article Refernece\n\n\nSEOYEON CHOI\n\n\n\n\nMay 6, 2023\n\n\nITSTGCN DCRNN\n\n\nSEOYEON CHOI\n\n\n\n\nMay 5, 2023\n\n\nITSTGCN LOADERS Version\n\n\nSEOYEON CHOI\n\n\n\n\nMay 4, 2023\n\n\nQuestions of PyTorch Geometric Temporal\n\n\nSEOYEON CHOI\n\n\n\n\nApr 29, 2023\n\n\nPadalme GSO_st\n\n\nSEOYEON CHOI\n\n\n\n\nApr 27, 2023\n\n\nToy Example Note\n\n\nGUEBIN CHOI\n\n\n\n\nApr 27, 2023\n\n\nToy Example Figure(Intro)\n\n\nSEOYEON CHOI\n\n\n\n\nApr 25, 2023\n\n\nNote_weight amatrix\n\n\nGUEBIN CHOI\n\n\n\n\nApr 10, 2023\n\n\nGCRN Review\n\n\nSEOYEON CHOI\n\n\n\n\nApr 10, 2023\n\n\nSTGCN Existing Method Review\n\n\nSEOYEON CHOI\n\n\n\n\nApr 10, 2023\n\n\nLebesgue’s decomposition theorem\n\n\nSEOYEON CHOI\n\n\n\n\nApr 10, 2023\n\n\nSelf-onsistent estimator\n\n\nSEOYEON CHOI\n\n\n\n\nApr 8, 2023\n\n\nITSTGCN\n\n\nSEOYEON CHOI\n\n\n\n\nApr 6, 2023\n\n\nMETRLADatasetLoader-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nApr 5, 2023\n\n\nSimulation\n\n\nSEOYEON CHOI\n\n\n\n\nApr 5, 2023\n\n\nSimulation\n\n\nSEOYEON CHOI\n\n\n\n\nApr 4, 2023\n\n\nNomalized Graph Laplacian\n\n\nSEOYEON CHOI\n\n\n\n\nMar 22, 2023\n\n\nSimualtionPlanner-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 20, 2023\n\n\ndata load, data save as pickle\n\n\nSEOYEON CHOI\n\n\n\n\nMar 18, 2023\n\n\nSimualtionPlanner-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 17, 2023\n\n\nITSTGCN-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 14, 2023\n\n\nSTGCN papers review\n\n\nSEOYEON CHOI\n\n\n\n\nMar 3, 2023\n\n\nSY 1st ITSTGCN\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 15, 2023\n\n\n2nd ITSTGCN\n\n\nGUEBIN CHOI\n\n\n\n\nFeb 15, 2023\n\n\n1st ITSTGCN\n\n\nGUEBIN CHOI\n\n\n\n\nFeb 7, 2023\n\n\nClass of Method(WikiMath) lag 1\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 6, 2023\n\n\nClass of Method(GNAR) lag 1 80% Missing repeat\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 6, 2023\n\n\nClass of Method(GNAR) lag 2\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 5, 2023\n\n\nChap 12.2 ~ 3: Power Spectral Density and its Estimators\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 3, 2023\n\n\nChap 12.2: Weakly Stationary Graph Processes\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 1, 2023\n\n\nChap 8.3: Discrete Fourier Transform\n\n\nSEOYEON CHOI\n\n\n\n\nJan 28, 2023\n\n\nClass of Method(GNAR) lag 1\n\n\nSEOYEON CHOI\n\n\n\n\nJan 28, 2023\n\n\nClass of Method(WikiMath) lag 4\n\n\nSEOYEON CHOI\n\n\n\n\nJan 26, 2023\n\n\nClass of Method\n\n\nGuebin Choi\n\n\n\n\nJan 21, 2023\n\n\nClass of Method\n\n\nSEOYEON CHOI\n\n\n\n\nJan 20, 2023\n\n\n1st ST-GCN Example dividing train and test\n\n\nSEOYEON CHOI\n\n\n\n\nJan 17, 2023\n\n\n2nd ST-GCN Example dividing train and test\n\n\nSEOYEON CHOI\n\n\n\n\nJan 11, 2023\n\n\nGCN Algorithm Example 1\n\n\nSEOYEON CHOI\n\n\n\n\nJan 5, 2023\n\n\nGNAR data\n\n\nSEOYEON CHOI\n\n\n\n\nJan 2, 2023\n\n\nquarto blog tips\n\n\nSEOYEON CHOI\n\n\n\n\nDec 31, 2022\n\n\nStudy for Spaces\n\n\nSEOYEON CHOI\n\n\n\n\nDec 29, 2022\n\n\n[IT-STGCN] STGCN 튜토리얼\n\n\n신록예찬, SEOYEON CHOI\n\n\n\n\nDec 28, 2022\n\n\nSimulation of geometric-temporal\n\n\nSEOYEON CHOI\n\n\n\n\nDec 27, 2022\n\n\nDiscrete Fourier Transform\n\n\nSEOYEON CHOI\n\n\n\n\nDec 21, 2022\n\n\nPyTorch ST-GCN Dataset\n\n\nSEOYEON CHOI\n\n\n\n\nDec 5, 2022\n\n\nGCN\n\n\nSEOYEON CHOI\n\n\n\n\nDec 5, 2022\n\n\nTORCH_GEOMETRIC.NN\n\n\nSEOYEON CHOI\n\n\n\n\nDec 1, 2022\n\n\nStudy\n\n\nSEOYEON CHOI\n\n\n\n\nDec 1, 2022\n\n\nGraph code\n\n\nSEOYEON CHOI\n\n\n\n\nNov 24, 2022\n\n\nClass code for Comparison Study\n\n\nSEOYEON CHOI\n\n\n\n\nNov 24, 2022\n\n\nEarthquake\n\n\nSEOYEON CHOI\n\n\n\n\nSep 2, 2022\n\n\nSimulation\n\n\nSEOYEON CHOI\n\n\n\n\nSep 1, 2022\n\n\nQuarto tip\n\n\nSEOYEON CHOI\n\n\n\n\nSep 1, 2022\n\n\nGODE\n\n\nSEOYEON CHOI\n\n\n\n\nApr 4, 2022\n\n\nIntroduction to Python 5wk\n\n\nSEOYEON CHOI\n\n\n\n\nMar 23, 2022\n\n\nIntroduction to Python 4wk\n\n\nSEOYEON CHOI\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Study/index.html",
    "href": "posts/Study/index.html",
    "title": "Study",
    "section": "",
    "text": "About Study"
  },
  {
    "objectID": "posts/Study/2023-04-10-GCRN_riview.html",
    "href": "posts/Study/2023-04-10-GCRN_riview.html",
    "title": "GCRN Review",
    "section": "",
    "text": "GCRN\nChat-GPT 참조\n논문 proposed method 에 추가\n사용한 기존 알고리즘(STGCN에 존재) 소개"
  },
  {
    "objectID": "posts/Study/2023-04-10-GCRN_riview.html#lstm",
    "href": "posts/Study/2023-04-10-GCRN_riview.html#lstm",
    "title": "GCRN Review",
    "section": "LSTM",
    "text": "LSTM\nGCRN에서 LSTM은 시계열 데이터와 그래프 데이터를 동시에 모델링하기 위해 사용됩니다. GCRN의 LSTM 레이어는 이전 타임스텝의 LSTM hidden state를 현재 타임스텝의 입력 그래프의 정보와 함께 사용하여 다음 hidden state를 생성합니다.\n구체적으로, GCRN의 LSTM 레이어에서는 입력 그래프의 adjacency matrix \\(A\\)를 이용하여 Graph Convolutional LSTM (GC-LSTM)을 구성합니다. GC-LSTM은 LSTM 셀과 유사한 구조를 가지면서도 입력과 출력에 대해 그래프 합성곱 연산을 수행합니다. 이를 통해 LSTM은 그래프 구조를 고려한 시계열 데이터 모델링이 가능해집니다.\n또한, GCRN에서는 GC-LSTM 레이어를 여러 층으로 쌓아서 더 복잡한 그래프와 시계열 데이터의 모델링이 가능하도록 합니다. 이러한 구조는 GCRN이 다양한 그래프 및 시계열 데이터에 적용 가능하도록 합니다."
  },
  {
    "objectID": "posts/Study/2023-04-10-GCRN_riview.html#chebyshev-다항식",
    "href": "posts/Study/2023-04-10-GCRN_riview.html#chebyshev-다항식",
    "title": "GCRN Review",
    "section": "Chebyshev 다항식",
    "text": "Chebyshev 다항식\nChebyshev 다항식을 이용한 그래프 합성곱 신경망은 GCRN 모델에서 사용되는 그래프 합성곱 신경망 레이어에서 적용됩니다. 이 방법은 그래프 신경망에서 효율적인 연산을 가능하게 하여 복잡한 그래프에서도 빠른 학습이 가능하도록 합니다.\nChebyshev 다항식은 그래프 신경망에서 Laplacian 행렬을 특정 다항식으로 근사하는 방법 중 하나입니다. Laplacian 행렬은 그래프에서 노드 간의 연결 관계를 나타내는 행렬이며, 이를 다항식으로 근사함으로써 그래프 합성곱 연산을 수행합니다.\n논문에서는 Chebyshev 다항식을 이용한 그래프 합성곱 신경망의 계산식을 다음과 같이 정의합니다.\n\\(H_{l+1} = \\sigma\\left(\\sum_{i=0}^{K-1}\\Theta_{i}^{(l)}T_{i}(L)H^{(l)}\\right)\\)\n여기서 \\(H_{l}\\)은 \\(l\\)번째 레이어에서의 입력 데이터, \\(H_{l+1}\\)은 \\(l+1\\)번째 레이어에서의 출력 데이터, \\(\\Theta_{i}^{(l)}\\)은 \\(i\\)번째 Chebyshev 다항식에 대한 가중치, \\(T_{i}(L)\\)은 Laplacian 행렬 \\(L\\)에 대한 \\(i\\)번째 Chebyshev 다항식, \\(\\sigma\\)는 활성화 함수입니다.\n이 식에서 \\(T_{i}(L)\\)은 재귀적인 방법으로 계산됩니다. 이를 위해서는 Laplacian 행렬 \\(L\\)에 대한 최대 \\(K\\)차 Chebyshev 다항식 \\(T_{k}(L)\\)이 필요합니다. 이 다항식은 다음과 같이 재귀적으로 정의됩니다.\n\\(T_{0}(L) = I\\), \\(T_{1}(L) = L\\), \\(T_{k}(L) = 2LT_{k-1}(L)-T_{k-2}(L)\\)\n여기서 \\(I\\)는 단위 행렬입니다. 따라서, \\(T_{i}(L)\\)을 계산하기 위해서는 \\(L\\)에 대한 \\(T_{i-1}(L)\\)과 \\(T_{i-2}(L)\\)이 필요합니다. 이를 이용하여 Chebyshev 다항식을 이용한 그래프 합성곱 신경망을 효율적으로 계산할 수 있습니다.\n\n체비셰브 예시\nChebyshev 다항식은 다음과 같은 식으로 정의됩니다.\n\\(T_{k}(x) = \\cos(k\\cos^{-1}(x)), \\quad x \\in [-1, 1]\\)\n따라서 예를 들어, \\(T_{2}(x)\\)를 구하고자 한다면 다음과 같은 계산을 수행할 수 있습니다.\n\\(T_{2}(x) = \\cos(2\\cos^{-1}(x))\\)\n우선 \\(\\cos^{-1}(x)\\)를 구합니다.\n\\(\\cos^{-1}(x) = \\theta \\Rightarrow \\cos(\\theta) = x\\)\n그리고 나서 \\(2\\theta\\)를 구합니다.\n\\(2\\theta = 2\\cos^{-1}(x)\\)\n마지막으로, \\(\\cos(2\\theta)\\)를 계산합니다.\n\\(\\cos(2\\theta) = 2\\cos^{2}(\\theta) - 1\\)\n따라서,\n\\(T_{2}(x) = \\cos(2\\cos^{-1}(x)) = 2x^{2}-1\\)\n이렇게 구한 \\(T_{2}(x)\\)는 Laplacian 행렬 \\(L\\)에 대한 2차 Chebyshev 다항식 \\(T_{2}(L)\\)의 계수로 사용될 수 있습니다.\nGConvGRU를 사용하였으며, Structured Sequence Modeling with Graph Convolutional Recurrent Networks 논문에서 제안된 모델\n이 논문은 그래프 합성곱 신경망 (Graph Convolutional Neural Network, GCN)을 이용한 시퀀스 모델링을 위해 제안된 구조인 그래프 합성곱 순환 신경망 (Graph Convolutional Recurrent Neural Network, GCRN)에 대한 연구입니다.\nGCRN은 시계열 데이터를 처리하기 위해 순환 신경망 (Recurrent Neural Network, RNN)과 같은 장기 의존성(long-term dependency)을 고려하는 방법으로 그래프 합성곱 신경망(GCN)을 확장합니다. 이를 위해 GCRN은 각 시간 스텝에서 그래프 상의 노드 간 상호 작용을 잘 나타낼 수 있는 새로운 그래프 합성곱 셀(GCN cell)을 도입합니다. 이 셀은 체비셰폴리노미알 필터 (Chebyshev Polynomial Filter)를 사용하여 그래프 신호의 재귀적인 확장을 수행하며, 게이트 메카니즘(Gate Mechanism)을 통해 순환 구조를 적용합니다.\n논문에서 실험은 여러 시퀀스 모델링 문제에서 GCRN의 성능을 검증합니다. 실험 결과 GCRN은 기존의 RNN과 GCN을 비롯한 다른 모델들과 비교하여 높은 예측 정확도를 보입니다. 이를 통해 GCRN이 그래프 시계열 데이터 분석 분야에서 유용하게 사용될 수 있다는 가능성을 제시합니다.\nChebyshev polynomial filter는 Laplacian 행렬과 함께 사용하여 그래프 신경망에서 필터링을 수행하는 데 사용됩니다. 이 필터는 임의의 그래프에 대해 일반적으로 적용될 수 있으며, 이전의 고유 벡터와 함께 Laplacian의 K차 필터링을 수행하는 데 사용됩니다. 이 때 K는 Chebyshev 다항식의 차수입니다.\nGCRN의 은닉 레이어는 입력 데이터를 처리하여 모델이 패턴을 인식하고 기억하는 데 필요한 정보를 추출하는 레이어입니다. 이 레이어에서는 게이트 메카니즘이 사용됩니다. 각 노드마다 하나의 게이트 메카니즘이 존재하며, 이를 통해 입력 데이터의 중요성을 동적으로 조절하면서 학습 가능한 가중치를 유지합니다.\n수식으로 표현하면, 은닉 레이어의 출력값 H는 다음과 같이 계산됩니다.\n\\(H = (1 - G) * f(XWx) + G * H_prev\\)\n여기서 X는 입력 데이터를, Wx는 입력 데이터와 연결된 가중치 매트릭스를 나타냅니다. f는 활성화 함수를 나타내며, 일반적으로 ReLU나 시그모이드 함수 등이 사용됩니다. G는 게이트 메카니즘을 나타내며, 이는 입력 데이터의 중요성을 동적으로 조절하는 가중치 매트릭스입니다. H_prev는 이전 시점(t-1)의 은닉 레이어의 출력값을 나타냅니다.\n게이트 메카니즘의 가중치 매트릭스 G는 다음과 같이 계산됩니다.\n\\(G = σ(XWg)\\)\n여기서 σ는 시그모이드 함수를 나타내며, X는 입력 데이터를, Wg는 게이트 메카니즘과 연결된 가중치 매트릭스를 나타냅니다.\n이를 통해 입력 데이터의 중요성을 조절하면서, 이전 시점의 은닉 레이어 출력값과 현재 입력 데이터를 결합하여 새로운 정보를 추출하는 것이 은닉 레이어의 역할입니다. 이를 통해 GCRN은 패턴을 인식하고 기억하는 능력을 갖게 되며, 이를 이용하여 다양한 응용 분야에서 활용될 수 있습니다.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\n# 입력 데이터의 차원\ninput_size = 10\n\n\n# 은닉 레이어의 차원\nhidden_size = 20\n\n\n# GCRN 모델 정의\nclass GCRN(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(GCRN, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.fc_x = nn.Linear(input_size, hidden_size)\n        self.fc_h_prev = nn.Linear(hidden_size, hidden_size)\n        self.fc_g = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x, h_prev):\n        xw = self.fc_x(x)\n        hw_prev = self.fc_h_prev(h_prev)\n        g = self.sigmoid(self.fc_g(x))\n        h = (1 - g) * self.relu(xw) + g * hw_prev\n        return h\n\n\n# 입력 데이터와 이전 시점의 은닉 레이어 출력값 정의\nx = torch.randn(1, input_size);x\n\ntensor([[ 0.3523, -1.2066,  1.0772, -0.4210,  0.0966, -0.0625,  1.2245, -1.5016,\n          0.9676,  0.7957]])\n\n\n\nh_prev = torch.randn(1, hidden_size);h_prev\n\ntensor([[-1.1932, -0.5705,  0.5479, -0.1685, -0.8700,  0.9897, -0.3543,  1.1368,\n          0.2002, -2.0535,  0.2089,  0.0197, -1.6354,  0.4046,  1.2525,  0.0061,\n          1.1829, -0.9437, -0.4646, -0.4378]])\n\n\n\n# GCRN 모델 생성 및 실행\nmodel = GCRN(input_size, hidden_size)\n\n\nh = model(x, h_prev)\n\n\nprint(h)\n\ntensor([[ 0.0327,  0.2234,  0.2752, -0.0619,  0.3870,  0.1196,  0.1651,  0.1339,\n          0.5128,  0.2298,  0.3821, -0.2497,  0.0820,  0.1898,  0.3247,  0.0905,\n          0.5839, -0.0843,  0.2252,  0.1654]], grad_fn=<AddBackward0>)"
  },
  {
    "objectID": "posts/Study/2023-04-10-STGCN_Existing_Method_Review.html",
    "href": "posts/Study/2023-04-10-STGCN_Existing_Method_Review.html",
    "title": "STGCN Existing Method Review",
    "section": "",
    "text": "Existing Method Review\n\nRELU의 핵심; 그레이언트 소실 문제 해결\n\n노드를 임베딩한다는 것은, 각 노드를 고정된 크기의 벡터로 변환하는 것을 말합니다. 이러한 노드 임베딩은 각 노드의 특성을 벡터 공간에서 나타내기 때문에, 노드 간의 유사성이나 관계를 계산하기 쉬워집니다. 노드 임베딩을 사용하면, 예를 들어 노드 간의 유사성을 측정하거나, 그래프 분류 및 예측 문제를 해결할 수 있습니다.\n\n\n1. GCN\nGCN(Graph Convolutional Network): 그래프를 표현하는 인접 행렬을 이용하여 노드 간 상호작용을 모델링하는 신경망 모델입니다.\n\n그래프 표현\n\n그래프 데이터를 인접 행렬(adjacency matrix)로 표현합니다.\n인접 행렬은 노드들 간의 연결 정보를 나타내며, 행렬의 (i, j) 요소는 노드 i와 노드 j가 연결되어 있으면 1, 아니면 0으로 표현됩니다.\n그래프 데이터를 인접 행렬(adjacency matrix) A로 표현합니다.\nA는 n x n 크기의 행렬이며, A[i,j]는 노드 i와 노드 j가 연결되어 있으면 1, 아니면 0으로 표현됩니다.\n\nGCN 레이어 구성\n\n입력으로 인접 행렬과 노드 특성 행렬(feature matrix)을 받습니다.\n노드 특성 행렬은 각 노드의 특성을 행렬로 나타낸 것입니다.\nGCN 레이어는 인접 행렬과 노드 특성 행렬을 이용하여 노드 임베딩(node embedding)을 학습합니다.\n노드 임베딩은 각 노드의 특성을 저차원 벡터로 나타낸 것입니다.\n입력으로 인접 행렬 A와 노드 특성 행렬 X을 받습니다.\n노드 특성 행렬 X는 n x d 크기의 행렬이며, X[i,:]는 노드 i의 d차원 특성 벡터입니다.\nGCN 레이어는 아래와 같이 정의됩니다.\n\\(H^{(l+1)} = \\sigma (\\bar{A} H^{(l)} W^{(l)})\\)\n여기서 \\(H^{(l)}\\)는 l번째 레이어의 출력으로 \\(n x d_l\\) 크기의 행렬입니다.\n\\(W^{(l)}\\)는 l번째 레이어의 가중치 행렬로, \\(d_l x d_{l+1}\\) 크기입니다.\n\\(\\tilde{A} = D^{-\\frac{1}{2}}(A+I)n^{\\frac{1}{2}}D^{-\\frac{1}{2}}\\)는 A에 정규화 과정을 거친 행렬입니다.\n여기서 D는 A의 차수(degree) 행렬로, D[i,i]는 노드 i의 차수(연결된 간선의 수)입니다.\n\\(\\sigma\\)는 활성화 함수로, ReLU 함수 등을 사용할 수 있습니다.\n\n활성화 함수\n\nGCN 레이어의 출력에 활성화 함수를 적용합니다.\n대표적인 활성화 함수로는 ReLU 함수가 있습니다.\n\n풀링\n\nGCN 레이어를 여러 개 쌓아서 그래프의 전체 구조를 반영하는 더 깊은 모델을 만들 수 있습니다.\n더 깊은 모델에서는 각 레이어의 출력을 풀링(pooling) 연산을 통해 전체 구조를 보존하면서 임베딩의 크기를 줄입니다.\nGCN 레이어를 여러 개 쌓아서 그래프의 전체 구조를 반영하는 더 깊은 모델을 만들 수 있습니다.\n더 깊은 모델에서는 각 레이어의 출력을 풀링(pooling) 연산을 통해 전체 구조를 보존하면서 임베딩의 크기를 줄입니다.\n대표적인 풀링 연산으로는 최대 풀링(max pooling)이 있습니다.\n\n손실 함수\n\nGCN 모델의 출력을 이용하여 예측 값을 구하고, 이를 실제 값과 비교하여 손실 함수를 계산합니다.\n대표적인 손실 함수로는 크로스 엔트로피(Cross-entropy) 손실 함수가 있습니다.\nGCN 모델의 출력을 이용하여 예측 값을 구하고, 이를 실제 값과 비교하여 손실 함수를 계산합니다.\n예측 값을 구하는 방법에는 softmax 함수 등을 사용할 수 있습니다.\n대표적인 손실 함수로는 크로스 엔트로피(Cross-entropy) 손실 함수가 있습니다.\n\n최적화\n\n손실 함수를 최소화하기 위해 역전파(backpropagation) 알고리즘을 이용하여 모델의 파라미터를 업데이트합니다.\n\n학습\n\n손실 함수를 최소화하는 가중치 행렬 \\(W^{(l)}\\)를 찾기 위해 역전파(backpropagation) 알고리즘을 사용하여 모델을 학습합니다.\n일반적인 최적화 알고리즘으로는 경사 하강법(Gradient Descent)이 사용됩니다.\n\n예측\n\n학습이 완료된 GCN 모델을 사용하여 새로운 그래프에 대한 예측 값을 구할 수 있습니다.\n입력 그래프의 인접 행렬 A와 노드 특성 행렬 X를 모델에 입력하고, 출력 값을 구합니다.\n출력 값은 보통 각 노드의 클래스에 대한 확률 값으로 나타내어지며, 분류 문제에서는 이를 이용하여 예측 클래스를 결정합니다.\n\nGCN의 활용\n\nGCN은 그래프 데이터를 다루는 다양한 문제에 활용됩니다.\n대표적으로는 노드 분류(node classification), 링크 예측(link prediction), 그래프 분류(graph classification) 등이 있습니다.\n또한 GCN은 컴퓨터 비전 분야에서의 CNN(Convolutional Neural Network)과 같은 역할을 수행하여, 그래프 데이터에 대한 특성 추출에 활용됩니다.\n논문\n\n“Semi-Supervised Classification with Graph Convolutional Networks” by Thomas N. Kipf and Max Welling, ICLR 2017.\n\n\n\n2. GAT\nGAT(Graph Attention Network): 노드 간 상호작용을 모델링할 때, 중요한 노드에 더 많은 가중치를 주어 상호작용을 모델링하는 신경망 모델입니다.\n입력: 그래프 \\(\\mathcal{G=(V, E)}\\), 각 노드 #v_i#의 특성 벡터 \\(h_i∈ℝ^d\\), 이웃 노드 집합 \\(N(i)∈V (v_i)\\)에 연결된 노드들의 집합), 가중치 행렬 \\(W \\in ℝ^{d(d)}\\), Attention 매개 변수 \\(W_a∈ℝ^{(2d)}×1\\)\n노드 i와 이웃 노드 j 간의 Attention 점수 계산:\n\\(a_{ij} = softmax(LeakyReLU(a([Wh_i, Wh_j])))\\)\n여기서 a는 하나의 Dense layer를 나타내며, LeakyReLU는 Leaky Rectified Linear Unit 함수입니다.\n노드 i의 새로운 표현 계산:\n\\(h_i' = σ(Σ_j a_ijWh_j)\\)\n여기서 σ는 활성화 함수로, 일반적으로 ReLU 함수가 사용됩니다.\n출력: 새로운 노드 표현 \\(h'_i\\)\n위의 알고리즘에서, 노드 i와 이웃 노드 j 간의 Attention 점수 a_ij는 각 노드 쌍 사이에서 얼마나 중요한 정보를 교환하는지를 나타냅니다. 이를 계산할 때, 입력 특성 벡터 h_i와 h_j를 각각 가중치 행렬 W를 이용하여 변환한 후, Attention 매개 변수 W_a를 사용하여 조정합니다. 이를 통해 중요한 노드에 더 많은 가중치를 부여하고, 이를 바탕으로 각 노드 간의 상호작용을 모델링합니다.\n\nLeakyReLU\n\nLeakyReLU(Leaky Rectified Linear Unit)는 ReLU(Rectified Linear Unit) 함수의 변형으로, 입력 값이 음수일 때 작은 기울기를 가지는 선형 함수입니다.\nReLU 함수는 입력 값이 0 이상일 경우, 0을 출력하고, 음수인 경우에는 기울기가 0이 되어 해당 노드의 기능을 강제로 제한하는 문제가 있습니다. 이를 해결하기 위해 LeakyReLU는 입력 값이 음수일 때, 작은 기울기를 가지도록 하여 제한을 완화합니다.\n따라서 GAT에서 사용되는 LeakyReLU는 양수 값에 대해서는 ReLU와 같은 함수를 수행하고, 음수 값에 대해서는 작은 기울기를 가지는 선형 함수를 수행합니다. 이를 통해 입력 값이 음수일 때도 노드의 기능이 유지되어, 더욱 효과적인 모델링을 가능하게 합니다.\n\nAttention 매개 변수 W_a\n\nAttention 매개 변수 W_a는 모델이 학습을 통해 자동으로 결정됩니다. 즉, 모델이 훈련 데이터를 통해 최적의 가중치 값을 찾아가는 과정에서, W_a도 함께 학습됩니다.\n일반적으로 모델의 손실 함수에는 W와 같은 가중치 행렬들과 함께 Attention 매개 변수 W_a를 고려하는 항이 포함됩니다. 이 항은 모델이 Attention 매개 변수를 학습하는 동안 중요한 역할을 합니다. 모델은 이 항을 최소화하면서 W와 W_a를 조절하여 입력 그래프에서 노드 간의 상호작용을 잘 모델링할 수 있는 최적의 파라미터 값을 찾아갑니다.\n따라서, 모델이 학습을 통해 훈련 데이터를 학습하면서 최적의 가중치 행렬과 Attention 매개 변수를 찾아내며, 이를 통해 입력 그래프에서 노드 간의 상호작용을 잘 모델링할 수 있게 됩니다.\n논문\n\nGraph Attention Networks\n\n\n\n3. TGCN\nTGCN(Temporal Graph Convolutional Network): 시계열 데이터와 그래프를 동시에 고려하여 모델링하는 신경망 모델입니다.\n\n입력 데이터를 받아 각 노드와 엣지에 대한 특성을 임베딩합니다.\n\n노드 임베딩: \\(H^{(0)} = [h_1^{(0)}, h_2^{(0)}, ..., h_N^{(0)}] \\in \\mathbb{R}^{N \\times F^{(0)}}\\)\n엣지 임베딩: \\(E^{(0)} = [e_1^{(0)}, e_2^{(0)}, ..., e_M^{(0)}] \\in \\mathbb{R}^{M \\times F_e^{(0)}}\\)\n\n시계열 데이터를 입력으로 받아 각 시간 단계별로 특성을 임베딩합니다.\n\n시계열 임베딩: \\(X^{(t)} \\in \\mathbb{R}^{N \\times F_x}\\), \\(t=1,...,T\\)\n\nTGCN 레이어를 여러 개 쌓아서 그래프와 시계열 정보를 모두 고려한 특성을 추출합니다.\n\nTGCN 레이어: \\(H^{(l)}, E^{(l)}, X^{(t)}, A \\rightarrow H^{(l+1)}\\)\n\\(H^{(l)}\\): 이전 레이어에서의 노드 특성\n\\(E^{(l)}\\): 이전 레이어에서의 엣지 특성\n\\(X^{(t)}\\): 현재 시간 단계의 시계열 특성\n\\(A\\): 인접 행렬\n\nTGCN 레이어를 모두 거친 후, 각 시간 단계별로 예측 값을 출력합니다.\n\n출력: \\(\\hat{Y}^{(t)} \\in \\mathbb{R}^{N \\times F_y}\\), \\(t=1,...,T\\)\n위의 식에서 A는 인접 행렬이며, D는 A의 차수 행렬(diagonal degree matrix)입니다. A의 i번째 행은 i번째 노드와 연결된 모든 노드의 정보를 담고 있으며, D의 i번째 대각선 원소는 i번째 노드와 연결된 모든 노드의 차수의 합을 나타냅니다.\n첫 번째 레이어에서는 인접 행렬 A와 입력 특성 행렬 X를 곱하여 히든 상태 행렬 H(1)을 생성합니다. 이때, 히든 상태 행렬의 크기는 (N, F(1))이며, N은 노드의 개수, F(1)은 첫 번째 레이어에서의 히든 유닛 수입니다.\n두 번째 레이어에서는 첫 번째 레이어에서 생성된 히든 상태 행렬 H(1)와 인접 행렬 A를 곱하여 두 번째 히든 상태 행렬 H(2)를 생성합니다. 이때, 두 번째 히든 상태 행렬의 크기는 (N, F(2))이며, F(2)는 두 번째 레이어에서의 히든 유닛 수입니다.\n마지막 레이어에서는 두 번째 히든 상태 행렬 H(2)를 사용하여 그래프 분류, 노드 분류 또는 링크 예측과 같은 다양한 그래프 기반 작업을 수행할 수 있습니다. 예를 들어, 그래프 분류에서는 H(2)를 사용하여 소프트맥스 함수를 통해 분류 결과를 계산할 수 있습니다.\nTGCN에서는 또한 시계열 데이터를 고려하기 위해, GCN의 입력 특성 행렬 X 대신 시계열 데이터를 입력으로 사용합니다. 따라서 입력은 시간 축을 가지는 3D 텐서 형태이며, (N, T, F)의 크기를 가집니다. T는 시간 스텝 수이며, F는 각 시간 스텝에서의 특성 수입니다. 이러한 입력과 인접 행렬을 사용하여 TGCN은 시계열 데이터와 그래프를 동시에 고려하여 모델링할 수 있습니다.\n\\(N\\): 노드 수\n\\(F\\): 입력 피쳐의 수\n\\(K\\): 인접 행렬의 차수 (degree)\n\\(T\\): 시간 단계 수\n\\(C\\): 출력 피쳐의 수\n\\(H\\): hidden state의 크기\n\\(W_{in} \\in \\mathbb{R}^{F \\times H}\\): 입력 피쳐에 대한 가중치 행렬\n\\(W_{out} \\in \\mathbb{R}^{H \\times C}\\): 출력 피쳐에 대한 가중치 행렬\n\\(\\boldsymbol{W} \\in \\mathbb{R}^{H \\times H \\times K}\\): 인접 행렬에 대한 가중치 텐서\n\\(\\boldsymbol{U} \\in \\mathbb{R}^{H \\times H}\\): 시간 축에 대한 가중치 행렬\n논문\n\nT-GCN: A Temporal Graph ConvolutionalNetwork for Traffic Prediction\n\n\n\n4. MTGNN\nMTGNN(Multi-Temporal Graph Neural Network): 여러 개의 그래프와 시계열 데이터를 모두 고려하여 모델링하는 신경망 모델입니다.\nMTGNN은 여러 개의 그래프와 시계열 데이터를 동시에 고려하여 모델링하는 신경망 모델입니다. MTGNN은 크게 두 가지 단계로 구성됩니다.\n공간 변환 (Spatial Transformation): 먼저, 모델은 각 시간 스텝에서의 그래프 정보를 공간 상의 정보로 변환합니다. 이를 위해 각 그래프는 노드 임베딩 벡터를 생성하는데, 이 벡터는 해당 노드와 인접한 노드들의 정보를 이용해 구성됩니다. 또한, 각 그래프는 그래프 임베딩 벡터를 생성하는데, 이 벡터는 해당 그래프와 유사한 다른 그래프들의 정보를 이용해 구성됩니다.\n시간 변환 (Temporal Transformation): 다음으로, 모델은 공간 정보를 시간적인 정보로 변환합니다. 이를 위해 모델은 시계열 데이터와 공간 정보를 함께 입력으로 받아, 시간에 따라 정보가 어떻게 변화하는지 학습합니다. 이 단계에서는 여러 개의 MTGNN 레이어가 사용됩니다. 각 레이어에서는 이전 스텝에서 생성된 그래프와 노드 임베딩 벡터, 그리고 현재 스텝에서의 시계열 데이터가 입력으로 사용됩니다. 각 레이어에서는 이전 정보를 기억하고 새로운 정보를 추가하는 방식으로 학습이 이루어집니다. 이 단계에서도, 그래프 임베딩 벡터는 현재 스텝에서의 그래프와 유사한 다른 그래프들의 정보를 이용해 업데이트됩니다.\nMTGNN은 이러한 방식으로 여러 개의 그래프와 시계열 데이터를 동시에 고려하여 모델링할 수 있습니다.\n입력:\n입력 데이터 X : T x N x F 크기의 3차원 텐서, T는 시간 단계, N은 노드 개수, F는 각 노드의 특징 벡터 크기입니다.\n인접 행렬 A : N x N 크기의 인접 행렬, 각 노드 사이의 관계를 나타냅니다.\n시계열 길이 L : 모델이 고려하는 시계열 데이터의 길이입니다.\nhidden state의 크기 H : LSTM 레이어에서 사용되는 은닉 상태의 크기입니다.\n출력의 크기 C : 예측하고자 하는 클래스의 개수입니다.\n출력:\n예측값 Y : T x C 크기의 2차원 텐서, T는 시간 단계, C는 출력 클래스의 개수입니다.\n\n인접 행렬 A를 정규화합니다.\n\nD : N x N 크기의 노드의 차수(diagonal degree matrix)를 대각선에 위치한 행렬로 나타냅니다.\n\\(A_hat : (D + I)^(-1/2) x A x (D + I)^(-1/2)\\)로 정규화된 인접 행렬을 계산합니다.\n여기서 I는 N x N 크기의 단위 행렬입니다.\n\n입력 데이터 X를 처리하기 위해 1D convolutional layer를 적용합니다.\n\n1D convolutional layer의 출력은 T x N x H 크기의 3차원 텐서입니다.\n\nGCLSTM 레이어를 적용합니다.\n\n각 시간 단계 t마다 다음과 같은 과정을 거칩니다.\n\n\\(X_t : T x N x F\\) 크기의 입력 데이터 X에서 시간 단계 t의 데이터를 가져옵니다.\n\n데이터 X_t와 이전 시간 단계의 은닉 상태 H_t-1을 이용하여, 각 노드의 hidden state \\(h_t^l\\)과 cell state \\(c_t^l\\)을 계산합니다.\n\\(h_t^l, c_t^l = LSTM(X_t, h_t-1^l, c_t-1^l)\\)\n이전 시간 단계에서의 hidden state \\(H_{t-1}\\)과 현재 시간 단계에서 계산된 hidden state \\(h_t^l\\)을 이용하여, attention score s_t^l을 계산합니다.\n\\(s_t^l = softmax(a([H_{t-1}, h_t^l]))\\)\n여기서 a는 fully connected layer입니다.\n계산된 attention score \\(s_t^l\\)을 이용하여, GCLSTM의 출력 \\(H_t^l\\)을 계산합니다.\n\\(H_t^l = s_t^l * h_t^l + (1-s_t^l) * H\\)\n\n\n식에서 \\(\\mathcal{A}\\)는 각 노드 간의 연결 관계를 나타내는 그래프의 인접 행렬(adjacency matrix)이고, \\(\\mathcal{D}\\)는 노드의 차수(degree)를 대각 원소로 갖는 대각 행렬(diagonal matrix)입니다.\n\\(\\mathbf{X}\\)는 입력 데이터로, \\(n \\times d\\) 크기의 행렬입니다. 이는 \\(n\\)개의 시계열 데이터 샘플을 나타내는데, 각 샘플은 \\(d\\)개의 변수를 가지고 있습니다. \\(\\mathbf{X}\\)는 첫 번째 레이어의 입력으로 사용됩니다.\n\\(\\mathbf{H}^{(0)}\\)는 레이어 0의 출력으로, 이전 시간 단계에서의 입력 데이터 \\(\\mathbf{X}\\)로 초기화됩니다.\n\\(\\mathbf{H}^{(l)}\\)는 레이어 \\(l\\)의 출력입니다. \\(l\\)번째 레이어의 입력은 이전 레이어의 출력 \\(\\mathbf{H}^{(l-1)}\\)과 이전 시간 단계에서의 레이어 \\(l-1\\)의 출력인 \\(\\mathbf{Z}^{(l-1)}\\)의 결합입니다.\n\\(\\mathbf{Z}^{(l)}\\)는 레이어 \\(l\\)에서의 중간 표현으로, 이전 시간 단계에서의 레이어 \\(l\\)의 출력인 \\(\\mathbf{Z}^{(l-1)}\\)와 이번 시간 단계에서의 레이어 \\(l\\)의 입력 \\(\\mathbf{H}^{(l)}\\)을 기반으로 계산됩니다.\n레이어 \\(l\\)에서의 출력 \\(\\mathbf{H}^{(l)}\\)은 \\(\\mathbf{Z}^{(l)}\\)에서 특정 행만 선택하여 구성되는데, 이는 특정 시계열 변수만 사용하고자 할 때 유용합니다.\n\\(\\sigma(\\cdot)\\)는 활성화 함수로, 일반적으로 ReLU 또는 Sigmoid 함수가 사용됩니다. \\(\\mathbf{W}\\)와 \\(\\mathbf{U}\\)는 레이어에서 사용되는 가중치 행렬입니다. \\(\\circ\\)는 element-wise 곱셈을 나타냅니다.\n마지막 레이어에서는 softmax 함수를 사용하여 각 클래스에 대한 확률 분포를 출력합니다. 이를 통해 다중 클래스 분류 문제를 해결할 수 있습니다.\n\n네, 인접한 노드와의 관계를 레이어마다 고려합니다. GNN은 그래프의 구조를 이용해 노드 간 상호작용을 모델링하는데, 인접한 노드끼리의 상호작용은 그래프의 연결관계에 의해 정의됩니다. 따라서 인접한 노드끼리의 정보를 이용해 노드의 임베딩을 갱신합니다. 이를 위해서는 인접한 노드의 특성을 수집하고, 해당 정보를 이용해 노드의 새로운 임베딩을 계산해야 합니다. 이 과정을 여러 레이어에서 반복하여 노드의 임베딩을 점점 더 복잡하게 구성해 나가는 것입니다.\n\n논문\n\nConnecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks\n\n\n\n5. STGAT\nSTGAT(Spatio-Temporal Graph Attention Network): STGCN과 유사한 방식으로, 그래프와 시계열 데이터를 고려하여 모델링하되, GAT과 유사한 가중치를 사용하여 모델링합니다.\nSTGAT는 먼저 교통 네트워크를 시공간 그래프로 표현합니다. 이 그래프는 노드(node)와 엣지(edge)로 이루어져 있으며, 각 노드는 교차로나 도로의 일부 구간을 나타내고, 각 엣지는 구간 간의 연결을 나타냅니다. 또한, 시간 축을 추가하여 각 구간의 트래픽 데이터를 시간에 따라 그래프의 각 노드에 매핑합니다.\n그 다음, STGAT는 그래프 내의 각 노드에 대한 특징을 추출합니다. 이를 위해 STGAT는 그래프 어텐션(graph attention) 메커니즘을 사용합니다. 이 메커니즘은 각 노드의 특징을 다른 노드의 특징과 조합하여 더 유의미한 특징을 추출합니다.\n마지막으로, STGAT는 추출된 특징을 사용하여 교통 예측 모델을 학습합니다. 이 모델은 시간에 따른 각 노드의 트래픽을 예측합니다. 이를 위해 STGAT는 순환 신경망(recurrent neural network)을 사용합니다. 이 신경망은 과거 시간의 데이터를 사용하여 현재 시간의 데이터를 예측합니다.\n따라서, STGAT는 교통 네트워크의 시공간 정보를 효과적으로 활용하여 높은 예측 성능을 달성하는 모델입니다.\n\n그래프 생성\n\n각 교차로나 도로 구간을 노드로 표현\n각 구간 간의 연결을 엣지로 표현\n각 노드에 시간 축 추가하여 시공간 그래프 생성\n\n그래프 어텐션\n\n그래프 내의 각 노드에 대해 인접한 노드의 특징을 조합하여 새로운 특징을 추출하는 과정\n노드 i의 새로운 특징 h_i를 계산하는 식은 다음과 같음:\n\\(h_i = σ(Σ_j(α_ij * W * h_j))\\)\n여기서, \\(h_j\\)는 j번째 노드의 특징, W는 가중치 행렬, \\(α_{ij}\\)는 노드 i와 j 간의 어텐션 가중치, σ는 활성화 함수\n\n시간 예측\n\nRNN을 사용하여 예측 모델 학습\n입력으로는 각 노드의 특징을 사용\n출력으로는 다음 시간대의 각 노드의 트래픽 예측 값을 사용\nRNN의 출력 식은 다음과 같음:\n\\(h_t = σ(W_h * h_t-1 + W_x * x_t + b)\\)\n여기서, h_t는 현재 시간대의 은닉 상태(hidden state), x_t는 입력 데이터, W_h와 W_x는 가중치 행렬, b는 편향\n논문 - Spatial-temporal Graph Attention Networks for Traffic Flow Forecasting\nSTGCN과 STGAT 차이점\n\n그래프 생성 방법\n\nSTGAT는 시간 축을 추가하여 각 노드에 매핑하는 방식으로 시공간 그래프를 생성합니다.\nSTGCN은 인접 행렬(adjacency matrix)을 사용하여 시공간 그래프를 생성합니다.\n\n그래프 어텐션 방법\n\nSTGAT는 인접한 노드의 특징을 조합하여 새로운 특징을 추출하는 그래프 어텐션 메커니즘을 사용합니다.\nSTGCN은 그래프 컨볼루션 연산(graph convolution operation)을 사용하여 새로운 특징을 추출합니다.\n\n모델 구조\n\nSTGAT는 그래프 어텐션과 RNN을 결합한 구조를 사용합니다.\nSTGCN은 그래프 컨볼루션 연산과 1D 컨볼루션(convolution)을 결합한 구조를 사용합니다.\n\n성능\n\nSTGAT와 STGCN은 각각 다른 데이터셋에서 평가되었으며, 성능은 데이터셋에 따라 다릅니다. 일반적으로 STGCN은 STGAT보다 더 높은 예측 성능을 보입니다.\n\nSTGAT는 시간 축을 추가하여 각 노드에 매핑하는 방식으로 시공간 그래프를 생성합니다.\n\nSTGAT에서는 교통 데이터의 시간 축을 고려하여 시공간 그래프를 생성합니다. 이를 위해, 각 노드는 시간 정보를 포함한 특징 벡터를 가지고 있으며, 시간 정보는 시간 축으로 매핑됩니다.\n예를 들어, 교차로를 노드로 표현하는 경우, 각 교차로에 대해 다양한 특징(예: 교차로의 위치, 크기, 인구 밀도 등)을 수집할 수 있습니다. 그리고 교차로 간의 연결을 엣지로 표현하여 시공간 그래프를 생성할 수 있습니다. 그러나 이 경우 각 교차로의 특징 벡터에는 시간 정보가 포함되어 있지 않습니다.\n따라서, STGAT에서는 각 교차로의 특징 벡터에 시간 정보를 추가하여 시간 축으로 매핑합니다. 예를 들어, 하루를 24개의 시간 구간으로 분할하여 각 교차로의 특징 벡터에 현재 시간대의 교통 데이터를 추가할 수 있습니다. 이렇게 시간 정보가 추가된 특징 벡터를 이용하여 시공간 그래프를 생성하면, 시간 축을 고려한 교통 예측이 가능해집니다."
  },
  {
    "objectID": "posts/Study/2022-04-03-(5주차) 4월2일.html",
    "href": "posts/Study/2022-04-03-(5주차) 4월2일.html",
    "title": "Introduction to Python 5wk",
    "section": "",
    "text": "소스코드 관리, 모듈, 패키지, 라이브러리\n\nhttps://guebin.github.io/IP2022/2022/04/03/(5%EC%A3%BC%EC%B0%A8)-4%EC%9B%942%EC%9D%BC.html\n\n강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-yzovneTfJptA4K705FOG1f\n\n- (1/7) intro\n- (2/7) import 사용방법, 도움말 작성기능\n- (3/7) import 사용시 주의점\n- (4/7) import 고급\n- (5/7) site-packages 1\n- (6/7) site-packages 2\n- (7/7) 모듈, 패키지, 라이브러리, 숙제설명\n\n\nintro\n- 현재 파이썬은 길이가 2인 벡터의 덧셈을 지원하지 않음\n\na=[1,2]\nb=[3,4]\na+b\n\n[1, 2, 3, 4]\n\n\n- 아래와 같은 기능을 구현하는 함수를 만들고 싶음\n[1,2], [3,4] -> [4,6]\n- 구현\n\ndef vec2_add(a,b): \n    return [a[0]+b[0], a[1]+b[1]]\n\n- test\n\na=[1,2]\nb=[3,4]\n\n\nvec2_add(a,b)\n\n[4, 6]\n\n\n\n\nmake myfuns.py\n- 생각해보니까 vec2_add는 내가 앞으로 자주 쓸 기능임\n- 그런데 현재 사용방법으로는 내가 노트북파일을 새로 만들떄마다 def vec2_add(a,b): 와 같은 형태로 vec2_add를 매번 정의해줘야 하는 불편한이 있다.\n\n해결1\n- 자주 사용하는 함수를 myfuns.py에 저장한다.\n# myfuns.py\ndef vec2_add(a,b): \n    return [a[0]+b[0], a[1]+b[1]]\n- %run myfuns를 실행\n준비: “00” -> 커널재시작\n\n%run myfuns \n\n\nvec2_add([1,2],[3,4])\n\n[4, 6]\n\n\n\n\n해결2\n- 자주 사용하는 함수를 myfuns.py에 저장한다.\n# myfuns.py\ndef vec2_add(a,b): \n    return [a[0]+b[0], a[1]+b[1]]\n- import myfuns를 이용\n(준비) “00” -> 커널재시작\n\nimport myfuns \n\n\na=[1,2]\nb=[3,4]\nmyfuns.vec2_add(a,b)\n\n[4, 6]\n\n\n\n\n\nimport 기본\n\n사용방법\n- 사용방법1\n준비: “00” -> 커널재시작\n\nimport myfuns \n\n\nmyfuns.vec2_add([1,2],[3,4]) \n\n[4, 6]\n\n\n\nmyfuns.vec2_add 의 의미: myfuns.py 라는 파일안에 vec2_add라는 함수가 있음. 그것을 실행하라.\n.의 의미: 상위.하위의 개념!\n\n(주의) 아래와 같이 사용불가능 하다.\n\nvec2_add([1,2],[3,4])\n\nNameError: name 'vec2_add' is not defined\n\n\n- 사용방법2\n준비: “00” -> 커널재시작\n\nfrom myfuns import vec2_add \n\n\nvec2_add([1,2],[3,4])\n\n[4, 6]\n\n\n(주의) 이 경우는 오히려 아래가 불가능함\n\nmyfuns.vec2_add([1,2],[3,4]) # myfuns안의 vec2_add만 임포트했지 myfuns자체를 임포트 한것은 아님 \n\nNameError: name 'myfuns' is not defined\n\n\n- 사용방법3\n준비: “00” -> 커널재시작\n\nimport myfuns\nfrom myfuns import vec2_add\n\n\nmyfuns.vec2_add([1,2],[3,4])\n\n[4, 6]\n\n\n\nvec2_add([1,2],[3,4])\n\n[4, 6]\n\n\n- 사용방법4\n준비: “00” -> 커널재시작\n\nfrom myfuns import vec2_add, vec2_sub \n\n\nvec2_add([1,2],[3,4])\n\n[4, 6]\n\n\n\nvec2_sub([1,2],[3,4])\n\n[-2, -2]\n\n\n- 사용방법5\n준비: “00” -> 커널재시작\n\nfrom myfuns import * #*는 all의 의미 \n\n\nvec2_add([1,2],[3,4])\n\n[4, 6]\n\n\n\nvec2_sub([1,2],[3,4])\n\n[-2, -2]\n\n\n- 사용방법6\n준비: “00” -> 커널재시작\n\nimport myfuns as mf \n\n\nmf.vec2_add([1,2],[3,4])\n\n[4, 6]\n\n\n\nmf.vec2_sub([1,2],[3,4])\n\n[-2, -2]\n\n\n(오히려 아래는 실행불가능)\n\nmyfuns.vec2_add([1,2],[3,4])\n\nNameError: name 'myfuns' is not defined\n\n\n\nmyfuns.vec2_sub([1,2],[3,4])\n\nNameError: name 'myfuns' is not defined\n\n\n- 잘못된 사용방법1\n준비: “00” -> 커널재시작\n\nimport myfuns as mf \nfrom mf import vec2_add \n\nModuleNotFoundError: No module named 'mf'\n\n\n- 사용방법7\n준비: “00” -> 커널재시작\n\nimport myfuns as mf \nfrom myfuns import vec2_add \n\n\nmf.vec2_add([1,2],[3,4])\n\n[4, 6]\n\n\n\nvec2_add([1,2],[3,4])\n\n[4, 6]\n\n\n- 사용방법8\n준비: “00” -> 커널재시작\n\nimport myfuns as mf \nfrom myfuns import vec2_add as add \n\n\nmf.vec2_add([1,2],[3,4])\n\n[4, 6]\n\n\n\nvec2_add([1,2],[3,4])\n\nNameError: name 'vec2_add' is not defined\n\n\n\nadd([1,2],[3,4])\n\n[4, 6]\n\n\n\n\n도움말 작성기능\n- mf란 무엇인가?\n준비: “00” -> 커널재시작\n\nimport myfuns as mf \n\n\nmf\n\n<module 'myfuns' from '/home/cgb3/Dropbox/07_lectures/IP2022/_notebooks/myfuns.py'>\n\n\n\nmf?\n\n\nType:        module\nString form: <module 'myfuns' from '/home/cgb3/Dropbox/07_lectures/IP2022/_notebooks/myfuns.py'>\nFile:        ~/Dropbox/07_lectures/IP2022/_notebooks/myfuns.py\nDocstring:   <no docstring>\n\n\n\n\n\ntype(mf)\n\nmodule\n\n\n\nmf의 타입은 모듈이라고 나옴, 현재 단계에서는 무엇인지 알기 어려움\n\n- Docstring의 내용을 채울 수 있을까?\n준비1: myfuns.py 파일을 아래와 같이 수정한다.\n준비2: “00” -> 커널재시작\n\nimport myfuns as mf \n\n\nmf?\n\n\nType:        module\nString form: <module 'myfuns' from '/home/cgb3/Dropbox/07_lectures/IP2022/_notebooks/myfuns.py'>\nFile:        ~/Dropbox/07_lectures/IP2022/_notebooks/myfuns.py\nDocstring:   이것은 길이가 2인 벡터의 합 혹은 차를 구하는 모듈입니다.\n\n\n\n\n\n\n주의점\n- myfuns.py는 최초 한번만 import 된다.\n준비: “00” -> 커널재시작\n\nimport myfuns\n\n\nmyfuns.vec2_add([1,2],[3,4])\n\n[4, 6]\n\n\nmyfuns.py파일을 열고 함수를 아래와 같이 바꾸자.\n\"\"\"이것은 길이가 2인 벡터의 합 혹은 차를 구하는 모듈입니다.\"\"\" \ndef vec2_add(a,b): \n    print(\"이것은 myfuns.py에 정의된 함수입니다\") \n    return [a[0]+b[0], a[1]+b[1]]\ndef vec2_sub(a,b): \n    return [a[0]-b[0], a[1]-b[1]]\n다시 myfuns를 로드하고 myfuns.vec2_add 를 실행하여 보자.\n\nimport myfuns\n\n\nmyfuns.vec2_add([1,2],[3,4])\n\n[4, 6]\n\n\n바뀐내용이 적용되지 않는다.\n커널을 다시 시작하고 임포트해보자.\n“00” -> 커널재시작\n\nimport myfuns\n\n\nmyfuns.vec2_add([1,2],[3,4])\n\n이것은 myfuns.py에 정의된 함수입니다\n\n\n[4, 6]\n\n\n- myfuns.py는 주피터노트북파일과 같은 폴더에 존재해야 한다.\n준비1: “00” -> 커널재시작\n준비2: myfuns.py을 복사하여 다른 폴더로 이동. 예를들면 IP0403 폴더를 만들고 그 폴더안에 myfuns.py파일을 복사해서 붙여넣은뒤에 파일이름을 myfuns2.py 로 변경.\n\nimport myfuns # 주피터노트북파일과 같은 폴더에 있는 myfuns는 잘 로드되지만 \n\n\nimport myfuns2 # 주피터노트북파일과 다른 폴더에 있는 myfuns2는 그렇지 않다. \n\nModuleNotFoundError: No module named 'myfuns2'\n\n\n- IP0403 폴더에 있는 myfuns2.py를 실행하기 위해서는 아래와 같이 할 수 있다.\n준비: “00” -> 커널재시작\n\nfrom IP0403 import myfuns2\n\n\nmyfuns2.vec2_add([1,2],[3,4]) \n\n이것은 myfuns2.py에 정의된 함수입니다\n\n\n[4, 6]\n\n\n- 아래도 가능하다.\n준비: “00” -> 커널재시작\n\nfrom IP0403.myfuns2 import vec2_add as add \n\n\nadd([1,2],[3,4])\n\n이것은 myfuns2.py에 정의된 함수입니다\n\n\n[4, 6]\n\n\n참고로 아래는 모두 정의되지 않음\n\nIP0403.myfuns2.vec2_add([1,2],[3,4]) \n\nNameError: name 'IP0403' is not defined\n\n\n\nmyfuns2.vec2_add([1,2],[3,4]) \n\nNameError: name 'myfuns2' is not defined\n\n\n\nvec2_add([1,2],[3,4]) \n\nNameError: name 'vec2_add' is not defined\n\n\n\n\n\nimport 고급\n\n폴더와 함께 사용할시\n- 언뜻 생각하면 아래가 가능할 것 같다.\nimport IP0403 \nIP0403.myfuns2.vec2_add([1,2],[3,4]) \n- 하지만 불가능하다.\n준비: “00” -> 커널재시작\n\nimport IP0403 \n\n\n되는거아냐?\n\n\nIP0403.myfuns2.vec2_add([1,2],[3,4])\n\nAttributeError: module 'IP0403' has no attribute 'myfuns2'\n\n\n\n여기서 불가능하다.\n\n- (암기) IP0403 폴더안에 __init__.py라는 파일을 만들고 내용에 아래와 같이 쓰면 가능하다.\n# ./IP0403/__init__.py \nfrom . import myfuns2\n준비1: 위의 지침을 따른다.\n준비2: “00” -> 커널재시작\n\nimport IP0403 \n\n\nIP0403.myfuns2.vec2_add([1,2],[3,4])\n\n이것은 myfuns2.py에 정의된 함수입니다\n\n\n[4, 6]\n\n\n컴퓨터 상식 - .: 현재폴더를 의미 - ..: 상위폴더를 의미 - ./myfuns.py: 현재폴더안에 있는 myfuns.py를 의미 - ./IP0403/myfuns2.py: 현재폴더만에 IP0403폴더안의 myfuns2.py 파일을 의미 - ../myfuns.py: 현재폴더보다 한단계상위폴더에 있는 myfuns.py를 의미 - cd ./IP0403: 현재폴더안에 있는 IP0403폴더로 이동해라. (cd IP0403으로 줄여쓸 수 있음) - cd .. 현재폴더보다 한단계 상위폴더로 이동하라.\n따라서 from . import myfuns2는 현재폴더에서 myfuns2를 찾아서 임포트 하라는 의미로 해석가능\n- 의미상으로 보면 아래가 실행가능할듯 한데 불가능하다.\n\n#import myfuns\nfrom . import myfuns\n\nImportError: attempted relative import with no known parent package\n\n\n\n\n\nsite-packages (실습금지)\n- 의문: 왜 현재폴더에 numpy.py라든가 numpy라는 이름의 폴더가 없는데도 import 가능한지?\n준비: “00” -> 커널재시작\n\nimport numpy as np\n\n\nimport IP0403 as ip \n\n\nip?\n\n\nType:        module\nString form: <module 'IP0403' from '/home/cgb3/Dropbox/07_lectures/IP2022/_notebooks/IP0403/__init__.py'>\nFile:        ~/Dropbox/07_lectures/IP2022/_notebooks/IP0403/__init__.py\nDocstring:   <no docstring>\n\n\n\n\n\nnp?\n\n\nType:        module\nString form: <module 'numpy' from '/home/cgb3/anaconda3/envs/py310/lib/python3.10/site-packages/numpy/__init__.py'>\nFile:        ~/anaconda3/envs/py310/lib/python3.10/site-packages/numpy/__init__.py\nDocstring:  \nNumPy\n=====\nProvides\n  1. An array object of arbitrary homogeneous items\n  2. Fast mathematical operations over arrays\n  3. Linear Algebra, Fourier Transforms, Random Number Generation\nHow to use the documentation\n----------------------------\nDocumentation is available in two forms: docstrings provided\nwith the code, and a loose standing reference guide, available from\n`the NumPy homepage <https://www.scipy.org>`_.\nWe recommend exploring the docstrings using\n`IPython <https://ipython.org>`_, an advanced Python shell with\nTAB-completion and introspection capabilities.  See below for further\ninstructions.\nThe docstring examples assume that `numpy` has been imported as `np`::\n  >>> import numpy as np\nCode snippets are indicated by three greater-than signs::\n  >>> x = 42\n  >>> x = x + 1\nUse the built-in ``help`` function to view a function's docstring::\n  >>> help(np.sort)\n  ... # doctest: +SKIP\nFor some objects, ``np.info(obj)`` may provide additional help.  This is\nparticularly true if you see the line \"Help on ufunc object:\" at the top\nof the help() page.  Ufuncs are implemented in C, not Python, for speed.\nThe native Python help() does not know how to view their help, but our\nnp.info() function does.\nTo search for documents containing a keyword, do::\n  >>> np.lookfor('keyword')\n  ... # doctest: +SKIP\nGeneral-purpose documents like a glossary and help on the basic concepts\nof numpy are available under the ``doc`` sub-module::\n  >>> from numpy import doc\n  >>> help(doc)\n  ... # doctest: +SKIP\nAvailable subpackages\n---------------------\ndoc\n    Topical documentation on broadcasting, indexing, etc.\nlib\n    Basic functions used by several sub-packages.\nrandom\n    Core Random Tools\nlinalg\n    Core Linear Algebra Tools\nfft\n    Core FFT routines\npolynomial\n    Polynomial tools\ntesting\n    NumPy testing tools\nf2py\n    Fortran to Python Interface Generator.\ndistutils\n    Enhancements to distutils with support for\n    Fortran compilers support and more.\nUtilities\n---------\ntest\n    Run numpy unittests\nshow_config\n    Show numpy build configuration\ndual\n    Overwrite certain functions with high-performance SciPy tools.\n    Note: `numpy.dual` is deprecated.  Use the functions from NumPy or Scipy\n    directly instead of importing them from `numpy.dual`.\nmatlib\n    Make everything matrices.\n__version__\n    NumPy version string\nViewing documentation using IPython\n-----------------------------------\nStart IPython with the NumPy profile (``ipython -p numpy``), which will\nimport `numpy` under the alias `np`.  Then, use the ``cpaste`` command to\npaste examples into the shell.  To see which functions are available in\n`numpy`, type ``np.<TAB>`` (where ``<TAB>`` refers to the TAB key), or use\n``np.*cos*?<ENTER>`` (where ``<ENTER>`` refers to the ENTER key) to narrow\ndown the list.  To view the docstring for a function, use\n``np.cos?<ENTER>`` (to view the docstring) and ``np.cos??<ENTER>`` (to view\nthe source code).\nCopies vs. in-place operation\n-----------------------------\nMost of the functions in `numpy` return a copy of the array argument\n(e.g., `np.sort`).  In-place versions of these functions are often\navailable as array methods, i.e. ``x = np.array([1,2,3]); x.sort()``.\nExceptions to this rule are documented.\n\n\n\n\n- 추측: ~/anaconda3/envs/py310/lib/python3.10/site-packages/를 찾아가보자. 그곳에 numpy폴더가 있을 것이다.\n\n!ls ~/anaconda3/envs/py310/lib/python3.10/site-packages | grep numpy\n\nnumpy\nnumpy-1.22.2.dist-info\n\n\n- 추측2: ~/anaconda3/envs/py310/lib/python3.10/site-packages/에 내가 자주 쓰는 기능을 폴더로 만들어서 모아두면 어디서든지 import 할 수 있다.\n\n!mkdir ~/anaconda3/envs/py310/lib/python3.10/site-packages/guebin # guebin 폴더 생성 \n\n\n!cp ./myfuns.py ~/anaconda3/envs/py310/lib/python3.10/site-packages/guebin \n# 현폴더에 있는 myfuns.py를 아까만든 guebin 폴더로 복사 \n\n\nfrom guebin import myfuns\n\n\nmyfuns?\n\n\nType:        module\nString form: <module 'guebin.myfuns' from '/home/cgb3/anaconda3/envs/py310/lib/python3.10/site-packages/guebin/myfuns.py'>\nFile:        ~/anaconda3/envs/py310/lib/python3.10/site-packages/guebin/myfuns.py\nDocstring:   이것은 길이가 2인 벡터의 합 혹은 차를 구하는 모듈입니다.\n\n\n\n\n\n!rm  ~/anaconda3/envs/py310/lib/python3.10/site-packages/guebin -rf # guebin 폴더삭제 \n\n- 추측3: guebin이 사라진 상태에서는 from guebin import myfuns 이 동작하지 않을 것이다.\n준비: “00” -> 커널재시작\n\nfrom guebin import myfuns\n\nModuleNotFoundError: No module named 'guebin'\n\n\n- 추측4: ~/anaconda3/envs/py310/lib/python3.10/site-packages/에서 numpy를 지운다면 numpy를 import할 수 없다.\n준비: “00” -> 커널재시작\n\nimport numpy as np\n\nModuleNotFoundError: No module named 'numpy'\n\n\n- 추측5: !pip install numpy를 하면 다시 폴더가 생길 것이다.\n\n!pip uninstall numpy -y \n\nFound existing installation: numpy 1.22.2\nUninstalling numpy-1.22.2:\n  Successfully uninstalled numpy-1.22.2\n\n\n\n!pip install numpy \n\nCollecting numpy\n  Downloading numpy-1.22.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n     |████████████████████████████████| 16.8 MB 11.4 MB/s eta 0:00:01\nInstalling collected packages: numpy\nSuccessfully installed numpy-1.22.3\n\n\n\n\n모듈, 패키지, 라이브러리?\n- 모듈의 개념은 아까 살펴본것과 같다. (import를 하여 생기게 되는 오브젝트)\n- 교수님들: 모듈이 모이면 패키지라고 부른다. 그리고 라이브러리는 패키지보다 큰 개념이다.\n- 그런데 구분이 모호하다.\n\nimport numpy as np\n\n\ntype(np)\n\nmodule\n\n\n- python에서의 numpy의 type은 모듈\n- 그런데 numpy package 라고 검색하면 검색이 된다.\n- 심지어 numpy library 라고 해도 검색가능\n- 내생각: 넘파이모듈, 넘파이패키지, 넘파이라이브러리 다 맞는 말임\n\n\n숙제\nmyfuns.py 도움말 만드는 예제에서\n이것은 길이가 2인 벡터의 합 혹은 차를 구하는 모듈입니다\n대신에\n이것은 길이가 2인 벡터의 합 혹은 차를 구하는 모듈입니다. (학번: 2022-43052) \n와 같이 출력되도록 하고 스크린샷 제출"
  },
  {
    "objectID": "posts/Study/2022-12-31-Space-study.html",
    "href": "posts/Study/2022-12-31-Space-study.html",
    "title": "Study for Spaces",
    "section": "",
    "text": "Spaces"
  },
  {
    "objectID": "posts/Study/2022-12-31-Space-study.html#내적공간",
    "href": "posts/Study/2022-12-31-Space-study.html#내적공간",
    "title": "Study for Spaces",
    "section": "내적공간",
    "text": "내적공간\n\\[|a||b| \\cos \\theta\\]\n\n길이 + 각의 개념\n\nProjection"
  },
  {
    "objectID": "posts/Study/2022-12-31-Space-study.html#바나흐공간",
    "href": "posts/Study/2022-12-31-Space-study.html#바나흐공간",
    "title": "Study for Spaces",
    "section": "바나흐공간",
    "text": "바나흐공간\n\\[|---|\\]\n\n길이 + 극한의 개념"
  },
  {
    "objectID": "posts/Study/2022-12-31-Space-study.html#노름공간",
    "href": "posts/Study/2022-12-31-Space-study.html#노름공간",
    "title": "Study for Spaces",
    "section": "노름공간",
    "text": "노름공간\n\\[|   |\\]\n\n길이의 개념"
  },
  {
    "objectID": "posts/Study/2022-12-31-Space-study.html#힐베르트공간유클리드-비유클리드-모두-존재",
    "href": "posts/Study/2022-12-31-Space-study.html#힐베르트공간유클리드-비유클리드-모두-존재",
    "title": "Study for Spaces",
    "section": "힐베르트공간(유클리드 + 비유클리드 모두 존재)",
    "text": "힐베르트공간(유클리드 + 비유클리드 모두 존재)\n퓨리에 해석 - 길이 + 각 + 극한의 개념"
  },
  {
    "objectID": "posts/Study/2023-04-08-wrting_down_algorithm.html",
    "href": "posts/Study/2023-04-08-wrting_down_algorithm.html",
    "title": "ITSTGCN",
    "section": "",
    "text": "Algorithm"
  },
  {
    "objectID": "posts/Study/2023-04-08-wrting_down_algorithm.html#padding",
    "href": "posts/Study/2023-04-08-wrting_down_algorithm.html#padding",
    "title": "ITSTGCN",
    "section": "padding",
    "text": "padding\n\nmindex = 입력값, 리스트 자료형\nf = TN \\(\\times\\) 1, 리스트 자료형\n\n보간법 이용 후 결측값 뒤, 앞으로 채우고 리스트화하여 FX 값으로 출력\n\nlags = up to writer’s choice\nT = Time\nN = Node\n\n\n\n\n\n\n\nTip\n\n\n\n\nlinear: 선형 보간법(기본값)\ntime: 시계열 데이터를 위한 선형 보간법\nindex: 인덱스를 사용하여 선형 보간법\npolynomial: 다항식 보간법\nspline: 스플라인 보간법\nbarycentric: 바리센트릭 보간법\nkrogh: Krogh 보간법\npiecewise_polynomial: 조각 다항식 보간법\npchip: PCHIP(Piecewise Cubic Hermite Interpolating Polynomial) 보간법\nakima: Akima 보간법\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nfillna(method='bfill')\n이 코드는 Pandas 라이브러리를 사용하여 데이터프레임의 결측값을 뒤 방향으로(backward) 채우는(fill) 메소드입니다.\n’bfill’은 ’backward fill’을 의미하며, 이 메소드는 결측값(missing value)을 다음으로 나타나는(non-missing) 값으로 대체합니다. 즉, 각 결측값은 그 이후에 오는 첫 번째 비 결측값으로 대체됩니다.\n\n\n\n\n\n\n\n\nNote\n\n\n\nfillna(method='ffill')\n이 코드는 Pandas 라이브러리를 사용하여 데이터프레임의 결측값을 앞 방향으로(forward) 채우는(fill) 메소드입니다.\n’ffill’은 ’forward fill’을 의미하며, 이 메소드는 결측값을 이전에 나타난(non-missing) 값으로 대체합니다. 즉, 각 결측값은 그 이전에 나타난 마지막 비 결측값으로 대체됩니다."
  },
  {
    "objectID": "posts/Study/2023-04-08-wrting_down_algorithm.html#rand_mindex",
    "href": "posts/Study/2023-04-08-wrting_down_algorithm.html#rand_mindex",
    "title": "ITSTGCN",
    "section": "rand_mindex",
    "text": "rand_mindex\n\nmissing_count = mrate \\(\\times\\) Time\nmindex = Index of random values\n\nblock에서 시뮬레이션 돌리려면 나중에 트레이닝에서 missing 뽑을 거니까 트레이닝$$0.8보다 작은 값으로 block 설정해야 함"
  },
  {
    "objectID": "posts/Study/2023-04-08-wrting_down_algorithm.html#make_psi",
    "href": "posts/Study/2023-04-08-wrting_down_algorithm.html#make_psi",
    "title": "ITSTGCN",
    "section": "make_Psi",
    "text": "make_Psi\n\nW = T \\(\\times\\) T\n\n대각선빼고 모두 1인, 모두 연결되었다고 가정하는 인접행렬(adjacency matrix)\n\nd = sum of each row data at W\n\n2차원 \\(\\to\\) 1차원\nT \\(\\times\\) 1\n\nD = 대각선 원소가 d값을 가진 2차원 행렬로 변환\n\nT \\(\\times\\) T\n\nL = \\(D^{1/2} (D-W)D^{1/2}\\)\nlamb, Psi = result of eigenvaluede composition at L\n\nlamb = T,\nPsi = T \\(\\times\\) T\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\\(L = D-W\\)\n\\(D\\) is a diagonal degree matrix.\n\\(W\\) is a weight matrix.\n\\(\\tilde{L} = D^{-1/2}LD^{-1/2}\\)\n\\(= D^{-1/2}(D-W)D^{-1/2}\\)\n\\(= D^{-1/2}DD^{-1/2}-D^{-1/2}WD^{-1/2}\\)\n\\(= I-D^{-1/2}WD^{-1/2}\\)\n\\(I\\) is an identity matrix.\n\\(\\tilde{L}\\) nomalizes weights using \\(D^{-1/2}WD^{-1/2}\\) by degree of each node and retain the information of graph structiure.\n\\(0 < L< 1\\)"
  },
  {
    "objectID": "posts/Study/2023-04-08-wrting_down_algorithm.html#trim",
    "href": "posts/Study/2023-04-08-wrting_down_algorithm.html#trim",
    "title": "ITSTGCN",
    "section": "trim",
    "text": "trim\n\nf = T \\(\\times\\) N \\(\\to\\) 벡터로 변환, ?$$1\n\n아래와 같이\n\nf = np.array([1,2]);f.shape\n\n(2,)\n\n\n\nif len(f.shape)==1: f = f.reshape(-1,1)\n\n\nf.shape\n\n(2, 1)\n\n\n\nT = Time\nN = Node\nPsi = Psi from make_Psi class\n\nT \\(\\times\\) T\n\nfbar = trans(Psi) \\(\\times\\) f = T \\(\\times\\) N\n\nPsi = T\\(\\times\\) T\nf = T \\(\\times\\) N\n\nfbar_threshed = fbar에서 node 수 대로 ebayes하여 step function을 detect 하여 행별로 stack 한 결과\n\nT \\(\\times\\) N\n\nfhat = Psi \\(\\times\\) fbar_threshed = T \\(\\times\\) N\n\nPsi = T \\(\\times\\)T\nfbar_threshed = T \\(\\times\\) N"
  },
  {
    "objectID": "posts/Study/2023-04-08-wrting_down_algorithm.html#update_from_freq_domain",
    "href": "posts/Study/2023-04-08-wrting_down_algorithm.html#update_from_freq_domain",
    "title": "ITSTGCN",
    "section": "update_from_freq_domain",
    "text": "update_from_freq_domain\n\nsignal = input\nsignal_trimed = trim(signal) = T\\(\\times\\)N\n\nmissing_index를 입력으로 바꿔서 signal_trimed로 결측 index의 값만 imputation"
  },
  {
    "objectID": "posts/Study/2023-04-08-wrting_down_algorithm.html#recurrentgcn",
    "href": "posts/Study/2023-04-08-wrting_down_algorithm.html#recurrentgcn",
    "title": "ITSTGCN",
    "section": "RecurrentGCN",
    "text": "RecurrentGCN\n\n\n\n\n\n\nTip\n\n\n\nGConvGRU를 사용하였으며, Structured Sequence Modeling with Graph Convolutional Recurrent Networks 논문에서 제안된 모델\n이 논문은 그래프 합성곱 신경망 (Graph Convolutional Neural Network, GCN)을 이용한 시퀀스 모델링을 위해 제안된 구조인 그래프 합성곱 순환 신경망 (Graph Convolutional Recurrent Neural Network, GCRN)에 대한 연구입니다.\nGCRN은 시계열 데이터를 처리하기 위해 순환 신경망 (Recurrent Neural Network, RNN)과 같은 장기 의존성(long-term dependency)을 고려하는 방법으로 그래프 합성곱 신경망(GCN)을 확장합니다. 이를 위해 GCRN은 각 시간 스텝에서 그래프 상의 노드 간 상호 작용을 잘 나타낼 수 있는 새로운 그래프 합성곱 셀(GCN cell)을 도입합니다. 이 셀은 체비셰폴리노미알 필터 (Chebyshev Polynomial Filter)를 사용하여 그래프 신호의 재귀적인 확장을 수행하며, 게이트 메카니즘(Gate Mechanism)을 통해 순환 구조를 적용합니다.\n논문에서 실험은 여러 시퀀스 모델링 문제에서 GCRN의 성능을 검증합니다. 실험 결과 GCRN은 기존의 RNN과 GCN을 비롯한 다른 모델들과 비교하여 높은 예측 정확도를 보입니다. 이를 통해 GCRN이 그래프 시계열 데이터 분석 분야에서 유용하게 사용될 수 있다는 가능성을 제시합니다.\nChebyshev polynomial filter는 Laplacian 행렬과 함께 사용하여 그래프 신경망에서 필터링을 수행하는 데 사용됩니다. 이 필터는 임의의 그래프에 대해 일반적으로 적용될 수 있으며, 이전의 고유 벡터와 함께 Laplacian의 K차 필터링을 수행하는 데 사용됩니다. 이 때 K는 Chebyshev 다항식의 차수입니다."
  },
  {
    "objectID": "posts/Study/2023-04-08-wrting_down_algorithm.html#stgcnlearner",
    "href": "posts/Study/2023-04-08-wrting_down_algorithm.html#stgcnlearner",
    "title": "ITSTGCN",
    "section": "StgcnLearner",
    "text": "StgcnLearner\ngetattr(dataset,'mindex',None)\ndataset에서 mindex 가져오기"
  },
  {
    "objectID": "posts/Study/2023-04-08-wrting_down_algorithm.html#itstgcnlearner",
    "href": "posts/Study/2023-04-08-wrting_down_algorithm.html#itstgcnlearner",
    "title": "ITSTGCN",
    "section": "ITStgcnLearner",
    "text": "ITStgcnLearner"
  },
  {
    "objectID": "posts/Study/2023-04-08-wrting_down_algorithm.html#gnarlearner",
    "href": "posts/Study/2023-04-08-wrting_down_algorithm.html#gnarlearner",
    "title": "ITSTGCN",
    "section": "GNARLearner",
    "text": "GNARLearner"
  },
  {
    "objectID": "posts/Study/2022-03-28-(4주차) 3월28일.html",
    "href": "posts/Study/2022-03-28-(4주차) 3월28일.html",
    "title": "Introduction to Python 4wk",
    "section": "",
    "text": "개발환경의 변천사, 1세대 프로그래머부터 5세대 프로그래머까지\n\nhttps://guebin.github.io/IP2022/2022/03/28/(4%EC%A3%BC%EC%B0%A8)-3%EC%9B%9428%EC%9D%BC.html\n\n강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-zcnjAged1xIatgznRTy93c\n\n- (1/8) 파이썬이 어려웠던 이유\n- (2/8) 1세대 프로그래머\n- (3/8) 1세대 프로그래머의 삶 with python\n- (4/8) 1세대 프로그래머의 삶 with ipython\n- (5/8) 2세대 프로그래머, 3세대 프로그래머 (1)\n- (6/8) 3세대 프로그래머(2), 4세대 프로그래머\n- (7/8) 5세대 프로그래머\n- (8/8) 다양한 개발환경 구축방법 다시 리뷰, 숙제설명\n\n\n파이썬이 어려웠던 이유\n- 파이썬 배우는 초보자에게 가장 어려운것! - 선생님마다 설치하는 방법이 모두 다름\n- 왜 저렇게 설치방법이 다른가? 왜 다른 방법으로 각각 파이썬을 실행하는가? 이런것이 너무 어려움 - 방법1: 파이썬프로그램 다운로드 -> 시작버튼 눌러서 설치 - 방법2: 아나콘다 설치 (그럼 자동으로 파이썬이 설치됨) - 방법3: 아나콘다 설치 + 가상환경 - …\n- 심지어 실행하는것도 다름 - 방법1: 파이썬 프롬프트 - 방법2: .py를 이용하여 실행? - 방법3: IDLE - 방법4: 파이참 - 방법5: 스파이더 - 방법6: Visual Studio Code - 방법7: 주피터노트북, 주피터랩 - 가상환경을 만들어서 해라.. - 아나콘다 네비게이터에 주피터가 있다.. - …\n- 머리아프니까 collab을 쓰라는 사람도 있음. 아니면 도커이미지를 줄테니까 그걸 쓰라는 사람도 있음. AWS를 쓰라는 사람도 있음.. \\(\\to\\) 이게 더 머리아픔\n- 핵심: 그냥 (1) 컴퓨터에 (2) 파이썬을 깔아서 (3) 실행하는 것임\n- 의문: 그런데 방법이 왜이렇게 많은가? 엑셀처럼 프로그램 설치하고 아이콘 더블클릭하면 끝나는 식으로 만들어야 하는것 아닌가?\n\n개발환경 구축방법이 많은 이유?\n- 파이썬 개발환경 구축은 수많은 방법이 있다.\n- 이는 마치 라면의 레시피를 검색하면 수많은 방법이 나오는것과 유사함. - 방법1: 스프를 먼저 넣고 끓인다음 라면을 넣어야 합니다. - 방법2: 양은냄비에 물넣고 물이 끊으면 라면과 스프를 같이 넣고 마지막에 계란을 넣는다. - 방법3: 먹다남은 삼겹살을 후라이팬에 볶은다음에 물을 붓고 라면을 넣는다. - 방법4: 용기에 라면+스프+뜨거운물 랩을 씌운뒤에 젓가락으로 구멍을 뚫고 전자렌지에 돌린다. - …\n- 우리는 모든 방법을 나열할 순 없지만 모든 방법을 이해할 수 있다. 왜냐하면 라면을 끓이는 공통적인 맥락을 우리는 알고 있으니까\n- 파이썬을 설치하는 다양한 방법 역시 공통맥락을 파악하면 이해하기 쉽다.\n- 제목적: 파이썬을 설치하고 실행하는 공통맥락을 설명하고 싶음\n- 설치하는 방법이 다양한 이유? 파이썬이 인기있음 + 다양한 방법을 설치를 하면 각자의 장점이 뚜렷해서\n\n\n\n1세대 프로그래머\n\npython\n- 윈도우에서 anaconda prompt 실행 -> python\n(base) C:\\Users\\python>python\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> [1,2,3]+[4]\n[1, 2, 3, 4]\n>>> a=[1,2,3]+[4]\n>>> a\n[1, 2, 3, 4]\n- 2개를 실행할 수도 있음. (두 환경은 각각 서로 독립적인 파이썬, 변수가 공유되지 않음) \\(\\star\\)\n- 아쉬운점: `?list’와 같이 도움말 기능이 동작하지 않음\n>>> ?list\n  File \"<stdin>\", line 1\n    ?list\n    ^\nSyntaxError: invalid syntax\n>>> \n\n\nipython\n- 윈도우에서 anaconda prompt 실행 -> ipython\n(base) C:\\Users\\python>ipython\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.29.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: a=[1,2,3]\n\nIn [2]: a\nOut[2]: [1, 2, 3]\n\nIn [3]: a+[4]\nOut[3]: [1, 2, 3, 4]\n- ?list가 가능\nIn [4]: ?list\nInit signature: list(iterable=(), /)\nDocstring:\nBuilt-in mutable sequence.\n\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\nType:           type\nSubclasses:     _HashedSeq, StackSummary, DeferredConfigList, SList, _ImmutableLineList, FormattedText, NodeList, _ExplodedList, Stack, _Accumulator, ...\n\n- 색깔이 알록달록해서 문법을 보기 편하다. (구문강조)\n\n\n1세대 프로그래머의 삶 with python\n- 1부터 10까지 합을 구하는 프로그램을 만들고 싶음\n- 시도1: python을 키고 아래와 같이 실행\n(base) C:\\Users\\python>python\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> total = 0\n>>> for i in range(10):\n...     total=total+i\n...\n>>> total\n45\n>>>\n- 반성: 정답은 55인데 45가 출력되었다! \\(\\to\\) range(10)을 range(1,11)으로 바꿔야겠다!\n- 시도2: range(1,11)을 바꿔야겠다고 생각하고 다시 입력하다가 오타가 발생\n>>> total =0\n>>> for i in range(1,11):\n...     total = totla +i\n...\n\n앗 totla이라고 잘못쳤다.\n\n- 반성: 다음에는 정신을 똑바로 차려야겠다.\n- 불편한점: … 다..\n\n\n1세대 프로그래머의 삶 with ipython\n- ipython을 사용한 프로그래머는 좀더 상황이 낫다\n(base) C:\\Users\\python>ipython\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.29.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: total = 0\n\nIn [2]: for i in range(1,11):\n   ...:     total = total + i\n   ...:\n\nIn [3]: total\nOut[3]: 55\n\n편한점1: 자동으로 들여쓰기가 되어서 편함\n편한점2: 화살표를 이용해서 for문을 쓰는 도중에 위아래로 이동가능\n불편한점1: 화살표로 이동할수는 있는데 마우스로는 이동할 수 없다.\n불편한점2: 내가 작성한 코드를 관리하기 어렵다.\n\n\n\n\n2세대 프로그래머: 메모장 + anconda prompt를 이용 (.py를 이용한 python활용)\n- 메모장을 키고 아래의 내용을 적는다.\ntotal = 0 \nfor i in range(1,11): \n    total = total + i\nprint(total)\n- 파일이름을 mysum.py로 저장한다.\n- anaconda prompt에서 mysum.py파일이 저장된 폴더로 이동 -> 실행\n(base) C:\\Users\\python>cd Desktop\n\n(base) C:\\Users\\python\\Desktop>dir\n C 드라이브의 볼륨에는 이름이 없습니다.\n 볼륨 일련 번호: 9AFD-A05F\n\n C:\\Users\\python\\Desktop 디렉터리\n\n2022-03-27  오전 11:32    <DIR>          .\n2022-03-27  오전 11:32    <DIR>          ..\n2022-03-27  오전 12:01             2,306 Chrome.lnk\n2022-03-26  오후 08:32             2,332 Microsoft Edge.lnk\n2022-03-27  오전 11:33                71 mysum.py\n               3개 파일               4,709 바이트\n               2개 디렉터리  743,643,467,776 바이트 남음\n\n(base) C:\\Users\\python\\Desktop>python mysum.py\n55\n\n(base) C:\\Users\\python\\Desktop>\n- 소감 - 편한점1: 마우스를 이용하여 이동가능 - 편한점2: 내가 작업한 내용은 바탕화면의 메모장에 저장이 되어있음 - 아쉬운점: ipython의 장점은 활용못함 (구문강조, 도움말기능)\n\n\n3세대 프로그래머: 메모장 + ipython\n- 전체적인 개발방식 - 메모장: 코드를 편집, 저장 - ipython: anaconda prompt처럼 메모장의 코드를 실행하고 결과를 확인 + 구문강조, 도움말확인기능 등을 이용하여 짧은 코드를 빠르게 작성\n- 기능 - ipython에서 !python mysum.py를 입력하면 anaconda prompt에서 python mysum.py를 입력한 것과 같은 효과 - ipython에서 %run mysum을 입력하면 메모장에서 mysum.py에 입력된 내용을 복사해서 ipython에 붙여넣어 실행한것과 같은 효과\n\n\n4세대 프로그래머: IDE(통합개발환경)를 사용\n- 메모장과 ipython을 하나로 통합한 프로그램이 등장! - jupyter notebook, jupyter lab - spyder - idle - VScode - …\n- 주피터의 트릭 (실제로 주피터는 ipython에 기생할 뿐 아무런 역할도 안해요)\n\n주피터를 실행\n새노트북을 생성 (파이썬으로 선택)\n\n\n컴퓨터는 내부적으로 ipython을 실행하고 그 ipython이랑 여러분이 방금만든 그 노트북과 연결\n\n\n처음보이는 cell에 1+1을 입력 -> 쉬프트엔터 -> 결과2가 출력\n\n\n처음보이는 cell하나 = 자동으로 열린 하나의 메모장\ncell 1+1을 입력 = 메모장에 1+1을 적음\n쉬프트+엔터후 결과2를 출력 = cell의 내용을 복사 -> ipython에 붙여넣음 -> ipython 계산된 결과를 복사 -> cell로 돌아와 붙여넣기\n\n\n새로운 cell을 추가하고 2+2을 입력 -> 쉬프트엔터 -> 결과4가 출력\n\n\n새로운 cell을 추가 = 새로운 메모장 추가\ncell 2+2을 입력 = 새로운 메모장에 2+2를 적음\n쉬프트+엔터후 결과4를 출력 = cell의 내용을 복사 -> ipython에 붙여넣음 -> ipython 계산된 결과를 복사 -> cell로 돌아와 붙여넣기\n\n- 중요한 사실들 - IDE는 내부적으로 연산을 수행하는 능력이 없다. (생각해볼것: 왜 R을 꼭 설치하고 Rstudio를 설치해야 했을까?) - 주피터에서 커널을 재시작한다는 의미는 메모장이 열린채로 ipython을 껐다가 다시 실행한다는 의미 - 주피터는 단순히 ’메모장의 내용을 복사하여 붙여넣는 기계’라고 볼 수 있다. 이렇게 생각하면 주피터는 꼭 ipython에 연결할 이유는 없다. 실제로 주피터에 R을 연결해서 쓸 수 있다. 즉 하나의 IDE가 여러개의 언어와 연결될 수 있다. - Jupyterlab이라는 프로그램은 크롬에 있는 내용과 ipython간의 통신을 제어하는 프로그램일 뿐이다.\n\n\n5세대 프로그래머: 가상컴퓨터(anaconda), 원격컴퓨터(server), 클라우드컴퓨터(colab)의 개념 등장\n- 지금까지는 ipython이 실행되는 컴퓨터와 크롬이 실행되는 컴퓨터가 동일하다는 전제였음.\n- 생각해보니까 어차피 ipython이 실행된 컴퓨터에서 내가 크롬에 입력한 명령 “전달”되기만 하면 되므로 꼭 같은 컴퓨터일 필요는 없다.\n\n모델1: 원격컴퓨터\n- 준비상태 - 전북대컴퓨터: ipython을 실행 + 이 컴퓨터는 인터넷 연결이 되어있어야함 - 우리집노트북: 크롬실행 + 이 컴퓨터도 인터넷이 연결되어 있어야함\n- 명령입력 - 우리집노트북 크롬에서 1+1을 입력하고 쉬프트 엔터를 누름\n- 우리집노트북 -> 전북대컴퓨터 - 우리집 노트북의 내부의 어떤프로그램은 1+1이라는 명령을 복사하여 카카오톡으로 전북대 컴퓨터에 전달 - 전북대 컴퓨터의 내부의 어떤프로그램은 1+1이라는 명령을 카톡으로 받아서 그것을 ipython에게 전달\n- 전북대컴퓨터 -> 우리집노트북 - 전북대컴퓨터 내부의 ipython은 2라는 출력결과를 계산함 - 전북대컴퓨터 내부의 어떤프로그램은 계산결과를 카톡으로 우리집 노트북에 알려줌 - 나는 우리집 노트북에서 계산결과를 받아볼 수 있다.\n\n\n모델2: 원격컴퓨터 + 가상컴퓨터\n- 준비상태 - 성능좋은 전북대 컴퓨터 1개 - 내 노트북 1개 (그냥 싸고 가벼운거) - 대학원생 아이패드 1개 (그냥 싸고 가벼운거)\n- 아이디어\n\n성능좋은 전북대 컴퓨터를 논리적으로 3개로 분리 \\(\\to\\) 이를 각각 (base) (py39jl17) (py38r40) 컴퓨터라고 하자.\n나는 (py39jl17)에 접속하여 파이썬 3.9와 줄리아 1.7을 설치한뒤 실습한다.\n대학원생은 (py38r40)에 접속하여 파이썬 3.8과 R 4.0을 설치하고 실습한다.\n(base)는 예비용으로 아무것도 설치안한 깨끗한 상태 유지\n내가 뭘 실수해서 (py39jl17)컴퓨터가 망가졌으나 (py38r40)은 아무 타격없다.\n나는 (py39jl17)를 삭제하고 (base)로 부터 다시 새로운 컴퓨터를 복사하여 (py39jl17)을 다시 만든다.\n\n\n\n모델3: 가상컴퓨터\n- 여러분들 사례 - 여러분들의 컴퓨터는 (base), (py39) 2개의 컴퓨터로 나누어져 있음 - 여러분들이 (py39)에만 주피터랩을 설치 - (py39)에 있는 ipython과 여러분의 크롬창이 서로 통신하면서 실습 - 장점: 서로 다른 환경에 서로다른 파이썬과 R등을 설치할 수 있다. \\(\\to\\) 패키지간의 충돌이 최소화 (파이썬 입문 수업을 듣고, 이후에 파이썬을 이용하는 어떤수업을 들음)\n\n\n모델4: 클라우드\n- 사례1 - 성능이 그저그런 컴퓨터 27개 - 대학원생을 포함하여 쓸 사람은 5명 - 한사람당 27/5(=5.4)대의 컴퓨터식 할당\n- 사례2: 구글코랩 - 구글에 여러가지 성능을 가진 컴퓨터가 \\(n\\)대 있음 - \\(m\\)명의 사람이 \\(n\\)대의 컴퓨터에 접속 - 적당히 컴퓨터 자언을 분배하여 사용\n\n\n\n요약 및 정리\n- 결국 (1) 컴퓨터에 (2) 파이썬을 설치하고 (3) 실행하는 과정은 생각보다 다양한 선택의 조합이 가능하다.\n\n그냥 내 노트북에 파이썬을 설치할지? 내 노트북안에 가상컴퓨터를 만들고 거기에 파이썬을 설치할지? 학교의 데스크탑에 파이썬을 설치하고 쓸지? 설치를 안하고 구글컴퓨터에 설치된 파이썬을 난 쓰기만 할지?\npython설치할지? ipython를 설치할지? 어차피 가상환경을 쓸꺼니가 anaconda를 설치할지? 아니면 코랩쓸꺼니까 설치안할지?\n어떤 IDE를 쓸지? IDE를 쓰지 않을지? 내가 IDE를 직접구성해서 만들지?\n\n하지만 공통적으로 관통하는 원리가 있다\n\n\n숙제\n- 주피터랩에서 ’myprod.py’파일을 만들고 1부터 5까지의 곱을 계산하는 코드를 작성후 %run myprod를 실행하여 출력결과를 확인"
  },
  {
    "objectID": "posts/Study/2023-03-16-stgcn_paper_review_1.html",
    "href": "posts/Study/2023-03-16-stgcn_paper_review_1.html",
    "title": "STGCN papers review",
    "section": "",
    "text": "STGCN papers review\n\nPyTorch Geometric Temporal\n이 데이터는 세 종류의 dataset을 가지고 있고, 각 데이터는 snapshot으로 구분되어 있음\n\nDynamic graph with temporal signal(A dynamic graph with a temporal signal)\n\n엣지 변함, 시그널 변함\n\nDynamic graph with static signal(A dynamic graph with a static signal)\n\n엣지 변함, 시그널 고정\n\nStatic graph with temporal signal(A static graph with a temporal signal)\n\n엣지 고정, 시그널 변함\n\n\nSpatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting\n\n!git add .\n!git commit -m .\n!git push"
  },
  {
    "objectID": "posts/Study/2023-02-01-ch8_DFT.html",
    "href": "posts/Study/2023-02-01-ch8_DFT.html",
    "title": "Chap 8.3: Discrete Fourier Transform",
    "section": "",
    "text": "Chap 8.3: Discrete Fourier Transform\n\n\nusing LinearAlgebra, FFTW\n\nShift Operator B가 존재할 떄,\n\n직교 행렬orthogonal\ncyclic shift operator(\\(BS_n = S_{n-1}\\)) \\(\\to\\) \\(S_{n-1}\\)은 vector space components\n\n\\(\\star\\) 시계열의 back shift operator 로 생각할 수 있고, foward shift operator도 가능하다.\n\\(\\star\\) cyclic operator이어야 하는 이유? 책의 정의 이용 및 back/forward shift operator는 고유분해 안 될 수도.\n이 행렬을 고유분해(full rank)하여 나온 고유값과 고유벡터가 존재한다.\n\nB= [0 0 0 0 1\n    1 0 0 0 0 \n    0 1 0 0 0\n    0 0 1 0 0\n    0 0 0 1 0] # cyclic shift operator B\nB'B # matrix B is orthogonal\n\n5×5 Matrix{Int64}:\n 1  0  0  0  0\n 0  1  0  0  0\n 0  0  1  0  0\n 0  0  0  1  0\n 0  0  0  0  1\n\n\n\ns = [1,2,3,4,5]\nB*s # matrix B is a cyclic shift operator\n\n5-element Vector{Int64}:\n 5\n 1\n 2\n 3\n 4\n\n\n\nB^2*s\n\n5-element Vector{Int64}:\n 4\n 5\n 1\n 2\n 3\n\n\n이 고유값\\(\\lambda\\), 고유벡터\\(\\psi\\)가 존재한다면 B는 \\(DFT^* \\Lambda DFT\\)로 표현 가능핟하다.\n\n\\(DFT^*\\)\n\nconjugate matrix\n\\(\\psi\\)인데 DFT로 표현 \\(\\to\\) 그래프 도메인으로 확장이 가능하기 때문\n\n\n여기서 \\(DFT^*\\)는 \\(\\psi^*_k = DFT_k = \\frac{1}{\\sqrt{N}} \\begin{bmatrix} 1 \\\\ \\dots \\\\ e^{-j\\frac{2\\pi}{N}(N-1)k} \\end{bmatrix}\\)로서 표현(\\(\\in C^N\\) 길이가 \\(N\\)인 vector(복소수))\n\nunitary and symmetric\n\nunitary \\(\\to\\) complex space에서 정규직교기저를 이루고, \\(A(A^*)^\\top = I, \\psi^{-1} = \\psi^*, \\psi^* \\psi = \\psi \\psi^* = I\\)\n\n위 \\(k\\)개의 벡터들은 spectral components 이다.\nthe complex exponential sinusodal functions\n\n여기서 \\(\\lambda\\)는 the frequencies of the signal 로서 정의될 수 있다.\n\n특징\n\ndistinct\npositive\nequally spaced\nincreasing from \\(0\\) ro \\(\\frac{N-1}{N}\\)\n\n\n\nλ, ψ = eigen(B)\n\nEigen{ComplexF64, ComplexF64, Matrix{ComplexF64}, Vector{ComplexF64}}\nvalues:\n5-element Vector{ComplexF64}:\n -0.8090169943749472 - 0.5877852522924725im\n -0.8090169943749472 + 0.5877852522924725im\n 0.30901699437494734 - 0.9510565162951536im\n 0.30901699437494734 + 0.9510565162951536im\n  0.9999999999999998 + 0.0im\nvectors:\n5×5 Matrix{ComplexF64}:\n  0.138197+0.425325im   0.138197-0.425325im  …  0.447214+0.0im\n -0.361803-0.262866im  -0.361803+0.262866im     0.447214+0.0im\n  0.447214-0.0im        0.447214+0.0im          0.447214+0.0im\n -0.361803+0.262866im  -0.361803-0.262866im     0.447214+0.0im\n  0.138197-0.425325im   0.138197+0.425325im     0.447214+0.0im\n\n\n\nB = ψ * Diagonal(λ) * ψ'\n\n5×5 Matrix{ComplexF64}:\n    2.498e-16-9.67158e-18im  …           1.0+1.81709e-18im\n          1.0+2.1793e-18im       5.55112e-16-1.09573e-17im\n -3.88578e-16-7.89355e-18im     -3.21902e-16+8.57473e-18im\n -4.16334e-16-8.06149e-18im       -4.996e-16-8.9293e-18im\n  2.99888e-16+1.53977e-17im      3.99189e-16+1.42498e-17im\n\n\n\nDFT = ψ'\n\n5×5 adjoint(::Matrix{ComplexF64}) with eltype ComplexF64:\n  0.138197-0.425325im  -0.361803+0.262866im  …  0.138197+0.425325im\n  0.138197+0.425325im  -0.361803-0.262866im     0.138197-0.425325im\n -0.361803-0.262866im  -0.361803+0.262866im     0.138197-0.425325im\n -0.361803+0.262866im  -0.361803-0.262866im     0.138197+0.425325im\n  0.447214-0.0im        0.447214-0.0im          0.447214-0.0im\n\n\n\\(B = \\psi \\Lambda \\psi^{-1}\\)\n\n\\(\\psi^H := DFT\\) 이렇게 정의한다면 F의 고유벡터의 conjugate\n\\(F\\) \\(\\to\\) \\(BF = I\\)\n\\(\\psi \\Lambda \\psi^H F = I\\)\n만약, \\(F\\)가 \\(\\psi^H\\Lambda^{-1}\\psi\\)라면, \\(\\psi \\Lambda \\psi^H\\psi^H\\Lambda \\psi = I\\)\n따라서 \\(F\\)는 \\(\\psi^{-1} \\Lambda^{-1} \\psi\\)로 고유분해\n\n\nx = [1,2-im,-im,-1+2im]\n\n4-element Vector{Complex{Int64}}:\n  1 + 0im\n  2 - 1im\n  0 - 1im\n -1 + 2im\n\n\n\n_DFT = reshape([i*j for i in 0:3 for j in 0:3], (4,4))\n_DFT\n\n4×4 Matrix{Int64}:\n 0  0  0  0\n 0  1  2  3\n 0  2  4  6\n 0  3  6  9\n\n\n\nf = x -> exp(-im * (2π/4) * x)\nDFT = _DFT .|> f\n\n4×4 Matrix{ComplexF64}:\n 1.0-0.0im           1.0-0.0im          …           1.0-0.0im\n 1.0-0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0-0.0im          -1.0-1.22465e-16im             -1.0-3.67394e-16im\n 1.0-0.0im  -1.83697e-16+1.0im              5.51091e-16-1.0im\n\n\n\nDFT * x\n\n4-element Vector{ComplexF64}:\n                   2.0 + 0.0im\n   -1.9999999999999998 - 2.0000000000000004im\n 8.881784197001252e-16 - 1.9999999999999998im\n    3.9999999999999987 + 4.000000000000001im\n\n\n\nfft(x)\n\n4-element Vector{ComplexF64}:\n  2.0 + 0.0im\n -2.0 - 2.0im\n  0.0 - 2.0im\n  4.0 + 4.0im\n\n\nDFT의 두 번쨰 정의\n\n복소수 sequence \\(\\{x_n\\}\\)을 규칙에 따라 \\(\\{X_k\\}\\)로 변환하는 것\n규칙: \\(x_k = \\sum^{N-1}_{n=0} x_n e^{-i\\frac{2\\pi}{N}kn}\\)\n\n특히, \\(k=0\\)이면 \\(X_0 = \\sum^{N-1}{n=0}x_n\\), constant term 이 되어 \\(\\beta_0\\)의 역할을 한다.\n\n\n행렬로 표현한다면, \\(\\begin{bmatrix}X_k \\\\ \\dots \\end{bmatrix} = DFT = \\begin{bmatrix}x_n \\\\ \\dots \\end{bmatrix}\\)\n\n\\(x_k = DFT^{-1}X_k\\)\n\n\\(x_k\\) = bias, 관측값\n\\(DFT^{-1}\\): 설명변수, unitary라 \\(DFT^{-1} = DFT = DFT^*\\), symmetric, orthogonal(설명변수가 독립적이라 다중공선성이 존재하지 않는다.)\n\n다중공선성이 있으면 각 설명변수의 설명이 안 될 수도 있고 그 설명변수를 해석하기도 어려워짐.\n\n\\(X_k = \\beta\\), codfficient(푸리에 변환의 결과이다)\n\n\nDFT 행렬의 특징\n\n유니터리unitary 행렬, 즉, \\(DFT^* = DFT, DFT^*DFT = I\\)\n대칭symmetric 행렬 \\(\\to\\) 그렇기 떄문에 이 행렬의 켤레전치는 \\(i = \\sqrt{-1}\\) 대신 \\(i\\)를 넣은 것과 같음.\n\n\ninverse DFT는 \\(i = -i\\)를 넣은 행렬, 즉 DFT의 켤레전치 = inverse DFT\n\n\\(DFT = \\frac{1}{\\sqrt{N}}\\begin{bmatrix} 1 & 1 & 1 & \\dots & 1 \\\\ 1 & e^{-i\\frac{2\\pi}{N}1} & e^{-i\\frac{2\\pi}{N}1} & \\dots & e^{-i\\frac{2\\pi}{N}(N-1)} \\\\ 1 & e^{-i\\frac{2\\pi}{N}2} & e^{-i\\frac{2\\pi}{N}4} & \\dots & e^{-i\\frac{2\\pi}{N}(2(N-1)} \\\\ \\dots & \\dots & \\dots & \\dots \\\\ 1 & e^{-i\\frac{2\\pi}{N}(N-1)} & e^{-i\\frac{2\\pi}{N}2(N-1)} & \\dots & e^{-i\\frac{2\\pi}{N}(N-1)^2}\\end{bmatrix}\\)\n\nDFT = (1/√4)*DFT # 위의 정의 충족위해 1/sqrt(4)곱함\nDFT'DFT .|> round # 유니터리행렬임을 확인!\n\n4×4 Matrix{ComplexF64}:\n  0.0+0.0im  -0.0-0.0im   0.0-0.0im   0.0-0.0im\n -0.0+0.0im   0.0+0.0im  -0.0-0.0im   0.0-0.0im\n  0.0+0.0im  -0.0+0.0im   0.0+0.0im  -0.0-0.0im\n  0.0+0.0im   0.0+0.0im  -0.0+0.0im   0.0+0.0im"
  },
  {
    "objectID": "posts/Study/2023-05-10-EbayesThreshold.html",
    "href": "posts/Study/2023-05-10-EbayesThreshold.html",
    "title": "EbayesThresh: R Programs for Empirical Bayes Thresholding, Review",
    "section": "",
    "text": "EbayesThresh\nrefer: CRAN, 교수님 블로그"
  },
  {
    "objectID": "posts/Study/2023-05-10-EbayesThreshold.html#the-bayesian-model",
    "href": "posts/Study/2023-05-10-EbayesThreshold.html#the-bayesian-model",
    "title": "EbayesThresh: R Programs for Empirical Bayes Thresholding, Review",
    "section": "The Bayesian model",
    "text": "The Bayesian model\n\\(X_i \\sim N(\\mu_i,1)\\)의 관측값이 있음.\n베이지안 context에서 sparsity희소성의 개념은 parameter \\(\\mu_i\\)의 사전 분포prior distribution에 의해 모델링됨.\n논문에서는 독립적인 사전 분포를 가진 이 \\(\\mu_i\\)를 모델링하는데, 각각 가지고 있는 mixture는 다음과 같음.\n\\[f_{prior}(\\mu) = (1- w) \\delta_0 (\\mu) + w \\gamma(\\mu)\\]\n\\(\\gamma\\)는 0이 아닌 부분으로 fixed unimodal symmetric density2로 가정한다.\n\nprior=“lapace”를 설정하면 척도 매개변수scale parameter a > 0의 Laplace 밀도를 사용.\n아래의 식(\\(\\mu_i\\)의 density function)에서 a는 기본 0.5로 지정되고, 변경도 가능\n\n\\[\\gamma_a(\\mu) = \\frac{1}{2} a \\exp(-a|\\mu|)\\]\nprior=“schy”로 설정하면 a는 무시되고 \\(\\mu\\)에 대한 \\(\\gamma\\)의 분포가 아래와 같음\n\\[\\mu|Θ = \\theta \\sim N(0,\\theta^{-1} -1) \\text{with} Θ \\sim \\text{Beta}(\\frac{1}{2},1)\\]\n\\(u^{-2}\\)는 코시 분포 Cauchy dustribution3와 같은 weight를 가짐 - 아래 density를 quasi-Cauchy density라 부름4. - the combination of heavy tails5\n\\[\\gamma(\\mu) = (2\\pi)^{-1/2} \\{ 1-|mu| \\tilde{\\phi}(\\mu)\\emptyset(\\mu)\\}\\]"
  },
  {
    "objectID": "posts/Study/2023-05-10-EbayesThreshold.html#thresholding-rules",
    "href": "posts/Study/2023-05-10-EbayesThreshold.html#thresholding-rules",
    "title": "EbayesThresh: R Programs for Empirical Bayes Thresholding, Review",
    "section": "Thresholding rules",
    "text": "Thresholding rules\n\\(X \\sim N(\\mu,1)\\)라 가정할 때, \\(X=x\\)에서 \\(\\mu\\) 조건부 posterior 분포를 찾을 수 있음.\n\\(\\hat{\\mu}(x;w)\\)를 사후분포posterior distribution의 중앙값이라고 정의하자.\n모든 고정된 \\(w\\)에 대해 \\(\\hat{\\mu}(x;w)\\)의 추정규칙은 \\(|x| \\le t(w)\\)인 경우에만 \\(\\hat{\\mu}(x;w)= 0\\)이 되도록 임계값 속성을 갖는 \\(x\\)의 monotonic function이다.\nobservation이 있을때, 각 observation \\(X_i\\)에 각각 베이지안 절차를 적용해서 corresponding parameter \\(\\mu_i\\)의 추정치를 구할 수 있다.\ndefault set threshrule=“dll”은 이 추정치로 사후 중위수 \\(\\hat{\\mu}(x;w)\\)을 사용\n\n\\(X_i\\)가 독립적인 경우 정확한 베이지안 절차\n\\(X_i\\)가 정확하게 독립적이지 않으면 추정 절차에서 정보의 손실이 있지만, 너무 많은 의존성dependence이 없다면 방법은 최소한 합리적인 결과at least reasonable results를 제공할 것임."
  },
  {
    "objectID": "posts/Study/2023-05-10-EbayesThreshold.html#choosing-the-threshold",
    "href": "posts/Study/2023-05-10-EbayesThreshold.html#choosing-the-threshold",
    "title": "EbayesThresh: R Programs for Empirical Bayes Thresholding, Review",
    "section": "Choosing the threshold",
    "text": "Choosing the threshold\nEmperical Bayes approach의 핵심은 weight \\(w\\)를 mixing 하거나 임계값 \\(t(w)\\)와 동일하게 선택하는 것\n\\(X_i\\)가 독립적이라고 가정하면 marginal maximum likelihood approach에 의해 \\(w\\) 추정할 수 있음.\n\\(g = \\gamma \\star \\emptyset\\)가 있을때, \\(\\star\\)는 convolution 임. observation \\(X_i\\)의 주변 밀도는 아래와 같음\n\\[(1-w)\\emptyset(x) + wg\\]\n\\(w\\)의 marginal maximum likelihood estimator \\(\\hat{w}\\)를 marginal log likelihood 의 maximizer로 정의하고, 이는 임계값이 \\(t(w) \\le \\sqrt{2 logn}\\)을 만족하는 w에 대한 제약 조건에 따름.\n\\[\\ell(w) = \\sum^n_{i=1} \\log \\{ (1-w) \\emptyset (X_i) + wg(X_i)\\}\\]\nderivative도함수 \\(\\ell'(w)\\)은 \\(w\\)의 monotonic function이므로 함수 g가 각각의 경우에 대해 다루기 쉽기 때문에 그 근은 수치적으로 매우 쉽게 찾을 수 있음.\n임계값에서 Bound \\(\\sqrt{2 log n}\\)는 표본 크기 n의 샘플에 대하여 universal threshold라고 부른다.\n점근적으로, n개의 독립적인 \\(N(0,1)\\) 확률변수random variable 시퀀스의 최대 절대값임.\nuniversal threshold를 사용하는 경우 높은 확률로 모든 영점 신호 값이 정확하게 추정.\n임계값을 통해 가능한 신호의 경제성을 활용하려면 일반 임계값보다 더 큰 임계값을 고려할 필요가 없음.\n사후 중위수 임계값에 대한 대안은 다음과 같이 \\(\\tau_b(w) > 0\\) 값으로 정의되는 베이즈 인자 임계값을 사용할 수 있음.\n\\[P(\\mu \\ne 0 |X = \\tau_b(w)) = 0.5\\]"
  },
  {
    "objectID": "posts/Study/2023-05-10-EbayesThreshold.html#wavelet-thresholding-and-other-extensions",
    "href": "posts/Study/2023-05-10-EbayesThreshold.html#wavelet-thresholding-and-other-extensions",
    "title": "EbayesThresh: R Programs for Empirical Bayes Thresholding, Review",
    "section": "Wavelet thresholding and other extensions",
    "text": "Wavelet thresholding and other extensions\n논문의 동기는 웨이블릿을 사용한 함수 추정\n일반적으로 실제 신호의 웨이블릿 계수는 미세 해상도 척도에서는 희소하고 거친 척도에서는 밀도가벼운 척도에서는 밀도가 높음.\n따라서 레벨별 임계값 레벨을 조정하는 임계값 선택 방법을 개발하는 것이 바람직함.\n웨이블릿 사례에서 EbayedThresh는 경험적 베이즈 방법을 변환의 각 레벨에 개별적으로 적용하는 것임."
  },
  {
    "objectID": "posts/Study/2023-05-10-EbayesThreshold.html#a-simple-illustrative-example",
    "href": "posts/Study/2023-05-10-EbayesThreshold.html#a-simple-illustrative-example",
    "title": "EbayesThresh: R Programs for Empirical Bayes Thresholding, Review",
    "section": "A simple illustrative example",
    "text": "A simple illustrative example\n\nn = 100\n10개는 3.5이고 나머지는 0으로 가정.\n신호에 normal independent noise 추가\nEbayesthresh사용해서 underlying mean vector 추정\n\n\\(\\to\\) Ebayes 결과는 아래의 line처럼 나타남.\n\nset.seed(1)\nmu <- c( rep(0, 90), rep(3.5, 10) )\nx  <- rnorm(100, mu)\nplot(x)\nlines(ebayesthresh(x))\n\n\n\n\n\n90개의 0 중 대부분은 정확하게 추정되지만 3개는 영점으로 추정되지 않았음.\n반대로 3.5와 같아야 하는 10개의 값(마지막으로 표시할 10개)은 모두 영점이 아닌 것으로 추정\n그림을 자세히 조사하면 그림의 첫 번째 부분에서 가장 큰 두 데이터 점이 마지막 세그먼트에서 가장 작은 두 데이터 점과 값이 매우 유사해 보임.\n오른쪽 패널에는 마지막 다섯 점의 평균만 0이 아닌 희소 표본sparse sample에 대한 동일한 절차의 효과가 표시됨\n이 방법의 적응성adaptivity은 이제 모든 0 평균이 정확하게 추정된다는 사실에 의해 입증 가능.\nthe price paid는 영이 아닌 값의 더 큰 축소와 평활화가 이루어진다는 것이고, 그 중 두 값은 0으로 추정\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/Study/2023-05-10-EbayesThreshold.html#varying-sparsity-in-a-larger-sample",
    "href": "posts/Study/2023-05-10-EbayesThreshold.html#varying-sparsity-in-a-larger-sample",
    "title": "EbayesThresh: R Programs for Empirical Bayes Thresholding, Review",
    "section": "Varying sparsity in a larger sample",
    "text": "Varying sparsity in a larger sample\n\nm <-50\nmu <- sample( c( runif(m,-5,5), rep(0,10000-m) ) ) \nx <- rnorm(10000, mu)\ntt <- tfromx(x)\n\nroutine 인 tformx는 노이즈 표준 편차를 1로 가정하여 threshold의 empirical bayes를 찾음.\n임계값이 발견되면 임계값 루틴(tformx)을 사용하여 원래 데이터의 임계값을 하드 임계값hard threshold6으로 설정하여 \\(\\mu\\)의 추정치를 찾을 수 있음\n\nmuhat <- threshld(x,tt)\n\n\nplot(muhat,type='l')\n\n\n\n\n단일 호출single call ebayeshresh(x, sdev=1, threshrule=\"hard\")를 통해 얻을 수 있는 동일한 추정 muhat을 보면, 뚜렷하게 threshold를 알 수 있는 것은 아님\n\n선택한 임계값을 가장 작은 평균 제곱 오차를 생성하는 임계값과 비교\n다음 루틴에서는 길이 10000의 신호를 0이 아닌 값이 균일하게 분포된 (-5,5)로 구성\n임계값의 경험적 베이즈 선택 tebayes와 비교하여 하드 임계값에 대한 최소 평균 제곱 오차를 달성하는 임계값 tbest를 반환\n제곱합 오차 rebayes 및 rbest도 반환\nEbayesThresh에서 사용된 두 가지 루틴\n\n경험적 베이즈 임계값을 찾기 위한 루틴 tfromx\n하드 임계값을 적용하는 루틴 임계값\n\n\n\nebdem1 <- function(m)\n{\nset.seed(1)\nzz <- rnorm(10000)\nmu <- c( runif(m,-5,5), rep(0,10000-m) )\nx <-mu+zz\ntt <- tfromx(x)\ntvec <- seq(from = 0, to = 5, by = 0.1)\nrvec <- rep(NA, 51)\nfor ( j in (1:51)) rvec[j] <- sum( (threshld(x, tvec[j]) - mu )^2 ) \n    reb <- sum( (threshld(x, tt) - mu )^2 )\nrbest <- min(rvec)\ntbest <- mean ( tvec[rvec==rbest] )\nreturn(list(tebayes=tt, rebayes=reb, tbest=tbest, rbest=rbest))\n}\n\n예를 들어, 매개 변수 m은 매우 희박한sparse(m = 5) 신호를 완전히 조밀한dense(m = 10000) 신호를 생성하도록 변경될 수 있음.\n\n루틴 ebdem1의 출력에 대한 두 개의 플롯\n왼쪽 그림은 경험적 베이즈 선택 tabayes를 이상적이지만 실제로는 도달할 수 없는 임계값 tbest와 비교\n오른쪽 그림에서는 하드 임계값 추정치의 제곱합 오차를 두 임계값과 비교\n\n\n\n\nimage.png\n\n\nHard threshold 사용하는 이유\n\nThe reason that we use hard thresholding for the Empirical Bayes threshold, rather than the default posterior median estimator, is that allows an honest comparison with the threshold chosen to minimize error under hard thresholding.\n\n하드 임계값에서 오류를 최소화하기 위해 선택한 임계값과 honest comparison 을 허락하기 때문"
  },
  {
    "objectID": "posts/Study/2023-05-10-EbayesThreshold.html#the-posterior-median-thresholding-function",
    "href": "posts/Study/2023-05-10-EbayesThreshold.html#the-posterior-median-thresholding-function",
    "title": "EbayesThresh: R Programs for Empirical Bayes Thresholding, Review",
    "section": "The posterior median thresholding function",
    "text": "The posterior median thresholding function\nroutine7 postmed를 사용하여 임계값 함수를 그릴 수 있는데, 이 루틴은 주어진 데이터와 prior distribution에서 주어진 mixing weight에서 posterior median function을 찾음\n\\(w\\) 0.02에 대해, quasi-Cauchy prior를 사용하여 아래처럼 계산할 수 있음\n\nplot(postmed( seq(from=-8, to=8, by=0.1), w=0.02, prior=\"cauchy\"),type='l')"
  },
  {
    "objectID": "posts/Study/2023-05-10-EbayesThreshold.html#other-control-parameters-in-the-packages-routines",
    "href": "posts/Study/2023-05-10-EbayesThreshold.html#other-control-parameters-in-the-packages-routines",
    "title": "EbayesThresh: R Programs for Empirical Bayes Thresholding, Review",
    "section": "Other control parameters in the package’s routines",
    "text": "Other control parameters in the package’s routines\nebayesthresh(x, a=NA)           # Laplace prior, scale factor also estimated\nebayesthresh(x, prior=\"cauchy\") # quasi-Cauchy prior\nebayesthresh(x, a=NA, threshrule=\"mean\") # Laplace prior, scale factor estimated\nebayesthresh(x, a=NA, threshrule=\"hard\") # use posterior mean as estimator\nebayesthresh(x) # Laplace prior, scale factor estimated\nebayesthresh(x, a=0.2) # use hard threshold with estimated threshold\ntuniv <- sqrt(2 * length(x)) # Laplace prior with a=0.5 (default)\nthreshld(x, tuniv) # Laplace prior with a=0.2\nthreshld(x, tuniv, hard=FALSE)  # soft thresholding with universal threshold"
  },
  {
    "objectID": "posts/Study/2023-05-10-EbayesThreshold.html#empirical-bayes-thresholding-of-the-discrete-wavelet-transform",
    "href": "posts/Study/2023-05-10-EbayesThreshold.html#empirical-bayes-thresholding-of-the-discrete-wavelet-transform",
    "title": "EbayesThresh: R Programs for Empirical Bayes Thresholding, Review",
    "section": "Empirical Bayes thresholding of the discrete wavelet transform",
    "text": "Empirical Bayes thresholding of the discrete wavelet transform\n해당 패키지 지원이 되지 않음..이 버전에\nThe default action of the routine ebayesthresh.wavelet is to assume that the original signal is observed with independent Gaussian noise with mean zero and constant variance.\nebayeshresh.wavelet 루틴의 기본 동작은 원래 신호가 평균 0 및 상수 분산을 갖는 독립적인 가우스 노이즈로 관찰된다고 가정하는 것"
  },
  {
    "objectID": "posts/Study/2023-05-10-EbayesThreshold.html#the-estimated-thresholds",
    "href": "posts/Study/2023-05-10-EbayesThreshold.html#the-estimated-thresholds",
    "title": "EbayesThresh: R Programs for Empirical Bayes Thresholding, Review",
    "section": "The estimated thresholds",
    "text": "The estimated thresholds\n\\(N(0,\\hat{\\sigma^2_i})\\)를 가정했을때, 분포가 0에 가까울때(6번), std를 0이 아니라 1로 설정하는 게 합리적.\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/Study/2023-05-10-EbayesThreshold.html#the-stationary-noise-model",
    "href": "posts/Study/2023-05-10-EbayesThreshold.html#the-stationary-noise-model",
    "title": "EbayesThresh: R Programs for Empirical Bayes Thresholding, Review",
    "section": "The stationary noise model",
    "text": "The stationary noise model\nJohnstone과 Silverman(1997)에 따르면 원래 노이즈가 정상성을 띄고 있지만 상관관계가 있는 데이터에 웨이블릿 임계값 방법을 사용할 것을 고려.\n노이즈가 독립적인 것처럼 웨이블릿 임계값을 수행하지만 다른 수준에서 다른 노이즈 분산을 허용하는 것이 적절한 접근임을 보임.\n이것은 각 수준에서 개별적으로 만들어진 중앙값 절대 편차 함수를 사용하여 분산을 추정하는 것과 일치\nebayesthresh.wavelet 패키지의 the parameter value vscale=“level”로 수준에 따라 임계값이 계산되기 때문에 level이 coarser(거칠수록) 임계값이 더 커짐\n\nscale이 커질 수록 sparse희소성이 감소한다는 점을 밝힘\n\n백색소음8\n정상소음9\nOverall the stationary noise plots remove some moderately high frequency effects still present in the white noise plots.\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/Study/2023-05-10-EbayesThreshold.html#the-translation-invariant-wavelet-transform",
    "href": "posts/Study/2023-05-10-EbayesThreshold.html#the-translation-invariant-wavelet-transform",
    "title": "EbayesThresh: R Programs for Empirical Bayes Thresholding, Review",
    "section": "The translation-invariant wavelet transform",
    "text": "The translation-invariant wavelet transform\n경험적 베이즈 접근법을 수행하는 가장 직관적인 방법은 독립적인 시퀀스처럼 각 수준에서 계수 벡터를 임계값으로 취하는 것.\nAs discussed in detail in Johnstone and Silverman (2005), the most straightforward way of applying the empirical Bayes approach is to threshold the coefficient vector at each level as if it were an independent sequence."
  },
  {
    "objectID": "posts/Study/2023-05-10-EbayesThreshold.html#smoothing-an-image",
    "href": "posts/Study/2023-05-10-EbayesThreshold.html#smoothing-an-image",
    "title": "EbayesThresh: R Programs for Empirical Bayes Thresholding, Review",
    "section": "Smoothing an image",
    "text": "Smoothing an image\n\n경험적 베이즈 접근법은 사용된 변환에 상관없이 동일하게 적용 가능\n추정되는 함수 또는 이미지의 표현에서 희소성을 활용할 것이므로, 이러한 유형의 이미지의 표현에 더 구체적으로 적합한 사전이 사용된다면 훨씬 더 나은 결과가 기대됨"
  },
  {
    "objectID": "posts/Study/2023-05-10-EbayesThreshold.html#increasing-sparsity-along-the-sequence",
    "href": "posts/Study/2023-05-10-EbayesThreshold.html#increasing-sparsity-along-the-sequence",
    "title": "EbayesThresh: R Programs for Empirical Bayes Thresholding, Review",
    "section": "Increasing sparsity along the sequence",
    "text": "Increasing sparsity along the sequence\n\\(\\mu_i\\)가 0인 prior probability이 \\(i\\)가 증가할수록 같이 증가하는 것으로 합리적으로 고려된다는 점에서 \\(i\\)가 증가함에 따라 더 sparse해진다고 여겨지는 single sequence10 \\(\\mu_i\\)가 있다고 가정하자.\n예를 들어, \\(\\mu_i\\)는 시퀀스(sequence)의 각 항(term)11이 관심 대상인 함수 또는 현상12의 다른 크기(scale)의 측면을 묘사하는 large-scale을 설명하는 함수의 사전13에서 계수일 수 있다.이 사전에 있는 변수들에 의해 우리는 모델링이 되고 예측을 한다.\nnatural approach는 \\(\\mu_i\\)를 이전과 같은 형태의 prior distribution14을 가지지만 \\(i\\)에 의존하는 가중치 \\(w\\)를 가지고 있어서 아래와 같은 prior density15를 모델링16하는 것이다.\n\n각 항이 가지는 중요도를 고려하여 i에 따라 가중치가 부여\n“\\(\\mu_i\\)”는 i번째 항에 해당하는 계수(coefficient)를 의미\n이러한 사전 분포는 모델링에 사용되는 데이터 이전에 존재하는 정보를 반영하며, 이를 기반으로 모델을 구성하는 과정에서 업데이트함.\n\n\\[(1-w_i)\\delta(u) + w_i\\gamma(u)\\]\n여기서 \\(w_i\\)를 구하려면 log marginal likelihood 를 최대화하는 것으로 \\(\\hat{w}_i\\)를 선택하면 된다.\n\n단, \\(w_1 \\ge w_2 \\ge \\dots \\ge w_n\\)\n\n\\[\\ell(w_1, \\dots, w_n) = \\sum^n_{i=1} log\\{(1-w_i)\\emptyset(x_i) + w_i g(x_i)\\}\\]\n일단 가중치가 추정되면, \\(\\mu_i\\)를 각각 추정할건데 mixing parameter \\(w_i\\)를 가진 베이지안 모델에 근거한 임계값 규칙을 사용.\nroutine wmonfromx\n\n\\(w_i\\)에 해당하는 모든 임계값들이 \\(\\sqrt{2 log n}\\)에 의해 제한된다는 조건에 따라 추정을 진행.\n\n\nset.seed(1)\npp <- 1- ((1:2000)/2000)^0.25\nmu <- runif(2000, -7,7)*rbinom(2000,1,pp)\n\n\nplot(mu,type='l')\n\n\n\n\n이 signal에서 0이 아닌 값이 [-7,7] 사이에 균등하게 분포되어 있음.\n증가하는 sparsity희소성은 특정 지점까지 0이 아닌 점의\n\n예를 들어, 0이 아닌 값의 절반 이상이 the first quarter of the sequence에서 발생함.\n이는 시퀀스의 처음 250개의 위치에서는 123개의 0이 아닌 값이 발생하지만, 마지막 250개의 위치에서는 단 8개만 발생한다는 것을 보여줌.\n시계열 데이터의 희소성과 노이즈가 추가된 데이터로부터 추정된 monotone weights를 사용한 추정 결과(아래)를 보여줌\n\n\nwmon   <- wmonfromx(x)\nthresh <- tfromw(wmon)\nmuhat  <- postmed(x,wmon)\n\n\npar(mfrow = c(1,3))\nplot(muhat)\nplot(thresh,type='l')\nplot(wmon)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMonotone weights는 Bayesian adaptive regression spline (BARS) 모형에서 사용되는 가중치(weight)입니다. BARS 모형은 회귀분석 모형으로, 자료에 대한 적합성을 높이기 위해 여러 개의 스플라인(spline) 함수를 결합하여 모델을 구성합니다. 이때, 스플라인 함수의 가중치가 작을수록 해당 함수는 모델에 미치는 영향력이 작아지며, 가중치가 크면 해당 함수의 영향력이 커집니다.\nMonotone weights는 이 가중치를 설정하는 방법 중 하나로, 스플라인 함수가 증가하는 방향으로만 움직이도록 제한합니다. 따라서 monotone weights를 사용하면 모델이 단조적으로 증가하는 형태를 띄게 되며, 시계열 데이터와 같이 시간적인 변화를 반영해야 하는 경우에 유용하게 사용됩니다.\n\n\n위에서 추정된 weights가 구간마다 일정한 값을 유지하다가 (constant), 갑자기 내려가는 (downward jumps) 구간이 있는 것을 확인할 수 있는데 이 추정 결과는 Monotonicity constraint가 적용된 시계열 데이터에서 특징적으로 나타남.\nMonotonicity constraint가 적용된 시계열 데이터에서는, 점프가 발생하는 위치가 인공적인 “levels”로 시계열을 분할하는 데 사용될 수 있음.\n:::{.callout-note} Monotonicity constraint란, 특정 변수의 값이 증가하거나 감소하는 방향으로 제한하는 제약조건입니다. 이는 데이터에서 관찰되는 특정 패턴을 모델링하고자 할 때 유용하게 사용됩니다.\n시계열 데이터에서는, 시간의 흐름에 따라 값이 증가하거나 감소하는 경향성이 있는 경우가 많습니다. 이때, Monotonicity constraint를 적용하면, 모델링에 더 적합한 결과를 얻을 수 있습니다.\n예를 들어, 시계열 데이터에서는 Monotonicity constraint를 사용하여 값이 증가하는 경향성이 있는 변수를 모델링할 수 있습니다. 이때, Monotonicity constraint를 적용하면 모델링에 불필요한 노이즈가 제거되고, 보다 정확한 예측 결과를 얻을 수 있습니다.\n예) 주식시장에서 어떤 종목이 상승하고 있을 때 상승한다는 monotonicity contraint를 적용하면 더 정확한 분석이 가능:::"
  },
  {
    "objectID": "posts/Study/2023-05-10-EbayesThreshold.html#parametric-dependence",
    "href": "posts/Study/2023-05-10-EbayesThreshold.html#parametric-dependence",
    "title": "EbayesThresh: R Programs for Empirical Bayes Thresholding, Review",
    "section": "Parametric dependence",
    "text": "Parametric dependence\nsequence를 따라 감소하도록 제한된 weight고려\nPossibility\n\n상수 \\(c_i\\)를 알고 있고,\n가중치가 \\(c_i\\)에 비례하여 비례proportional 상수를 추정할 수 있기를 바라는 것\n\nEbayesThresh 패키지의 zetafromx 루틴으로 구현\n\nbasic model = \\(w_i = c_i \\zeta\\), \\(c_i\\)는 상수\n\\(w_{lo}\\)를 가중치 \\(t(w{lo}=\\sqrt{2 log n}\\)을 가지는 가중치로 생각.\n\\(w_{lo} \\le w_i \\le 1\\)으로 조건 주리 위해 아래 모델을 가정\n\n\\[w_i(\\zeta) = median \\{ w_{lo} , c_i \\zeta, 1\\}\\]\n\n\\(\\zeta\\)는 비례 상수가 아니라 가중치 파라미터가 됨.\n\\(\\gamma\\)와 \\(\\emptyset\\)을 convolution한 것을 \\(g\\)라 놓고, 아래처럼 marginal log likelihood function으로 \\(\\zeta\\)를 구할 수 있음.\n\n\\[\\ell(\\zeta) = \\sum_i log[ \\{1- w_i (\\zeta) \\} \\emptyset (z_i) + w_i(\\zeta) \\gamma (z_i)]\\]\n\nwts   <- zetafromx(x, cs=1/(1:2000))$w #   find weights\nmuhat <- postmed(x, wts) #   carry out estimation\nthresh<- tfromw(wts) #   find thresholds\n\n위처럼 역수 취해서 감소하게 만들어서 routine 이용하여 \\(\\ell(\\zeta)\\)찾을 수 있음"
  },
  {
    "objectID": "posts/Study/2023-04-04-nomalized graph laplacian.html",
    "href": "posts/Study/2023-04-04-nomalized graph laplacian.html",
    "title": "Nomalized Graph Laplacian",
    "section": "",
    "text": "Graph Laplacian\n\nhttps://en.wikipedia.org/wiki/Laplacian_matrix\n\\(L = D-W\\)\n\\(D\\) is a diagonal degree matrix.\n\\(W\\) is a weight matrix.\n\\(\\tilde{L} = D^{-1/2}LD^{-1/2}\\)\n\\(= D^{-1/2}(D-W)D^{-1/2}\\)\n\\(= D^{-1/2}DD^{-1/2}-D^{-1/2}WD^{-1/2}\\)\n\\(= I-D^{-1/2}WD^{-1/2}\\)\n\\(I\\) is an identity matrix.\n\\(\\tilde{L}\\) nomalizes weights using \\(D^{-1/2}WD^{-1/2}\\) by degree of each node and retain the information of graph structiure.\n\\(0 < L< 1\\)"
  },
  {
    "objectID": "posts/Study/2023-04-23-lebesque_decompposition_therem.html",
    "href": "posts/Study/2023-04-23-lebesque_decompposition_therem.html",
    "title": "Lebesgue’s decomposition theorem",
    "section": "",
    "text": "Existing Method Review\n\nLebesgue’s decomposition theorem\n\n르벡 분해 이론\n\n\\(\\mathcal{v}\\)는 아래와 같이 분해할 수 있다.\n\n\\(\\mathcal{v = v_{count} + v_{sing} + v_{pp}}\\)\n\n\\(\\mathcal{v_{count}}\\)는 절대 연속함수\n\\(\\mathcal{v_{sing}}\\)는 singular 연속 함수\n\\(\\mathcal{v_{pp}}\\)는 이산형 확률변수\n\n\n절대연속 메져는 Radon–Nikodym theorem에 의해 분해되고, 이산 메져는 쉽게 이해된다(?)"
  },
  {
    "objectID": "posts/Study/2023-04-12-.html",
    "href": "posts/Study/2023-04-12-.html",
    "title": "Self-onsistent estimator",
    "section": "",
    "text": "Existing Method Review\n\nLebesgue’s decomposition theorem\nIn this section, we propose a procedure using STGCN by coupling a concept of self-consistency (Tarpey and Flury, 1996; Lee and Meng, 2005) and a thresholding of periodogram by Emperical beyes Thresholding. First, we consider the self-consistency principle for estimating(or forecasting) when facing missing data. Tarpey and Flury (1996) introduced the self-consistency as a fundamental concept in statistics, which is inspired by Hastie and Stuetzle (1989) for developing principal curves. Further, Lee and Meng (2007) considered a self-consistent regression estimator with incomplete data. They proposed an estimate \\(\\hat{f}\\) of the underlying obs function \\(f\\) given observed data \\(x_{obs}\\) that is the solution of the following self-consistent equation\n\\[E(\\hat{f}_{com} | x_{obs}, f = \\hat{f}_{obs}) = \\hat{f}_{obs}\\]\nwhere \\(\\hat{f}\\) denotes an estimate of \\(f\\) based on the hypothetical complete data \\(x_{com} = (x_{obs},x_{mis})\\) and \\(x_{mis}\\) is missing data, and \\(E(·)\\) denotes the expectation operator. We note that \\(x_{mis}\\) is not available in practice. Moreover, Lee and Meng (2007) suggested that the above equation provides a way to obtain a “best” incomplete-data estimator \\(\\hat{f}\\) by simply using the corresponding obs complete-data procedure that computes \\(\\hat{f}\\). By considering that the above equation does not depend on the method for estimation, it can be applicable for STGCN procedure with missing data. In practice, the missing values \\(\\{ f(n) : n \\in M \\}\\) are not available, and thus, given imputed values \\(\\{ \\tilde{f}(n) : n \\in M \\}\\), we obtain an estimated complete dataset \\(\\{ \\hat{f}(n)\\} = \\{ f(n) : n \\in O \\} \\cup \\{ \\tilde{f}(n) : n \\in M \\}\\).\nA simple and fasr way to implement that may be an interactive algorithm with updating \\(\\{ \\tilde{f}^{(l)} (n) : n \\in M \\}\\) by STGCN."
  },
  {
    "objectID": "posts/Study/2023-02-02-ch12.2_Weakly Stationary Graph Processes.html",
    "href": "posts/Study/2023-02-02-ch12.2_Weakly Stationary Graph Processes.html",
    "title": "Chap 12.2: Weakly Stationary Graph Processes",
    "section": "",
    "text": "Chap 12.2: Weakly Stationary Graph Processes\n\n\nusing LinearAlgebra, FFTW\n\nSimultaneiusly Diagnalizable\n\nmatrix A, B가 고유분해 가능할때, \\(B = V_B \\Lambda_B V_B^{-1}, A = V_A \\Lambda_A V_A^{-1}\\)와 같이 표현할 수 있고, 특히 \\(V_A = V_B\\)라면,\n\n\\(AB = V \\Lambda_A V^{-1}V \\Lambda_B V^{-1}, BA = V \\Lambda_B V^{-1}V \\Lambda_A V^{-1}\\)\n\\(V \\Lambda_A \\Lambda_B V^{-1} = AB = BA = V \\Lambda_B \\Lambda_A V^{-1}\\)\n\n\\(\\Lambda\\)는 diagonal matrix라 가능하다.\n\n\n\nCommute\n\n\\(AB = BA\\) 가 가능할때,\n\nShift invariant filter\n\n\\(z h(z) = h(z)z, h(z) = h_0 Z^0 + h_1 z^{-1} + \\dots + h_{N-1}z^{-(N-1)} \\to\\) z-transform\n\\(Bh(B) = h(B)B, H = h(B) = h_0 B^0 + h_1 B^1 + \\dots h_{N-1}B^{N-1} \\to\\) cycluc shift\n\n\\(h(B) = \\frac{1}{3} + \\frac{1}{3}B + \\frac{1}{3} B^2\\)\n\n\\(\\to\\) matrix : 성질 1. 자료 2. 변환\n\n\n\n\\(x\\)가 정상시계열이라면, 모든 \\(l = 0,1,2,\\dots\\)에 대하여 \\(E(XX^H) = E((B^l X)(B^lX)^H)\\)가 성립한다.\n\\(x\\)가 정상시계열이라면, \\(X = Hn\\)(단, \\(n\\)은 white noise를 만족하는 shift invariant operator)\n\n\\(H = \\sum^{N-1}_{l=0} h_l B^l\\) 이 존재한다.\n\n\\(\\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\dots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix}\\frac{1}{3} &\\frac{1}{3} & 0 & 0 & \\dots \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & \\dots & \\dots \\\\ \\dots & \\dots & \\dots & \\dots \\\\ \\dots &\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\end{bmatrix} \\times \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\dots \\\\ \\epsilon_n \\end{bmatrix}\\)\n\\(x_1 = (\\epsilon_1 + \\epsilon_2) \\times \\frac{1}{3}\\)\n\\(x_2 = (\\epsilon_1 + \\epsilon_2 + \\epsilon_3) \\times \\frac{1}{3}\\)\n\\(\\dots\\)\n\\(x_t = \\frac{1}{3}\\epsilon_t + \\frac{1}{3}\\epsilon_{t-1} + \\dots + \\frac{1}{3}\\epsilon_{t-N}\\)\n\\(E(e^xt)\\)\n\n\\(E(n) = 0, E(nn^H) = I\\)\n\n\\(E(XX^T) = E(Hnn^TH^T) = HE(nn^T)H = HH^T\\)\n\\(n = \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\dots \\\\ \\epsilon_n \\end{pmatrix}\\)\n\\(nn^T = \\begin{bmatrix} \\epsilon_1\\epsilon_1 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 \\\\ \\epsilon_2\\epsilon_1 & \\epsilon_2\\epsilon_2 & \\epsilon_2\\epsilon_3 \\\\ \\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3\\epsilon_3 \\end{bmatrix}\\)\n\\(E(\\begin{bmatrix} \\epsilon_1\\epsilon_1 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 \\\\ \\epsilon_2\\epsilon_1 & \\epsilon_2\\epsilon_2 & \\epsilon_2\\epsilon_3 \\\\ \\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3\\epsilon_3 \\end{bmatrix}) = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} = I\\)\nex1)\n\\(x(1,2,3) \\xrightarrow[]{h} x^2(1,4,9) = \\tilde{x}\\)\n\\(h(x) = x^2\\)\nex2)\n\\(x(1,2,3) \\xrightarrow[]{h} 2x(2,4,6) = x^2\\)\n\\(h(x) = 2x\\)\n\nx = [1,2,3]\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\n\nH = [2 0 0 \n    0 2 0 \n    0 0 2 ] \n\n3×3 Matrix{Int64}:\n 2  0  0\n 0  2  0\n 0  0  2\n\n\n\nH*x\n\n3-element Vector{Int64}:\n 2\n 4\n 6\n\n\n\nB = [0 1 0 \n    0 0 1 \n    1 0 0 ] \n\n3×3 Matrix{Int64}:\n 0  1  0\n 0  0  1\n 1  0  0\n\n\n\nH*B\n\n3×3 Matrix{Int64}:\n 0  2  0\n 0  0  2\n 2  0  0\n\n\n\nB*H\n\n3×3 Matrix{Int64}:\n 0  2  0\n 0  0  2\n 2  0  0\n\n\n\\(HB = BH\\)\n\nB*x\n\n3-element Vector{Int64}:\n 2\n 3\n 1\n\n\n\nH*B*x\n\n3-element Vector{Int64}:\n 4\n 6\n 2\n\n\nex3)\n\\(x(1,2,3) \\xrightarrow[]{h} \\tilde{x}(1,0,0)\\)\n\nH = [1 0 0 \n    0 0 0 \n    0 0 0 ] \n\n3×3 Matrix{Int64}:\n 1  0  0\n 0  0  0\n 0  0  0\n\n\n\nx = [1,2,3]\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\n\nB = [0 1 0 \n    0 0 1 \n    1 0 0 ] \n\n3×3 Matrix{Int64}:\n 0  1  0\n 0  0  1\n 1  0  0\n\n\n\nH*B\n\n3×3 Matrix{Int64}:\n 0  1  0\n 0  0  0\n 0  0  0\n\n\n\nB*H\n\n3×3 Matrix{Int64}:\n 0  0  0\n 0  0  0\n 1  0  0\n\n\n\\(HB \\neq BH\\)\n\nH*x\n\n3-element Vector{Int64}:\n 1\n 0\n 0\n\n\n\nB*x\n\n3-element Vector{Int64}:\n 2\n 3\n 1\n\n\n\nH*B*x\n\n3-element Vector{Int64}:\n 2\n 0\n 0\n\n\n\nB*H*x\n\n3-element Vector{Int64}:\n 0\n 0\n 1\n\n\n\nZ-transform\nhttps://en.wikipedia.org/wiki/Z-transform\n어떠한 연속함수 \\(f(x)\\)가 있을때 아래와 같은 변환을 정의할 수 있다.\n\n\\(\\int^{\\infty}_{-\\infty} f(x) \\times e^{-\\text{ 복소수 }x} dx \\sim E(e^{-tx}) \\to\\) 라플라스\n\\(\\int^{\\infty}_{-\\infty}f(x) e^{- \\text{ 순허수 }x} dx \\to\\) 퓨리에 변환\n\n\n여기서 \\(f(x)\\)가 확률밀도함수를 의미할 수도 있으나 signal을 의미할 수도 있다.\nsignal을 의미하는 경우 중 특히 이산 signal을 의미할 수도 있다.\n예를 들어서 \\(f(0) = x_0, f(1) = x_1 , \\dots\\)\n\n이 경우 라플라스 transform은 \\(\\int^{\\infty}_{-\\infty}f(t) e^{-st}dt\\),\n이산형이면 \\(\\sum^{\\infty}_{t=0} x(t) e^{-st} = \\sum^{\\infty}_{t=0} x(t) z^{-t}\\)\n\n단, \\(e^{s} = z\\)\n\n\n\n\\(x\\)가 정상시계열이라면, 임의의 \\(n\\)에 대하여 \\(x = Hn\\)을 만족하는 shift invariant operator H가 항상 존재한다.\n\\(C_x = E(XX^H) = HH^H\\)라 표현 가능, \\(H,B\\)는 같은 고유행렬을 가진다. \\(\\to\\) \\(C_x\\)는 \\(B\\)와 같은 고유행렬을 가진다. \\(\\to\\) simultaneously diagonalizable 도 만족\n\\(C_x = \\psi \\times \\text{ diagonal matrix } \\times \\psi^H = DFT^H \\times \\text{ diagonal matrix } \\times DFT\\)\n\n\\(DFT^H = DFT\\)니까 순서는 상관이 없음\n\n\\(x\\)가 정상시계열이라면 \\(C_x\\)는 DFT행렬로 대각화가 가능하다.\n\\(C_x = E(XX^T)\\)\n\\(X_1,X_2,X_3 = X\\)일 떄, \\(cov = \\begin{bmatrix} cov(x_1,x_1) & cov(x_1,x_2) & \\dots \\\\ cov(x_2,x_1) & cov(x_2,x_2) & \\dots \\\\ cov(x_3,x_1) & cov(x_3,x_2) & \\dots\\end{bmatrix}\\)\n\\(cov(x_1,x_2) = E(x_1,x_2) - E(x_1)E(x_2) = E(x_1x_2) - 0\\)\n\\(cov(x) = \\begin{bmatrix} E(x_1,x_1) & E(x_1,x_2) & \\dots \\\\ E(x_2,x_1) & E(x_2,x_2) & \\dots \\\\ E(x_3,x_1) & E(x_3,x_2) & \\dots\\end{bmatrix}\\)\n그런데 \\(XX^T = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\\\dots\\end{bmatrix}\\begin{bmatrix} x_1 & x_2 & x_3 & \\dots\\end{bmatrix} = \\begin{bmatrix} x_1x_1 & x_1x_2 & x_1x_3\\\\x_2x_1 & x_2x_2 & x_2x_3 \\\\x_3x_1 & x_3x_2 & x_3x_3 \\end{bmatrix}\\)\n\\(E(XX^T) = E(\\begin{bmatrix} x_1x_1 & x_1x_2 & x_1x_3\\\\x_2x_1 & x_2x_2 & x_2x_3 \\\\x_3x_1 & x_3x_2 & x_3x_3 \\end{bmatrix}) = cov(X) = C_x\\)"
  },
  {
    "objectID": "posts/Study/2023-02-05-ch12_2_3_Power Spectral Density and its Estimators.html",
    "href": "posts/Study/2023-02-05-ch12_2_3_Power Spectral Density and its Estimators.html",
    "title": "Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "",
    "text": "Chap 12.2 ~ 3: Power Spectral Density and its Estimators\n\\(\\star\\) 정상시계열은 유한차수의 ARMA로 근사할 수 있다.\n\\(C_x = E(XX^\\top)\\)에 ACF모든 정보가 함유되어 있고, 이 떄 \\(X\\)는 확률벡터이다.\n\\(XX^\\top = \\begin{bmatrix} X_t X_t & X_t X_{t-1} & \\dots \\\\ X_{t-1} X_t & X_{t-1} X_{t-1} & \\dots \\\\ \\dots & \\dots & \\dots \\end{bmatrix}\\)\n또한 이 \\(C_x\\)는 \\(C_x = Vdiag(p)V^{H}\\)로 분해 가능 \\(\\to\\) 양정치 행렬, 고유분해 가능\nSpectral analysis \\(C_x\\)에서 \\(p\\)를 특정한 뒤 \\(p\\)에서 \\(C_x\\) 해석하는 방법론\n\\(p = E((V^{H}X))^2\\)\n\\(\\to V^{H}_X = DFT\\times X\\)이므로\n\\(DFT: \\sin, \\cos\\) 주기의 조합으로 모든 시계열 설명 가능\n\\(\\star\\) graph shift operator, GSO, \\(S = V\\Lambda V^{H}\\)\n\\(\\tilde{x} = GFT{x} = V^Hx\\)"
  },
  {
    "objectID": "posts/Study/2023-02-05-ch12_2_3_Power Spectral Density and its Estimators.html#p-표현식",
    "href": "posts/Study/2023-02-05-ch12_2_3_Power Spectral Density and its Estimators.html#p-표현식",
    "title": "Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "\\(p\\) 표현식",
    "text": "\\(p\\) 표현식\n\n\\(C_x = Vdiag(p)V^H \\sim diag(V^HC_xV), C_x = E(XX^H) \\sim \\frac{1}{R}\\sum^R_{r=1} x_i x_r^H\\)\n\n\n\\(\\hat{p}_{cg} = diag(V^H \\hat{C}_x V) := diag[V^H[\\frac{1}{R}\\sum^R_{r=1} x_r x_r^H] V]\\)\n\\(\\hat{p}_{cg} = diag(V^H \\hat{C}_x V) = diag[V^H x_r x_r^H V]\\)\n\n어차피 대각선 원소에 정보가 있어서 대각 행렬로 산출되지 않았을때, 대각 행렬이 아닌 원소 버려도 영향없음\n\n\n\n\\(p=E[(V^Hx)^2] \\sim p=E[(V^Hx)^2] \\sim \\frac{1}{R}\\sum^R_{i=1}|V^Hx_r|^2\\)\n\n\n\\(\\hat{p}_{pg} = \\frac{1}{R}\\sum^R_{r=1} |V^H x_r |^2\\)\n\\(\\hat{p}_{cg} = |V^H x_r |^2\\)\n\n퓨리에 변환에서 \\(\\tilde{x} = V^H x\\) 결과에 절대값 취하고 제곱한 것과 같다.\n\n\n\n\\(C_x = \\sum^N_{i=1} p_ivec(v_i,V^H_i) = G_{np}P \\sim C_x = V diag(p)V^H, \\hat{C}_x G_{np}p\\)\n\n\n3의 \\(p\\)가 회귀에서 \\(\\beta\\) 역할, \\(G_{np}\\) 회귀에서 설명변수 역할.\n\\(G_{np} V^* \\otimes V\\), 크로네커 곱 \\(\\begin{pmatrix}A & C \\\\ B & D \\end{pmatrix}\\begin{pmatrix}A & B & C \\\\ D & E & F \\\\G & H & I \\end{pmatrix}\\begin{pmatrix} AA & AB & AC & BA & BB & BC &\\dots \\\\ \\dots\\end{pmatrix}\\)\n\n\nA = [1 3\n    4 5]\nvec(A)\n\n4-element Vector{Int64}:\n 1\n 4\n 3\n 5\n\n\n\nB = [1 2 3\n    4 5 6\n    7 8 9]\n\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\n\n\nkron(A,B) # 크로네커 곱  kronecker product\n\n4×4 Matrix{Int64}:\n  0   5   0  10\n  6   7  12  14\n  0  15   0  20\n 18  21  24  28\n\n\n\nC = [1 2 3]\nD = [2 4 6\n    2 5 7]\n\n2×3 Matrix{Int64}:\n 2  4  6\n 2  5  7\n\n\n\nhcat([kron(C[:,i],D[:,i]) for i in 1:size(C)[2]]...)\n\n2×3 Matrix{Int64}:\n 2   8  18\n 2  10  21\n\n\n\ncolumnwise_kron = \n(C,D) -> hcat([kron(C[:,i],D[:,i]) for i in 1:size(C)[2]]...) # column-wise kronecker product\n\n#11 (generic function with 1 method)\n\n\n\ncolumnwise_kron(C,D)\n\n2×3 Matrix{Int64}:\n 2   8  18\n 2  10  21"
  },
  {
    "objectID": "posts/Study/2023-02-05-ch12_2_3_Power Spectral Density and its Estimators.html#property-12.1",
    "href": "posts/Study/2023-02-05-ch12_2_3_Power Spectral Density and its Estimators.html#property-12.1",
    "title": "Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "Property 12.1",
    "text": "Property 12.1\n\\(f_T = f_x * f_y \\to M_T = M_x \\bullet M_y\\)\n마치 \\(f_x\\)를 \\(M_x\\)로 바꾸고 \\(f_Y\\)를 \\(M_Y\\)로 바꾸고 \\(f_T\\)를 \\(M_T\\)로 바꾸고 \\(*\\)연산은 \\(\\bullet\\) 연산으로 바꾸면 되는듯이 보이고, 실제로 그렇다.\n\\(Hx = h * x \\to Hx = \\tilde{h} \\bullet \\tilde{x}\\)\ntime domain 에서 convolution 연산은 freq-domain 에서 * 연산과 같음\n예시: X의 pdf 와 Y의 pdf를 각각 f,g 라고 하자. 그리고 mgf 를 각각 F,G 라고 하자.\ntime domain: X+Y의 pdf는 conv(f,g)\nfreq domain: X+Y의 mgf는 F*G\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nhttps://dlsun.github.io/probability/sums-continuous.html\n\nSums of Continuous Random Variables\n\n\n\nimage.png\n\n\nhttps://en.wikipedia.org/wiki/Convolution_theorem\n\n\nConvolution Theorem\nIn mathematics, the convolution theorem states that under suitable conditions the Fourier transform of a convolution of two functions (or signals) is the pointwise product of their Fourier transforms. More generally, convolution in one domain (e.g., time domain) equals point-wise multiplication in the other domain (e.g., frequency domain). Other versions of the convolution theorem are applicable to various Fourier-related transforms.\n\n\n\nimage.png\n\n\nhttps://faculty.math.illinois.edu/~hildebr/370/370mgfproblems.pdf\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/Study/2023-02-05-ch12_2_3_Power Spectral Density and its Estimators.html#r의-개념",
    "href": "posts/Study/2023-02-05-ch12_2_3_Power Spectral Density and its Estimators.html#r의-개념",
    "title": "Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "\\(R\\)의 개념",
    "text": "\\(R\\)의 개념\n확률변수\n예) 동전 던지기를 한다고 했을때 사건, event \\(\\to\\) H 앞, T 뒤\n확률벡터\n예) 동전 던지기를 두 번 했을 떄, 나올 수 있는 집합,\n\n\\(HH\\)\n\\(HT\\)\n\\(TH\\)\n\\(TT\\)\n\n동전을 100회 던질때, 이떄 각 사건은 독립.\n\\(X = \\{ X_1, X_2, \\dots, X_{100}\\}\\)\n\\(E(X_{77}) = 0.5\\)일 것이다.\n이게 정말 \\(0.5\\)인지 시뮬레이션으로 확인해본다면,\n\n100회의 평균을 제시하는 방법(R=1)\nsimulation을 N번 하여 그 중 77번째의 값을 평균내는 방법(R=N)\n\n이 있는데, 2번의 방법은 현실적으로 불가능. 따라서 1번을 제시하는 경우가 많다.\n\nlet \n    N=100\n    n=1\n    p=0.5\n    X=rand(Binomial(n,p),N)\n    \"\"\"\n    mean: $(n*p),simulation: $(mean(X))\"\"\"\nend\n\n\"mean: 0.5,simulation: 0.49\"\n\n\n- 1번의 방법\n\nhistogram(rand(Bernoulli(0.5),10000),bins=0:0.01:1.01)\n\n\n\n\n\n\n\nimage.png\n\n\n\\(R\\) = 1\n\n실제로, 한 관측치에서 시계열 하나만 갖을 수 있고, 여러개의 시계열을 가진다는 것은 비현실적이다.\n책에서 돌린 \\(R=100\\)의 개념은 시뮬레이션으로서 가능,\n\n왼쪽 위 사진: \\(R=1\\)때 분산이 크고, \\(R=100\\)일때 TRUE값이 잘 맞음\n왼쪽 아래 사진: \\(R=1\\)일때 분산이 작아지고, R=100일때 TRUE값과 편차bias가 생겼다.\n현실적으로 \\(R=1\\)만 얻어지기 때문에 위(periodogram)보다 아래(Win. Period) 방법을 쓰는 것이 더 낫다고 저자는 제안함\n\n\n\\(\\star R=1000\\) 일때도 동일하다고 말하기 위해서 정상의 개념이 필요하다, 정상성을 띄고 있어야 시계열을 많이 가지고 있어도 일정하다고 말할 수 있기 때문"
  },
  {
    "objectID": "posts/Quarto_tip/index.html",
    "href": "posts/Quarto_tip/index.html",
    "title": "Quarto tip",
    "section": "",
    "text": "About tips of quarto blog"
  },
  {
    "objectID": "posts/Quarto_tip/2023-01-02-quarto_tips.html",
    "href": "posts/Quarto_tip/2023-01-02-quarto_tips.html",
    "title": "quarto blog tips",
    "section": "",
    "text": "note, tip, warning, caution, and important.\n\n\ncallout\nThere are three types of callouts.\n:::{.callout-note}\nNote that there are five types of callouts, including:\n`note`, `tip`, `warning`, `caution`, and `important`.\n:::\n\n:::{.callout-tip}\n## Tip With Caption\n\nThis is an example of a callout with a caption.\n:::\n\n\ncollapse\n\n\n\n\n\n\n\ntype\ninfo\n\n\n\n\ndefault\nThe default appearance with colored header and an icon.\n\n\nsimple\nA lighter weight appearance that doesn’t include a colored header background.\n\n\nminimal\nA minimal treatment that applies borders to the callout, but doesn’t include a header background color or icon.\n\n\n\n::: {.callout-note appearance=\"simple\"}\n\n## Pay Attention\n\nUsing callouts is an effective way to highlight content that your reader give special consideration or attention.\n\n:::\n\n\nLists\nMarkdown Syntax Output\n* unordered list\n    + sub-item 1\n    + sub-item 2\n        - sub-sub-item 1\n1. ordered list\n2. item 2\n    i) sub-item 1\n         A.  sub-sub-item 1\n\n\nVideo\n\n\n\nSub figures\n::: {#fig-elephants layout-ncol=2}\n\n![Surus](surus.png){#fig-surus}\n\n![Hanno](hanno.png){#fig-hanno}\n\nFamous Elephants\n:::\n\n\nFigure Panels\n::: {layout-ncol=2}\n![Surus](surus.png)\n\n![Hanno](hanno.png)\n:::"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html",
    "title": "PyTorch ST-GCN Dataset",
    "section": "",
    "text": "PyTorch Geometric Temporal Dataset\nhttps://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/dataset.html#module-torch_geometric_temporal.dataset.chickenpox\n논문\n|Dataset|Signal|Graph|Frequency|𝑇|ㅣ𝑉ㅣ | |:–:|:–:||:–:||:–:| |Chickenpox Hungary|Temporal|Static|Weekly|522|20| |Windmill Large|Temporal|Static|Hourly|17,472|319| |Windmill Medium|Temporal|Static|Hourly|17,472|26| |Windmill Small|Temporal|Static|Hourly|17,472|11| |Pedal Me Deliveries|Temporal|Static|Weekly|36|15| |Wikipedia Math|Temporal|Static|Daily|731|1,068| |Twitter Tennis RG|Static|Dynamic|Hourly|120|1000| |Twitter Tennis UO|Static|Dynamic|Hourly|112|1000| |Covid19 England|Temporal|Dynamic|Daily|61|129| |Montevideo Buses|Temporal|Static|Hourly|744|675| |MTM-1 Hand Motions|Temporal|Static|1/24 Seconds|14,469|21|"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#chickenpoxdatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#chickenpoxdatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "ChickenpoxDatasetLoader",
    "text": "ChickenpoxDatasetLoader\nChickenpox Hungary\n\nA spatiotemporal dataset about the officially reported cases of chickenpox in Hungary. The nodes are counties and edges describe direct neighbourhood relationships. The dataset covers the weeks between 2005 and 2015 without missingness.\n\n데이터정리\n\nT = 519\nN = 20 # number of nodes\nE = 102 # edges\n\\(f(v,t)\\)의 차원? (1,)\n시간에 따라서 Number of nodes가 변하는지? False\n시간에 따라서 Number of nodes가 변하는지? False\nX: (20,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (20,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 20\n\nvertices are counties\n\n-Edges : 102\n\nedges are neighbourhoods\n\n- Time : 519\n\nbetween 2004 and 2014\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import  ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset(lags=1)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n519\n\n\n\n(data[0][1]).x.type,(data[0][1]).edge_index.type,(data[0][1]).edge_attr.type,(data[0][1]).y.type\n\n(<function Tensor.type>,\n <function Tensor.type>,\n <function Tensor.type>,\n <function Tensor.type>)\n\n\n\nmax((data[4][1]).x[0])\n\ntensor(2.1339)\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(20)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[519, Data(x=[20, 1], edge_index=[2, 102], edge_attr=[102], y=[20])]\n\n\n\nlen(data[0][1].edge_index[0])\n\n102\n\n\n\nedge_list=[]\nfor i in range(519):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(20, 61)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import  ChickenpoxDatasetLoader\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.0011,  0.0286,  0.3547,  0.2954]), tensor(0.7106))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0]\n\ntensor([0.0286, 0.3547, 0.2954, 0.7106])\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0]\n\ntensor([ 0.3547,  0.2954,  0.7106, -0.6831])\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 20개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{20}\\}, t=1,2,\\dots,519\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:52<00:00,  1.05s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([20, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 102])\n\n\n\n_edge_attr.shape\n\ntorch.Size([102])\n\n\n\n_y.shape\n\ntorch.Size([20])\n\n\n\n_x.shape\n\ntorch.Size([20, 4])\n\n\nx\n\nVertex features are lagged weekly counts of the chickenpox cases (we included 4 lags). y\nThe target is the weekly number of cases for the upcoming week\n\n\n_x\n\ntensor([[-1.0814e-03,  2.8571e-02,  3.5474e-01,  2.9544e-01],\n        [-7.1114e-01, -5.9843e-01,  1.9051e-01,  1.0922e+00],\n        [-3.2281e+00, -2.2910e-01,  1.6103e+00, -1.5487e+00],\n        [ 6.4750e-01, -2.2117e+00, -9.6858e-01,  1.1862e+00],\n        [-1.7302e-01, -9.4717e-01,  1.0347e+00, -6.3751e-01],\n        [ 3.6345e-01, -7.5468e-01,  2.9768e-01, -1.6273e-01],\n        [-3.4174e+00,  1.7031e+00, -1.6434e+00,  1.7434e+00],\n        [-1.9641e+00,  5.5208e-01,  1.1811e+00,  6.7002e-01],\n        [-2.2133e+00,  3.0492e+00, -2.3839e+00,  1.8545e+00],\n        [-3.3141e-01,  9.5218e-01, -3.7281e-01, -8.2971e-02],\n        [-1.8380e+00, -5.8728e-01, -3.5514e-02, -7.2298e-02],\n        [-3.4669e-01, -1.9827e-01,  3.9540e-01, -2.4774e-01],\n        [ 1.4219e+00, -1.3266e+00,  5.2338e-01, -1.6374e-01],\n        [-7.7044e-01,  3.2872e-01, -1.0400e+00,  3.4945e-01],\n        [-7.8061e-01, -6.5022e-01,  1.4361e+00, -1.2864e-01],\n        [-1.0993e+00,  1.2732e-01,  5.3621e-01,  1.9023e-01],\n        [ 2.4583e+00, -1.7811e+00,  5.0732e-02, -9.4371e-01],\n        [ 1.0945e+00, -1.5922e+00,  1.3818e-01,  1.1855e+00],\n        [-7.0875e-01, -2.2460e-01, -7.0875e-01,  1.5630e+00],\n        [-1.8228e+00,  7.8633e-01, -5.6172e-01,  1.2647e+00]])\n\n\n\n_y\n\ntensor([ 0.7106, -0.0725,  2.6099,  1.7870,  0.8024, -0.2614, -0.8370,  1.9674,\n        -0.4212,  0.1655,  1.2519,  2.3743,  0.7877,  0.4531, -0.1721, -0.0614,\n         1.0452,  0.3203, -1.3791,  0.0036])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#pedalmedatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#pedalmedatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "PedalMeDatasetLoader",
    "text": "PedalMeDatasetLoader\nPedal Me Deliveries\n\nA dataset about the number of weekly bicycle package deliveries by Pedal Me in London during 2020 and 2021. Nodes in the graph represent geographical units and edges are proximity based mutual adjacency relationships.\n\n데이터정리\n\nT = 33\nV = 지역의 집합\nN = 15 # number of nodes\nE = 225 # edges\n\\(f(v,t)\\)의 차원? (1,) # number of deliveries\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (15,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (15,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 15\n\nvertices are localities\n\n-Edges : 225\n\nedges are spatial_connections\n\n- Time : 33\n\nbetween 2020 and 2021\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import  PedalMeDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = PedalMeDatasetLoader()\n\ndataset = loader.get_dataset(lags=1)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n33\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([15, 1]), torch.Size([2, 225]), torch.Size([225]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(15)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[33, Data(x=[15, 1], edge_index=[2, 225], edge_attr=[225], y=[15])]\n\n\n\nedge_list=[]\nfor i in range(33):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(15, 120)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import  PedalMeDatasetLoader\nloader = PedalMeDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([ 3.0574, -0.0477, -0.3076,  0.2437]), tensor(-0.2710))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0], (data[1][1]).y[0]\n\n(tensor([-0.0477, -0.3076,  0.2437, -0.2710]), tensor(0.2490))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0], (data[2][1]).y[0]\n\n(tensor([-0.3076,  0.2437, -0.2710,  0.2490]), tensor(-0.0357))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 15개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{15}\\}, t=1,2,\\dots,33\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:03<00:00, 16.04it/s]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([15, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 225])\n\n\n\n_edge_attr.shape\n\ntorch.Size([225])\n\n\n\n_y.shape\n\ntorch.Size([15])\n\n\n\n_x.shape\n\ntorch.Size([15, 4])\n\n\nx\n\nVertex features are lagged weekly counts of the delivery demands (we included 4 lags).\n주마다 배달 수요의 수가 얼마나 될지 percentage로, t-4시점까지?\n\ny\n\nThe target is the weekly number of deliveries the upcoming week. Our dataset consist of more than 30 snapshots (weeks).\n그 다음주에 배달의 수가 몇 퍼센트로 발생할지?\n\n\n_x[0:3]\n\ntensor([[ 3.0574, -0.0477, -0.3076,  0.2437],\n        [ 3.2126,  0.1240,  0.0764,  0.5582],\n        [ 1.9071, -0.8883,  1.5280, -0.7184]])\n\n\n\n_y\n\ntensor([-0.2710,  0.0888,  0.4733,  0.0907, -0.3129,  0.1184,  0.5886, -0.6571,\n         0.2647,  0.2338,  0.1720,  0.5720, -0.9568, -0.4138, -0.5271])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#wikimathsdatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#wikimathsdatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "WikiMathsDatasetLoader",
    "text": "WikiMathsDatasetLoader\nWikipedia Math\n\nContains Wikipedia pages about popular mathematics topics and edges describe the links from one page to another. Features describe the number of daily visits between 2019 and 2021 March.\n\n데이터정리\n\nT = 722\nV = 위키피디아 페이지\nN = 1068 # number of nodes\nE = 27079 # edges\n\\(f(v,t)\\)의 차원? (1,) # 해당페이지를 유저가 방문한 횟수\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (1068,8) (N,8), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3),f(v,t_4),f(v,t_5),f(v,t_6),f(v,t_7)\\)\ny: (1068,) (N,), \\(f(v,t_8)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 1068\n\nvertices are Wikipedia pages\n\n-Edges : 27079\n\nedges are links between them\n\n- Time : 722\n\nWikipedia pages between March 16th 2019 and March 15th 2021\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import  WikiMathsDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n722\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([1068, 8]), torch.Size([2, 27079]), torch.Size([27079]))\n\n\n\n(data[10][1]).x\n\ntensor([[ 0.4972,  0.6838,  0.7211,  ..., -0.8513,  0.1881,  1.3820],\n        [ 0.5457,  0.6016,  0.7071,  ..., -0.4599, -0.6089, -0.0626],\n        [ 0.6305,  1.1404,  0.8779,  ..., -0.5370,  0.7422,  0.3862],\n        ...,\n        [ 0.8699,  0.5451,  1.9254,  ..., -0.8351,  0.3828,  0.3828],\n        [ 0.2451,  0.9629,  1.0526,  ..., -0.9213,  0.8731, -0.1138],\n        [ 0.0200, -0.0871,  0.2342,  ..., -0.4712,  0.0717,  0.2859]])\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(1068)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(722):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(1068, 27079)\n\n\n\nnx.draw(G,node_color='green',node_size=100,width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\nnp.where(data[11][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\nnp.where(data[11][1].edge_index != data[20][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\nhttps://www.kaggle.com/code/mapologo/loading-wikipedia-math-essentials\n\nfrom torch_geometric_temporal.dataset import  WikiMathsDatasetLoader\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset(lags=8)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.4323, -0.4739,  0.2659,  0.4844,  0.5367,  0.6412,  0.2179, -0.7617]),\n tensor(-0.4067))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3,,x_4,x_5,x_6,x_7\\)\ny:= \\(x_9\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-0.4739,  0.2659,  0.4844,  0.5367,  0.6412,  0.2179, -0.7617, -0.4067]),\n tensor(0.3064))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8\\)\ny:= \\(x_9\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([ 0.2659,  0.4844,  0.5367,  0.6412,  0.2179, -0.7617, -0.4067,  0.3064]),\n tensor(0.4972))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9\\)\ny:=\\(x_{10}\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 1068개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7) \\to (x_8)\\)\n\\((x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8) \\to (x_9)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{1068}\\}, t=1,2,\\dots,722\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=8, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [09:28<00:00, 11.37s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([1068, 8])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 27079])\n\n\n어떤 페이지에 refer가 되었는지\n\n_edge_index[0][:5],_edge_index[1][:5]\n\n(tensor([0, 0, 0, 0, 0]), tensor([1, 2, 3, 4, 5]))\n\n\n\n_edge_attr.shape\n\ntorch.Size([27079])\n\n\n\n_edge_attr[:5]\n\ntensor([1., 4., 2., 2., 5.])\n\n\n\nWeights represent the number of links found at the source Wikipedia page linking to the target Wikipedia page.\n\n가중치는 엣지별 한 페이지에 refer되었는지, 몇 번 되었나 수 나옴\n\n_y.shape\n\ntorch.Size([1068])\n\n\n\n_x.shape\n\ntorch.Size([1068, 8])\n\n\nx\n\nlag 를 몇으로 지정하느냐에 따라 다르게 추출\n\ny\n\nThe target is the daily user visits to the Wikipedia pages between March 16th 2019 and March 15th 2021 which results in 731 periods.\n매일 위키피디아 해당 페이지에 몇 명의 유저가 방문하는지!\n음수가 왜 나오지..\n\n\n_x[0:3]\n\ntensor([[-0.4323, -0.4739,  0.2659,  0.4844,  0.5367,  0.6412,  0.2179, -0.7617],\n        [-0.4041, -0.4165, -0.0751,  0.1484,  0.4153,  0.4464, -0.3916, -0.8137],\n        [-0.3892,  0.0634,  0.5913,  0.5370,  0.4646,  0.2776, -0.0724, -0.8116]])\n\n\n\n_y[:3]\n\ntensor([-0.4067, -0.1620, -0.4043])\n\n\n\ny_hat[:3].data\n\ntensor([[-0.0648],\n        [ 0.0314],\n        [-1.0724]])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#windmilloutputlargedatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#windmilloutputlargedatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "WindmillOutputLargeDatasetLoader",
    "text": "WindmillOutputLargeDatasetLoader\nHourly energy output of windmills from a European country for more than 2 years. Vertices represent 319 windmills and weighted edges describe the strength of relationships. The target variable allows for regression tasks.\n데이터정리\n\nT = 17470\nV = 풍력발전소\nN = 319 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (319,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (319,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 319\n\nvertices represent 319 windmills\n\n-Edges : 101761\n\nweighted edges describe the strength of relationships.\n\n- Time : 17470\n\nmore than 2 years\n\n\nfrom torch_geometric_temporal.dataset import  WindmillOutputLargeDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WindmillOutputLargeDatasetLoader()\n\ndataset = loader.get_dataset(lags=1)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n17470\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([319, 1]), torch.Size([2, 101761]), torch.Size([101761]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(319)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[17470, Data(x=[319, 1], edge_index=[2, 101761], edge_attr=[101761], y=[319])]\n\n\ntime이 너무 많아서 일부만 시각화함!!\n\nedge_list=[]\nfor i in range(1000):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(319, 51040)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import  WindmillOutputLargeDatasetLoader\nloader = WindmillOutputLargeDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.5711, -0.7560,  2.6278, -0.8674]), tensor(-0.9877))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0], (data[1][1]).y[0]\n\n(tensor([-0.7560,  2.6278, -0.8674, -0.9877]), tensor(-0.8583))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([ 2.6278, -0.8674, -0.9877, -0.8583]), tensor(0.4282))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 319개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{319}\\}, t=1,2,\\dots,17470\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(5)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 5/5 [1:06:03<00:00, 792.70s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([319, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 101761])\n\n\n\n_edge_attr.shape\n\ntorch.Size([101761])\n\n\n\n_y.shape\n\ntorch.Size([319])\n\n\n\n_x.shape\n\ntorch.Size([319, 4])\n\n\nx\n\n\n\ny\n\nThe target variable allows for regression tasks.\n\n\n_x[0:3]\n\ntensor([[-0.5711, -0.7560,  2.6278, -0.8674],\n        [-0.6936, -0.7264,  2.4113, -0.6052],\n        [-0.8666, -0.7785,  2.2759, -0.6759]])\n\n\n\n_y[0]\n\ntensor(-0.9877)"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#windmilloutputmediumdatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#windmilloutputmediumdatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "WindmillOutputMediumDatasetLoader",
    "text": "WindmillOutputMediumDatasetLoader\nHourly energy output of windmills from a European country for more than 2 years. Vertices represent 26 windmills and weighted edges describe the strength of relationships. The target variable allows for regression tasks.\n데이터정리\n\nT = 17470\nV = 풍력발전소\nN = 319 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (319,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (319,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 26\n\nvertices represent 26 windmills\n\n-Edges : 225\n\nweighted edges describe the strength of relationships\n\n- Time : 676\n\nmore than 2 years\n\n\nfrom torch_geometric_temporal.dataset import  WindmillOutputMediumDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WindmillOutputMediumDatasetLoader()\n\ndataset = loader.get_dataset(lags=1)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n17470\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([26, 1]), torch.Size([2, 676]), torch.Size([676]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(26)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[17470, Data(x=[26, 1], edge_index=[2, 676], edge_attr=[676], y=[26])]\n\n\n\nedge_list=[]\nfor i in range(17463):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(26, 351)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import  WindmillOutputMediumDatasetLoader\nloader = WindmillOutputMediumDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.2170, -0.2055, -0.1587, -0.1930]), tensor(-0.2149))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-0.2055, -0.1587, -0.1930, -0.2149]), tensor(-0.2336))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([-0.1587, -0.1930, -0.2149, -0.2336]), tensor(-0.1785))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 26개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{26}\\}, t=1,2,\\dots,177470\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(5)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 5/5 [03:23<00:00, 40.73s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([26, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 676])\n\n\n\n_edge_attr.shape\n\ntorch.Size([676])\n\n\n\n_y.shape\n\ntorch.Size([26])\n\n\n\n_x.shape\n\ntorch.Size([26, 4])\n\n\nx\n\n\n\ny\n\nThe target variable allows for regression tasks.\n\n\n_x[0:3]\n\ntensor([[-0.2170, -0.2055, -0.1587, -0.1930],\n        [-0.1682, -0.2708, -0.1051,  1.1786],\n        [ 1.1540, -0.6707, -0.8291, -0.6823]])\n\n\n\n_y[0]\n\ntensor(-0.2149)"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#windmilloutputsmalldatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#windmilloutputsmalldatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "WindmillOutputSmallDatasetLoader",
    "text": "WindmillOutputSmallDatasetLoader\nHourly energy output of windmills from a European country for more than 2 years. Vertices represent 11 windmills and weighted edges describe the strength of relationships. The target variable allows for regression tasks.\n데이터정리\n\nT = 17470\nV = 풍력발전소\nN = 11 # number of nodes\nE = 121 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (11,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (11,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 11\n\nvertices represent 11 windmills\n\n-Edges : 121\n\nweighted edges describe the strength of relationships\n\n- Time : 17470\n\nmore than 2 years\n\n\nfrom torch_geometric_temporal.dataset import WindmillOutputSmallDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WindmillOutputSmallDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n17463\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([11, 8]), torch.Size([2, 121]), torch.Size([121]))\n\n\n\ndata[-1]\n\n[17463, Data(x=[11, 8], edge_index=[2, 121], edge_attr=[121], y=[11])]\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(11)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(17463):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(11, 66)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import WindmillOutputSmallDatasetLoader\nloader = WindmillOutputSmallDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([ 0.8199, -0.4972,  0.4923, -0.8299]), tensor(-0.6885))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-0.4972,  0.4923, -0.8299, -0.6885]), tensor(0.7092))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([ 0.4923, -0.8299, -0.6885,  0.7092]), tensor(-0.9356))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 11개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{11}\\}, t=1,2,\\dots,17463\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(5)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 5/5 [02:55<00:00, 35.01s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([11, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 121])\n\n\n\n_edge_attr.shape\n\ntorch.Size([121])\n\n\n\n_y.shape\n\ntorch.Size([11])\n\n\n\n_x.shape\n\ntorch.Size([11, 4])\n\n\nx\n\n\n\ny\n\nThe target variable allows for regression tasks.\n\n\n_x[0:3]\n\ntensor([[ 0.8199, -0.4972,  0.4923, -0.8299],\n        [ 1.1377, -0.3742,  0.3668, -0.8333],\n        [ 0.9979, -0.5643,  0.4070, -0.8918]])\n\n\n\n_y\n\ntensor([-0.6885, -0.6594, -0.6303, -0.6983, -0.5416, -0.6186, -0.6031, -0.7580,\n        -0.6659, -0.5948, -0.5088])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#metrladatasetloader_real-world-traffic-dataset",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#metrladatasetloader_real-world-traffic-dataset",
    "title": "PyTorch ST-GCN Dataset",
    "section": "METRLADatasetLoader_real world traffic dataset",
    "text": "METRLADatasetLoader_real world traffic dataset\nA traffic forecasting dataset based on Los Angeles Metropolitan traffic conditions. The dataset contains traffic readings collected from 207 loop detectors on highways in Los Angeles County in aggregated 5 minute intervals for 4 months between March 2012 to June 2012.\n데이터정리\n\nT = 33\nV = 구역\nN = 207 # number of nodes\nE = 225\n\\(f(v,t)\\)의 차원? (3,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (207,4) (N,2,12), \\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11}\\)\ny: (207,) (N,), \\((x_{12})\\)\n예제코드적용가능여부: No\n\nhttps://arxiv.org/pdf/1707.01926.pdf\n- Nodes : 207\n\nvertices are localities\n\n-Edges : 225\n\nedges are spatial_connections\n\n- Time : 33\n\nbetween 2020 and 2021\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import METRLADatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = METRLADatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n34248\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([207, 2, 12]), torch.Size([2, 1722]), torch.Size([1722]))\n\n\n\ndata[-1]\n\n[34248,\n Data(x=[207, 2, 12], edge_index=[2, 1722], edge_attr=[1722], y=[207, 12])]\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(20)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(1000):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(207, 1520)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n논문 내용 중\n\n\n\nimage.png\n\n\n\nfrom torch_geometric_temporal.dataset import METRLADatasetLoader\nloader = METRLADatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\n\n\n\n\n\nNote\n\n\n\nlags option 없어서 error 뜸 : get_dataset() got an unexpected keyword argument ‘lags’\n\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([[ 0.5332,  0.4486,  0.5146, -2.6522, -2.6522,  0.1847,  0.6383,  0.4961,\n           0.7497,  0.4899,  0.5751,  0.4280],\n         [-1.7292, -1.7171, -1.7051, -1.6930, -1.6810, -1.6689, -1.6569, -1.6448,\n          -1.6328, -1.6207, -1.6087, -1.5966]]),\n tensor([0.3724, 0.2452, 0.4961, 0.6521, 0.1126, 0.5311, 0.5091, 0.4713, 0.4218,\n         0.3909, 0.4761, 0.5641]))\n\n\n\\(t=0\\)에서 \\(X,Z\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11}\\)\nZ:= \\(z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11}\\)\ny:= \\(x_{12}\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([[ 0.4486,  0.5146, -2.6522, -2.6522,  0.1847,  0.6383,  0.4961,  0.7497,\n           0.4899,  0.5751,  0.4280,  0.3724],\n         [-1.7171, -1.7051, -1.6930, -1.6810, -1.6689, -1.6569, -1.6448, -1.6328,\n          -1.6207, -1.6087, -1.5966, -1.5846]]),\n tensor([ 0.2452,  0.4961,  0.6521,  0.1126,  0.5311,  0.5091,  0.4713,  0.4218,\n          0.3909,  0.4761,  0.5641, -0.0022]))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12}\\)\nZ:= \\(z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12}\\)\ny:= \\(x_{13}\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([[ 0.5146, -2.6522, -2.6522,  0.1847,  0.6383,  0.4961,  0.7497,  0.4899,\n           0.5751,  0.4280,  0.3724,  0.2452],\n         [-1.7051, -1.6930, -1.6810, -1.6689, -1.6569, -1.6448, -1.6328, -1.6207,\n          -1.6087, -1.5966, -1.5846, -1.5725]]),\n tensor([ 0.4961,  0.6521,  0.1126,  0.5311,  0.5091,  0.4713,  0.4218,  0.3909,\n          0.4761,  0.5641, -0.0022,  0.4218]))\n\n\n\nX:= \\(x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12},x_{13}\\)\nZ:= \\(z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12},z_{13}\\)\ny:= \\(x_{14}\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 207개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11} \\to (x_{12})\\)\n\\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12},z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12} \\to (x_{13})\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{207}\\}, t=1,2,\\dots,34248\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=1, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([207, 2, 12])\n\n\n\nnode 207개, traffic sensor 2개\n\n\n_edge_index.shape\n\ntorch.Size([2, 1722])\n\n\n\n_edge_attr.shape\n\ntorch.Size([1722])\n\n\n\n_y.shape\n\ntorch.Size([207, 12])\n\n\n\n_x.shape\n\ntorch.Size([207, 2, 12])\n\n\ny\n\ntraffic speed\n\n\n_x[0]\n\ntensor([[ 0.5332,  0.4486,  0.5146, -2.6522, -2.6522,  0.1847,  0.6383,  0.4961,\n          0.7497,  0.4899,  0.5751,  0.4280],\n        [-1.7292, -1.7171, -1.7051, -1.6930, -1.6810, -1.6689, -1.6569, -1.6448,\n         -1.6328, -1.6207, -1.6087, -1.5966]])\n\n\n\n_y[0]\n\ntensor([0.3724, 0.2452, 0.4961, 0.6521, 0.1126, 0.5311, 0.5091, 0.4713, 0.4218,\n        0.3909, 0.4761, 0.5641])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#pemsbaydatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#pemsbaydatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "PemsBayDatasetLoader",
    "text": "PemsBayDatasetLoader\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1111/tgis.12644\nA traffic forecasting dataset as described in Diffusion Convolution Layer Paper.\nThis traffic dataset is collected by California Transportation Agencies (CalTrans) Performance Measurement System (PeMS). It is represented by a network of 325 traffic sensors in the Bay Area with 6 months of traffic readings ranging from Jan 1st 2017 to May 31th 2017 in 5 minute intervals.\n데이터정리\n\nT = 17470\nV = 풍력발전소\nN = 325 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (325,2,12) (N,2,12),\n\n\\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11}\\)\n\\(z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11}\\)\n\ny: (325,) (N,2,12),\n\n\\(x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23},x_{24}\\)\n\\(z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23},z_{24}\\)\n\n예제코드적용가능여부: No\n\n- Nodes : 325\n\nvertices are sensors\n\n-Edges : 2694\n\nweighted edges are between seonsor paris measured by the road nretwork distance\n\n- Time : 52081\n\n6 months of traffic readings ranging from Jan 1st 2017 to May 31th 2017 in 5 minute intervals\n\n\nfrom torch_geometric_temporal.dataset import PemsBayDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = PemsBayDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n52081\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([325, 2, 12]), torch.Size([2, 2694]), torch.Size([2694]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(325)).tolist()\n\n\ndata[-1]\n\n[52081,\n Data(x=[325, 2, 12], edge_index=[2, 2694], edge_attr=[2694], y=[325, 2, 12])]\n\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(1000):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(325, 2404)\n\n\n\nnx.draw(G,node_color='green',node_size=50,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import PemsBayDatasetLoader\nloader = PemsBayDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([[ 0.9821,  0.9928,  1.0251,  1.0574,  1.0466,  1.0681,  0.9821,  1.0251,\n           1.0143,  0.9928,  0.9498,  0.9821],\n         [-1.6127, -1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397, -1.5275,\n          -1.5153, -1.5032, -1.4910, -1.4788]]),\n tensor([[ 1.0143,  0.9821,  0.9821,  1.0036,  1.0143,  0.9605,  0.9498,  1.0251,\n           0.9928,  0.9928,  0.9498,  0.9928],\n         [-1.4667, -1.4545, -1.4423, -1.4302, -1.4180, -1.4058, -1.3937, -1.3815,\n          -1.3694, -1.3572, -1.3450, -1.3329]]))\n\n\n\\(t=0\\)에서 \\(X,Z\\)와 \\(y,s\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11}\\)\nZ:= \\(z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11}\\)\ny:= \\(x_{12},x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23}\\)\ns:= \\(z_{12},z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23}\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([[ 0.9928,  1.0251,  1.0574,  1.0466,  1.0681,  0.9821,  1.0251,  1.0143,\n           0.9928,  0.9498,  0.9821,  1.0143],\n         [-1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397, -1.5275, -1.5153,\n          -1.5032, -1.4910, -1.4788, -1.4667]]),\n tensor([[ 0.9821,  0.9821,  1.0036,  1.0143,  0.9605,  0.9498,  1.0251,  0.9928,\n           0.9928,  0.9498,  0.9928,  0.9821],\n         [-1.4545, -1.4423, -1.4302, -1.4180, -1.4058, -1.3937, -1.3815, -1.3694,\n          -1.3572, -1.3450, -1.3329, -1.3207]]))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12}\\)\nZ:= \\(z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12}\\)\ny:= \\(x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23},x_{24}\\)\ns:= \\(z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23},z_{24}\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([[ 1.0251,  1.0574,  1.0466,  1.0681,  0.9821,  1.0251,  1.0143,  0.9928,\n           0.9498,  0.9821,  1.0143,  0.9821],\n         [-1.5883, -1.5762, -1.5640, -1.5518, -1.5397, -1.5275, -1.5153, -1.5032,\n          -1.4910, -1.4788, -1.4667, -1.4545]]),\n tensor([[ 0.9821,  1.0036,  1.0143,  0.9605,  0.9498,  1.0251,  0.9928,  0.9928,\n           0.9498,  0.9928,  0.9821,  1.0143],\n         [-1.4423, -1.4302, -1.4180, -1.4058, -1.3937, -1.3815, -1.3694, -1.3572,\n          -1.3450, -1.3329, -1.3207, -1.3085]]))\n\n\n\nX:= \\(x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12},x_{13}\\)\nZ:= \\(z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12},z_{13}\\)\ny:= \\(x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23},x_{24},x_{25}\\)\ns:= \\(z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23},z_{24},z_{25}\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 325개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11} \\to x_{12},x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23}\\)\n\\(z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11} \\to z_{12},z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23}\\)\n\\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12} \\to x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23},x_{24}\\)\n\\(z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12} \\to z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23},z_{24}\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{325}\\}, t=1,2,\\dots,52081\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([325, 2, 12])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 2694])\n\n\n\n_edge_attr.shape\n\ntorch.Size([2694])\n\n\n\n_y.shape\n\ntorch.Size([325, 2, 12])\n\n\n\n_x.shape\n\ntorch.Size([325, 2, 12])\n\n\nx\n\n.!\n\ny\n\ncapturing temporal dependencies..?\n\nedges connect sensors\nFor instance, the traffic conditions on one road on Wednesday at 3:00 p.m. are similar to the traffic conditions on Thursday at the same time.\n\n_x[0:3]\n\ntensor([[[ 0.9821,  0.9928,  1.0251,  1.0574,  1.0466,  1.0681,  0.9821,\n           1.0251,  1.0143,  0.9928,  0.9498,  0.9821],\n         [-1.6127, -1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397,\n          -1.5275, -1.5153, -1.5032, -1.4910, -1.4788]],\n\n        [[ 0.6054,  0.5839,  0.6592,  0.6269,  0.6808,  0.6377,  0.6700,\n           0.6054,  0.6162,  0.6162,  0.5839,  0.5947],\n         [-1.6127, -1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397,\n          -1.5275, -1.5153, -1.5032, -1.4910, -1.4788]],\n\n        [[ 0.9390,  0.9175,  0.8960,  0.9175,  0.9067,  0.9175,  0.9175,\n           0.8852,  0.9283,  0.8960,  0.9067,  0.8960],\n         [-1.6127, -1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397,\n          -1.5275, -1.5153, -1.5032, -1.4910, -1.4788]]])\n\n\n\n_y[0]\n\ntensor([[ 1.0143,  0.9821,  0.9821,  1.0036,  1.0143,  0.9605,  0.9498,  1.0251,\n          0.9928,  0.9928,  0.9498,  0.9928],\n        [-1.4667, -1.4545, -1.4423, -1.4302, -1.4180, -1.4058, -1.3937, -1.3815,\n         -1.3694, -1.3572, -1.3450, -1.3329]])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#englandcoviddatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#englandcoviddatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "EnglandCovidDatasetLoader",
    "text": "EnglandCovidDatasetLoader\nCovid19 England\n\nA dataset about mass mobility between regions in England and the number of confirmed COVID-19 cases from March to May 2020 [38]. Each day contains a different mobility graph and node features corresponding to the number of cases in the previous days. Mobility stems from Facebook Data For Good 1 and cases from gov.uk 2\n\nhttps://arxiv.org/pdf/2009.08388.pdf\n데이터정리\n\nT = 52\nV = 지역\nN = 129 # number of nodes\nE = 2158\n\\(f(v,t)\\)의 차원? (1,) # 코로나확진자수\n시간에 따라서 Number of nodes가 변하는지? False\n시간에 따라서 Number of nodes가 변하는지? False\nX: (20,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (20,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 129\n\nvertices are correspond to the number of COVID-19 cases in the region in the past window days.\n\n-Edges : 2158\n\nthe spatial edges capture county-to-county movement at a specific date, and a county is connected to a number of past instances of itself with temporal edges.\n\n- Time : 52\n\nfrom 3 March to 12 of May\n\n\nfrom torch_geometric_temporal.dataset import EnglandCovidDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = EnglandCovidDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n52\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([129, 8]), torch.Size([2, 2158]), torch.Size([2158]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(129)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[52, Data(x=[129, 8], edge_index=[2, 1424], edge_attr=[1424], y=[129])]\n\n\n\nlen(data[0][1].edge_index[0])\n\n2158\n\n\n\nedge_list=[]\nfor i in range(52):\n    for j in range(100):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(129, 1230)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[2][1].edge_index !=data[2][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import EnglandCovidDatasetLoader\nloader = EnglandCovidDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-1.4697, -1.9283, -1.6990, -1.8137]), tensor(-1.8137))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-1.9283, -1.6990, -1.8137, -1.8137]), tensor(-0.8965))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([-1.6990, -1.8137, -1.8137, -0.8965]), tensor(-1.1258))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 129개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{129}\\}, t=1,2,\\dots,52\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:07<00:00,  6.30it/s]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([129, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 2158])\n\n\n\n_edge_attr.shape\n\ntorch.Size([2158])\n\n\n\n_y.shape\n\ntorch.Size([129])\n\n\n\ny_hat.shape\n\ntorch.Size([129, 1])\n\n\n\n_x.shape\n\ntorch.Size([129, 4])\n\n\nx\n\n\n\ny\n\n\n\nThe node features correspond to the number of COVID-19 cases in the region in the past window days.\nThe task is to predict the number of cases in each node after 1 day\n\n_x[0:3]\n\ntensor([[-1.4697, -1.9283, -1.6990, -1.8137],\n        [-1.2510, -1.1812, -1.3208, -1.1812],\n        [-1.0934, -1.0934, -1.0934, -1.0934]])\n\n\n\n_y[:3]\n\ntensor([-1.8137, -1.3208, -1.0934])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#montevideobusdatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#montevideobusdatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "MontevideoBusDatasetLoader",
    "text": "MontevideoBusDatasetLoader\nhttps://www.fing.edu.uy/~renzom/msc/uploads/msc-thesis.pdf\nMontevideo Buses\n\nA dataset about the hourly passenger inflow at bus stop level for eleven bus lines from the city of Montevideo. Nodes are bus stops and edges represent connections between the stops; the dataset covers a whole month of traffic patterns.\n\n데이터정리\n\nT = 739\nV = 버스정류장\nN = 675 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # passenger inflow\n시간에 따라서 Number of nodes가 변하는지? False\n시간에 따라서 Number of nodes가 변하는지? False\nX: (675,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (675,,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 675\n\nvertices are bus stops\n\n-Edges : 690\n\nedges are links between bus stops when a bus line connects them and the weight represent the road distance\n\n- Time : 739\n\nhourly inflow passenger data at bus stop level for 11 bus lines during October 2020 from Montevideo city (Uruguay).\n\n\nfrom torch_geometric_temporal.dataset import MontevideoBusDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = MontevideoBusDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n739\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([675, 4]), torch.Size([2, 690]), torch.Size([690]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(675)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(739):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(675, 690)\n\n\n\nnx.draw(G,node_color='green',node_size=50,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import MontevideoBusDatasetLoader\nloader = MontevideoBusDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.4200, -0.4200, -0.4200, -0.4200]), tensor(-0.4200))\n\n\n\\(t=0\\)에서 \\(X\\)와 \\(y\\)를 정리하면 아래와 같음.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-0.4200, -0.4200, -0.4200, -0.4200]), tensor(-0.4200))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([-0.4200, -0.4200, -0.4200, -0.4200]), tensor(-0.4200))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\n하나의 노드에 길이가 \\(T\\)인 시계열이 맵핑되어 있음. (노드는 총 675개)\n각 노드마다 아래와 같은 과정으로 예측이 됨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{675}\\}, t=1,2,\\dots,739\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [01:51<00:00,  2.23s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([675, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 690])\n\n\n\n_edge_attr.shape\n\ntorch.Size([690])\n\n\n\n_y.shape\n\ntorch.Size([675])\n\n\n\n_x.shape\n\ntorch.Size([675, 4])\n\n\nx\n\n\n\ny\n\nThe target is the passenger inflow.\nThis is a curated dataset made from different data sources of the Metropolitan Transportation System (STM) of Montevide\n\n\n_x[0:3]\n\ntensor([[-0.4200, -0.4200, -0.4200, -0.4200],\n        [-0.0367, -0.0367, -0.0367, -0.0367],\n        [-0.2655, -0.2655, -0.2655, -0.2655]])\n\n\n\n_y[:3]\n\ntensor([-0.4200, -0.0367, -0.2655])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#twittertennisdatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#twittertennisdatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "TwitterTennisDatasetLoader",
    "text": "TwitterTennisDatasetLoader\nhttps://appliednetsci.springeropen.com/articles/10.1007/s41109-018-0080-5?ref=https://githubhelp.com\nTwitter Tennis RG and UO\n\nTwitter mention graphs of major tennis tournaments from 2017. Each snapshot contains the graph of popular player or sport news accounts and mentions between them [5, 6]. Node labels encode the number of mentions received and vertex features are structural properties\n\n데이터정리\n\nT = 52081\nV = 트위터계정\n\nN = 1000 # number of nodes\nE = 119 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # passenger inflow\n시간에 따라서 N이 변하는지? ??\n시간에 따라서 E가 변하는지? True\nX: ?\ny: ?\n예제코드적용가능여부: No\n\n- Nodes : 1000\n\nvertices are Twitter accounts\n\n-Edges : 119\n\nedges are mentions between them\n\n- Time : 52081\n\nTwitter mention graphs related to major tennis tournaments from 2017\n\n\nfrom torch_geometric_temporal.dataset import TwitterTennisDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = TwitterTennisDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n119\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([1000, 16]), torch.Size([2, 89]), torch.Size([89]))\n\n\n\ndata[0][1].x[0]\n\ntensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\ndata[0][1].edge_index[0]\n\ntensor([ 42, 909, 909, 909, 233, 233, 450, 256, 256, 256, 256, 256, 434, 434,\n        434, 233, 233, 233, 233, 233, 233, 233,   9,   9, 355,  84,  84,  84,\n         84, 140, 140, 140, 140,   0, 140, 238, 238, 238, 649, 875, 875, 234,\n         73,  73, 341, 341, 341, 341, 341, 417, 293, 991,  74, 581, 282, 162,\n        144, 383, 383, 135, 135, 910, 910, 910, 910, 910,  87,  87,  87,  87,\n          9,   9, 934, 934, 162, 225,  42, 911, 911, 911, 911, 911, 911, 911,\n        911, 498, 498,  64, 435])\n\n\n\ndata[0][1].edge_attr\n\ntensor([2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 3., 2., 1., 1., 1., 1., 2., 2., 2., 1., 1., 1., 3.])\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(1000)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(119):\n    for j in range(40):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(1000, 2819)\n\n\n\nnx.draw(G,node_color='green',node_size=50,width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nlen(data[2][1].edge_index[0])\n\n67\n\n\n\nlen(data[0][1].edge_index[0])\n\n89\n\n\n다름..\n\n\nfrom torch_geometric_temporal.dataset import TwitterTennisDatasetLoader\nloader = TwitterTennisDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n tensor(4.8363))\n\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n tensor(4.9200))\n\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n tensor(6.5539))\n\n\n\n(data[3][1]).x[0],(data[3][1]).y[0]\n\n(tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n tensor(6.9651))\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([1000, 16])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 89])\n\n\n\n_edge_attr.shape\n\ntorch.Size([89])\n\n\n\n_y.shape\n\ntorch.Size([1000])\n\n\n\n_x.shape\n\ntorch.Size([1000, 16])\n\n\nx\n\n\n\ny\n\n\n\n\n_x[0:3]\n\ntensor([[0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n\n\n\n_y[0]\n\ntensor(4.8363)"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#mtmdatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#mtmdatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "MTMDatasetLoader",
    "text": "MTMDatasetLoader\nMTM-1 Hand Motions\n\nA temporal dataset of MethodsTime Measurement-1 [36] motions, signalled as consecutive graph frames of 21 3D hand key points that were acquired via MediaPipe Hands [64] from original RGB-Video material. Node features encode the normalized xyz-coordinates of each finger joint and the vertices are connected according to the human hand structure.\n\n데이터정리\n\nT = 14452\nV = 손의 shape에 대응하는 dot\n\nN = 325 # number of nodes\nE = 19 = N^2 # edges\n\\(f(v,t)\\)의 차원? (Grasp, Release, Move, Reach, Poision, -1)\n시간에 따라서 N이 변하는지? ??\n시간에 따라서 E가 변하는지? ??\nX: ?\ny: ?\n예제코드적용가능여부: No\n\n- Nodes : 325\n\nvertices are are the finger joints of the human hand\n\n-Edges : 19\n\nedges are the bones connecting them\n\n- Time : 14452\n\nfrom torch_geometric_temporal.dataset import MTMDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = MTMDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n14452\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([3, 21, 16]), torch.Size([2, 19]), torch.Size([19]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(21)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(14452):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(21, 19)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntime별 같은 edge 정보를 가지고 있나 확인\n\nnp.where(data[0][1].edge_index != data[12][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import MTMDatasetLoader\nloader = MTMDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n tensor([0., 0., 1., 0., 0., 0.]))\n\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n tensor([0., 0., 1., 0., 0., 0.]))\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([3, 21, 16])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 19])\n\n\n\n_edge_attr.shape\n\ntorch.Size([19])\n\n\n\n_y.shape\n\ntorch.Size([16, 6])\n\n\n\n_x.shape\n\ntorch.Size([3, 21, 16])\n\n\nx\n\nThe data x is returned in shape (3, 21, T),\n\ny\n\nThe targets are manually labeled for each frame, according to one of the five MTM-1 motions (classes ): Grasp, Release, Move, Reach, Position plus a negative class for frames without graph signals (no hand present).\nthe target is returned one-hot-encoded in shape (T, 6).\n\n\n_x[0]\n\ntensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n\n\n\n_y[0]\n\ntensor([0., 0., 1., 0., 0., 0.])"
  },
  {
    "objectID": "posts/GCN/index.html",
    "href": "posts/GCN/index.html",
    "title": "GCN",
    "section": "",
    "text": "About GCN Study"
  },
  {
    "objectID": "posts/GCN/2023-01-26-guebin.html",
    "href": "posts/GCN/2023-01-26-guebin.html",
    "title": "Class of Method",
    "section": "",
    "text": "Class"
  },
  {
    "objectID": "posts/GCN/2023-01-26-guebin.html#시나리오1-baseline",
    "href": "posts/GCN/2023-01-26-guebin.html#시나리오1-baseline",
    "title": "Class of Method",
    "section": "시나리오1 (Baseline)",
    "text": "시나리오1 (Baseline)\n시나리오1\n\nmissing rate: 0%\n보간방법: None\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [01:39<00:00,  2.00s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nGNAR 으로 적합 + 예측\n-\n\n\n결과시각화\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n mse(train) = {1:.2f}, mse(test) = {2:.2f}'.format(i,train_mse_eachnode[i],test_mse_eachnode[i]))\n    a.plot(range(1,160),stgcn_train[:,i],label='STCGCN (train)',color='C0')\n    a.plot(range(161,200),stgcn_test[:,i],label='STCGCN (test)',color='C1')\n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario1: STGCN \\n missing=0% \\n interpolation=None \\n mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n\".format(train_mse_total,test_mse_total),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-01-26-guebin.html#시나리오2",
    "href": "posts/GCN/2023-01-26-guebin.html#시나리오2",
    "title": "Class of Method",
    "section": "시나리오2",
    "text": "시나리오2\n시나리오2\n\nmissing rate: 50%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.5)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [01:31<00:00,  1.82s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [01:29<00:00,  1.80s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\n결과시각화\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(161,200),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(161,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-01-26-guebin.html#시나리오3",
    "href": "posts/GCN/2023-01-26-guebin.html#시나리오3",
    "title": "Class of Method",
    "section": "시나리오3",
    "text": "시나리오3\n시나리오3\n\nmissing rate: 80%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [01:27<00:00,  1.76s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [01:32<00:00,  1.86s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\n결과시각화\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(161,200),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(161,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario3: \\n missing=80% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-03-18-SimulationPlanner-Tutorial.html",
    "href": "posts/GCN/2023-03-18-SimulationPlanner-Tutorial.html",
    "title": "SimualtionPlanner-Tutorial",
    "section": "",
    "text": "table\nGNN-Ensemble: Towards Random Decision Graph Neural Networks\nhttps://papers.nips.cc/papers/search?q=GNN"
  },
  {
    "objectID": "posts/GCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_rand",
    "href": "posts/GCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_rand",
    "title": "SimualtionPlanner-Tutorial",
    "section": "PLNR_STGCN_RAND",
    "text": "PLNR_STGCN_RAND\n\nimport pandas as pd\n\n\ndata_chickenpox = pd.read_csv('./simulation_results/Real_simulation/chikenpox_Simulation.csv')\n\n\ndata_chickenpox['mrate'].unique()\n\narray([0.3       , 0.4       , 0.5       , 0.28776978, 0.        ,\n       0.8       , 0.9       ])\n\n\n\nplans_stgcn_rand = {\n    'max_iteration': 30, \n    'method': ['STGCN', 'IT-STGCN'],\n    'RecurrentGCN' : ['DCRNN'],\n    'mrate': [0,0.3,0.4,0.5,0.8,0.9],\n    'lags': [4], \n    'nof_filters': [16], \n    'inter_method': ['linear','cubic'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader1,dataset_name='chickenpox')\n\n\nplnr.simulate()\n\n/home/csy/Dropbox/blog/posts/GCN/itstgcnadd/learners.py:101: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343998658/work/torch/csrc/utils/tensor_new.cpp:245.)\n  self.lags = torch.tensor(train_dataset.features).shape[-1]\n\n\n1/30 is done\n20/50\n\n\n\nplans_stgcn_rand = {\n    'max_iteration': 10, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mrate': [0.3],\n    'lags': [2, 4], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader3,dataset_name='wikimath')\n\n\nplnr.simulate()\n\n11/50\n\n\n\nplans_stgcn_rand = {\n    'max_iteration': 1, \n    'method': ['STGCN', 'IT-STGCN'], \n    'RecurrentGCN' : ['DCRNN'],\n    'mrate': [0],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 1, \n    'method': ['STGCN', 'IT-STGCN'], \n    'RecurrentGCN' : ['DCRNN'],\n    'mrate': [0.7],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader6,dataset_name='windmillsmall')\nplnr.simulate()"
  },
  {
    "objectID": "posts/GCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_manual",
    "href": "posts/GCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_manual",
    "title": "SimualtionPlanner-Tutorial",
    "section": "PLNR_STGCN_MANUAL",
    "text": "PLNR_STGCN_MANUAL\n\nmy_list = [[] for _ in range(20)] #chickenpox\nanother_list = list(range(100,400))\nmy_list[1] = another_list\nmy_list[3] = another_list\nmy_list[5] = another_list\nmy_list[7] = another_list\nmy_list[9] = another_list\nmy_list[11] = another_list\nmy_list[13] = another_list\nmy_list[15] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(15)] #pedalme\nanother_list = list(range(5,35))\nmy_list[2] = another_list\nmy_list[4] = another_list\nmy_list[7] = another_list\nmy_list[11] = another_list\nmindex = my_list\n\n\nimport random\nmy_list = [[] for _ in range(1068)] # wikimath\nanother_list = random.sample(range(570), 72)\n# my_list에서 250개 요소 무작위 선택\nselected_indexes = random.sample(range(len(my_list)), 250)\n# 선택된 요소에 해당하는 값들을 another_list에 할당\nfor index in selected_indexes:\n    my_list[index] = another_list\n\n\n# _data = itstgcn.load_data('./data/fivenodes.pkl')\n# _edges = torch.tensor(_data['edges']).nonzero().tolist()\n# _FX = _data['f'].tolist()\n# _node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \n# data_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n# loader = itstgcn.DatasetLoader(data_dict)\n# data_dict = itstgcn.load_data('./data/fivenodes.pkl')\n# loader = itstgcn.DatasetLoader(data_dict)\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]\n# mindex= [list(range(50,150)),[],list(range(50,90)),list(range(50,150)),[]] # node 2\nplans_stgcn_block = {\n    'max_iteration': 30, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [4,8], \n    'nof_filters': [12], \n    'inter_method': ['cubic'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader1,dataset_name='chickenpox')\n\n\nplnr.simulate(mindex=mindex,mtype='block')\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader2,dataset_name='pedalme')\n\n\nplnr.simulate(mindex=mindex,mtype='block')\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]\n# mindex= [list(range(50,150)),[],list(range(50,90)),list(range(50,150)),[]] # node 2\nplans_stgcn_block = {\n    'max_iteration': 10, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [2,4], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader3,dataset_name='wikimath')\n\n\nplnr.simulate(mindex=mindex,mtype='block')"
  },
  {
    "objectID": "posts/GCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_rand",
    "href": "posts/GCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_rand",
    "title": "SimualtionPlanner-Tutorial",
    "section": "PLNR_GNAR_RAND",
    "text": "PLNR_GNAR_RAND\n\n# _data = itstgcn.load_data('./data/fivenodes.pkl')\n# _edges = torch.tensor(_data['edges']).nonzero().tolist()\n# _FX = _data['f'].tolist()\n# _node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \n# data_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n# loader = itstgcn.DatasetLoader(data_dict)\n# data_dict=itstgcn.load_data('./data/fivenodes.pkl')\n# loader = itstgcn.DatasetLoader(data_dict)\n\n\nplans_gnar_rand = {\n    'max_iteration': 30, \n#    'method': ['GNAR'], \n    'mrate': [0.1],\n    'lags': [4], \n#    'nof_filters': [8,16], \n    'inter_method': ['linear','cubic'],\n#    'epoch': [1]\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_RAND(plans_gnar_rand,loader2,dataset_name='pedalme')\nplnr.simulate()\n\n\nplans_gnar_rand = {\n    'max_iteration': 3, \n#    'method': ['GNAR'], \n    'mrate': [0],\n    'lags': [2,4], \n#    'nof_filters': [8,16], \n    'inter_method': ['linear'],\n#    'epoch': [1]\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_RAND(plans_gnar_rand,loader3,dataset_name='wikimath')\nplnr.simulate()\n\n\nplans_gnar_rand = {\n    'max_iteration': 3, \n#    'method': ['GNAR'], \n    'mrate': [0,0.3],\n    'lags': [8], \n#    'nof_filters': [8,16], \n    'inter_method': ['linear'],\n#    'epoch': [1]\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_RAND(plans_gnar_rand,loader5,dataset_name='windmillmedium')\nplnr.simulate()"
  },
  {
    "objectID": "posts/GCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_block",
    "href": "posts/GCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_block",
    "title": "SimualtionPlanner-Tutorial",
    "section": "PLNR_GNAR_BLOCK",
    "text": "PLNR_GNAR_BLOCK\n\n# _data = itstgcn.load_data('./data/fivenodes.pkl')\n# _edges = torch.tensor(_data['edges']).nonzero().tolist()\n# _FX = _data['f'].tolist()\n# _node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \n# data_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n# loader = itstgcn.DatasetLoader(data_dict)\n# loader = itstgcn.load_data('./data/fivenodes.pkl')\n\n\n Nodes : 26\n\nvertices represent 26 windmills\n-Edges : 676\n\nweighted edges describe the strength of relationships\n- Time : 17464\n\n\nmy_list = [[] for _ in range(26)] #medium\nanother_list = list(range(1000,2000))+list(range(4000,5000))+list(range(7000,8000)) #17464\n\nfor i in np.array(random.sample(range(0, 26), 15)):\n    my_list[i] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(20)] #chickenpox\nanother_list = list(range(100,400))\nmy_list[2] = another_list\nmy_list[4] = another_list\nmy_list[6] = another_list\nmy_list[8] = another_list\nmy_list[10] = another_list\nmy_list[12] = another_list\nmy_list[14] = another_list\nmy_list[16] = another_list\nmindex = my_list\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]\n# mindex= [list(range(50,150)),[],list(range(50,90)),list(range(50,150)),[]] # node 2\nplans_gnar_block = {\n    'max_iteration': 3, \n    'method': ['GNAR'], \n    'mindex': [mindex],\n    'lags': [4], \n    'inter_method': ['cubic','linear'],\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_MANUAL(plans_gnar_block,loader1,dataset_name='chickenpox')\nplnr.simulate(mindex,mtype='block')\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]\n# mindex= [list(range(50,150)),[],list(range(50,90)),list(range(50,150)),[]] # node 2\nplans_gnar_block = {\n    'max_iteration': 3, \n    'method': ['GNAR'], \n    'mindex': [mindex],\n    'lags': [2,4], \n    'inter_method': ['linear'],\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_MANUAL(plans_gnar_block,loader3,dataset_name='wikimath')\nplnr.simulate(mindex,mtype='block')"
  },
  {
    "objectID": "posts/GCN/2023-04-06-METRLADatasetLoader.html",
    "href": "posts/GCN/2023-04-06-METRLADatasetLoader.html",
    "title": "METRLADatasetLoader-Tutorial",
    "section": "",
    "text": "METRLADatasetLoader\n\n\nimport torch\nfrom IPython.display import clear_output\npt_version = torch.__version__\nprint(pt_version)\n\n1.10.1\n\n\n\nimport numpy as np\nfrom torch_geometric_temporal.dataset import METRLADatasetLoader\nfrom torch_geometric_temporal.signal import StaticGraphTemporalSignal\n\nloader = METRLADatasetLoader()\ndataset = loader.get_dataset(num_timesteps_in=12, num_timesteps_out=12)\n\n#print(\"Dataset type:  \", dataset)\n#print(\"Number of samples / sequences: \",  len(set(dataset)))\n\n\nimport seaborn as sns\n# Visualize traffic over time\nsensor_number = 1\nhours = 24\nsensor_labels = [bucket.y[sensor_number][0].item() for bucket in list(dataset)[:hours]]\nsns.lineplot(data=sensor_labels)\n\n<AxesSubplot:>\n\n\n\n\n\n\nfrom torch_geometric_temporal.signal import temporal_signal_split\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)\n\n#print(\"Number of train buckets: \", len(set(train_dataset)))\n#print(\"Number of test buckets: \", len(set(test_dataset)))\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import A3TGCN\n\nclass TemporalGNN(torch.nn.Module):\n    def __init__(self, node_features, periods):\n        super(TemporalGNN, self).__init__()\n        # Attention Temporal Graph Convolutional Cell\n        self.tgnn = A3TGCN(in_channels=node_features, \n                           out_channels=32, \n                           periods=periods)\n        # Equals single-shot prediction\n        self.linear = torch.nn.Linear(32, periods)\n\n    def forward(self, x, edge_index):\n        \"\"\"\n        x = Node features for T time steps\n        edge_index = Graph edge indices\n        \"\"\"\n        h = self.tgnn(x, edge_index)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\nTemporalGNN(node_features=2, periods=12)\n\nTemporalGNN(\n  (tgnn): A3TGCN(\n    (_base_tgcn): TGCN(\n      (conv_z): GCNConv(2, 32)\n      (linear_z): Linear(in_features=64, out_features=32, bias=True)\n      (conv_r): GCNConv(2, 32)\n      (linear_r): Linear(in_features=64, out_features=32, bias=True)\n      (conv_h): GCNConv(2, 32)\n      (linear_h): Linear(in_features=64, out_features=32, bias=True)\n    )\n  )\n  (linear): Linear(in_features=32, out_features=12, bias=True)\n)\n\n\n\n# GPU support\ndevice = torch.device('cpu') # cuda\nsubset = 2000\n\n# Create model and optimizers\nmodel = TemporalGNN(node_features=2, periods=12).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nmodel.train()\n\nprint(\"Running training...\")\nfor epoch in range(10): \n    loss = 0\n    step = 0\n    for snapshot in train_dataset:\n        snapshot = snapshot.to(device)\n        # Get model predictions\n        y_hat = model(snapshot.x, snapshot.edge_index)\n        # Mean squared error\n        loss = loss + torch.mean((y_hat-snapshot.y)**2) \n        step += 1\n        if step > subset:\n          break\n\n    loss = loss / (step + 1)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    print(\"Epoch {} train MSE: {:.4f}\".format(epoch, loss.item()))\n\nRunning training...\nEpoch 0 train MSE: 0.7596\nEpoch 1 train MSE: 0.7398\nEpoch 2 train MSE: 0.7205\nEpoch 3 train MSE: 0.6996\nEpoch 4 train MSE: 0.6759\nEpoch 5 train MSE: 0.6495\nEpoch 6 train MSE: 0.6221\nEpoch 7 train MSE: 0.5963\nEpoch 8 train MSE: 0.5743\nEpoch 9 train MSE: 0.5573\n\n\n\nmodel.eval()\nloss = 0\nstep = 0\nhorizon = 288\n\n# Store for analysis\npredictions = []\nlabels = []\n\nfor snapshot in test_dataset:\n    snapshot = snapshot.to(device)\n    # Get predictions\n    y_hat = model(snapshot.x, snapshot.edge_index)\n    # Mean squared error\n    loss = loss + torch.mean((y_hat-snapshot.y)**2)\n    # Store for analysis below\n    labels.append(snapshot.y)\n    predictions.append(y_hat)\n    step += 1\n    if step > horizon:\n          break\n\nloss = loss / (step+1)\nloss = loss.item()\nprint(\"Test MSE: {:.4f}\".format(loss))\n\nTest MSE: 0.6738"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html",
    "href": "posts/GCN/2023-04-27-simulation_table.html",
    "title": "Simulation Tables",
    "section": "",
    "text": "Simulation Tables"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#baseline",
    "href": "posts/GCN/2023-04-27-simulation_table.html#baseline",
    "title": "Simulation Tables",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n  \n    \n      \n      nof_filters\n      method\n      lags\n      mean\n      std\n    \n  \n  \n    \n      0\n      12.0\n      IT-STGCN\n      2\n      1.168\n      0.030\n    \n    \n      1\n      12.0\n      STGCN\n      2\n      1.173\n      0.036\n    \n    \n      2\n      16.0\n      IT-STGCN\n      2\n      1.166\n      0.039\n    \n    \n      3\n      16.0\n      STGCN\n      2\n      1.165\n      0.040\n    \n  \n\n\n\n\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','lags'])['mse'].std().reset_index(),\n         on=['nof_filters','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n  \n    \n      \n      nof_filters\n      lags\n      mean\n      std\n    \n  \n  \n    \n      0\n      12.0\n      2\n      1.170\n      0.033\n    \n    \n      1\n      16.0\n      2\n      1.165\n      0.039\n    \n  \n\n\n\n\n\npd.merge(data_DCRNN_fivenodes.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','lags'])['mse'].mean().reset_index(),\n         data_DCRNN_fivenodes.query(\"dataset=='fivenodes' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','lags'])['mse'].std().reset_index(),\n         on=['nof_filters','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n  \n    \n      \n      nof_filters\n      lags\n      mean\n      std\n    \n  \n  \n    \n      0\n      16\n      2\n      1.247\n      0.005"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#random",
    "href": "posts/GCN/2023-04-27-simulation_table.html#random",
    "title": "Simulation Tables",
    "section": "Random",
    "text": "Random\n\ndata.query(\"dataset=='fivenodes' and mtype=='rand'and method=='GNAR'\")['mse'].unique().round(3)\n\narray([1.407])\n\n\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==12\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      nof_filters\n      method\n      lags\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.70\n      12.0\n      IT-STGCN\n      2\n      1.200\n      0.070\n    \n    \n      1\n      0.70\n      12.0\n      STGCN\n      2\n      1.213\n      0.083\n    \n    \n      4\n      0.75\n      12.0\n      IT-STGCN\n      2\n      1.188\n      0.060\n    \n    \n      5\n      0.75\n      12.0\n      STGCN\n      2\n      1.239\n      0.102\n    \n    \n      8\n      0.80\n      12.0\n      IT-STGCN\n      2\n      1.221\n      0.083\n    \n    \n      9\n      0.80\n      12.0\n      STGCN\n      2\n      1.226\n      0.105\n    \n    \n      12\n      0.85\n      12.0\n      IT-STGCN\n      2\n      1.227\n      0.085\n    \n    \n      13\n      0.85\n      12.0\n      STGCN\n      2\n      1.291\n      0.252\n    \n  \n\n\n\n\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters!=12\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      nof_filters\n      method\n      lags\n      mean\n      std\n    \n  \n  \n    \n      2\n      0.70\n      16.0\n      IT-STGCN\n      2\n      1.201\n      0.068\n    \n    \n      3\n      0.70\n      16.0\n      STGCN\n      2\n      1.227\n      0.094\n    \n    \n      6\n      0.75\n      16.0\n      IT-STGCN\n      2\n      1.231\n      0.110\n    \n    \n      7\n      0.75\n      16.0\n      STGCN\n      2\n      1.201\n      0.072\n    \n    \n      10\n      0.80\n      16.0\n      IT-STGCN\n      2\n      1.232\n      0.092\n    \n    \n      11\n      0.80\n      16.0\n      STGCN\n      2\n      1.292\n      0.148\n    \n    \n      14\n      0.85\n      16.0\n      IT-STGCN\n      2\n      1.286\n      0.297\n    \n    \n      15\n      0.85\n      16.0\n      STGCN\n      2\n      1.362\n      0.239\n    \n  \n\n\n\n\n\npd.merge(data_DCRNN_fivenodes.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].mean().reset_index(),\n         data_DCRNN_fivenodes.query(\"dataset=='fivenodes' and mtype=='rand'\").groupby(['mrate','nof_filters','method','lags'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters!=12 and mrate!=0.3\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      nof_filters\n      method\n      lags\n      mean\n      std\n    \n  \n  \n    \n      2\n      0.8\n      16\n      IT-STGCN\n      2\n      1.478\n      1.245\n    \n    \n      3\n      0.8\n      16\n      STGCN\n      2\n      1.491\n      0.302"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#block",
    "href": "posts/GCN/2023-04-27-simulation_table.html#block",
    "title": "Simulation Tables",
    "section": "Block",
    "text": "Block\n\ndata.query(\"dataset=='fivenodes' and mtype=='block'and method=='GNAR'\")['mse'].unique().round(3)\n\narray([1.407])\n\n\n\ndata.query(\"dataset=='fivenodes' and mtype=='block'\")\n\n\n\n\n\n  \n    \n      \n      dataset\n      method\n      RecurrentGCN\n      mrate\n      mtype\n      lags\n      nof_filters\n      inter_method\n      epoch\n      mse\n    \n  \n  \n    \n      600\n      fivenodes\n      GNAR\n      GConvGRU\n      0.125\n      block\n      2\n      NaN\n      cubic\n      NaN\n      1.406830\n    \n    \n      601\n      fivenodes\n      GNAR\n      GConvGRU\n      0.125\n      block\n      2\n      NaN\n      linear\n      NaN\n      1.406830\n    \n    \n      602\n      fivenodes\n      GNAR\n      GConvGRU\n      0.125\n      block\n      2\n      NaN\n      cubic\n      NaN\n      1.406830\n    \n    \n      603\n      fivenodes\n      GNAR\n      GConvGRU\n      0.125\n      block\n      2\n      NaN\n      linear\n      NaN\n      1.406830\n    \n    \n      604\n      fivenodes\n      GNAR\n      GConvGRU\n      0.125\n      block\n      2\n      NaN\n      cubic\n      NaN\n      1.406830\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      967\n      fivenodes\n      IT-STGCN\n      GConvGRU\n      0.300\n      block\n      2\n      16.0\n      linear\n      150.0\n      1.135442\n    \n    \n      968\n      fivenodes\n      STGCN\n      GConvGRU\n      0.300\n      block\n      2\n      12.0\n      linear\n      150.0\n      1.203593\n    \n    \n      969\n      fivenodes\n      STGCN\n      GConvGRU\n      0.300\n      block\n      2\n      16.0\n      linear\n      150.0\n      1.220799\n    \n    \n      970\n      fivenodes\n      IT-STGCN\n      GConvGRU\n      0.300\n      block\n      2\n      12.0\n      linear\n      150.0\n      1.111655\n    \n    \n      971\n      fivenodes\n      IT-STGCN\n      GConvGRU\n      0.300\n      block\n      2\n      16.0\n      linear\n      150.0\n      1.197438\n    \n  \n\n372 rows × 10 columns\n\n\n\n\npd.merge(data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='fivenodes' and mtype=='block'\").groupby(['mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters','mrate']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n  \n    \n      \n      mrate\n      nof_filters\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.125\n      12.0\n      IT-STGCN\n      4.308\n      3.333\n    \n    \n      1\n      0.125\n      12.0\n      STGCN\n      6.722\n      5.755\n    \n    \n      2\n      0.125\n      16.0\n      IT-STGCN\n      4.633\n      3.737\n    \n    \n      3\n      0.125\n      16.0\n      STGCN\n      6.858\n      5.814\n    \n    \n      4\n      0.300\n      12.0\n      IT-STGCN\n      1.178\n      0.032\n    \n    \n      5\n      0.300\n      12.0\n      STGCN\n      1.232\n      0.040\n    \n    \n      6\n      0.300\n      16.0\n      IT-STGCN\n      1.163\n      0.050\n    \n    \n      7\n      0.300\n      16.0\n      STGCN\n      1.232\n      0.053\n    \n  \n\n\n\n\n\nepoch 별 보기\n\ndf1 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_random_epoch50.csv')\ndf2 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_random_epoch100.csv')\ndf3 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_random_epoch150.csv')\ndf4 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_random_epoch200.csv')\n\n\ndf_gnar = pd.read_csv('./simulation_results/fivenodes/fivenodes_GNAR_random.csv')\n\n\ndata_temp = pd.concat([df1,df2,df3,df4,df_gnar],axis=0)\n\nSTGCN은 nearest에서 mse가 낮았다.\n\ndata_temp.query(\"method=='STGCN' and mtype=='rand' and mrate==0.8 and lags==2 and inter_method=='linear' and nof_filters==4\").\\\ngroupby(['method','epoch','mrate','lags','nof_filters','inter_method'])['mse'].mean().reset_index()['mse'].mean()\n\n1.182556539773941\n\n\n\ndata_temp.query(\"method=='STGCN' and mtype=='rand' and mrate==0.8 and lags==2 and inter_method=='linear' and nof_filters==4\").\\\ngroupby(['method','epoch','mrate','lags','nof_filters','inter_method'])['mse'].mean().reset_index()['mse'].std()\n\n0.012169932740213692\n\n\n\ndata_temp.query(\"method=='IT-STGCN' and mtype=='rand' and mrate==0.8 and lags==2 and inter_method=='linear' and nof_filters==4\").\\\ngroupby(['method','epoch','mrate','lags','nof_filters','inter_method'])['mse'].mean().reset_index()['mse'].mean()\n\n1.1747438261906304\n\n\n\ndata_temp.query(\"method=='IT-STGCN' and mtype=='rand' and mrate==0.8 and lags==2 and inter_method=='linear' and nof_filters==4\").\\\ngroupby(['method','epoch','mrate','lags','nof_filters','inter_method'])['mse'].mean().reset_index()['mse'].std()\n\n0.007602895892378366"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#baseline-1",
    "href": "posts/GCN/2023-04-27-simulation_table.html#baseline-1",
    "title": "Simulation Tables",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==16\")\n\n\n\n\n\n  \n    \n      \n      nof_filters\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      16.0\n      IT-STGCN\n      1.008\n      0.010\n    \n    \n      1\n      16.0\n      STGCN\n      1.009\n      0.008\n    \n  \n\n\n\n\n\npd.merge(data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].mean().reset_index(),\n         data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype!='rand' and mtype!='block'\").groupby(['nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==16\")\n\n\n\n\n\n  \n    \n      \n      nof_filters\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      16\n      IT-STGCN\n      0.953\n      0.005\n    \n    \n      1\n      16\n      STGCN\n      0.953\n      0.006"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#random-1",
    "href": "posts/GCN/2023-04-27-simulation_table.html#random-1",
    "title": "Simulation Tables",
    "section": "Random",
    "text": "Random\n\ndata.query(\"dataset=='chickenpox' and mtype=='rand'and method=='GNAR'\")['mse'].unique().round(3)\n\narray([1.427])\n\n\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.3 and nof_filters==16\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      inter_method\n      nof_filters\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.3\n      cubic\n      16.0\n      IT-STGCN\n      1.019\n      0.011\n    \n    \n      1\n      0.3\n      cubic\n      16.0\n      STGCN\n      1.059\n      0.013\n    \n    \n      6\n      0.3\n      linear\n      16.0\n      IT-STGCN\n      1.015\n      0.009\n    \n    \n      7\n      0.3\n      linear\n      16.0\n      STGCN\n      1.040\n      0.014\n    \n  \n\n\n\n\n\npd.merge(data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.3 and nof_filters==16\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      inter_method\n      nof_filters\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.3\n      cubic\n      16\n      IT-STGCN\n      0.985\n      0.008\n    \n    \n      1\n      0.3\n      cubic\n      16\n      STGCN\n      1.053\n      0.008\n    \n    \n      2\n      0.3\n      linear\n      16\n      IT-STGCN\n      0.983\n      0.007\n    \n    \n      3\n      0.3\n      linear\n      16\n      STGCN\n      1.028\n      0.012\n    \n  \n\n\n\n\n\npd.merge(data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.4 and nof_filters==16\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      inter_method\n      nof_filters\n      method\n      mean\n      std\n    \n  \n  \n    \n      4\n      0.4\n      cubic\n      16\n      IT-STGCN\n      0.995\n      0.009\n    \n    \n      5\n      0.4\n      cubic\n      16\n      STGCN\n      1.069\n      0.011\n    \n    \n      6\n      0.4\n      linear\n      16\n      IT-STGCN\n      0.994\n      0.008\n    \n    \n      7\n      0.4\n      linear\n      16\n      STGCN\n      1.038\n      0.011\n    \n  \n\n\n\n\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.4 and nof_filters==16\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      inter_method\n      nof_filters\n      method\n      mean\n      std\n    \n  \n  \n    \n      12\n      0.4\n      cubic\n      16.0\n      IT-STGCN\n      1.021\n      0.009\n    \n    \n      13\n      0.4\n      cubic\n      16.0\n      STGCN\n      1.084\n      0.025\n    \n    \n      18\n      0.4\n      linear\n      16.0\n      IT-STGCN\n      1.020\n      0.009\n    \n    \n      19\n      0.4\n      linear\n      16.0\n      STGCN\n      1.051\n      0.014\n    \n  \n\n\n\n\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.5 and nof_filters==16\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      inter_method\n      nof_filters\n      method\n      mean\n      std\n    \n  \n  \n    \n      24\n      0.5\n      cubic\n      16.0\n      IT-STGCN\n      1.027\n      0.012\n    \n    \n      25\n      0.5\n      cubic\n      16.0\n      STGCN\n      1.128\n      0.042\n    \n    \n      30\n      0.5\n      linear\n      16.0\n      IT-STGCN\n      1.026\n      0.014\n    \n    \n      31\n      0.5\n      linear\n      16.0\n      STGCN\n      1.071\n      0.016\n    \n  \n\n\n\n\n\npd.merge(data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.5 and nof_filters==16\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      inter_method\n      nof_filters\n      method\n      mean\n      std\n    \n  \n  \n    \n      8\n      0.5\n      cubic\n      16\n      IT-STGCN\n      1.011\n      0.007\n    \n    \n      9\n      0.5\n      cubic\n      16\n      STGCN\n      1.080\n      0.019\n    \n    \n      10\n      0.5\n      linear\n      16\n      IT-STGCN\n      1.008\n      0.007\n    \n    \n      11\n      0.5\n      linear\n      16\n      STGCN\n      1.055\n      0.010\n    \n  \n\n\n\n\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.8 and nof_filters==16\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      inter_method\n      nof_filters\n      method\n      mean\n      std\n    \n  \n  \n    \n      36\n      0.8\n      cubic\n      16.0\n      IT-STGCN\n      1.206\n      0.117\n    \n    \n      37\n      0.8\n      cubic\n      16.0\n      STGCN\n      1.266\n      0.152\n    \n    \n      42\n      0.8\n      linear\n      16.0\n      IT-STGCN\n      1.101\n      0.034\n    \n    \n      43\n      0.8\n      linear\n      16.0\n      STGCN\n      1.166\n      0.059\n    \n  \n\n\n\n\n\npd.merge(data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.8 and nof_filters==16\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      inter_method\n      nof_filters\n      method\n      mean\n      std\n    \n  \n  \n    \n      12\n      0.8\n      cubic\n      16\n      IT-STGCN\n      1.181\n      0.142\n    \n    \n      13\n      0.8\n      cubic\n      16\n      STGCN\n      1.417\n      0.663\n    \n    \n      14\n      0.8\n      linear\n      16\n      IT-STGCN\n      1.058\n      0.015\n    \n    \n      15\n      0.8\n      linear\n      16\n      STGCN\n      1.102\n      0.027\n    \n  \n\n\n\n\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.9 and nof_filters==16\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      inter_method\n      nof_filters\n      method\n      mean\n      std\n    \n  \n  \n    \n      48\n      0.9\n      cubic\n      16.0\n      IT-STGCN\n      1.228\n      0.199\n    \n    \n      49\n      0.9\n      cubic\n      16.0\n      STGCN\n      1.283\n      0.222\n    \n    \n      54\n      0.9\n      linear\n      16.0\n      IT-STGCN\n      1.251\n      0.106\n    \n    \n      55\n      0.9\n      linear\n      16.0\n      STGCN\n      1.265\n      0.148\n    \n  \n\n\n\n\n\npd.merge(data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].mean().reset_index(),\n         data_DCRNN_chickenpox.query(\"dataset=='chickenpox' and mtype=='rand'\").groupby(['mrate','inter_method','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.9 and nof_filters==16\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      inter_method\n      nof_filters\n      method\n      mean\n      std\n    \n  \n  \n    \n      16\n      0.9\n      cubic\n      16\n      IT-STGCN\n      2.372\n      1.841\n    \n    \n      17\n      0.9\n      cubic\n      16\n      STGCN\n      1.596\n      0.648\n    \n    \n      18\n      0.9\n      linear\n      16\n      IT-STGCN\n      1.090\n      0.045\n    \n    \n      19\n      0.9\n      linear\n      16\n      STGCN\n      1.179\n      0.127"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#block-1",
    "href": "posts/GCN/2023-04-27-simulation_table.html#block-1",
    "title": "Simulation Tables",
    "section": "Block",
    "text": "Block\n\ndata.query(\"dataset=='chickenpox' and mtype=='block'and method=='GNAR'\")['mse'].unique()\n\narray([1.42749429])\n\n\n\npd.merge(data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='chickenpox' and mtype=='block'\").groupby(['inter_method','mrate','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','inter_method','mrate','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"nof_filters==16\")\n\n\n\n\n\n  \n    \n      \n      inter_method\n      mrate\n      nof_filters\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      cubic\n      0.288\n      16.0\n      IT-STGCN\n      1.052\n      0.028\n    \n    \n      1\n      cubic\n      0.288\n      16.0\n      STGCN\n      1.052\n      0.023\n    \n    \n      6\n      linear\n      0.288\n      16.0\n      IT-STGCN\n      1.008\n      0.005\n    \n    \n      7\n      linear\n      0.288\n      16.0\n      STGCN\n      1.011\n      0.008"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#baseline-2",
    "href": "posts/GCN/2023-04-27-simulation_table.html#baseline-2",
    "title": "Simulation Tables",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','lags','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n  \n    \n      \n      lags\n      nof_filters\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      4\n      12.0\n      IT-STGCN\n      1.241\n      0.04\n    \n    \n      1\n      4\n      12.0\n      STGCN\n      1.271\n      0.04\n    \n  \n\n\n\n\n\n(1.241+1.271)/2\n\n1.256\n\n\n\npd.merge(data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index(),\n         data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype!='rand' and mtype!='block'\").groupby(['lags','nof_filters','method'])['mse'].std().reset_index(),\n         on=['method','lags','nof_filters']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n  \n    \n      \n      lags\n      nof_filters\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      4\n      12\n      IT-STGCN\n      1.204\n      0.020\n    \n    \n      1\n      4\n      12\n      STGCN\n      1.203\n      0.022\n    \n  \n\n\n\n\n\n(1.204+1.203)/2\n\n1.2035"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#random-2",
    "href": "posts/GCN/2023-04-27-simulation_table.html#random-2",
    "title": "Simulation Tables",
    "section": "Random",
    "text": "Random\n\ndata.query(\"dataset=='pedalme' and method=='GNAR' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index().query(\" lags==4\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      inter_method\n      method\n      mse\n    \n  \n  \n    \n      0\n      0.3\n      4\n      cubic\n      GNAR\n      1.302679\n    \n    \n      1\n      0.3\n      4\n      linear\n      GNAR\n      1.302679\n    \n    \n      2\n      0.4\n      4\n      cubic\n      GNAR\n      1.302679\n    \n    \n      3\n      0.4\n      4\n      linear\n      GNAR\n      1.302679\n    \n    \n      4\n      0.5\n      4\n      cubic\n      GNAR\n      1.302679\n    \n    \n      5\n      0.5\n      4\n      linear\n      GNAR\n      1.302679\n    \n    \n      6\n      0.6\n      4\n      cubic\n      GNAR\n      1.302679\n    \n    \n      7\n      0.6\n      4\n      linear\n      GNAR\n      1.302679\n    \n  \n\n\n\n\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.3 and lags==4\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      inter_method\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.3\n      4\n      cubic\n      GNAR\n      1.303\n      0.000\n    \n    \n      1\n      0.3\n      4\n      cubic\n      IT-STGCN\n      1.314\n      0.109\n    \n    \n      2\n      0.3\n      4\n      cubic\n      STGCN\n      1.363\n      0.115\n    \n    \n      3\n      0.3\n      4\n      linear\n      GNAR\n      1.303\n      0.000\n    \n    \n      4\n      0.3\n      4\n      linear\n      IT-STGCN\n      1.323\n      0.094\n    \n    \n      5\n      0.3\n      4\n      linear\n      STGCN\n      1.380\n      0.127\n    \n  \n\n\n\n\n\npd.merge(data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.3 and  lags==4\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      inter_method\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.3\n      4\n      cubic\n      IT-STGCN\n      1.223\n      0.031\n    \n    \n      1\n      0.3\n      4\n      cubic\n      STGCN\n      1.248\n      0.039\n    \n    \n      2\n      0.3\n      4\n      linear\n      IT-STGCN\n      1.227\n      0.026\n    \n    \n      3\n      0.3\n      4\n      linear\n      STGCN\n      1.242\n      0.031\n    \n    \n      4\n      0.3\n      4\n      nearest\n      IT-STGCN\n      1.231\n      0.032\n    \n    \n      5\n      0.3\n      4\n      nearest\n      STGCN\n      1.229\n      0.032\n    \n  \n\n\n\n\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.4 and lags==4\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      inter_method\n      method\n      mean\n      std\n    \n  \n  \n    \n      10\n      0.4\n      4\n      cubic\n      GNAR\n      1.303\n      0.000\n    \n    \n      11\n      0.4\n      4\n      cubic\n      IT-STGCN\n      1.331\n      0.112\n    \n    \n      12\n      0.4\n      4\n      cubic\n      STGCN\n      1.342\n      0.108\n    \n    \n      13\n      0.4\n      4\n      linear\n      GNAR\n      1.303\n      0.000\n    \n    \n      14\n      0.4\n      4\n      linear\n      IT-STGCN\n      1.375\n      0.154\n    \n    \n      15\n      0.4\n      4\n      linear\n      STGCN\n      1.397\n      0.193\n    \n  \n\n\n\n\n\npd.merge(data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\" mrate==0.4 and lags==4\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      inter_method\n      method\n      mean\n      std\n    \n  \n  \n    \n      6\n      0.4\n      4\n      cubic\n      IT-STGCN\n      1.230\n      0.030\n    \n    \n      7\n      0.4\n      4\n      cubic\n      STGCN\n      1.257\n      0.051\n    \n    \n      8\n      0.4\n      4\n      linear\n      IT-STGCN\n      1.231\n      0.032\n    \n    \n      9\n      0.4\n      4\n      linear\n      STGCN\n      1.251\n      0.040\n    \n    \n      10\n      0.4\n      4\n      nearest\n      IT-STGCN\n      1.235\n      0.031\n    \n    \n      11\n      0.4\n      4\n      nearest\n      STGCN\n      1.241\n      0.033\n    \n  \n\n\n\n\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.5 and lags==4\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      inter_method\n      method\n      mean\n      std\n    \n  \n  \n    \n      20\n      0.5\n      4\n      cubic\n      GNAR\n      1.303\n      0.000\n    \n    \n      21\n      0.5\n      4\n      cubic\n      IT-STGCN\n      1.328\n      0.108\n    \n    \n      22\n      0.5\n      4\n      cubic\n      STGCN\n      1.367\n      0.114\n    \n    \n      23\n      0.5\n      4\n      linear\n      GNAR\n      1.303\n      0.000\n    \n    \n      24\n      0.5\n      4\n      linear\n      IT-STGCN\n      1.377\n      0.138\n    \n    \n      25\n      0.5\n      4\n      linear\n      STGCN\n      1.326\n      0.129\n    \n  \n\n\n\n\n\npd.merge(data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\" mrate==0.5 and lags==4\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      inter_method\n      method\n      mean\n      std\n    \n  \n  \n    \n      12\n      0.5\n      4\n      cubic\n      IT-STGCN\n      1.251\n      0.034\n    \n    \n      13\n      0.5\n      4\n      cubic\n      STGCN\n      1.279\n      0.095\n    \n    \n      14\n      0.5\n      4\n      linear\n      IT-STGCN\n      1.241\n      0.037\n    \n    \n      15\n      0.5\n      4\n      linear\n      STGCN\n      1.268\n      0.052\n    \n    \n      16\n      0.5\n      4\n      nearest\n      IT-STGCN\n      1.245\n      0.034\n    \n    \n      17\n      0.5\n      4\n      nearest\n      STGCN\n      1.256\n      0.043\n    \n  \n\n\n\n\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.6 and lags==4\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      inter_method\n      method\n      mean\n      std\n    \n  \n  \n    \n      30\n      0.6\n      4\n      cubic\n      GNAR\n      1.303\n      0.000\n    \n    \n      31\n      0.6\n      4\n      cubic\n      IT-STGCN\n      1.300\n      0.063\n    \n    \n      32\n      0.6\n      4\n      cubic\n      STGCN\n      1.352\n      0.106\n    \n    \n      33\n      0.6\n      4\n      linear\n      GNAR\n      1.303\n      0.000\n    \n    \n      34\n      0.6\n      4\n      linear\n      IT-STGCN\n      1.416\n      0.169\n    \n    \n      35\n      0.6\n      4\n      linear\n      STGCN\n      1.326\n      0.106\n    \n  \n\n\n\n\n\npd.merge(data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_DCRNN_pedalme.query(\"dataset=='pedalme' and mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"mrate==0.6 and  lags==4\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      inter_method\n      method\n      mean\n      std\n    \n  \n  \n    \n      18\n      0.6\n      4\n      cubic\n      IT-STGCN\n      1.259\n      0.052\n    \n    \n      19\n      0.6\n      4\n      cubic\n      STGCN\n      1.313\n      0.193\n    \n    \n      20\n      0.6\n      4\n      linear\n      IT-STGCN\n      1.243\n      0.036\n    \n    \n      21\n      0.6\n      4\n      linear\n      STGCN\n      1.280\n      0.064\n    \n    \n      22\n      0.6\n      4\n      nearest\n      IT-STGCN\n      1.254\n      0.037\n    \n    \n      23\n      0.6\n      4\n      nearest\n      STGCN\n      1.271\n      0.050"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#block-2",
    "href": "posts/GCN/2023-04-27-simulation_table.html#block-2",
    "title": "Simulation Tables",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='pedalme' and mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      inter_method\n      method\n      mean\n      std\n    \n  \n  \n    \n      4\n      0.143\n      4\n      cubic\n      GNAR\n      1.303\n      0.000\n    \n    \n      5\n      0.143\n      4\n      cubic\n      IT-STGCN\n      1.284\n      0.053\n    \n    \n      6\n      0.143\n      4\n      cubic\n      STGCN\n      1.288\n      0.071\n    \n    \n      7\n      0.143\n      4\n      linear\n      GNAR\n      1.303\n      0.000\n    \n    \n      14\n      0.286\n      4\n      cubic\n      GNAR\n      1.303\n      0.000\n    \n    \n      15\n      0.286\n      4\n      cubic\n      IT-STGCN\n      1.304\n      0.050\n    \n    \n      16\n      0.286\n      4\n      cubic\n      STGCN\n      1.377\n      0.061\n    \n    \n      17\n      0.286\n      4\n      linear\n      GNAR\n      1.303\n      0.000\n    \n    \n      18\n      0.286\n      4\n      linear\n      IT-STGCN\n      1.335\n      0.062\n    \n    \n      19\n      0.286\n      4\n      linear\n      STGCN\n      1.350\n      0.056"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#w_st",
    "href": "posts/GCN/2023-04-27-simulation_table.html#w_st",
    "title": "Simulation Tables",
    "section": "W_st",
    "text": "W_st\n\ndata_pedalme_wst = pd.read_csv('./simulation_results/Real_simulation/pedalme_Simulation_itstgcnsnd.csv')\n\n\npd.merge(data_pedalme_wst.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedalme_wst.query(\"mtype=='rand'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      inter_method\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.3\n      4\n      cubic\n      IT-STGCN\n      1.353\n      0.141\n    \n    \n      1\n      0.3\n      4\n      cubic\n      STGCN\n      1.360\n      0.131\n    \n    \n      2\n      0.3\n      4\n      linear\n      IT-STGCN\n      1.337\n      0.122\n    \n    \n      3\n      0.3\n      4\n      linear\n      STGCN\n      1.353\n      0.117\n    \n    \n      4\n      0.3\n      4\n      nearest\n      IT-STGCN\n      1.316\n      0.122\n    \n    \n      5\n      0.3\n      4\n      nearest\n      STGCN\n      1.403\n      0.134\n    \n    \n      12\n      0.4\n      4\n      cubic\n      IT-STGCN\n      1.332\n      0.166\n    \n    \n      13\n      0.4\n      4\n      cubic\n      STGCN\n      1.344\n      0.123\n    \n    \n      14\n      0.4\n      4\n      linear\n      IT-STGCN\n      1.355\n      0.139\n    \n    \n      15\n      0.4\n      4\n      linear\n      STGCN\n      1.393\n      0.168\n    \n    \n      16\n      0.4\n      4\n      nearest\n      IT-STGCN\n      1.386\n      0.128\n    \n    \n      17\n      0.4\n      4\n      nearest\n      STGCN\n      1.341\n      0.129\n    \n    \n      24\n      0.5\n      4\n      cubic\n      IT-STGCN\n      1.312\n      0.152\n    \n    \n      25\n      0.5\n      4\n      cubic\n      STGCN\n      1.362\n      0.129\n    \n    \n      26\n      0.5\n      4\n      linear\n      IT-STGCN\n      1.344\n      0.177\n    \n    \n      27\n      0.5\n      4\n      linear\n      STGCN\n      1.335\n      0.117\n    \n    \n      28\n      0.5\n      4\n      nearest\n      IT-STGCN\n      1.335\n      0.153\n    \n    \n      29\n      0.5\n      4\n      nearest\n      STGCN\n      1.350\n      0.129\n    \n    \n      36\n      0.6\n      4\n      cubic\n      IT-STGCN\n      1.346\n      0.151\n    \n    \n      37\n      0.6\n      4\n      cubic\n      STGCN\n      1.398\n      0.103\n    \n    \n      38\n      0.6\n      4\n      linear\n      IT-STGCN\n      1.365\n      0.177\n    \n    \n      39\n      0.6\n      4\n      linear\n      STGCN\n      1.353\n      0.087\n    \n    \n      40\n      0.6\n      4\n      nearest\n      IT-STGCN\n      1.402\n      0.269\n    \n    \n      41\n      0.6\n      4\n      nearest\n      STGCN\n      1.339\n      0.111\n    \n    \n      48\n      0.7\n      4\n      cubic\n      IT-STGCN\n      1.377\n      0.173\n    \n    \n      49\n      0.7\n      4\n      cubic\n      STGCN\n      1.363\n      0.097\n    \n    \n      50\n      0.7\n      4\n      linear\n      IT-STGCN\n      1.355\n      0.144\n    \n    \n      51\n      0.7\n      4\n      linear\n      STGCN\n      1.288\n      0.063\n    \n    \n      52\n      0.7\n      4\n      nearest\n      IT-STGCN\n      1.383\n      0.157\n    \n    \n      53\n      0.7\n      4\n      nearest\n      STGCN\n      1.334\n      0.124\n    \n  \n\n\n\n\n\npd.merge(data_pedalme_wst.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].mean().reset_index(),\n         data_pedalme_wst.query(\"mtype=='block'\").groupby(['mrate','lags','inter_method','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags','inter_method']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==4\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      inter_method\n      method\n      mean\n      std\n    \n  \n  \n    \n      6\n      0.286\n      4\n      cubic\n      IT-STGCN\n      1.260\n      0.063\n    \n    \n      7\n      0.286\n      4\n      cubic\n      STGCN\n      1.417\n      0.065\n    \n    \n      8\n      0.286\n      4\n      linear\n      IT-STGCN\n      1.276\n      0.065\n    \n    \n      9\n      0.286\n      4\n      linear\n      STGCN\n      1.288\n      0.055\n    \n    \n      10\n      0.286\n      4\n      nearest\n      IT-STGCN\n      1.275\n      0.061\n    \n    \n      11\n      0.286\n      4\n      nearest\n      STGCN\n      1.312\n      0.061"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#baseline-3",
    "href": "posts/GCN/2023-04-27-simulation_table.html#baseline-3",
    "title": "Simulation Tables",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index().round(3).query(\"lags==8\")\n\n\n\n\n\n  \n    \n      \n      lags\n      nof_filters\n      method\n      mse\n    \n  \n  \n    \n      4\n      8\n      12.0\n      IT-STGCN\n      0.771\n    \n    \n      5\n      8\n      12.0\n      STGCN\n      0.772\n    \n  \n\n\n\n\n\ndata_DCRNN_wikimath.query(\"dataset=='wikimath' and mrate==0\").groupby(['lags','nof_filters','method'])['mse'].mean().reset_index().round(3).query(\"lags==8\")\n\n\n\n\n\n  \n    \n      \n      lags\n      nof_filters\n      method\n      mse\n    \n  \n  \n    \n      0\n      8\n      12\n      IT-STGCN\n      0.778\n    \n    \n      1\n      8\n      12\n      STGCN\n      0.759"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#random-3",
    "href": "posts/GCN/2023-04-27-simulation_table.html#random-3",
    "title": "Simulation Tables",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==8\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      method\n      mean\n      std\n    \n  \n  \n    \n      6\n      0.3\n      8\n      IT-STGCN\n      0.781\n      0.012\n    \n    \n      7\n      0.3\n      8\n      STGCN\n      0.779\n      0.013\n    \n    \n      14\n      0.5\n      8\n      IT-STGCN\n      0.802\n      0.041\n    \n    \n      15\n      0.5\n      8\n      STGCN\n      0.806\n      0.020\n    \n  \n\n\n\n\n\npd.merge(data_DCRNN_wikimath.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data_DCRNN_wikimath.query(\"dataset=='wikimath' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3).query(\"lags==8\")\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.3\n      8\n      IT-STGCN\n      0.759\n      0.021\n    \n    \n      1\n      0.3\n      8\n      STGCN\n      0.774\n      0.030"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#block-3",
    "href": "posts/GCN/2023-04-27-simulation_table.html#block-3",
    "title": "Simulation Tables",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='wikimath' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.003841\n      2\n      IT-STGCN\n      0.810475\n      0.033897\n    \n    \n      1\n      0.003841\n      2\n      STGCN\n      0.801502\n      0.015510\n    \n    \n      2\n      0.003841\n      4\n      IT-STGCN\n      0.779852\n      0.013188\n    \n    \n      3\n      0.003841\n      4\n      STGCN\n      0.779816\n      0.019309"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#missing-values-on-the-same-nodes",
    "href": "posts/GCN/2023-04-27-simulation_table.html#missing-values-on-the-same-nodes",
    "title": "Simulation Tables",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\ndata_wikimath_st = pd.read_csv('./simulation_results/Real_simulation/wikimath_GSO_st.csv')\n\n\npd.merge(data_wikimath_st.groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n        data_wikimath_st.groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.123\n      4\n      IT-STGCN\n      0.774\n      0.008\n    \n    \n      1\n      0.123\n      4\n      STGCN\n      0.766\n      0.010\n    \n    \n      2\n      0.738\n      4\n      IT-STGCN\n      0.851\n      0.029\n    \n    \n      3\n      0.738\n      4\n      STGCN\n      0.831\n      0.031"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#baseline-4",
    "href": "posts/GCN/2023-04-27-simulation_table.html#baseline-4",
    "title": "Simulation Tables",
    "section": "Baseline",
    "text": "Baseline\n\npd.merge(data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n  \n    \n      \n      lags\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      8\n      GNAR\n      1.649\n      0.000\n    \n    \n      1\n      8\n      IT-STGCN\n      1.006\n      0.006\n    \n    \n      2\n      8\n      STGCN\n      1.001\n      0.003\n    \n  \n\n\n\n\n\npd.merge(data_DCRNN_windmillsmall.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].mean().reset_index(),\n         data_DCRNN_windmillsmall.query(\"dataset=='windmillsmall' and mrate==0\").groupby(['lags','method'])['mse'].std().reset_index(),\n         on=['method','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n  \n    \n      \n      lags\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      8\n      IT-STGCN\n      1.000\n      NaN\n    \n    \n      1\n      8\n      STGCN\n      1.001\n      NaN"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#random-4",
    "href": "posts/GCN/2023-04-27-simulation_table.html#random-4",
    "title": "Simulation Tables",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.7\n      8\n      GNAR\n      1.649\n      0.000\n    \n    \n      1\n      0.7\n      8\n      IT-STGCN\n      1.178\n      0.054\n    \n    \n      2\n      0.7\n      8\n      STGCN\n      1.410\n      0.075\n    \n  \n\n\n\n\n\npd.merge(data_DCRNN_windmillsmall.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data_DCRNN_windmillsmall.query(\"dataset=='windmillsmall' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n  \n    \n      \n      mean\n      mrate\n      lags\n      method\n      std"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#block-4",
    "href": "posts/GCN/2023-04-27-simulation_table.html#block-4",
    "title": "Simulation Tables",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='windmillsmall' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'}).round(3)\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.325\n      8\n      GNAR\n      1.649\n      0.000\n    \n    \n      1\n      0.325\n      8\n      IT-STGCN\n      1.015\n      0.009\n    \n    \n      2\n      0.325\n      8\n      STGCN\n      1.017\n      0.008"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#baseline-5",
    "href": "posts/GCN/2023-04-27-simulation_table.html#baseline-5",
    "title": "Simulation Tables",
    "section": "Baseline",
    "text": "Baseline\n\nround(data.query(\"dataset=='monte' and mrate==0\")['mse'].mean(),3),round(data_monte.query(\"mrate==0\")['mse'].std(),3)\n\n(0.97, 0.024)"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#random-5",
    "href": "posts/GCN/2023-04-27-simulation_table.html#random-5",
    "title": "Simulation Tables",
    "section": "Random",
    "text": "Random\n\npd.merge(data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='rand'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.3\n      4\n      GNAR\n      1.061937\n      0.000000\n    \n    \n      1\n      0.3\n      4\n      IT-STGCN\n      0.971925\n      0.001871\n    \n    \n      2\n      0.3\n      4\n      STGCN\n      0.965885\n      0.002552\n    \n    \n      3\n      0.3\n      8\n      GNAR\n      1.068464\n      0.000000\n    \n    \n      4\n      0.3\n      8\n      IT-STGCN\n      0.974867\n      0.003233\n    \n    \n      5\n      0.3\n      8\n      STGCN\n      0.972051\n      0.002383\n    \n    \n      6\n      0.4\n      4\n      GNAR\n      1.061937\n      0.000000\n    \n    \n      7\n      0.4\n      4\n      IT-STGCN\n      0.976348\n      0.001374\n    \n    \n      8\n      0.4\n      4\n      STGCN\n      0.967480\n      0.002974\n    \n    \n      9\n      0.4\n      8\n      GNAR\n      1.068464\n      0.000000\n    \n    \n      10\n      0.4\n      8\n      IT-STGCN\n      0.978809\n      0.002110\n    \n    \n      11\n      0.4\n      8\n      STGCN\n      0.973098\n      0.002613\n    \n    \n      12\n      0.8\n      4\n      GNAR\n      1.061937\n      0.000000\n    \n    \n      13\n      0.8\n      4\n      IT-STGCN\n      1.006583\n      0.003297\n    \n    \n      14\n      0.8\n      4\n      STGCN\n      0.999715\n      0.006909\n    \n    \n      15\n      0.8\n      8\n      GNAR\n      1.068464\n      0.000000\n    \n    \n      16\n      0.8\n      8\n      IT-STGCN\n      1.004450\n      0.002953\n    \n    \n      17\n      0.8\n      8\n      STGCN\n      1.005238\n      0.005777\n    \n    \n      18\n      0.9\n      4\n      GNAR\n      1.061937\n      0.000000\n    \n    \n      19\n      0.9\n      4\n      IT-STGCN\n      1.034162\n      0.005611\n    \n    \n      20\n      0.9\n      4\n      STGCN\n      1.034995\n      0.006551\n    \n    \n      21\n      0.9\n      8\n      GNAR\n      1.068464\n      0.000000\n    \n    \n      22\n      0.9\n      8\n      IT-STGCN\n      1.030283\n      0.007979\n    \n    \n      23\n      0.9\n      8\n      STGCN\n      1.031538\n      0.009285"
  },
  {
    "objectID": "posts/GCN/2023-04-27-simulation_table.html#block-5",
    "href": "posts/GCN/2023-04-27-simulation_table.html#block-5",
    "title": "Simulation Tables",
    "section": "Block",
    "text": "Block\n\npd.merge(data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].mean().reset_index(),\n         data.query(\"dataset=='monte' and mtype=='block'\").groupby(['mrate','lags','method'])['mse'].std().reset_index(),\n         on=['method','mrate','lags']).rename(columns={'mse_x':'mean','mse_y':'std'})\n\n\n\n\n\n  \n    \n      \n      mrate\n      lags\n      method\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.149142\n      4\n      GNAR\n      1.061937\n      0.000000\n    \n    \n      1\n      0.149142\n      4\n      IT-STGCN\n      0.963990\n      0.002194\n    \n    \n      2\n      0.149142\n      4\n      STGCN\n      0.965297\n      0.001611\n    \n    \n      3\n      0.149142\n      8\n      GNAR\n      1.068464\n      0.000000\n    \n    \n      4\n      0.149142\n      8\n      IT-STGCN\n      0.971647\n      0.002860\n    \n    \n      5\n      0.149142\n      8\n      STGCN\n      0.971700\n      0.001672"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html",
    "href": "posts/GCN/2023-01-05-GNAR.html",
    "title": "GNAR data",
    "section": "",
    "text": "GNAR\nGNAR"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#gnar-network-example",
    "href": "posts/GCN/2023-01-05-GNAR.html#gnar-network-example",
    "title": "GNAR data",
    "section": "2.3 GNAR network example",
    "text": "2.3 GNAR network example\n\nedge(list)\ndist(list)\n\n\n%%R\nplot(fiveNet, vertex.label = c(\"A\", \"B\", \"C\", \"D\", \"E\"))\n\n\n\n\n\n%%R\nsummary(\"fiveNet\")\n\n   Length     Class      Mode \n        1 character character \n\n\nother examples\n\nigraphtoGNAR or GNARtoigraph쓰는 예제\n\n\n%%R\nfiveNet2 <- GNARtoigraph(net = fiveNet)\nsummary(fiveNet2)\n\nIGRAPH 2b4460d U-W- 5 5 -- \n+ attr: weight (e/n)\n\n\n\n%%R\nfiveNet3 <- igraphtoGNAR(fiveNet2)\nall.equal(fiveNet, fiveNet3)\n\n[1] TRUE\n\n\n\n%%R\nprint(igraphtoGNAR(fiveNet2))\n\nGNARnet with 5 nodes \nedges:1--4 1--5 2--3 2--4 3--2 3--4 4--1 4--2 4--3 5--1 \n     \n edges of each of length  1 \n\n\nedge들 보고 싶을 때\nwhereas the reverse conversion would be performed as\n\n%%R\ng <- make_ring(10)\nprint(igraphtoGNAR(g))\n\nGNARnet with 10 nodes \nedges:1--2 1--10 2--1 2--3 3--2 3--4 4--3 4--5 5--4 5--6 \n     6--5 6--7 7--6 7--8 8--7 8--9 9--8 9--10 10--1 10--9 \n     \n edges of each of length  1 \n\n\n\n%%R\nmake_ring(10)\n\nIGRAPH 22f6be5 U--- 10 10 -- Ring graph\n+ attr: name (g/c), mutual (g/l), circular (g/l)\n+ edges from 22f6be5:\n [1] 1-- 2 2-- 3 3-- 4 4-- 5 5-- 6 6-- 7 7-- 8 8-- 9 9--10 1--10\n\n\n이어진 방향으로 각각의 edge를 만들어주는 게 igrapphtoGNAR이다\nGNARtoigraph function으로 높은 수준의 이웃 구조를 포함한 그래프를 추출할 수 있다.\n\nas.matrix or matrixtoGNAR로 인접 행렬 구할 수 있음\n\nwe can prosucean adjacency matrix for the fiveNet obeject with\n\n%%R\nas.matrix(fiveNet)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    1    1\n[2,]    0    0    1    1    0\n[3,]    0    1    0    1    0\n[4,]    1    1    1    0    0\n[5,]    1    0    0    0    0\n\n\nand an example converting a weighted adjacency matrix to a GNARnet object is\n\n%%R\nadj <- matrix(runif(9), ncol = 3, nrow = 3)\nadj[adj < 0.3] <- 0\nprint(matrixtoGNAR(adj))\n\nWARNING: diagonal entries present in original matrix, these will be removed\nGNARnet with 3 nodes \nedges:1--2 1--3 2--1 2--3 3--1 3--2 \n edges of unequal lengths"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#example-gnar-model-fitting",
    "href": "posts/GCN/2023-01-05-GNAR.html#example-gnar-model-fitting",
    "title": "GNAR data",
    "section": "2.4. Example: GNAR model fitting",
    "text": "2.4. Example: GNAR model fitting\n\nGNAR로 fit과 predict 가능\n\n\n%%R\ndata(\"fiveNode\")\nanswer <- GNARfit(vts = fiveVTS, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nanswer\n\nModel: \nGNAR(2,[1,1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\n dmatalpha1  dmatbeta1.1   dmatalpha2  dmatbeta2.1  \n    0.20624      0.50277      0.02124     -0.09523  \n\n\n\n\n파라메터 4개 가지고 있음\n\n\n%%R\nlayout(matrix(c(1, 2), 2, 1))\nplot(fiveVTS[, 1], ylab = \"Node A Time Series\")\nlines(fitted(answer)[, 1], col = 2)\nplot(fiveVTS[, 2], ylab = \"Node B Time Series\")\nlines(fitted(answer)[, 2], col = 2)\n\n\n\n\n\n%%R\nlayout(matrix(c(1, 2), 2, 1))\nplot(fiveVTS[, 3], ylab = \"Node C Time Series\")\nlines(fitted(answer)[, 3], col = 2)\nplot(fiveVTS[, 4], ylab = \"Node D Time Series\")\nlines(fitted(answer)[, 4], col = 2)\n\n\n\n\n\n각 노드의 time series(검정), fitted values from ‘answer’ model overlaid in red\n\n\n%%R\nmyresiduals <- residuals(answer)[, 1]\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(residuals(answer)[, 1]), ylab = \"`answer' model residuals\")\nhist(residuals(answer)[, 1], main = \"\", xlab = \"`answer' model residuals\")\n\n\n\n\n\n%%R\nmyresiduals <- residuals(answer)[, 2]\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(residuals(answer)[, 1]), ylab = \"`answer' model residuals\")\nhist(residuals(answer)[, 2], main = \"\", xlab = \"`answer' model residuals\")\n\n\n\n\n\n%%R\nmyresiduals <- residuals(answer)[, 3]\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(residuals(answer)[, 1]), ylab = \"`answer' model residuals\")\nhist(residuals(answer)[, 3], main = \"\", xlab = \"`answer' model residuals\")\n\n\n\n\n\n%%R\nmyresiduals <- residuals(answer)[, 4]\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(residuals(answer)[, 1]), ylab = \"`answer' model residuals\")\nhist(residuals(answer)[, 4], main = \"\", xlab = \"`answer' model residuals\")\n\n\n\n\n\nresidual plots from ‘answer’ model fit. Top: time sereies, Bottom: Histogram"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#example-gnar-data-simulation-on-a-given-network",
    "href": "posts/GCN/2023-01-05-GNAR.html#example-gnar-data-simulation-on-a-given-network",
    "title": "GNAR data",
    "section": "2.5. Example: GNAR data simulation on a given network",
    "text": "2.5. Example: GNAR data simulation on a given network\n\nfiveNet 네트워크를 사용하여 네트워크 시계열 시뮬레이션 진행\n두 시뮬레이션 모두 sigma argument를 사용하여 표준 편차가 제어되는 표준 정규 노이즈를 사용하여 생성된다.\n\n\n%%R\nset.seed(10)\nfiveVTS2 <- GNARsim(n = 200, net = fiveNet, alphaParams = list(c(0.4, 0, -0.6, 0, 0)), betaParams = list(c(0.3)))\n\n\nfiveVTS2 네트워크를 사용하여 시뮬레이션 된 것이다보니 파라메터 계수 비슷\n\n\n%%R\nprint(GNARfit(vts = fiveVTS2, net = fiveNet, alphaOrder = 1, betaOrder = 1, globalalpha = FALSE))\n\nModel: \nGNAR(1,[1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\ndmatalpha1node1  dmatalpha1node2  dmatalpha1node3  dmatalpha1node4  \n        0.45902          0.13133         -0.49166          0.03828  \ndmatalpha1node5      dmatbeta1.1  \n        0.02249          0.24848  \n\n\n\n\n%%R\nset.seed(10)\nfiveVTS3 <- GNARsim(n = 200, net = fiveNet, alphaParams = list(rep(0.2, 5), rep(0.3, 5)), betaParams = list(c(0.2, 0.3), c(0)))\nprint(GNARfit(vts = fiveVTS3, net = fiveNet, alphaOrder = 2, betaOrder = c(2,0)))\n\nModel: \nGNAR(2,[2,0]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\n dmatalpha1  dmatbeta1.1  dmatbeta1.2   dmatalpha2  \n     0.2537       0.1049       0.3146       0.2907  \n\n\n\n\n%%R\nfiveVTS4 <- simulate(GNARfit(vts = fiveVTS2, net = fiveNet, alphaOrder = 1, betaOrder = 1, globalalpha = FALSE), n = 200)\nprint(GNARfit(vts = fiveVTS4, net = fiveNet, alphaOrder = 1, betaOrder = 1, globalalpha = FALSE))\n\nModel: \nGNAR(1,[1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\ndmatalpha1node1  dmatalpha1node2  dmatalpha1node3  dmatalpha1node4  \n      0.4478300       -0.0008695       -0.4822675        0.0523652  \ndmatalpha1node5      dmatbeta1.1  \n     -0.0063702        0.2249530  \n\n\n\n\n위와 같이 GNAR 모델에 있는 시계열을 simulate하기 위해 GNARfit object에 대해 simulate S3 method 사용할 수 있다"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#missing-data-and-changing-connection-weights-with-gnar-models",
    "href": "posts/GCN/2023-01-05-GNAR.html#missing-data-and-changing-connection-weights-with-gnar-models",
    "title": "GNAR data",
    "section": "2.6 Missing data and changing connection weights with GNAR models",
    "text": "2.6 Missing data and changing connection weights with GNAR models\n\nThe flexibility of GNAR modelling이 의미하는 것은 연결 가중치를 바꾸지 않고 변하는 네트워크로 missing data 를 모델링 할 수 있다.\n한 노드가 missing data 구간이 생기면 그 구간에서만 네트워크를 변화하여 weight가 변경된다.\n\n\n%%R\nfiveVTS0 <- fiveVTS\nfiveVTS0[50:150, 3] <- NA\nnafit <- GNARfit(vts = fiveVTS0, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(fitted(nafit)[, 3]), ylab = \"Node C fitted values\")\nplot(ts(fitted(nafit)[, 4]), ylab = \"Node D fitted values\")"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#stationary-conditions-for-a-gnar-process-with-fixed-network",
    "href": "posts/GCN/2023-01-05-GNAR.html#stationary-conditions-for-a-gnar-process-with-fixed-network",
    "title": "GNAR data",
    "section": "2.7. Stationary conditions for a GNAR process with fixed network",
    "text": "2.7. Stationary conditions for a GNAR process with fixed network\nTheorem 1\n\nGiven an unchanging network, \\(\\mathcal{G}\\) a sufficient condition for the GNAT model (1) to be stationary is\n\n\\[\\sum^p_{j=1}(|\\alpha_{i,j}| + \\sum^{C}_{c=1} \\sum^{s_j}_{r=1} |\\beta_{j,t,c}|)<1 , \\forall_i \\in 1,\\dots, N\\]\n위 조건을 GNARsim을 이용하여 확인 할 수 있다.\n\n%%R\nset.seed(10)\nfiveVTS4 <- GNARsim(n = 200, net = fiveNet, alphaParams = list(rep(0.2, 5)), betaParams = list(c(0.85)))\nc(mean(fiveVTS4[1:50, ]), mean(fiveVTS4[51:100, ]), mean(fiveVTS4[101:150, ]), mean(fiveVTS4[151:200, ]))\n\n[1]    -120.511   -1370.216  -15725.884 -180319.140\n\n\n\nThe mean increases rapidly indicating nonstationarity.\n평균이 빠르게 증가하는 것으로 보아 정상성을 띄고 있지 않다."
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#benefits-of-our-model-and-comparisons-to-others",
    "href": "posts/GCN/2023-01-05-GNAR.html#benefits-of-our-model-and-comparisons-to-others",
    "title": "GNAR data",
    "section": "2.8. Benefits of our model and comparisons to others",
    "text": "2.8. Benefits of our model and comparisons to others"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#order-selection",
    "href": "posts/GCN/2023-01-05-GNAR.html#order-selection",
    "title": "GNAR data",
    "section": "3.1. Order selection",
    "text": "3.1. Order selection\nBayesian information criterion\n\\[BIC(p,s) = ln|\\sum^{\\hat{}}_{p,s}| + T^{-1} M ln(T)\\]\n\n%%R\nBIC(GNARfit())\n\n[1] -0.003953124\n\n\n\n%%R\nBIC(GNARfit(betaOrder = c(2, 1)))\n\n[1] 0.02251406\n\n\nAkaike information criterion\n\\[AIC(p,s) = ln|\\sum^{\\hat{}}_{p,s}| + 2T^{-1} M\\]\n\n%%R\nAIC(GNARfit())\n\n[1] -0.06991947\n\n\n\n%%R\nAIC(GNARfit(betaOrder = c(2, 1)))\n\n[1] -0.05994387"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#model-selection-on-a-wind-network-time-series",
    "href": "posts/GCN/2023-01-05-GNAR.html#model-selection-on-a-wind-network-time-series",
    "title": "GNAR data",
    "section": "3.2. Model selection on a wind network time series",
    "text": "3.2. Model selection on a wind network time series\nthe data suite vswind that contains a number of R objects pertaining to 721 wind speeds taken at each of 102 weather stations in England and Wales. The suite contains the vector time series vswindts, the associated network vswindnet, a character vector of the weather station location names in vswindnames and coordinates of the stations in the two column matrix vswindcoords. The data originate from the UK Met Office site http://wow.metoffice.gov.uk and full details can be found in the vswind help file in the GNAR package.\n\nnodes : 102\ntime step : 721\n\n\n%%R\noldpar <- par(cex = 0.75)\nwindnetplot()\npar(oldpar)\n\n\n\n\nPlot of the wind speed network\n\nblue numbers are relative distance between sites\nlabels are the site name\n\n\n%%R\nBIC(GNARfit(vts = vswindts, net = vswindnet, alphaOrder = 1, betaOrder = 0))\n\n[1] -233.3848\n\n\n\n%%R \nfiveFit <- GNARfit(fiveVTS[1:160,],net=fiveNet, alphaOrder=2, betaOrder=c(2,0)) #learn \ndim(fitted(fiveFit))\n\n[1] 158   5\n\n\n\n%%R\ndummyFit <- GNARfit(fiveVTS,net=fiveNet, alphaOrder=2, betaOrder=c(2,0)) #learn \ndummyFit$mod$coefficients <- fiveFit$mod$coefficients\n\n\n%%R\nfitted(dummyFit)[161:200]\n\n [1]  0.01093152  0.07611113  0.50989356  0.84380803  0.90488185 -0.12703505\n [7] -0.57721780 -0.36681689 -0.26281975 -0.47712098 -0.62293008 -0.58121816\n[13] -0.81149078 -0.45403821 -0.60487041  0.28617606  0.20580455  0.19341988\n[19]  0.35296420  0.15628117  0.68350847  0.49043974  0.29627168 -0.35666858\n[25] -0.47565960  0.06692171 -0.14924170 -0.36616239 -0.49994894  0.22625500\n[31] -0.08023045  0.25371268 -0.47415540 -0.99390660 -1.16821429 -0.18438203\n[37] -1.10766872 -0.76969390  0.71828989  0.69737474\n\n\n\n%%R\nfitted(fiveFit)\n\n               [,1]         [,2]         [,3]          [,4]         [,5]\n  [1,]  0.300764573  0.731759572  0.645747192  0.6590441235  0.329408573\n  [2,]  0.584155581  0.709939766  0.504838042  0.4006542005  0.002404727\n  [3,]  0.248206643 -0.053672929  0.089912292  0.3569760840  0.886027241\n  [4,]  0.431422029 -0.596332784 -0.445500333 -0.1950437084  0.998677531\n  [5,]  0.369251545 -0.131646096 -0.151852774 -0.0307057532  0.602574799\n  [6,] -0.281163998  0.372761568  0.470612441  0.0640593261 -0.793452225\n  [7,] -0.629184394  0.122210560 -0.064188223  0.2614228537  0.027403171\n  [8,]  0.176102146  0.054605602  0.001654112 -0.0679337406 -0.041235943\n  [9,] -0.168610998 -0.212095723 -0.204125726 -0.2125261302 -0.010579734\n [10,] -0.161421278 -0.007465513  0.149274060 -0.0249484567 -0.327041759\n [11,]  0.359655447 -0.077039236 -0.039580520 -0.0457447866  0.112500734\n [12,] -0.015390804 -0.224700395 -0.236137628 -0.1566997305 -0.032236173\n [13,] -0.254595294 -0.052425427 -0.222029463 -0.1955623614 -0.090852345\n [14,] -0.133378083 -0.231175869 -0.262111004 -0.4557269171 -0.613390251\n [15,] -0.299492590 -1.329777415 -1.157380728 -0.7395871328  0.367735930\n [16,] -0.092939379 -0.649485715 -0.649077093 -0.5677985865  0.018220159\n [17,] -0.197734475  0.006055154  0.003767933  0.0142878952  0.017169293\n [18,] -0.593128663 -0.029057099  0.019930836 -0.1995170085 -0.699678586\n [19,] -0.406803183 -0.080878255 -0.076916012 -0.0142207300  0.084031465\n [20,]  0.195661742  0.636973567  0.514808183  0.1521298877 -0.565262518\n [21,] -0.646798007  0.034595763 -0.038468526 -0.2844049647 -1.200687294\n [22,] -0.858660467 -0.279197317 -0.100300563 -0.1774862659 -0.600631944\n [23,]  0.072500668  0.171516441  0.137895886  0.1253750409  0.109017776\n [24,]  0.361169443 -0.134591857 -0.172907391 -0.0276254736  0.454886946\n [25,] -0.555837928  0.021772877  0.086135588 -0.5401601843 -1.758757275\n [26,] -0.907025497 -0.516889787 -0.601369437 -0.4907248867 -0.518976355\n [27,] -0.577788165 -0.883706611 -0.990274375 -0.5655498386  0.219126828\n [28,] -0.381253476 -1.020667212 -0.974967606 -0.8163078225 -0.224880458\n [29,] -0.418889129 -1.136651964 -1.192973176 -0.8278078873  0.075705061\n [30,] -0.430848760 -0.711242749 -0.645456670 -0.7835485785 -0.902099485\n [31,] -0.408762558 -0.330133901 -0.313267801 -0.2950574688 -0.516672981\n [32,] -0.397456978 -0.177503203 -0.123190622  0.0851676997  0.175790185\n [33,] -0.321504797  0.137826136  0.185516044 -0.0469442492 -0.541534195\n [34,] -0.446890348  0.425984103  0.440639199  0.0859296090 -0.911239457\n [35,] -0.409664931  0.661582101  0.615592428  0.4648149587 -0.620671093\n [36,] -0.178853576  1.134703079  1.170584884  0.7788603566 -0.614547790\n [37,] -0.187107405  0.975055141  0.894986103  0.7556786981  0.299963760\n [38,] -0.421512891  0.230443777  0.311288224  0.3783662241  0.481463020\n [39,]  0.551866636  0.010017627 -0.031712684 -0.0291730137  0.299117468\n [40,] -0.415705371 -0.084974037 -0.102059950  0.0729357811 -0.004680301\n [41,] -0.596582277 -0.181138675 -0.037759798 -0.0710576429 -0.300907209\n [42,] -0.264388277  0.136784757  0.077185718  0.0270845136 -0.311590329\n [43,]  0.413516614  0.932841430  0.935079115  0.7161523403 -0.158898407\n [44,]  0.095801809  0.920995408  0.955252555  0.8459697803 -0.127453246\n [45,]  0.975373534  1.209886345  1.185240223  1.0015804496  0.169827608\n [46,]  0.618868253 -0.058442807 -0.058935046  0.2370795943  0.914954875\n [47,]  0.366854292  0.267881271  0.247344186  0.2400219950  0.479987689\n [48,] -0.150157867 -0.777932308 -0.717286974 -0.5794468369  0.195377648\n [49,]  0.075029321 -0.299959011 -0.355342761 -0.1920419709  0.334423437\n [50,]  0.386593451 -0.135877992 -0.072542184  0.2518678243  0.984484742\n [51,]  0.697822864 -0.311896474 -0.295669625 -0.2502382955  0.585030936\n [52,]  0.062654477 -0.191584061 -0.107314176  0.2208996954  1.056837103\n [53,]  0.728773427 -0.477836693 -0.494169917 -0.4843342717  0.451358938\n [54,]  0.585069845  0.665543312  0.719452401  0.5820396956  0.545931719\n [55,]  1.079661317  0.127297862  0.038038488  0.0677187351  0.907180658\n [56,]  0.651000271  0.443573833  0.461237609  0.2318123507  0.355931847\n [57,]  0.047340441  0.061483244 -0.142562719 -0.0074741198  0.203598849\n [58,]  0.020722674 -0.855956709 -0.772986200 -0.7280158105 -0.072139463\n [59,] -0.197216261 -0.894976325 -0.740954981 -0.7972459012 -0.237856418\n [60,] -0.335837112 -0.488349114 -0.651565734 -0.7163113371 -0.703717928\n [61,] -0.573434288 -1.566963776 -1.470953366 -0.9358838106  0.340002593\n [62,] -0.019967493 -1.006103326 -1.203028246 -0.7206978782  0.426166645\n [63,]  0.289165755 -0.676589963 -0.529091393 -0.3863962807  0.580747013\n [64,] -0.063683833 -0.320450285 -0.325613255 -0.2974046265 -0.105675029\n [65,] -0.112032969 -0.527479112 -0.523398055 -0.1590403400  0.496544412\n [66,] -0.286173235 -0.053400092  0.018863005 -0.0124208740 -0.118724115\n [67,] -0.499744957  0.041067235  0.076504951 -0.2048682261 -0.787362589\n [68,] -0.547870145 -0.732546310 -0.770409578 -0.7369449312 -0.331821251\n [69,] -0.905056316 -0.829491825 -0.850502405 -0.7607730212 -0.656264847\n [70,] -0.246440697 -0.279850678 -0.269853546 -0.5660758221 -1.111827018\n [71,] -0.186335645  0.010799425 -0.077495418  0.0601490253  0.102437820\n [72,]  0.010493539 -0.479453043 -0.453954591 -0.3426073144 -0.040090049\n [73,] -0.673580794 -0.734494634 -0.564315445 -0.8190023610 -1.019947630\n [74,] -0.640032299 -0.558498277 -0.596977420 -0.5456733025 -0.090276605\n [75,] -0.464960459 -0.529199551 -0.552331950 -0.8884973812 -1.066744453\n [76,] -0.887182659 -0.141258133 -0.110786546  0.0237603066  0.003998690\n [77,] -0.073105627 -0.207336194 -0.187193599 -0.6930066146 -1.105251019\n [78,] -0.689981040  0.027486354  0.151691133  0.2650861359 -0.203709649\n [79,]  0.258382277  0.652801148  0.545006830  0.3925781271 -0.249181067\n [80,]  0.751377138  0.239900459  0.276168396  0.6057405726  1.262601102\n [81,]  0.631390471  0.411822991  0.543334476  0.2438621480  0.112347717\n [82,] -0.034969641 -0.104184784 -0.097705561  0.1498464790  0.895833940\n [83,]  0.120113065  0.049356967  0.046034277 -0.0721309377  0.217972070\n [84,] -0.200386785 -0.203495710 -0.239586254 -0.1172887981  0.323191787\n [85,]  0.185644181 -0.395187380 -0.370733604 -0.5009300085 -0.211102667\n [86,] -0.237732516  0.276454796  0.287754635  0.2080093644 -0.223951494\n [87,]  0.216163467  0.192130141  0.177949800 -0.0753244143 -0.351364565\n [88,] -0.142649364 -0.186692214 -0.168987298 -0.1166303744  0.011404103\n [89,]  0.548618117  0.747707315  0.770581155  0.5221810333  0.172075628\n [90,]  0.028104354  0.688465275  0.636215214  0.5500772784  0.059464762\n [91,] -0.495363724 -0.071919605 -0.008154852 -0.4353087653 -1.271143508\n [92,] -1.107134875 -0.172815824 -0.303687490 -0.0844525666 -0.236711607\n [93,] -0.149354271  0.009431534  0.092873697 -0.4137230613 -1.079758945\n [94,] -0.480127162 -0.759207617 -0.733556252 -0.1155717520  0.856016011\n [95,] -0.068768871 -0.262250231 -0.276510238 -0.4068034977 -0.281317568\n [96,] -0.204766674 -0.466093264 -0.516820813 -0.4012743931 -0.036227691\n [97,]  0.450365055 -0.345449271 -0.277951368 -0.0735386028  0.626942400\n [98,]  0.825252322 -0.102070954  0.029346323  0.2224587296  0.891243876\n [99,]  0.918255191  0.974393754  0.785132509  0.9588950601  0.747082329\n[100,]  1.161471681  0.803193156  1.046464965  1.2474096963  1.536524225\n[101,]  0.796513581  0.943461644  0.698756745  0.5968311778  0.275247231\n[102,]  0.189772234  0.088167353  0.193522913 -0.1055246595 -0.205361362\n[103,] -0.486312684  0.528260131  0.609425778  0.4533837262 -0.448633125\n[104,] -0.780106474  0.619638040  0.487082196  0.2699333424 -1.074329863\n[105,]  0.001495171  0.043686101  0.071429149 -0.0068869393  0.109059794\n[106,]  0.412811020  0.589258087  0.534530373  0.1827023688 -0.169191222\n[107,]  0.230033356  0.369815780  0.441499631  0.4808802972  0.619074058\n[108,]  0.887957665  0.299276196  0.242694214  0.2120811348  0.437908215\n[109,] -0.003445154 -0.934051132 -0.730038077 -0.3220444395  0.707556500\n[110,] -0.138607950 -0.723059594 -0.801323554 -0.7383525593 -0.366013353\n[111,] -0.761794858 -0.776583246 -0.767390838 -0.6281202398 -0.286591883\n[112,] -0.143995206 -0.994654745 -0.891931956 -0.8145750541  0.194529857\n[113,] -0.158840376 -0.789802525 -0.720611341 -0.3434589195  0.705106074\n[114,]  0.213701938  0.094954893 -0.066337346 -0.1715778242 -0.301015012\n[115,]  0.353147878 -0.162438159 -0.035453134  0.2417453098  0.692188468\n[116,] -0.259026322 -0.062564122 -0.069539905  0.0494333669  0.016725122\n[117,]  0.439567933  0.334238798  0.123452083  0.2899531832  0.536404389\n[118,]  0.243694767 -0.136731679 -0.062368473 -0.0505163356  0.329751821\n[119,] -0.221760052 -0.034088224 -0.073079140 -0.0719693132  0.080492952\n[120,] -0.639870092 -0.346958117 -0.350162454 -0.6158933852 -1.052705642\n[121,]  0.191514800  0.198936857  0.188570749  0.0576140540 -0.098783032\n[122,]  0.497040376  0.104911046  0.183534206  0.3233013200  0.725368718\n[123,]  0.912741276  0.906110114  0.847102372  0.9050828507  0.840894148\n[124,]  0.761509505  0.486180954  0.657445208  0.6485641139  0.813518998\n[125,]  0.316821416 -0.083720193 -0.173493262 -0.1007134067  0.356082268\n[126,] -0.067185056 -0.753094489 -0.702750670 -0.2621385119  1.046954137\n[127,] -0.091228436 -0.675109353 -0.703565297 -0.5564405787  0.258988050\n[128,] -0.109253436 -0.444143304 -0.399640932 -0.3580561559 -0.035762392\n[129,] -0.232521560 -0.039990247 -0.095241200 -0.0443450553 -0.430664437\n[130,]  0.123446724  0.200420996  0.173518282  0.5184071044  0.313636742\n[131,]  0.285159008 -0.032242166  0.027550780  0.1048459050 -0.132334038\n[132,] -0.029094326  0.291860928  0.394652562  0.4297945912  0.077465921\n[133,]  0.607314455  0.854271821  0.946947611  0.7446008823  0.229008010\n[134,]  0.114274696  0.395094364  0.372741933  0.2933528915  0.073135295\n[135,]  0.058334948  0.020883512 -0.064495939 -0.0002935146  0.194116690\n[136,] -0.485203794  0.162128088  0.243616443  0.0173331790 -0.588154182\n[137,]  0.361414229  0.497203208  0.361158226  0.5018799742  0.573470360\n[138,]  0.819903948  0.366313425  0.363516294  0.3796662479  0.699684917\n[139,] -0.216165855 -0.335708047 -0.251520773 -0.2682301442  0.062768414\n[140,] -0.454294227 -0.359066363 -0.294969357 -0.4097376437 -0.479590775\n[141,] -0.062418172  0.009466708 -0.051169781  0.1398314442  0.275918751\n[142,]  0.546927988  0.424770686  0.388169659  0.1288546957 -0.347653081\n[143,] -0.311508529  0.345041027  0.500511866  0.3303953879 -0.189245985\n[144,]  0.156321321  0.623518356  0.518171727  0.4513620372  0.284295098\n[145,]  0.039622964  0.299954776  0.356381605  0.3469508290  0.326305082\n[146,]  0.579791654  0.695810253  0.652099702  0.5011115910  0.023065334\n[147,]  0.554655907  0.093490122  0.153079897  0.4182759318  0.767121199\n[148,]  0.384057236 -0.065678272 -0.023948715  0.0451867298  0.454508452\n[149,]  0.447874514  0.392017250  0.266888939  0.3039632644  0.540801889\n[150,]  0.497810670  0.980726335  1.014061377  0.9632510637  0.560362395\n[151,]  0.044270389  0.550595322  0.526083974  0.5090146051 -0.039702900\n[152,]  0.128989148  0.501717816  0.494582107  0.2658346703 -0.283886186\n[153,]  0.003640933  0.471547028  0.593579063  0.5105159483  0.179982706\n[154,]  0.207081614  0.420074580  0.265684904  0.2159979026 -0.161101019\n[155,] -0.478137997  0.240399554  0.349195339  0.3926704926 -0.290283502\n[156,] -0.368038032  0.588505317  0.539132547  0.2403574312 -0.831187194\n[157,] -0.452959327  0.212753187  0.402307299  0.1713284561 -0.415735323\n[158,]  0.013726373  0.132970935  0.077022243  0.0243587773 -0.211000744\n\n\n\n%%R \nfiveVTS\n#gdpVTSn2[52,]\n\n               [,1]         [,2]         [,3]         [,4]        [,5]\n  [1,] -0.106526553  1.077613724 -0.244694569  0.933710066  0.44443593\n  [2,]  0.664495737  0.935476457  1.402823610  0.826656526  0.02097885\n  [3,] -0.255977521 -1.478171940  2.160938890  1.746276180  0.80188586\n  [4,]  1.684436464  0.180662387  0.398879113 -0.418094544  0.28037722\n  [5,]  1.451205104  0.584157057 -1.700218367 -1.219724913  1.63810802\n  [6,]  0.728846398  0.536253148 -0.730310794 -0.297010221  1.04864255\n  [7,] -1.526780869  1.957817106 -0.506088728  0.470747730 -0.63167929\n  [8,]  1.269035583 -0.519870107  0.773038576 -0.176065931 -3.13354575\n  [9,] -0.403340381 -0.398760476  1.358212255 -0.755544421  1.58592625\n [10,] -0.057805476 -0.932876792  0.163580296 -0.299486667 -0.31232748\n [11,] -0.498505318  1.005323196 -0.762045202  0.136725053 -0.43412619\n [12,]  0.057551667  0.252414834 -1.585146661  0.892773199  0.64534882\n [13,] -0.235969810  0.168738145 -0.927906887 -0.201182245  0.28394360\n [14,] -0.574265201 -0.451671164  1.523378623 -1.466113999  0.88143283\n [15,] -1.120369546 -2.167955049 -0.290951250  0.951438750 -0.63430591\n [16,]  0.734315543 -1.074175821 -2.550164018 -1.843026440  0.16372069\n [17,] -0.300708705 -0.215965234 -1.251829204 -1.147084336  0.81200151\n [18,]  0.168111491  0.617846030 -0.067553835 -0.316813889 -0.57552628\n [19,] -1.130660662  1.049382642 -0.219923688 -0.838450813 -0.84289855\n [20,]  0.491603006  0.911202204 -0.037939755 -1.146905077 -0.70878526\n [21,] -1.282680651 -0.037695723  1.273889958  1.222120228  0.48792144\n [22,] -1.977860407 -1.476586870  0.618312041  0.541447907 -1.59494685\n [23,] -0.566157633  1.007129193 -0.421914435 -1.255494830 -1.57780757\n [24,]  0.427751755  0.446864227 -0.006556415  0.235046101 -0.13251371\n [25,]  0.588963964 -0.510977075 -0.208215798 -0.038859202  1.03505405\n [26,] -3.577108873  0.063814248 -0.764773488  0.750897963 -0.57677829\n [27,] -0.762998219 -0.715903539  0.070974165 -1.767866460 -0.85886671\n [28,]  1.154873570 -2.505711766 -0.235496918 -1.464120269 -1.74689908\n [29,] -0.272693870 -1.857460255 -1.012583601 -1.478438760 -0.20065611\n [30,]  0.395708110 -2.601309968 -1.102332663 -1.418525692 -0.67074519\n [31,] -1.632021790 -1.389226367 -1.406639927 -0.164463613 -0.57992151\n [32,] -0.593868925 -0.604746854 -0.889344852  0.170550009 -1.19152282\n [33,]  0.902385866  0.859337939 -0.216488189 -1.019066503 -1.20331830\n [34,] -0.755599082  1.205454399 -0.315142793 -0.182216870 -0.76219579\n [35,] -1.402617202  1.327218071  0.030109089  0.508358610 -1.19204930\n [36,] -0.248901260  0.835443000  0.673836075  1.249100802 -2.50681088\n [37,] -0.821889741  2.683690391  1.991361276  0.628557722 -0.63183483\n [38,]  0.509052231  2.387571425  3.198218488 -1.313720751  0.41614740\n [39,]  1.760264680  1.329747672  0.589082315 -0.959062662 -2.17248032\n [40,]  0.407692882 -0.494160904 -0.336544078  0.518871030  1.17098422\n [41,]  0.529542953 -0.109371157  0.276546799 -0.441917032 -1.77465267\n [42,]  0.071422121  1.390859571 -0.656166400 -1.112883997 -1.53889722\n [43,] -0.038685141  0.343631869 -0.121075284  0.189519172 -1.28519302\n [44,]  0.032507365  1.105775541  0.745569895  2.313785056 -0.54606867\n [45,]  0.326298291  1.672376452  0.867801558  1.596593083 -1.38530798\n [46,]  0.493735214  0.828445070  0.665517459  3.545540325  0.17892418\n [47,]  1.046193130  0.182022439  0.076358398 -0.564071723  2.31894117\n [48,]  0.487801423  0.402192928  0.660265277  0.160859127  0.85916399\n [49,]  0.012036883 -0.575802156 -1.375502706 -1.555497631  0.78253503\n [50,]  0.566249612 -0.679123506 -0.342116629 -0.293370938  0.16791547\n [51,]  1.983899460  0.325870466 -0.462247924 -0.144103964  0.20428669\n [52,]  0.082477869 -0.015707410 -0.827420937 -0.533338717  3.03848854\n [53,]  2.025321199  1.260059506 -0.667274520 -0.983423623 -0.25069462\n [54,] -0.182207953 -0.492329932 -1.572486971 -0.318050791  3.11590615\n [55,]  0.310016519  2.163157480  0.539075569  0.775034293  1.55599197\n [56,]  0.344219807 -0.289984010  0.019376746  0.352604618  3.88803701\n [57,] -0.628460347  0.908826573  0.839215153  0.387888048  2.79852674\n [58,]  0.219514752 -1.949662023  1.297952270  0.245509848 -0.10715958\n [59,] -0.703231049 -1.872499373 -0.948118550 -0.956147553  1.46327573\n [60,] -1.037484518 -0.353586885 -2.197040404 -1.022195776  1.03019923\n [61,] -1.671104120 -1.807816077 -0.456726722 -0.275561042  0.23349098\n [62,]  1.082454650 -2.163914674 -2.772699788 -1.867484810 -1.15276058\n [63,]  0.733526181 -2.901286567 -0.144692022 -1.533368920  0.64538823\n [64,]  0.607622782 -0.564034684 -1.024446572 -0.860302664  1.46464103\n [65,] -0.249447697 -0.622497600 -0.860157861  0.059602510 -0.28629810\n [66,]  1.242991882 -0.553213122 -0.784868076 -0.866755329 -0.50726271\n [67,]  0.025808026  1.134045937 -0.194833475 -0.766397319 -0.66714031\n [68,] -1.280038469  0.969797971 -0.529341149 -0.274648131 -0.88451275\n [69,] -0.899437152 -0.455454165 -0.889218336 -2.081201461  0.63782940\n [70,] -0.485225451 -1.579881065 -1.542956678 -0.728773133 -2.63529295\n [71,] -2.232840088 -0.666470900 -0.802215707  0.306471316  0.36000506\n [72,]  0.263283350 -0.473210196  0.858443996 -0.330691066 -0.23528971\n [73,]  0.194868023 -1.673792947 -1.157081284  0.464290012 -0.67196384\n [74,] -2.374209088  0.746939110 -1.641811447 -1.805924541  0.66216561\n [75,] -0.319164807  0.368646446 -0.675529010 -2.103798682  0.06414596\n [76,] -2.347006749 -0.926133386 -1.280596322 -0.435692565  0.21824222\n [77,]  0.446994651  1.473521493  0.734103122 -2.231131628 -1.36931119\n [78,] -2.402766578 -0.673636271 -1.514644362  0.766704957  0.57180703\n [79,]  0.591165442  1.638210347 -0.954659887  0.008337226 -2.90507678\n [80,] -0.284550885  0.812008860  0.745209777  1.099796776  0.12632330\n [81,]  2.404372924  0.494780967 -0.137672593  0.738809147  0.71597343\n [82,] -0.732204526  2.210038243 -0.355431422  0.339909653  2.46725315\n [83,]  1.437145004  1.547713162 -0.345952409 -1.578834604  0.52850356\n [84,] -0.084957358  1.234464257 -0.032386550 -0.956802127  1.29018233\n [85,]  0.630013261  0.172600953 -0.127816246 -1.056826347 -0.23795324\n [86,] -0.893457396 -0.660542921 -1.265061454  0.015612038  1.27234265\n [87,] -0.287301062  0.963686285  0.359594848  0.203580641 -0.80393259\n [88,] -1.133229025  0.151632482 -0.040554491  0.515841576  1.28839412\n [89,] -0.023953670 -0.117144878 -0.537169309 -0.192949617 -0.19885872\n [90,] -0.238994400  1.725878556  1.063475592  0.828811176  1.72692730\n [91,]  0.299232378  0.799624873  1.181753039  0.831726730 -0.84857446\n [92,] -2.481179710  0.113529319 -0.650053446  0.011672273 -0.33558946\n [93,]  0.312713660 -0.079515578  1.502108851 -2.180229770 -2.24161884\n [94,] -1.996496328 -0.407588562 -0.652585366  0.923280995 -0.15832811\n [95,]  2.250167112 -0.220512659 -0.805730284 -2.066199413 -1.28057608\n [96,] -0.763689773 -0.031020794 -0.208759418 -0.867474784  0.72626616\n [97,]  0.014431435 -1.336163148 -0.640654650 -0.333381100 -0.44435745\n [98,]  0.957691513 -0.361694475 -0.967050395  0.060214711  1.10246612\n [99,]  1.469745730  0.554733531 -1.990318074  1.277950650  0.91469843\n[100,]  1.356192755  0.363980993  1.646543813  2.153855538  0.49502358\n[101,]  3.001712443  2.666746291 -0.368684582  1.849950555  0.63675201\n[102,] -0.281234965  0.456235005  2.292473984  0.854928226  2.22619190\n[103,] -1.140727890  0.202030354 -0.219402824  0.310787559  1.38376301\n[104,] -0.101523413  1.693526806  0.088496042  0.849098718 -2.58113447\n[105,] -1.122014513  0.674017745  1.696074464  0.065284585 -2.46042966\n[106,] -0.050503692  0.476461163  0.741019471 -1.033544711  1.30742678\n[107,] -0.977589842  0.119899249  1.188456997  1.081889577  1.41965239\n[108,]  0.909422801  1.255343375  0.842509208 -0.192153747  0.71392520\n[109,]  0.441028047 -0.896440332 -0.270300473  1.985698469  1.24604331\n[110,]  1.237351903  0.285784091 -2.573210003 -1.364613642  0.30715435\n[111,] -0.812621272 -1.216797535 -1.946208168 -0.293442424  0.01613793\n[112,] -0.569590223 -0.000629766 -0.657769831 -2.422099355 -0.23850071\n[113,]  0.013135413 -0.269047393 -2.386679520 -1.612243344  1.04220642\n[114,]  1.497050345  0.473005141 -2.121414725 -1.449151157 -0.36453856\n[115,] -0.826489548 -0.635578799  0.160746530  0.588310936  0.64139235\n[116,]  1.714414681 -0.069917852 -1.565924860  1.120471391 -0.78042307\n[117,]  0.090005879  1.200734542  0.270198304 -1.404710351 -0.03628579\n[118,]  1.236752409 -1.518803758  1.237344754  1.129076932 -0.20967169\n[119,] -0.087766705  0.109758177  0.830463404 -1.215657793  2.10369911\n[120,]  0.171155851 -0.458798255  0.673636072 -0.491339577 -0.55057403\n[121,] -2.015292503 -0.870762059 -0.029309395 -0.793820162 -0.46857349\n[122,] -0.362231614 -0.364385616  0.389845581  0.862317056  0.58137591\n[123,]  1.134908449  0.528983153 -0.215140490  0.381732812  0.95051905\n[124,]  1.457741194  0.847372382  1.281677103  1.882417946  0.73169697\n[125,]  1.108355891  2.153722703 -0.310021511  0.663789759  1.50520899\n[126,]  0.071438586 -0.058006918 -0.325876606 -0.374044712  1.44845100\n[127,]  1.793527240  0.106114670 -0.893934806 -2.276541783  0.63834022\n[128,]  0.315117203 -0.717323756 -0.955019446 -1.370627155  0.39051426\n[129,] -0.043340209 -0.312945747 -1.197591049 -0.317055511 -0.20153262\n[130,] -0.102618844 -0.880170227 -0.604543790  1.138606188 -2.06784587\n[131,]  1.740909147 -0.637353850 -0.010562729  1.561563247 -2.32157478\n[132,]  0.188345255 -0.469433386 -0.998242748  1.333047029 -0.61688863\n[133,]  0.478267920  1.814870921 -0.214383410  0.211463195 -0.68793981\n[134,]  0.520210151  2.448452650 -0.463708150  1.952197392  0.18188706\n[135,] -0.269362229  1.911952464  0.242272223 -0.444547682  1.07677467\n[136,]  0.423190923 -0.410880998 -0.195993929  0.257239829 -0.28741994\n[137,] -1.026903990  1.526871656  0.360358335 -0.772639199 -0.55385790\n[138,]  1.450078506 -0.349330702  1.034884543  1.105119122 -0.44915787\n[139,]  0.550436921  0.247527774  1.257978907  0.192344787  2.62949378\n[140,] -0.191655801  0.134136826 -0.521197521 -1.048113725  0.16877213\n[141,] -0.791900380  0.321889000 -1.170523314 -0.640876817 -0.67475039\n[142,]  0.971146450  0.021298110 -0.039896748  0.046666821 -0.88362522\n[143,] -0.982608481 -0.377262589  0.171316614  1.854723760  0.99834845\n[144,] -0.495627812  2.483176264  0.354246741 -0.684488580 -0.02501307\n[145,]  0.561938527  0.993967044  1.211151634  0.257789007  0.12399797\n[146,]  0.709311790  1.260796758  0.500729309 -0.317030392 -0.10472747\n[147,]  0.242781056  0.024085820  0.200161589  2.499565048 -0.35615091\n[148,]  1.391448147  0.433674956 -0.406153463  0.535043661  0.73457838\n[149,]  0.412636662  0.657392857 -0.602350630 -0.222007914  1.28589205\n[150,]  0.464724442  0.241309629  1.384164088 -0.007823340  1.51425328\n[151,]  1.387500492  1.166650199  1.425112356  1.867514447 -0.82096695\n[152,]  0.324362289  0.546923900  1.123115609  0.598062895 -0.86820404\n[153,] -0.642847662  0.488927474  1.005998211  0.623605903  0.36832761\n[154,]  0.580613501  1.826643913  0.215912542  0.360628058 -0.61370894\n[155,] -0.144307876 -0.646151661  0.724633376  1.186826797 -0.31028196\n[156,]  0.546715317  0.754951418 -0.046634000  0.644574743 -3.02529497\n[157,] -1.292512590  1.254904828  1.493495999 -0.128867226 -0.51111891\n[158,] -0.178360098  1.914596966 -0.981080025  0.228541535 -1.75337640\n[159,] -0.292207540  0.598820101 -0.516651587  0.274304089  0.03373131\n[160,] -2.308524149  0.208852278  1.461230272 -0.500911249 -2.27810147\n[161,] -1.379791032  0.145882158  0.915799429  1.484776097 -3.50041126\n[162,] -0.687780533 -0.916656103  1.270414228  1.016692379 -0.33067553\n[163,]  0.719998169  2.802816864  1.671148575 -0.623046401  0.58186844\n[164,]  0.720895216 -0.441022235  1.355521153  0.169312739  1.37392204\n[165,]  1.255917246  1.237433250 -1.261445845  1.156903886  1.34803048\n[166,]  1.705742677 -2.082080117  1.178644006  0.035092003  2.31810280\n[167,] -1.241297515 -1.810899663  0.429577096 -1.605573403  1.82238571\n[168,] -1.733594362 -1.512615033 -0.660630798 -0.541034771 -0.51548359\n[169,]  0.792480615  1.111823458 -0.087410649 -0.471304848 -1.46894677\n[170,] -1.308490289 -0.820921717 -1.060340478 -0.848828375  0.61581232\n[171,] -0.151371781 -1.755296255 -2.178401224 -1.133293893 -0.73148664\n[172,] -1.440949271 -1.701461336  0.086620787 -0.574783622 -0.96759066\n[173,] -0.602659336  0.088764382 -1.505800731 -1.165425847 -0.71480221\n[174,]  1.275259704 -0.410086120  0.494580410 -2.991666233 -1.27793672\n[175,] -1.404478046 -0.271241519  0.442854733 -0.222198583 -0.71844124\n[176,] -0.656202533  1.203821570  0.978017687  0.126331498 -1.96978752\n[177,] -0.394150009 -0.161910841  2.176312978  0.695232617  0.89852498\n[178,]  0.942635640  0.627373149  1.433906899 -0.365531957  0.60603188\n[179,] -0.023229999  1.377273430 -0.409007667  1.061813530 -0.29478037\n[180,]  0.365704922  1.211613739 -1.923486162  0.212759987  0.96275639\n[181,] -0.005420862 -0.765759383 -1.517079711  0.902219156 -0.34235656\n[182,]  0.024961781  0.098786009 -0.242548348  0.230247435  2.58670863\n[183,]  0.827409379 -0.019279260  1.645100501 -0.517327835  1.97867114\n[184,]  0.446528306 -0.641671765  0.183926490  0.194955794  0.61934460\n[185,] -0.852794309 -0.150650614 -1.175556623 -0.749290811 -0.16935739\n[186,]  0.545053341 -0.164000579 -0.705192833 -2.243859141 -0.09017822\n[187,] -1.237547900  0.639970678 -0.071693146  0.802596589  0.36885061\n[188,] -0.400597906 -0.490175656  1.285472941  0.566619402 -0.76045074\n[189,] -0.346030030 -0.031140831  0.468958960 -0.626935003 -0.58954715\n[190,] -0.242598074 -0.643482354  0.637618122  0.729482578 -2.59565491\n[191,]  1.906927330  1.295421628 -0.473359922  0.009433931 -0.45087521\n[192,]  0.247556013  0.380624945  0.511373051 -0.303734068 -0.35216521\n[193,]  0.109182478 -1.095266046 -1.117244679  0.260695357  0.63022764\n[194,] -0.428068355  0.957493744  1.147709215 -1.806721502  0.20508136\n[195,] -2.391696093  1.006982896  0.469035946 -1.029263987 -1.23612823\n[196,] -2.210774865 -1.403493412  0.556772208 -0.600485513 -2.42048578\n[197,] -0.722601932 -0.414053540  0.988636770 -0.610321642  0.58772892\n[198,] -2.938590715 -0.559749691  0.078615069  0.010633188 -2.36841490\n[199,] -0.985951916  0.215394351 -0.119048423  0.779733910 -2.98799845\n[200,]  0.447242609  0.404155027  0.719959458 -0.442525233 -1.78159592\n\n\n\n%%R\nBIC(GNARfit(vts = vswindts, net = vswindnet, alphaOrder = 1, betaOrder = 0, globalalpha = FALSE))\n\n[1] -233.1697\n\n\n\n%%R\nBIC.Alpha2.Beta <- matrix(0, ncol = 15, nrow = 15)\nfor(b1 in 0:14)\n    for(b2 in 0:14)\n        BIC.Alpha2.Beta[b1 + 1, b2 + 1] <- BIC(GNARfit(vts = vswindts,\n                    net = vswindnet, alphaOrder = 2, betaOrder = c(b1, b2)))\ncontour(0:14, 0:14, log(251 + BIC.Alpha2.Beta), xlab = \"Lag 1 Neighbour Order\", ylab = \"Lag 2 Neighbour Order\")\n\nException ignored from cffi callback <function _processevents at 0x7f1829767f70>:\nTraceback (most recent call last):\n  File \"/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/rpy2/rinterface_lib/callbacks.py\", line 277, in _processevents\n    try:\nKeyboardInterrupt: \nException ignored from cffi callback <function _processevents at 0x7f1829767f70>:\nTraceback (most recent call last):\n  File \"/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/rpy2/rinterface_lib/callbacks.py\", line 277, in _processevents\n    try:\nKeyboardInterrupt: \nException ignored from cffi callback <function _processevents at 0x7f1829767f70>:\nTraceback (most recent call last):\n  File \"/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/rpy2/rinterface_lib/callbacks.py\", line 277, in _processevents\n    try:\nKeyboardInterrupt: \n\n\n\na set of GNAR(2,[b1,b2]) models with b1, b2 ranging from zero to 14\nContour plot of BIC values for the two-lag autoregressive model incorporating b1-stage and b2-stage neighbours at time lags one and two. Values shown are log(251 + BIC) to display clearer contours.\n\n이해 덜 됨..\n\nincreasing the lag two neighbour sets beyond first stage neighbours would appear to increase the BIC for those lag one neighbour stages greater than five\n\nchatGPT\n이 문장을 조금 더 자세히 설명하면, BIC(Bayesian Information Criterion)는 모델을 선택할 때 사용하는 지표로서, 우리가 선택한 모델이 얼마나 적합한지를 측정합니다. 이 문장에서는, 이웃 집합의 대기 시간이 증가할수록 BIC 값이 증가할 것이라고 언급하고 있습니다. 이는 우리가 선택한 모델이 적합하지 않을 가능성이 있다는 의미입니다. 그래프를 보고 있을 때, 수평 윤곽선은 BIC 값이 0인 스테이지를 의미합니다. 이는 우리가 선택한 모델이 완벽하게 적합한다는 의미입니다.\n\n%%R\ngoodmod <- GNARfit(vts = vswindts, net = vswindnet, alphaOrder = 2, betaOrder = c(5, 1))\ngoodmod\n\nModel: \nGNAR(2,[5,1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\n dmatalpha1  dmatbeta1.1  dmatbeta1.2  dmatbeta1.3  dmatbeta1.4  dmatbeta1.5  \n    0.56911      0.10932      0.03680      0.02332      0.02937      0.04709  \n dmatalpha2  dmatbeta2.1  \n    0.23424     -0.04872"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#constructing-a-network-to-aid-prediction",
    "href": "posts/GCN/2023-01-05-GNAR.html#constructing-a-network-to-aid-prediction",
    "title": "GNAR data",
    "section": "3.3. Constructing a network to aid prediction",
    "text": "3.3. Constructing a network to aid prediction\nWe propose a network construction method that uses prediction error, but note here that our scope is not to estimate an underlying network, but merely to find a structure that is useful in the task of prediction.\nwe use a prediction error measure, understood as the sum of squared differences between the observations and the estimates:\n\\[\\sum^N_{i=1} (X_{i,t} - \\hat{X}_{i,t})^2\\]\n\n%%R\nprediction <- predict(GNARfit(vts = fiveVTS[1:199,], net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1)))\nprediction\n\nTime Series:\nStart = 1 \nEnd = 1 \nFrequency = 1 \n    Series 1  Series 2  Series 3  Series 4   Series 5\n1 -0.6427718 0.2060671 0.2525534 0.1228404 -0.8231921"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#oecd-gdp-network-structure-aids-prediction",
    "href": "posts/GCN/2023-01-05-GNAR.html#oecd-gdp-network-structure-aids-prediction",
    "title": "GNAR data",
    "section": "4. OECD GDP: Network structure aids prediction",
    "text": "4. OECD GDP: Network structure aids prediction\nGOP growth rate time series\n\n35 countries from the OECD website\ntime series : 1961 - 2013\nT = 52\nNodes = 35\nIn this data set 20.8% (379 out of 1820) of the observations were missing due to some nodes not being included from the start.\nwe do not uese covariate information, so C=1\n\n\n%%R\nlibrary(\"fields\")\nlayout(matrix(c(1, 2), nrow = 1, ncol = 2), widths = c(4.5, 1))\nimage(t(apply(gdpVTS, 1, rev)), xaxt = \"n\", yaxt = \"n\", col = gray.colors(14), xlab = \"Year\", ylab = \"Country\")\naxis(side = 1, at = seq(from = 0, to = 1, length = 52), labels = FALSE, col.ticks = \"grey\")\naxis(side = 1, at = seq(from = 0, to = 1, length = 52)[5*(1:11)], labels = (1:52)[5*(1:11)])\naxis(side = 2, at = seq(from = 1, to = 0, length = 35), labels = colnames(gdpVTS), las = 1, cex = 0.8)\nlayout(matrix(1))\nimage.plot(zlim = range(gdpVTS, na.rm = TRUE), legend.only = TRUE, col = gray.colors(14))\n\nR[write to console]: Loading required package: spam\n\nR[write to console]: Spam version 2.8-0 (2022-01-05) is loaded.\nType 'help( Spam)' or 'demo( spam)' for a short introduction \nand overview of this package.\nHelp for individual functions is also obtained by adding the\nsuffix '.spam' to the function name, e.g. 'help( chol.spam)'.\n\nR[write to console]: \nAttaching package: ‘spam’\n\n\nR[write to console]: The following objects are masked from ‘package:base’:\n\n    backsolve, forwardsolve\n\n\nR[write to console]: Loading required package: viridis\n\nR[write to console]: Loading required package: viridisLite\n\nR[write to console]: \nTry help(fields) to get started.\n\n\n\n\n\n\nHeat plot(grey scale) of the differenced time series,\n\nwhite space indicates missing time series observations"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#finding-a-network-to-aid-prediction",
    "href": "posts/GCN/2023-01-05-GNAR.html#finding-a-network-to-aid-prediction",
    "title": "GNAR data",
    "section": "4.1. Finding a network to aid prediction",
    "text": "4.1. Finding a network to aid prediction\n\n%%R\nnet1 <- seedToNet(seed.no = seed.nos[1], nnodes = 35, graph.prob = 0.15)\nnet2 <- seedToNet(seed.no = seed.nos[2], nnodes = 35, graph.prob = 0.15)\n\n\n%%R\nlayout(matrix(c(2, 1), 1, 2))\npar(mar=c(0,1,0,1))\nplot(net1, vertex.label = colnames(gdpVTS), vertex.size = 0)\nplot(net2, vertex.label = colnames(gdpVTS), vertex.size = 0)\n\n\n\n\n\nErdos-Renyi random graphs xonstructed from the first two elements of the seed.nos variable with 35 nodes and connection probability 0.15.\n자기회귀 모델인 GNAR 모델을 예측에 사용할 때, 어떤 네트워크가 가장 적합한지 조사해야 함.\n이때 각 노드의 자기 상관 함수를 이용한 초기 분석 결과, 2차 자기회귀 구성 요소가 충분할 것으로 예상되어 p = 2까지의 GNAR 모델을 시험함.\n각 시간 지연에서 최대 2개의 이웃 집합을 포함함.\n이에 따라 아래와 같은 GNAR 모델이 시험됨.\n\nGNAR(1, [0]), GNAR(1, [1]), GNAR(2, [0, 0]), GNAR(2, [1, 0]), GNAR(2, [1, 1]), GNAR(2, [2, 0]), GNAR(2, [2, 1]), 그리고 GNAR(2, [2, 2])가 시험되며, 각각 individual-\\(\\alpha\\)와 global-\\(\\alpha\\) GNAR 모델로 적합함.\n총 16개의 모델이 생성됨.\n이 중에서 전체 GDP 예측에 사용할 GNAR 모델을 선택할 것.\n연결 확률이 0.15인 10,000개의 임의의 양방향 네트워크를 생성하고, 위에서 언급한 GNAR 모델을 이용해 예측할 것.\n그래서 이 예제는 상당한 계산 시간이 필요(데스크탑 PC에서 약 90분).\n이를 위해 아래 코드에는 일부 분석만 포함.\n계산 상의 이유로, 우선 각 노드에서 표준 편차로 나눠서 잔차가 각 노드에서 동일한 분산을 가지게 함.\nseedSim 함수는 예측값과 원래 값의 제곱 차이의 합을 출력하고, 이를 예측 정확도의 측정 기준으로 사용\n\n\n\n%%R\ngdpVTSn <- apply(gdpVTS, 2, function(x){x / sd(x[1:50], na.rm = TRUE)})\nalphas <- c(rep(1, 2), rep(2, 6))\nbetas <- list(c(0), c(1), c(0, 0), c(1, 0), c(1, 1), c(2, 0), c(2, 1), c(2, 2))\nseedSim <- function(seedNo, modelNo, globalalpha){\n    net1 <- seedToNet(seed.no = seedNo, nnodes = 35, graph.prob = 0.15)\n    gdpPred <- predict(GNARfit(vts = gdpVTSn[1:50, ], net = net1,\n                               alphaOrder = alphas[modelNo], betaOrder = betas[[modelNo]],\n                               globalalpha = globalalpha))\n    return(sum((gdpPred - gdpVTSn[51, ])^2))\n    }\n\n\n%%R\nseedSim(seedNo = seed.nos[1], modelNo = 1, globalalpha = TRUE)\n\n[1] 23.36913\n\n\n\n%%R\nseedSim(seed.nos[1], modelNo = 3, globalalpha = TRUE)\n\n[1] 11.50739\n\n\n\n%%R\nseedSim(seed.nos[1], modelNo = 3, globalalpha = FALSE)\n\n[1] 18.96766\n\n\n\n\n\nimage.png\n\n\n\n10,000개의 임의의 네트워크와 16개의 모델로부터 시뮬레이션한 예측 오류의 박스 그래프\n(계산 시간이 길어(90분) 코드는 생략).\n일반적으로 global-α 모델은 더 낮은 예측 오류를 일으킴.\n그래서 이 버전의 GNAR 모델을 사용할 것.\n그림 9에서 첫 번째 모델인 GNAR(1, [0])과 세 번째 모델인 GNAR(2, [0, 0])의 경우, “박스 그래프”는 인접한 매개변수가 적합되지 않아 결과가 전부 동일해 짧은 수평선으로 표시됨.\n다른 global-α 모델들은 이 안에 포함되어 있기 때문에, global-α GNAR(2, [2, 2])의 예측 오류가 최소가 되는 임의의 그래프를 선택할 것.\n이는 seed.nos[921]에서 생성된 네트워크가 선택되게 됩니다.\n\n\n%%R\nnet921 <- seedToNet(seed.no = seed.nos[921], nnodes = 35, graph.prob = 0.15)\nlayout(matrix(c(1), 1, 1))\nplot(net921, vertex.label = colnames(gdpVTS), vertex.size = 0)\n\n\n\n\nRandomly generated un-weighted and un-directed graph over the OECD ountries that minimises the prediction error at t = 51 using GNAR(2, [2, 2]).\n\nseed.nos[921]에서 생성된 네트워크\n네트워크에는 전부 2개 이상의 이웃을 가지고 있는 countries들이 있고, 총 97개의 edges이 있음.\n이 “921” 네트워크는 GDP 예측을 위해 생성되었기 때문에, 찾은 네트워크에 인식 가능한 구조가 있지 않을 것이라고 예상할 수 있음\n그러나 미국, 멕시코, 캐나다는 각각 8개, 8개, 6개의 edge을 가지고 있어 매우 잘 연결되어 있음.\n스웨덴과 칠레도 잘 연결되어 있으며, 각각 8개와 7개의 edge을 가지고 있습니다.\n예측 성능이 유사한 적은 개수의 edge를 가진 네트워크를 찾기 위해 테스트 될 수 있지만, 여기서는 전체 선택된 네트워크를 그대로 사용.\n이 네트워크를 사용하면 BIC를 이용해 최적의 GNAR 순서를 선택할 수 있음.\n\n\n%%R\nres <- rep(NA, 8)\nfor(i in 1:8){\n    res[i] <- BIC(GNARfit(gdpVTSn[1:50, ],\n                          net = seedToNet(seed.nos[921], nnodes = 35, graph.prob = 0.15),\n                          alphaOrder = alphas[i], betaOrder = betas[[i]]))}\norder(res)\n\n[1] 6 3 4 7 8 5 1 2\n\n\n\n%%R\nsort(res)\n\n[1] -64.44811 -64.32155 -64.18751 -64.12683 -64.09656 -63.86919 -60.67858\n[8] -60.54207"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#results-and-comparisons",
    "href": "posts/GCN/2023-01-05-GNAR.html#results-and-comparisons",
    "title": "GNAR data",
    "section": "4.2. Results and comparisons",
    "text": "4.2. Results and comparisons\n\n이전 섹션의 모델을 사용해 t=52일 때의 값을 예측\n이 예측 오류를 표준 AR과 VAR 모델을 사용해 찾은 예측 오류와 비교\nGNAR 예측은 선택된 네트워크(seed.nos[921]에 해당)를 가진 GNAR(2, [2, 0]) 모델을 t=51까지의 데이터에 적합시키고, t=52일 때의 값을 예측\n우선 series를 정규화한 다음, 모델 적합으로부터 SSE를 계산합니다.\n\n\n%%R\ngdpVTSn2 <- apply(gdpVTS, 2, function(x){x / sd(x[1:51], na.rm = TRUE)})\ngdpFit <- GNARfit(gdpVTSn2[1:51,], net = net921, alphaOrder = 2, betaOrder = c(2, 0))\nsummary(gdpFit)\n\n\nCall:\nlm(formula = yvec2 ~ dmat2 + 0)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4806 -0.5491 -0.0121  0.5013  3.1208 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \ndmat2alpha1  -0.41693    0.03154 -13.221  < 2e-16 ***\ndmat2beta1.1 -0.12662    0.05464  -2.317   0.0206 *  \ndmat2beta1.2  0.28044    0.06233   4.500  7.4e-06 ***\ndmat2alpha2  -0.33282    0.02548 -13.064  < 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.8926 on 1332 degrees of freedom\n  (23 observations deleted due to missingness)\nMultiple R-squared:  0.1859,    Adjusted R-squared:  0.1834 \nF-statistic: 76.02 on 4 and 1332 DF,  p-value: < 2.2e-16\n\nGNAR BIC: -62.86003\n\n\n\n%%R\nsum((predict(gdpFit) - gdpVTSn2[52, ])^2)\n\n[1] 5.737203\n\n\n이 GNAR 모델의 적합된 매개변수는 \\(\\alpha^1 = - 0.42, \\beta^1,1 = - 0.13, \\beta^1,2 = 0.28\\), 그리고 \\(\\alpha^2 = - 0.33\\)입니다.\n\n\n\nModel\nparameters\nprediction error\n\n\n\n\nGNAR(2,[2,0])\n4\n5.7\n\n\nIndividual AR(2)\n38\n8.1\n\n\nVAR(1)\n199\n26.2\n\n\n\nEstimated prediction error of differenced real GDP change at t = 52 for all 35 countries.\n우리의 방법과 CRAN forecast 패키지의 버전 8.0에서의 forecast.ar()과 auto.arima() 함수를 사용해 각 노드별로 AR 모델을 적합한 결과를 비교\n\n섹션 4.1의 자기상관 분석을 고려해 각각 35개의 개별 모델의 최대 AR 순서를 p=2로 설정\n\n\n%%R\nlibrary(\"forecast\")\narforecast <- apply(gdpVTSn2[1:51, ], 2, function(x){\n            forecast(auto.arima(x[!is.na(x)], d= ,D=0,max.p = 2,max.q=0,\n                                max.P=0,max.Q = 0,stationary = TRUE, seasonal = FALSE), ic = \"bic\",\n                     allowmean = FALSE, allowdraft = FALSE, trace = FALSE, h=1)$mean\n})\nsum((arforecast - gdpVTSn2[52, ])^2)\n\nR[write to console]: Registered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n\n[1] 7.8974\n\n\nWe fit the model using the VAR function and then use the restrict function to reduce dimensionality further, by setting to zero any coefficient whose associated absolute t-statistic value is less than two.\n\n%%R\nlibrary(\"vars\")\ngdpVTSn2.0 <- gdpVTSn2\ngdpVTSn2.0[is.na(gdpVTSn2.0)] <- 0\nvarforecast <- predict(restrict(VAR(gdpVTSn2.0[1:51, ], p = 1, type = \"none\")), n.ahead = 1)\n\ncompute the prediction error\n\n%%R\ngetfcst <- function(x){return(x[1])}\nvarforecastpt <- unlist(lapply(varforecast$fcst, getfcst))\nsum((varforecastpt - gdpVTSn2.0[52, ])^2)\n\n[1] 26.19805\n\n\nGNAR 모델은 AR과 VAR 결과보다 적은 예측 오류를 제공합니다. 이는 AR과 비교했을 때 29%가 줄어들고, VAR과 비교했을 때 78%가 줄어듭니다.\n위 절차를 반복해 2단계 앞으로의 예측을 기반으로 분석을 수행합니다.\n이 경우 다른 네트워크가 GNAR(2,[2,2]) 모델의 예측 오류를 최소화합니다.\n그러나 BIC 단계에서 GNAR(2,[0,0]) 모델이 최적으로 적합된 것을 식별하였고, 이는 네트워크 회귀 매개변수를 포함하지 않는 모델입니다.\n\n%%R\nlibrary(\"vars\")\ngdpVTSn2.0 <- gdpVTSn2\ngdpVTSn2.0[is.na(gdpVTSn2.0)] <- 0\nvarforecast <- predict(restrict(VAR(gdpVTSn2.0[1:51, ], p = 1, type = \"none\")), n.ahead = 40)"
  },
  {
    "objectID": "posts/GCN/2023-05-06-article_refer.html",
    "href": "posts/GCN/2023-05-06-article_refer.html",
    "title": "ITSTGCN Article Refernece",
    "section": "",
    "text": "summerizing it\n\n\n\n\n\n\n\nNote\n\n\n\n\n글이 진행되는 순서로 작성함.\n\n\n\n\nIntroduction\nlittle1989analysis\n\nThe analysis of social science data with missing values\n\nLittle, Roderick JA and Rubin, Donald B\n\n\n위 논문은 구하지 못했고, 아래 논문에서 refer 건 것 보고 작성\nEvaluating the Influence of Missing Data on Classification Algorithms in Data Mining Applications by Luciano C. Blomberg\n내용\n\nFurthermore, distribution of missing data is another aspect that may influence the effectiveness of classification algorithms. Litle and Rubin [1987] presented three different mechanisms by which the missing data are distributed:\n\nli2018missing\n\nMissing value imputation for traffic-related time series data based on a multi-view learning method\n\nLi, Linchao and Zhang, Jian and Wang, Yonggang and Ran, Bin\n\n\n내용\n\nIn this circumstance, the difficulty of imputation is increased as we may not be able to find stable inputs for a model.\n\nSpatiotemporal Data\natluri2018spatio\n\nSpatio-temporal data mining: A survey of problems and methods\n\nAtluri, Gowtham and Karpatne, Anuj and Kumar, Vipin\n\n\n내용\n\n3.2. Data type 내용\n\nwang2020deep\n\nDeep learning for spatio-temporal data mining: A survey\n\nWang, Senzhang and Cao, Jiannong and Yu, Philip\n\n\n내용\n\n2.1. Spatio-Temporal Data Types 항목 내용에서 STData 종류\n\nyu2017spatio\n\nSpatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting\n\nYu, Bing and Yin, Haoteng and Zhu, Zhanxing\n\n\n내용\n\nThe linear interpolation method is used to fill missing values after data cleaning.\n\nguo2019attention\n\nAttention based spatial-temporal graph convolutional networks for traffic flow forecasting\n\nGuo, Shengnan and Lin, Youfang and Feng, Ning and Song, Chao and Wan, Huaiyu\n\n\n내용\n\nThe missing values are filled by the linear interpolation.\n\nbai2020adaptive\n\nAdaptive graph convolutional recurrent network for traffic forecasting\n\nBai, Lei and Yao, Lina and Li, Can and Wang, Xianzhi and Wang, Can\n\n\n내용\n\nThe missing values in the datasets are filled by linear interpolation.\n\nli2019predicting\n\nPredicting path failure in time-evolving graphs\n\nLi, Jia and Han, Zhichao and Cheng, Hong and Su, Jiao and Wang, Pengyun and Zhang, Jianfeng and Pan, Lujia\n\n\n내용\n\nWe replace missing values with 0, and normalize the features to the range\n\nzhao2019t\n\nT-GCN: A Temporal Graph Convol- utional Network for Traffic Prediction\n\nZhao, Ling and Song, Yujiao and Zhang, Chao and Liu, Yu and Wang, Pu and Lin, Tao and Deng, Min and Li, Haifeng\n\n\n내용\n\nwe used the linear interpolation method to fill missing values.\n\nbaraldi2010introduction\n\nAn introduction to modern missing data analyses\n\nBaraldi, Amanda N and Enders, Craig K\n\n\n내용\n\nAn overview of traditional missing data techniques 항목\n\nshi2022air\n\nAir Quality Prediction Model Based on Spatio-temporal Graph Convolution Neural Networks\n\nShi, Weidi and Song, Anjun\n\n\n내용\n\nDue to weather reasons and corrosion or damage of some sensors, some data were lost and abnormal. Therefore, the data needs to be cleaned. We used the average imputation method to fill some lost data and deleted the data which lost too many values.\n\nbatista2003analysis\n\nAn analysis of four missing data treatment methods for supervised learning\n\nBatista, Gustavo EAPA and Monard, Maria Carolina\n\n\n내용\n\nIMPUTATION METHODS\nImputation methods involve replacing missing values with estimated ones based on information available in the data set. There are many options varying from naive methods, such as mean imputation, to some more robust methods based on relationships among attributes. A description of some widely used imputation methods follows:\n\nblomberg2013evaluating\n\nEvaluating the performance of regression algorithms on datasets with missing data\n\nBlomberg, Luciano Costa and Hemerich, Daiane and Ruiz, Duncan Dubugras Alcoba\n\n\n내용\n\nfor example, replaces the missing values with means or modes.\n\ndonders2006gentle\n\nReview: A gentle introduction to imputation of missing values\n\nDonders, A Rogier T and Van Der Heijden, Geert JMG and Stijnen, Theo and Moons, Karel GM\n\n\n내용\n\nthe indicator method and overall mean imputation, give biased results.\nThe mean of the standard errors is a measure for the uncertainty in the estimated associations caused by sampling the study subjects from a source popu- lation. Additionally, the standard deviation of the multiple estimated associations (e.g., regression coefficients) reflects the differences between the imputed data sets, i.e., the un- certainty in the estimated underlying distributions of the variables with missing values.\n\nbaraldi2010introduction\n\nAn introduction to modern missing data analyses\n\nBaraldi, Amanda N and Enders, Craig K\n\n\n내용\n\nTo illustrate the bias that can result from the use of traditional missing data methods, we use the artificial math performance data set found in Table 1.\n\n\n\nProposed Methods\nSelf-Consistent Estimator"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html",
    "href": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html",
    "title": "Class of Method(GNAR) lag 1",
    "section": "",
    "text": "GNAR fiveNet,fivenodes"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html#시나리오1-baseline",
    "href": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html#시나리오1-baseline",
    "title": "Class of Method(GNAR) lag 1",
    "section": "시나리오1 (Baseline)",
    "text": "시나리오1 (Baseline)\n시나리오1\n\nmissing rate: 0%\n보간방법: None\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:34<00:00,  1.43it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nGNAR 으로 적합 + 예측\n-\n\n%load_ext rpy2.ipython\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython\n\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\nlibrary(tidyverse)\n\n\n%R -i fiveVTS_train\n\n\n%%R\nanswer <- GNARfit(vts = fiveVTS_train, net = fiveNet, alphaOrder = 1, betaOrder = c(1))\nprediction <- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((fiveVTS_test - gnar_test.reshape(-1,5))**2).mean(axis=0)\ntest_mse_total_gnar = ((fiveVTS_test - gnar_test.reshape(-1,5))**2).mean()\n\n\n\n결과시각화\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.9994323113693153, 1.2692101967317866)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n mse(train) = {3:.2f}, mse(test) = {4:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(range(1,160),stgcn_train[:,i],label='STCGCN (train)',color='C0')\n    a.plot(range(160,199),stgcn_test[:,i],label='STCGCN (test)',color='C0')\n    a.plot(range(1,160),gnar_train.reshape(-1,5)[:,i],label='GNAR (train)',color='C1')\n    a.plot(range(161,201),gnar_test.reshape(-1,5)[:,i],label='GNAR (test)',color='C1')\n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario1: STGCN \\n missing=0% \\n interpolation=None \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n GNAR: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html#시나리오2",
    "href": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html#시나리오2",
    "title": "Class of Method(GNAR) lag 1",
    "section": "시나리오2",
    "text": "시나리오2\n시나리오2\n\nmissing rate: 50%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.5)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:35<00:00,  1.41it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [00:37<00:00,  1.33it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nX_train1 = np.array(X).squeeze()\nX_test1 =  np.array(XX).squeeze()\n\n\n%R -i X_train1\n\n\n%%R\nanswer <- GNARfit(vts = X_train1, net = fiveNet, alphaOrder = 1, betaOrder = c(1))\nprediction <- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((X_test1 - gnar_test[1:,:])**2).mean(axis=0)\ntest_mse_total_gnar = ((X_test1 - gnar_test[1:,:])**2).mean()\n\n\n\n결과시각화\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.7473098322871093, 1.3231643342748722)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(160,199),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(161,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(1,159),gnar_train[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(161,201),gnar_test[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html#시나리오3",
    "href": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html#시나리오3",
    "title": "Class of Method(GNAR) lag 1",
    "section": "시나리오3",
    "text": "시나리오3\n시나리오3\n\nmissing rate: 80%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:35<00:00,  1.39it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [00:37<00:00,  1.33it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nX_train1 = np.array(X).squeeze()\nX_test1 =  np.array(XX).squeeze()\n\n\n%R -i X_train1\n\n\n%%R\nanswer <- GNARfit(vts = X_train1, net = fiveNet, alphaOrder = 1, betaOrder = c(1))\nprediction <- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((X_test1 - gnar_test[1:,:])**2).mean(axis=0)\ntest_mse_total_gnar = ((X_test1 - gnar_test[1:,:])**2).mean()\n\n\n\n결과시각화\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.38358787816283946, 1.3239931193379793)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(160,199),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(161,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(1,159),gnar_train[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(161,201),gnar_test[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario3: \\n missing=80% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html#시나리오4",
    "href": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html#시나리오4",
    "title": "Class of Method(GNAR) lag 1",
    "section": "시나리오4",
    "text": "시나리오4\n시나리오4\n\nmissing rate: 30%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.3)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:27<00:00,  1.85it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [00:27<00:00,  1.79it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nX_train1 = np.array(X).squeeze()\nX_test1 =  np.array(XX).squeeze()\n\n\n%R -i X_train1\n\n\n%%R\nanswer <- GNARfit(vts = X_train1, net = fiveNet, alphaOrder = 1, betaOrder = c(1))\nprediction <- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((X_test1 - gnar_test[1:,:])**2).mean(axis=0)\ntest_mse_total_gnar = ((X_test1 - gnar_test[1:,:])**2).mean()\n\n\n\n결과시각화\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.7978462123549198, 1.3146463350699074)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(160,199),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(161,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(1,159),gnar_train[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(160,200),gnar_test[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario3: \\n missing=80% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-03-22-SimulationPlanner-Tutorial_test_test.html",
    "href": "posts/GCN/2023-03-22-SimulationPlanner-Tutorial_test_test.html",
    "title": "SimualtionPlanner-Tutorial",
    "section": "",
    "text": "table"
  },
  {
    "objectID": "posts/GCN/2023-03-22-SimulationPlanner-Tutorial_test_test.html#plnr_stgcn_rand",
    "href": "posts/GCN/2023-03-22-SimulationPlanner-Tutorial_test_test.html#plnr_stgcn_rand",
    "title": "SimualtionPlanner-Tutorial",
    "section": "PLNR_STGCN_RAND",
    "text": "PLNR_STGCN_RAND\n\nplans_stgcn_rand = {\n    'max_iteration': 30, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mrate': [0,0.3,0.5],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader1,dataset_name='chickenpox')\n\n\nplnr.simulate()\n\n12/50\n\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader2,dataset_name='pedalme')\n\n\nplnr.simulate()\n\n\nplans_stgcn_rand = {\n    'max_iteration': 10, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mrate': [0],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader3,dataset_name='wikimath')\n\nplnr.simulate()\n\n11/50\n\n\n\nplans_stgcn_rand = {\n    'max_iteration': 10, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mrate': [0],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader3,dataset_name='wikimath')\n\nplnr.simulate()\n\n/home/csy/Dropbox/blog/posts/GCN/itstgcn/learners.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/utils/tensor_new.cpp:201.)\n  self.lags = torch.tensor(train_dataset.features).shape[-1]\n\n\n1/10 is done\n2/10 is done\n45/50\n\n\n\nplans_stgcn_rand = {\n    'max_iteration': 1, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mrate': [0],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader5,dataset_name='windmillmedium')\nplnr.simulate()\n\n/home/csy/Dropbox/blog/posts/GCN/itstgcn/learners.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343998658/work/torch/csrc/utils/tensor_new.cpp:245.)\n  self.lags = torch.tensor(train_dataset.features).shape[-1]\n\n\n24/50\n\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader5,dataset_name='windmillmedium')\nplnr.simulate()\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader10,dataset_name='monte')\nplnr.simulate()"
  },
  {
    "objectID": "posts/GCN/2023-03-22-SimulationPlanner-Tutorial_test_test.html#plnr_stgcn_manual",
    "href": "posts/GCN/2023-03-22-SimulationPlanner-Tutorial_test_test.html#plnr_stgcn_manual",
    "title": "SimualtionPlanner-Tutorial",
    "section": "PLNR_STGCN_MANUAL",
    "text": "PLNR_STGCN_MANUAL\n\nmy_list = [[] for _ in range(20)] #chickenpox\nanother_list = list(range(100,200))\nmy_list[10] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(15)] #pedalme\nanother_list = list(range(5,25))\nmy_list[1] = another_list\nmy_list[3] = another_list\nmy_list[5] = another_list\nmy_list[7] = another_list\nmy_list[9] = another_list\nmy_list[11] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(1068)] #wikimath\nanother_list = list(range(200,500))\nmy_list[10] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(26)] #windmilmedi\nanother_list = list(range(200,500)) # 676*0.8 = 540.8\nmy_list[10] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(11)] #windmilsmall\nanother_list = list(range(5000,10000)) # 17470*0.8 = 13976.0\nmy_list[10] = another_list\nmindex = my_list\n\n\nimport numpy as np\n\n\nimport random\n\n\nmy_list = [[] for _ in range(675)] #monte\nanother_list = list(range(200,350)) #743\n\nfor i in np.array(random.sample(range(0, 675), 400)):\n    my_list[i] = another_list\nmindex = my_list\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]  # node 1\n# mindex= [list(range(10,100)),[],list(range(50,80)),[],[]] # node 2\n# mindex= [list(range(10,100)),[],list(range(50,80)),list(range(50,150)),[]] # node3\nplans_stgcn_block = {\n    'max_iteration': 30, \n    'method': ['STGCN', 'IT-STGCN'], \n    'RecurrentGCN' : ['DCRNN'],\n    'mindex': [mindex],\n    'lags': [2], \n    'nof_filters': [16], \n    'inter_method': ['linear','cubic','nearest'],\n    'epoch': [150]\n}\n\n\nplnr = itstgcnadd.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader,dataset_name='fivenodes')\nplnr.simulate(mindex=mindex,mtype='block')\n\n/home/csy/Dropbox/blog/posts/GCN/itstgcnadd/utils.py:71: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343998658/work/torch/csrc/utils/tensor_new.cpp:245.)\n  lags = torch.tensor(train_dataset.features).shape[-1]\n\n\n1/30 is done\n2/30 is done\n3/30 is done\n4/30 is done\n143/150\n\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader1,dataset_name='chickenpox')\nplnr.simulate(mindex=mindex,mtype='block')\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader2,dataset_name='pedalme')\nplnr.simulate(mindex=mindex,mtype='block')\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader3,dataset_name='wikimath')\nplnr.simulate(mindex=mindex,mtype='block')\n\n1/30 is done\n2/30 is done\n36/50\n\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader5,dataset_name='windmiloutputmedium')\nplnr.simulate(mindex=mindex,mtype='block')\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader6,dataset_name='windmiloutputsmall')\nplnr.simulate(mindex=mindex,mtype='block')\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader10,dataset_name='monte')\nplnr.simulate(mindex=mindex,mtype='block')\n\n/home/csy/Dropbox/blog/posts/GCN/itstgcn/utils.py:71: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/utils/tensor_new.cpp:201.)\n  lags = torch.tensor(train_dataset.features).shape[-1]\n\n\n35/50\n\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader10,dataset_name='monte')\nplnr.simulate(mindex=mindex,mtype='block')\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader10,dataset_name='monte')\nplnr.simulate(mindex=mindex,mtype='block')\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader10,dataset_name='monte')\nplnr.simulate(mindex=mindex,mtype='block')\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader10,dataset_name='monte')\nplnr.simulate(mindex=mindex,mtype='block')\n\n\nimport random\n\n\nmy_list = [[] for _ in range(1068)] #wikimath\nanother_list = random.sample(range(0, 576), 432)\nfor i in range(0, 1068):\n    my_list[i] = another_list\nmindex = my_list\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]  # node 1\n# mindex= [list(range(10,100)),[],list(range(50,80)),[],[]] # node 2\n# mindex= [list(range(10,100)),[],list(range(50,80)),list(range(50,150)),[]] # node3\nplans_stgcn_block = {\n    'max_iteration': 10, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [4], \n    'nof_filters': [12], \n    'inter_method': ['nearest','linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader3,dataset_name='wikimath')\nplnr.simulate(mindex=mindex,mtype='block')\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]  # node 1\n# mindex= [list(range(10,100)),[],list(range(50,80)),[],[]] # node 2\n# mindex= [list(range(10,100)),[],list(range(50,80)),list(range(50,150)),[]] # node3\nplans_stgcn_block = {\n    'max_iteration': 10, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [4], \n    'nof_filters': [12], \n    'inter_method': ['nearest','linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader3,dataset_name='wikimath')\nplnr.simulate(mindex=mindex,mtype='block')\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]  # node 1\n# mindex= [list(range(10,100)),[],list(range(50,80)),[],[]] # node 2\n# mindex= [list(range(10,100)),[],list(range(50,80)),list(range(50,150)),[]] # node3\nplans_stgcn_block = {\n    'max_iteration': 10, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [4], \n    'nof_filters': [12], \n    'inter_method': ['nearest','linear'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader3,dataset_name='wikimath')\nplnr.simulate(mindex=mindex,mtype='block')"
  },
  {
    "objectID": "posts/GCN/2023-03-22-SimulationPlanner-Tutorial_test_test.html#plnr_gnar_rand",
    "href": "posts/GCN/2023-03-22-SimulationPlanner-Tutorial_test_test.html#plnr_gnar_rand",
    "title": "SimualtionPlanner-Tutorial",
    "section": "PLNR_GNAR_RAND",
    "text": "PLNR_GNAR_RAND\n\nplans_gnar_rand = {\n    'max_iteration': 30, \n#    'method': ['GNAR'], \n    'mrate': [0.8, 0.9],\n    'lags': [4], \n#    'nof_filters': [8,16], \n    'inter_method': ['cubic','linear'],\n#    'epoch': [1]\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_RAND(plans_gnar_rand,loader1,dataset_name='chickenpox')\nplnr.simulate()\n\n\nplnr = itstgcn.planner.PLNR_GNAR_RAND(plans_gnar_rand,loader2,dataset_name='pedalme')\nplnr.simulate()\n\n\nplnr = itstgcn.planner.PLNR_GNAR_RAND(plans_gnar_rand,loader3,dataset_name='wikimath')\nplnr.simulate()\n\n\nplnr = itstgcn.planner.PLNR_GNAR_RAND(plans_gnar_rand,loader5,dataset_name='windmiloutputmedium')\nplnr.simulate()\n\n\nplnr = itstgcn.planner.PLNR_GNAR_RAND(plans_gnar_rand,loader6,dataset_name='windmiloutputsmall')\nplnr.simulate()"
  },
  {
    "objectID": "posts/GCN/2023-03-22-SimulationPlanner-Tutorial_test_test.html#plnr_gnar_block",
    "href": "posts/GCN/2023-03-22-SimulationPlanner-Tutorial_test_test.html#plnr_gnar_block",
    "title": "SimualtionPlanner-Tutorial",
    "section": "PLNR_GNAR_BLOCK",
    "text": "PLNR_GNAR_BLOCK\n\nmy_list = [[] for _ in range(20)] #chickenpox\nanother_list = list(range(100,200))\nmy_list[10] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(15)] #pedalme\nanother_list = list(range(5,25))\nmy_list[1] = another_list\nmy_list[3] = another_list\nmy_list[5] = another_list\nmy_list[7] = another_list\nmy_list[9] = another_list\nmy_list[11] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(1068)] #wikimath\nanother_list = list(range(10,20))\nmy_list[10] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(26)] #windmilmedi\nanother_list = list(range(200,500)) # 676*0.8 = 540.8\nmy_list[10] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(11)] #windmilsmall\nanother_list = list(range(5000,10000)) # 17470*0.8 = 13976.0\nmy_list[10] = another_list\nmindex = my_list\n\n\nmy_list = [[] for _ in range(675)] #monte\nanother_list = list(range(200,350)) #743\n\nfor i in np.array(random.sample(range(0, 675), 400)):\n    my_list[i] = another_list\nmindex = my_list\n\n\n# mindex = [[],[],list(range(50,250)),[],[]]\n# mindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\n# mindex= [list(range(10,100)),[],list(range(50,80)),list(range(50,150)),[]]\nplans_gnar_block = {\n    'max_iteration': 3, \n    'method': ['GNAR'], \n    'mindex': [mindex],\n    'lags': [4,8], \n    'inter_method': ['linear'],\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_MANUAL(plans_gnar_block,loader1,dataset_name='chickenpox')\nplnr.simulate(mindex,mtype='block')\n\n\nplnr = itstgcn.planner.PLNR_GNAR_MANUAL(plans_gnar_block,loader2,dataset_name='pedalme')\nplnr.simulate(mindex,mtype='block')\n\n\n# mindex = [[],[],list(range(50,250)),[],[]]\n# mindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\n# mindex= [list(range(10,100)),[],list(range(50,80)),list(range(50,150)),[]]\nplans_gnar_block = {\n    'max_iteration': 3, \n    'method': ['GNAR'], \n    'mindex': [mindex],\n    'lags': [8], \n    'inter_method': ['linear'],\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_MANUAL(plans_gnar_block,loader3,dataset_name='wikimath')\nplnr.simulate(mindex,mtype='block')\n\n/home/csy/Dropbox/blog/posts/GCN/itstgcn/utils.py:71: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343998658/work/torch/csrc/utils/tensor_new.cpp:245.)\n  lags = torch.tensor(train_dataset.features).shape[-1]\n\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n1/3 is done\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n2/3 is done\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n3/3 is done\nAll results are stored in ./simulation_results/2023-05-14_19-46-46.csv\n\n\n\nplnr = itstgcn.planner.PLNR_GNAR_MANUAL(plans_gnar_block,loader5,dataset_name='windmiloutputmedium')\nplnr.simulate(mindex,mtype='block')\n\n\nplnr = itstgcn.planner.PLNR_GNAR_MANUAL(plans_gnar_block,loader6,dataset_name='windmiloutputsmall')\nplnr.simulate(mindex,mtype='block')\n\n\nplnr = itstgcn.planner.PLNR_GNAR_MANUAL(plans_gnar_block,loader10,dataset_name='monte')\nplnr.simulate(mindex,mtype='block')"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "",
    "text": "Try to divide train and test(GNAR fivenet)"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "1) ST-GCN",
    "text": "1) ST-GCN\n\nmean_f_fiveVTS_train = torch.tensor(fiveVTS_train_mean).reshape(160,5,1).float()\n\n\nmean_X_fiveVTS = mean_f_fiveVTS_train[:159,:,:]\nmean_y_fiveVTS = mean_f_fiveVTS_train[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(mean_X_fiveVTS,mean_y_fiveVTS)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:27<00:00,  1.84it/s]\n\n\n\nmean_fhat_fiveVTS = torch.stack([model(xt, edge_index, edge_attr) for xt in mean_X_fiveVTS]).detach().numpy()\n\n\nxt_test = torch.tensor(fiveVTS_test.reshape(40,5,1)[:-1,:,:]).float()\n\n\nmean_fhat_fiveVTS_forecast = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],mean_fhat_fiveVTS_forecast);\n\n\n\n\n\nvis2(fiveVTS_train_mean,mean_fhat_fiveVTS);"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "2) Fourier transform",
    "text": "2) Fourier transform\n\nw=np.zeros((159*N,159*N))\n\n\nfor i in range(159*N):\n    for j in range(159*N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\n# np.fft(mean_fhat_fiveVTS[:,0,0])\n\n\n# mean_fhat_fiveVTS.shape\n\n\n# fft_result =np.stack([np.fft.fft(mean_fhat_fiveVTS[:,n,0]) for n in range(N)]).T\n\n\n# plt.plot(abs(fft_result[:,0])**2)\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\n\n\nfhatbar = Psi.T @ mean_fhat_fiveVTS.reshape(159*N,1)"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "3) Ebayes",
    "text": "3) Ebayes\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\nfhatbar_threshed = ebayesthresh(FloatVector(fhatbar))\n\n\nplt.plot(fhatbar)\nplt.plot(fhatbar_threshed)"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "4) Inverse Fourier transform",
    "text": "4) Inverse Fourier transform\n\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_mean_spatio_temporal = fhatbarhat.reshape(159,N,1)\n\n\nvis2(mean_fhat_fiveVTS,fhatbarhat_mean_spatio_temporal.reshape(159,5));"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-1",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-1",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "5) ST-GCN",
    "text": "5) ST-GCN\n\nfiveVTS_train_mean[seed_number1,0] = fhatbarhat_mean_spatio_temporal[seed_number1,0,0]\nfiveVTS_train_mean[seed_number2,1] = fhatbarhat_mean_spatio_temporal[seed_number2,1,0]\nfiveVTS_train_mean[seed_number3,2] = fhatbarhat_mean_spatio_temporal[seed_number3,2,0]\nfiveVTS_train_mean[seed_number4,3] = fhatbarhat_mean_spatio_temporal[seed_number4,3,0]\nfiveVTS_train_mean[seed_number5,4] = fhatbarhat_mean_spatio_temporal[seed_number5,4,0]\nvis(fiveVTS_train_mean);\n\n\n\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(mean_X_fiveVTS,mean_y_fiveVTS)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:26<00:00,  1.88it/s]\n\n\n\nmean_fhat_spatio_temporal = torch.stack([model(xt, edge_index, edge_attr) for xt in mean_X_fiveVTS]).detach().numpy()\n\n\nmean_fhat_spatio_temporal_test = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],mean_fhat_spatio_temporal_test);\n\n\n\n\n\nvis2(fhatbarhat_mean_spatio_temporal,mean_fhat_spatio_temporal);\n\n\n\n\n\n\nfor i in tqdm(range(50)):\n    ## GFT \n    fhatbar = Psi.T @ mean_fhat_fiveVTS.reshape(159*N,1)\n\n    ## Ebayes\n    ebayesthresh = importr('EbayesThresh').ebayesthresh\n    fhatbar_threshed = ebayesthresh(FloatVector(fhatbar))\n    #plt.plot(fhatbar)\n    #plt.plot(fhatbar_threshed)\n\n    ## inverse GFT \n    fhatbarhat = Psi @ fhatbar_threshed\n    fhatbarhat_mean_spatio_temporal = fhatbarhat.reshape(159,N,1)\n    #vis2(mean_fhat_fiveVTS,fhatbarhat_mean_spatio_temporal.reshape(159,5));\n\n    ## STGCN \n    fiveVTS_train_mean[seed_number1,0] = fhatbarhat_mean_spatio_temporal[seed_number1,0,0]\n    fiveVTS_train_mean[seed_number2,1] = fhatbarhat_mean_spatio_temporal[seed_number1,1,0]\n    fiveVTS_train_mean[seed_number3,2] = fhatbarhat_mean_spatio_temporal[seed_number1,2,0]\n    fiveVTS_train_mean[seed_number4,3] = fhatbarhat_mean_spatio_temporal[seed_number1,3,0]\n    fiveVTS_train_mean[seed_number5,4] = fhatbarhat_mean_spatio_temporal[seed_number1,4,0]\n    #vis(fiveVTS_train_mean);\n\n    #model = RecurrentGCN(node_features=1, filters=4)\n\n    #optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n    #model.train()\n    for epoch in range(1):\n        for time, (xt,yt) in enumerate(zip(mean_X_fiveVTS,mean_y_fiveVTS)):\n            y_hat = model(xt, edge_index, edge_attr)\n            cost = torch.mean((y_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n    mean_fhat_spatio_temporal = torch.stack([model(xt, edge_index, edge_attr) for xt in mean_X_fiveVTS]).detach().numpy()\n    mean_fhat_spatio_temporal_test = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n    #vis2(fiveVTS_test[1:],mean_fhat_spatio_temporal_test);\n    #vis2(fiveVTS_train_backup,mean_fhat_spatio_temporal);\n\n100%|██████████| 50/50 [00:55<00:00,  1.10s/it]\n\n\n\nvis2(fiveVTS_train_backup,mean_fhat_spatio_temporal);\n\n\n\n\n\nvis2(fiveVTS_train_backup,mean_fhat_spatio_temporal);"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-1",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-1",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "6) Fourier transform",
    "text": "6) Fourier transform\n\nw=np.zeros((159*N,159*N))\n\n\nfor i in range(159*N):\n    for j in range(159*N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\n\n\nfhatbar = Psi.T @ mean_fhat_spatio_temporal.reshape(159*N,1)\npower = fhatbar**2"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-1",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-1",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "7) Ebayes",
    "text": "7) Ebayes\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\nfhatbar_threshed = ebayesthresh(FloatVector(fhatbar))\n\n\nplt.plot(fhatbar)\n\n\n\n\n\nplt.plot(fhatbar)\nplt.plot(fhatbar_threshed)"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-1",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-1",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "8) Inverse Fourier transform",
    "text": "8) Inverse Fourier transform\n\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_mean_spatio_temporal2 = fhatbarhat.reshape(159,N,1)\n\n\nvis2(mean_fhat_spatio_temporal,fhatbarhat_mean_spatio_temporal2.reshape(159,5));"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-2",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-2",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "9) ST-GCN",
    "text": "9) ST-GCN\n\nfiveVTS_train_mean[seed_number1,0] = fhatbarhat_mean_spatio_temporal[seed_number1,0,0]\nfiveVTS_train_mean[seed_number2,1] = fhatbarhat_mean_spatio_temporal[seed_number2,1,0]\nfiveVTS_train_mean[seed_number3,2] = fhatbarhat_mean_spatio_temporal[seed_number3,2,0]\nfiveVTS_train_mean[seed_number4,3] = fhatbarhat_mean_spatio_temporal[seed_number4,3,0]\nfiveVTS_train_mean[seed_number5,4] = fhatbarhat_mean_spatio_temporal[seed_number5,4,0]\nvis(fiveVTS_train_mean);\n\n\n\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(mean_X_fiveVTS,mean_y_fiveVTS)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:27<00:00,  1.84it/s]\n\n\n\nmean_fhat_spatio_temporal2 = torch.stack([model(xt, edge_index, edge_attr) for xt in mean_X_fiveVTS]).detach().numpy()\n\n\nmean_fhat_spatio_temporal_test2 = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],mean_fhat_spatio_temporal_test2);\n\n\n\n\n\nvis2(fhatbarhat_mean_spatio_temporal2,mean_fhat_spatio_temporal2);"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-2",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-2",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "10) Fourier transform",
    "text": "10) Fourier transform\n\nw=np.zeros((159*N,159*N))\n\n\nfor i in range(159*N):\n    for j in range(159*N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\n\n\nfhatbar = Psi.T @ mean_fhat_spatio_temporal.reshape(159*N,1)\npower = fhatbar**2"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-2",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-2",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "11) Ebayes",
    "text": "11) Ebayes\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\nfhatbar_threshed = ebayesthresh(FloatVector(fhatbar))"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-2",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-2",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "12) Inverse Fourier transform",
    "text": "12) Inverse Fourier transform\n\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_mean_spatio_temporal3 = fhatbarhat.reshape(159,N,1)"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-3",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-3",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "13) ST-GCN",
    "text": "13) ST-GCN\n\nfiveVTS_train_mean[seed_number1,0] = fhatbarhat_mean_spatio_temporal[seed_number1,0,0]\nfiveVTS_train_mean[seed_number2,1] = fhatbarhat_mean_spatio_temporal[seed_number2,1,0]\nfiveVTS_train_mean[seed_number3,2] = fhatbarhat_mean_spatio_temporal[seed_number3,2,0]\nfiveVTS_train_mean[seed_number4,3] = fhatbarhat_mean_spatio_temporal[seed_number4,3,0]\nfiveVTS_train_mean[seed_number5,4] = fhatbarhat_mean_spatio_temporal[seed_number5,4,0]\nvis(fiveVTS_train_mean);\n\n\n\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(mean_X_fiveVTS,mean_y_fiveVTS)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:26<00:00,  1.86it/s]\n\n\n\nmean_fhat_spatio_temporal3 = torch.stack([model(xt, edge_index, edge_attr) for xt in mean_X_fiveVTS]).detach().numpy()\n\n\nmean_fhat_spatio_temporal_test3 = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],mean_fhat_spatio_temporal_test3);\n\n\n\n\n\nvis2(fhatbarhat_mean_spatio_temporal3,mean_fhat_spatio_temporal3);\n\n\n\n\n\none = []\nfor i in range(N):\n    one.append(np.mean((fiveVTS_test[1:,i] - mean_fhat_fiveVTS_forecast.reshape(39,5)[:,i])))\n\n\ntwo = []\nfor i in range(N):\n    two.append(np.mean((fiveVTS_test[1:,i] - mean_fhat_spatio_temporal_test.reshape(39,5)[:,i])))\n\n\nthree = []\nfor i in range(N):\n    three.append(np.mean((fiveVTS_test[1:,i] - mean_fhat_spatio_temporal_test2.reshape(39,5)[:,i])))\n\n\nfour = []\nfor i in range(N):\n    four.append(np.mean((fiveVTS_test[1:,i] - mean_fhat_spatio_temporal_test3.reshape(39,5)[:,i])))\n\n\npd.DataFrame({'one':one,'two':two,'three':three,'four':four})\n\n\n\n\n\n  \n    \n      \n      one\n      two\n      three\n      four\n    \n  \n  \n    \n      0\n      -0.196310\n      -0.189000\n      -0.173563\n      -0.200559\n    \n    \n      1\n      -0.161632\n      -0.135003\n      -0.142250\n      -0.159892\n    \n    \n      2\n      0.079347\n      0.106893\n      0.108179\n      0.079011\n    \n    \n      3\n      -0.267653\n      -0.244438\n      -0.248220\n      -0.269292\n    \n    \n      4\n      -0.162464\n      -0.135709\n      -0.130221\n      -0.167336"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-4",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-4",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "1) ST-GCN",
    "text": "1) ST-GCN\n\nlinear_f_fiveVTS_train = torch.tensor(linear_fiveVTS_train).reshape(160,5,1).float()\n\n\nlinear_X_fiveVTS = linear_f_fiveVTS_train[:159,:,:]\nlinear_y_fiveVTS = linear_f_fiveVTS_train[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(linear_X_fiveVTS,linear_y_fiveVTS)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nlinear_fhat_fiveVTS = torch.stack([model(xt, edge_index, edge_attr) for xt in linear_X_fiveVTS]).detach().numpy()\n\n\nxt_test = torch.tensor(fiveVTS_test.reshape(40,5,1)[:-1,:,:]).float()\n\n\nlinear_fhat_fiveVTS_forecast = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],linear_fhat_fiveVTS_forecast);\n\n\nvis2(linear_fiveVTS_train,linear_f_fiveVTS_train);"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-3",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-3",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "2) Fourier transform",
    "text": "2) Fourier transform\n\nw=np.zeros((159*N,159*N))\n\n\nfor i in range(159*N):\n    for j in range(159*N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\n\n\nfhatbar = Psi.T @ linear_fhat_fiveVTS.reshape(159*N,1)\npower = fhatbar**2"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-3",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-3",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "3) Ebayes",
    "text": "3) Ebayes\n\nplt.plot(fhatbar.reshape(159,5)[:,0]**2)\n\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\nfhatbar_threshed = ebayesthresh(FloatVector(fhatbar))\n\n\nplt.plot(fhatbar)\nplt.plot(fhatbar_threshed)"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-3",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-3",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "4) Inverse Fourier transform",
    "text": "4) Inverse Fourier transform\n\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_linear_spatio_temporal = fhatbarhat.reshape(159,N,1)\n\n\nvis2(linear_fhat_fiveVTS,fhatbarhat_linear_spatio_temporal.reshape(159,5));"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-5",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-5",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "5) ST-GCN",
    "text": "5) ST-GCN\n\nlinear_spatio_temporal = torch.tensor(fhatbarhat_linear_spatio_temporal).reshape(159,5,1).float()\n\n\nlinear_X_spatio_temporal = linear_spatio_temporal[:158,:,:]\nlinear_y_spatio_temporal = linear_spatio_temporal[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(linear_X_spatio_temporal,linear_y_spatio_temporal)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nlinear_fhat_spatio_temporal = torch.stack([model(xt, edge_index, edge_attr) for xt in linear_X_spatio_temporal]).detach().numpy()\n\n\nlinear_fhat_spatio_temporal_test = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],linear_fhat_spatio_temporal_test);\n\n\nvis2(fhatbarhat_linear_spatio_temporal,linear_fhat_spatio_temporal);"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-4",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-4",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "6) Fourier transform",
    "text": "6) Fourier transform\n\nw=np.zeros((158*N,158*N))\n\n\nfor i in range(158*N):\n    for j in range(158*N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\n\n\nfhatbar = Psi.T @ linear_fhat_spatio_temporal.reshape(158*N,1)\npower = fhatbar**2"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-4",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-4",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "7) Ebayes",
    "text": "7) Ebayes\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\nfhatbar_threshed = ebayesthresh(FloatVector(fhatbar))\n\n\nplt.plot(fhatbar)\nplt.plot(fhatbar_threshed)"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-4",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-4",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "8) Inverse Fourier transform",
    "text": "8) Inverse Fourier transform\n\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_linear_spatio_temporal2 = fhatbarhat.reshape(158,N,1)\n\n\nvis2(linear_fhat_spatio_temporal,fhatbarhat_linear_spatio_temporal2.reshape(158,5));"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-6",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-6",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "9) ST-GCN",
    "text": "9) ST-GCN\n\nlinear_spatio_temporal2 = torch.tensor(fhatbarhat_linear_spatio_temporal2).reshape(158,5,1).float()\n\n\nlinear_X_spatio_temporal2 = linear_spatio_temporal2[:157,:,:]\nlinear_y_spatio_temporal2 = linear_spatio_temporal2[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(linear_X_spatio_temporal2,linear_y_spatio_temporal2)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nlinear_fhat_spatio_temporal2 = torch.stack([model(xt, edge_index, edge_attr) for xt in linear_X_spatio_temporal2]).detach().numpy()\n\n\nlinear_fhat_spatio_temporal_test2 = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],linear_fhat_spatio_temporal_test2);\n\n\nvis2(fhatbarhat_linear_spatio_temporal2,linear_fhat_spatio_temporal2);\n\n\none = []\nfor i in range(N):\n    one.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_fiveVTS_forecast.reshape(39,5)[:,i])))\n\n\ntwo = []\nfor i in range(N):\n    two.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_spatio_temporal_test.reshape(39,5)[:,i])))\n\n\nthree = []\nfor i in range(N):\n    three.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_spatio_temporal_test2.reshape(39,5)[:,i])))\n\n\npd.DataFrame({'one':one,'two':two,'three':three})"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-5",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-5",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "10) Fourier transform",
    "text": "10) Fourier transform\n\nw=np.zeros((157*N,157*N))\n\n\nfor i in range(157*N):\n    for j in range(157*N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\n\n\nfhatbar = Psi.T @ linear_fhat_spatio_temporal2.reshape(157*N,1)"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-5",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-5",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "11) Ebayes",
    "text": "11) Ebayes\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\nfhatbar_threshed = ebayesthresh(FloatVector(fhatbar))\n\n\nplt.plot(fhatbar)\nplt.plot(fhatbar_threshed)"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-5",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-5",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "12) Inverse Fourier transform",
    "text": "12) Inverse Fourier transform\n\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_linear_spatio_temporal3 = fhatbarhat.reshape(157,N,1)\n\n\nvis2(linear_fhat_spatio_temporal2,fhatbarhat_linear_spatio_temporal3.reshape(157,5));"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-7",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-7",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "13) ST-GCN",
    "text": "13) ST-GCN\n\nlinear_spatio_temporal3 = torch.tensor(fhatbarhat_linear_spatio_temporal3).reshape(157,5,1).float()\n\n\nlinear_X_spatio_temporal3 = linear_spatio_temporal3[:156,:,:]\nlinear_y_spatio_temporal3 = linear_spatio_temporal3[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(linear_X_spatio_temporal3,linear_y_spatio_temporal3)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nlinear_fhat_spatio_temporal3 = torch.stack([model(xt, edge_index, edge_attr) for xt in linear_X_spatio_temporal3]).detach().numpy()\n\n\nlinear_fhat_spatio_temporal_test3 = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],linear_fhat_spatio_temporal_test3);\n\n\nvis2(fhatbarhat_linear_spatio_temporal3,linear_fhat_spatio_temporal3);\n\n\none = []\nfor i in range(N):\n    one.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_fiveVTS_forecast.reshape(39,5)[:,i])))\n\n\ntwo = []\nfor i in range(N):\n    two.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_spatio_temporal_test.reshape(39,5)[:,i])))\n\n\nthree = []\nfor i in range(N):\n    three.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_spatio_temporal_test2.reshape(39,5)[:,i])))\n\n\nfour = []\nfor i in range(N):\n    four.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_spatio_temporal_test3.reshape(39,5)[:,i])))\n\n\npd.DataFrame({'one':one,'two':two,'three':three,'four':four})"
  },
  {
    "objectID": "posts/GCN/2023-05-11-Keras_st.html",
    "href": "posts/GCN/2023-05-11-Keras_st.html",
    "title": "STGCN on Keras",
    "section": "",
    "text": "Keras\n\n\nimport\n\nimport itstgcn \nimport torch\nimport numpy as np\n\n\nimport matplotlib.pyplot as plt\n\n\nimport random\n\n\nimport pickle\nimport pandas as pd\n\ndef load_data(fname):\n    with open(fname, 'rb') as outfile:\n        data_dict = pickle.load(outfile)\n    return data_dict\n\ndef save_data(data_dict,fname):\n    with open(fname,'wb') as outfile:\n        pickle.dump(data_dict,outfile)\n\nfrom plotnine import *\n\n\n\nExample\n\nT = 200\nt = np.arange(T)/T * 10\nx = 0.1*np.sin(2*t)+0.2*np.sin(4*t)+0.1*np.sin(8*t)+0.2*np.sin(16*t)\neps_x  = np.random.normal(size=T)*0\ny = x.copy()\n# for i in range(2,T):\n#     y[i] = 0.35*x[i-1] - 0.15*x[i-2] + 0.5*np.cos(0.5*t[i]) \neps_y  = np.random.normal(size=T)*0\nx = x*0.35\ny = y*0.3\nplt.plot(t,x,color='C0',lw=5)\nplt.plot(t,x+eps_x,alpha=0.5,color='C0')\nplt.plot(t,y,color='C1',lw=5)\nplt.plot(t,y+eps_y,alpha=0.5,color='C1')\n_node_ids = {'node1':0, 'node2':1}\n\n_FX1 = np.stack([x+eps_x,y+eps_y],axis=1).tolist()\n\n_edges1 = torch.tensor([[0,1],[1,0]]).tolist()\n\ndata_dict1 = {'edges':_edges1, 'node_ids':_node_ids, 'FX':_FX1}\n\n# save_data(data_dict1, './data/toy_example1.pkl')\n\ndata1 = pd.DataFrame({'x':x,'y':y,'xer':x,'yer':y})\n\nsave_data(data1, './data/toy_example_true1.csv')"
  },
  {
    "objectID": "posts/GCN/2023-03-17-ITSTGCN-Tutorial.html",
    "href": "posts/GCN/2023-03-17-ITSTGCN-Tutorial.html",
    "title": "ITSTGCN-Tutorial",
    "section": "",
    "text": "edit\n\n\nimport\n\nimport itstgcn \nimport torch\n\n\n\n예제1: vanilla STGCN\n- 데이터\n\ndata_dict = itstgcn.load_data('./data/fivenodes.pkl')\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 학습\n\nlrnr = itstgcn.StgcnLearner(train_dataset,dataset_name='five_nodes')\n\n/home/csy/Dropbox/blog/posts/GCN/itstgcn/learners.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/utils/tensor_new.cpp:201.)\n  self.lags = torch.tensor(train_dataset.features).shape[-1]\n\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) \nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제2: padding missing values\n- 데이터\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 임의로 결측치 발생\n\nmindex = itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex=mindex,mtype='rand')\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nfig \n\n\n\n\n- 적절한 method로 결측치를 채움 (default 는 linear)\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n다른 method로 결측치를 채울수도 있음. 사용할 수 있는 방법들은 아래에 정리되어 있음\n\nref: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html\n\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss,interpolation_method='nearest')\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss,interpolation_method='quadratic')\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss,interpolation_method='cubic')\n\n\nfig = itstgcn.utils.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.utils.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n- 블락으로 결측치 발생\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\n\n\nfig = itstgcn.utils.plot(torch.tensor(train_dataset_miss.targets),'o')\nfig \n\n\n\n\n\n\n예제3: vanilla STGCN with random missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.learners.StgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=5,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제4: vanilla STGCN with block missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.StgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=5,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제5: threshold example (random)\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 결측치 발생 및 패딩\n\nmindex=itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss)\n\n\nf_miss,_ = itstgcn.convert_train_dataset(train_dataset_miss)\nf_padded,_ = itstgcn.convert_train_dataset(train_dataset_padded)\n\n\nfig = itstgcn.utils.plot(f_miss,'o')\nitstgcn.utils.plot_add(fig,f_padded,'--x',alpha=0.5)\n\n\n\n\n- update by frequency thresholding\n\nfig = itstgcn.plot(f_miss,'o',alpha=0.5)\nitstgcn.plot_add(fig,f_padded,'--x',alpha=0.5)\nf_updated = itstgcn.update_from_freq_domain(f_padded,train_dataset_padded.mindex)\nitstgcn.plot_add(fig,f_updated,'-')\n\n\n\n\n\n\n예제6: threshold example (block)\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 결측치 발생 및 패딩\n\nmindex=[list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss)\n\n\nf_miss,_ = itstgcn.convert_train_dataset(train_dataset_miss)\nf_padded,_ = itstgcn.convert_train_dataset(train_dataset_padded)\n\n\nfig = itstgcn.plot(f_miss,'o')\nitstgcn.plot_add(fig,f_padded,'--x',alpha=0.5)\n\n\n\n\n- update by frequency thresholding\n\nfig = itstgcn.plot(f_miss,'o',alpha=0.5)\nitstgcn.plot_add(fig,f_padded,'--x',alpha=0.5)\nf_updated = itstgcn.update_from_freq_domain(f_padded,train_dataset_padded.mindex)\nitstgcn.plot_add(fig,f_updated,'-')\n\n\n\n\n\n\n예제7: iterative thresholded STGCN (IT-STGCN) with random missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.ITStgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제8: iterative thresholded STGCN (IT-STGCN) with block missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.ITStgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제9: GNAR (random missing)\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex=itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.GNARLearner(train_dataset_padded)\n\n\nlrnr.learn()\n\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제10: GNAR (block missing)\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex=[list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.GNARLearner(train_dataset_padded)\n\n\nlrnr.learn()\n\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-01-18-Algorithm_traintest_2.html",
    "href": "posts/GCN/2023-01-18-Algorithm_traintest_2.html",
    "title": "2nd ST-GCN Example dividing train and test",
    "section": "",
    "text": "Try to divide train and test(ST-GCN WikiMathsDatasetLoader)"
  },
  {
    "objectID": "posts/GCN/2023-01-18-Algorithm_traintest_2.html#train",
    "href": "posts/GCN/2023-01-18-Algorithm_traintest_2.html#train",
    "title": "2nd ST-GCN Example dividing train and test",
    "section": "Train",
    "text": "Train\n\ndata_train=[]\nfor time, snapshot in enumerate(train_dataset):\n    data_train.append([time,snapshot])\n\n\ndata_train[0][1].x.shape,data_train[0][1].y.shape,data_train[0][1].edge_index.shape,data_train[0][1].edge_attr.shape\n\n(torch.Size([1068, 1]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n583\n\n\n\nT_train = time\nN = len(data[0][1].x)\n\n\nedge_index = data_train[0][1].edge_index\nedge_attr = data_train[0][1].edge_attr\n\n\nx_train = []\nfor i in range(time):\n    x_train.append(data_train[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_train = data_tensor.reshape(time,1068,-1)\nx_train.shape\n\ntorch.Size([583, 1068, 1])\n\n\n\ny_train = []\nfor i in range(time):\n    y_train.append(data_train[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_train = data_tensor.reshape(time,1068)\ny_train.shape\n\ntorch.Size([583, 1068])\n\n\n\nx_train.shape, y_train.shape\n\n(torch.Size([583, 1068, 1]), torch.Size([583, 1068]))"
  },
  {
    "objectID": "posts/GCN/2023-01-18-Algorithm_traintest_2.html#test",
    "href": "posts/GCN/2023-01-18-Algorithm_traintest_2.html#test",
    "title": "2nd ST-GCN Example dividing train and test",
    "section": "Test",
    "text": "Test\n\ndata_test=[]\nfor time, snapshot in enumerate(test_dataset):\n    data_test.append([time,snapshot])\n\n\ndata_test[0][1].x.shape,data_test[0][1].y.shape,data_test[0][1].edge_index.shape,data_test[0][1].edge_attr.shape\n\n(torch.Size([1068, 1]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n145\n\n\n\nT_test = time\n\n\nx_test = []\nfor i in range(time):\n    x_test.append(data_test[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_test = data_tensor.reshape(time,1068,-1)\nx_test.shape\n\ntorch.Size([145, 1068, 1])\n\n\n\ny_test = []\nfor i in range(time):\n    y_test.append(data_test[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_test = data_tensor.reshape(time,1068)\ny_test.shape\n\ntorch.Size([145, 1068])\n\n\n\nx_test.shape, y_test.shape\n\n(torch.Size([145, 1068, 1]), torch.Size([145, 1068]))"
  },
  {
    "objectID": "posts/GCN/2023-01-18-Algorithm_traintest_2.html#st-gcn",
    "href": "posts/GCN/2023-01-18-Algorithm_traintest_2.html#st-gcn",
    "title": "2nd ST-GCN Example dividing train and test",
    "section": "ST-GCN",
    "text": "ST-GCN\n\nmean_f_train = x_train_mean.reshape(T_train,N,1).float()\n\n\nmean_X = mean_f_train[:438,:,:]\nmean_y = mean_f_train[145:,:,:]\n\n\nmean_X.shape,mean_y.shape\n\n(torch.Size([438, 1068, 1]), torch.Size([438, 1068, 1]))\n\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(mean_X,mean_y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [04:17<00:00,  5.15s/it]\n\n\n\nmean_X_fore = mean_f_train[438:,:]\n\n\nmean_fhat = torch.stack([model(xt, edge_index, edge_attr) for xt in mean_X_fore]).detach().numpy()\n\n\nmean_X_fore.shape,x_test.shape\n\n(torch.Size([145, 1068, 1]), torch.Size([145, 1068, 1]))"
  },
  {
    "objectID": "posts/GCN/2023-01-18-Algorithm_traintest_2.html#st-gcn-1",
    "href": "posts/GCN/2023-01-18-Algorithm_traintest_2.html#st-gcn-1",
    "title": "2nd ST-GCN Example dividing train and test",
    "section": "ST-GCN",
    "text": "ST-GCN\n\nlinear_f_train = x_train_linear.clone()\n\n\nlinear_X = linear_f_train[:438,:,:]\nlinear_y = linear_f_train[145:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(linear_X,linear_y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [04:20<00:00,  5.22s/it]\n\n\n\nlinear_X_fore = linear_f_train[438:,:]\n\n\nlinear_X_fore.shape\n\ntorch.Size([145, 1068, 1])\n\n\n\nlinear_fhat = torch.stack([model(xt, edge_index, edge_attr) for xt in linear_X_fore]).detach().numpy()\n\n\nlinear_X_fore.shape,x_test.shape\n\n(torch.Size([145, 1068, 1]), torch.Size([145, 1068, 1]))"
  },
  {
    "objectID": "posts/GCN/2023-05-05-ITSTGCN_LOADER.html",
    "href": "posts/GCN/2023-05-05-ITSTGCN_LOADER.html",
    "title": "ITSTGCN LOADERS Version",
    "section": "",
    "text": "edit"
  },
  {
    "objectID": "posts/GCN/2023-05-05-ITSTGCN_LOADER.html#import",
    "href": "posts/GCN/2023-05-05-ITSTGCN_LOADER.html#import",
    "title": "ITSTGCN LOADERS Version",
    "section": "import",
    "text": "import\n\nimport itstgcntry\nimport torch\n\n\nfrom torch_geometric_temporal.dataset import EnglandCovidDatasetLoader\n\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\n\n\nfrom torch_geometric_temporal.dataset import METRLADatasetLoader\n\n\nfrom torch_geometric_temporal.dataset import PedalMeDatasetLoader\n\n\nfrom torch_geometric_temporal.dataset import WindmillOutputLargeDatasetLoader\n\n\nfrom torch_geometric_temporal.dataset import WindmillOutputMediumDatasetLoader\n\n\nfrom torch_geometric_temporal.dataset import WindmillOutputSmallDatasetLoader\n\n\nfrom torch_geometric_temporal.dataset import MontevideoBusDatasetLoader\n\n\nfrom torch_geometric_temporal.dataset import WikiMathsDatasetLoader\n\n\nfrom torch_geometric_temporal.dataset import PemsBayDatasetLoader\n\n\nfrom torch_geometric_temporal.dataset import MTMDatasetLoader\n\n\nfrom torch_geometric_temporal.dataset import TwitterTennisDatasetLoader"
  },
  {
    "objectID": "posts/GCN/2023-05-05-ITSTGCN_LOADER.html#iterative-thresholded-stgcn-it-stgcn-with-random-missing",
    "href": "posts/GCN/2023-05-05-ITSTGCN_LOADER.html#iterative-thresholded-stgcn-it-stgcn-with-random-missing",
    "title": "ITSTGCN LOADERS Version",
    "section": "iterative thresholded STGCN (IT-STGCN) with random missing",
    "text": "iterative thresholded STGCN (IT-STGCN) with random missing\n- data\n\nloader = ChickenpoxDatasetLoader()\n\n\ndataset = loader.get_dataset(lags=1)\n\n\ntrain_dataset, test_dataset = itstgcntry.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = itstgcntry.rand_mindex(train_dataset,mrate=0.8)\n\n\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='rand',Datatyp='StaticGraphTemporalSignal_lags')\n\n\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss,Datatyp='StaticGraphTemporalSignal_lags') # padding(train_dataset_miss,method='linear'와 같음)a\n\n- 학습\n\nlrnr = itstgcntry.ITStgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(Datatyp='StaticGraphTemporalSignal_lags',filters=4,epoch=5,lr=0.01)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcntry.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=5,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(20)\nfig.set_figheight(20)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-05-05-ITSTGCN_LOADER.html#iterative-thresholded-stgcn-it-stgcn-with-block-missing",
    "href": "posts/GCN/2023-05-05-ITSTGCN_LOADER.html#iterative-thresholded-stgcn-it-stgcn-with-block-missing",
    "title": "ITSTGCN LOADERS Version",
    "section": "iterative thresholded STGCN (IT-STGCN) with block missing",
    "text": "iterative thresholded STGCN (IT-STGCN) with block missing\n- data\n\nloader = ChickenpoxDatasetLoader()\n\n\ndataset = loader.get_dataset(lags=1)\n\n\ntrain_dataset, test_dataset = itstgcntry.temporal_signal_split(dataset, train_ratio=0.8)\n\n\ntrain_dataset.\n\n<torch_geometric_temporal.signal.static_graph_temporal_signal.StaticGraphTemporalSignal at 0x7f9b4c9aefa0>\n\n\n\nmy_list = [[] for _ in range(20)] #chickenpox\nanother_list = list(range(100,350))\nmy_list[1] = another_list\nmy_list[2] = another_list\nmy_list[5] = another_list\nmindex = my_list\n\n\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='block',Datatyp='StaticGraphTemporalSignal_lags')\n\n\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss,Datatyp='StaticGraphTemporalSignal_lags') # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcntry.ITStgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(Datatyp='StaticGraphTemporalSignal_lags',filters=4,epoch=5,lr=0.01)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcntry.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=5,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(20)\nfig.set_figheight(20)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-05-05-ITSTGCN_LOADER.html#iterative-thresholded-stgcn-it-stgcn-with-random-missing-1",
    "href": "posts/GCN/2023-05-05-ITSTGCN_LOADER.html#iterative-thresholded-stgcn-it-stgcn-with-random-missing-1",
    "title": "ITSTGCN LOADERS Version",
    "section": "iterative thresholded STGCN (IT-STGCN) with random missing",
    "text": "iterative thresholded STGCN (IT-STGCN) with random missing\n- data\n\nloader = MTMDatasetLoader()\n\n\ndataset = loader.get_dataset(frames=1)\n\n\ntrain_dataset, test_dataset = itstgcntry.temporal_signal_split(dataset, train_ratio=0.8)\n\n\ntorch.tensor(train_dataset[0].x.T).shape,torch.tensor(train_dataset.targets).shape\n\n/tmp/ipykernel_10613/1932757802.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  torch.tensor(train_dataset[0].x.T).shape,torch.tensor(train_dataset.targets).shape\n\n\n(torch.Size([1, 20]), torch.Size([416, 20]))\n\n\n\ntorch.tensor(train_dataset[0].x.T).reshape(1,-1).shape,torch.tensor(train_dataset.targets).shape\n\n/tmp/ipykernel_10613/1932757802.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  torch.tensor(train_dataset[0].x.T).shape,torch.tensor(train_dataset.targets).shape\n\n\n(torch.Size([1, 21, 3]), torch.Size([11574, 1, 6]))\n\n\n\ntrain_dataset.snapshot_count\n\n11574\n\n\n\ntrain_dataset.edge_index\n\narray([[ 0,  0,  0,  0,  1,  2,  3,  6,  6,  7,  9, 10, 11, 13, 14, 15,\n        17, 18, 19],\n       [ 1,  5,  9, 17,  2,  3,  4,  5,  7,  8, 10, 11, 12, 14, 15, 16,\n        18, 19, 20]])\n\n\n\ntorch.tensor(train_dataset.features).shape\n\ntorch.Size([11574, 3, 21, 1])\n\n\n\nmindex = itstgcntry.rand_mindex(train_dataset,mrate=0.8)\n\nRuntimeError: Sizes of tensors must match except in dimension 0. Expected size 21 but got size 1 for tensor number 1 in the list.\n\n\n\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='rand',Datatyp='StaticGraphTemporalSignal_lags')\n\n\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss,Datatyp='StaticGraphTemporalSignal_lags') # padding(train_dataset_miss,method='linear'와 같음)a\n\n- 학습\n\nlrnr = itstgcntry.ITStgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(Datatyp='StaticGraphTemporalSignal_lags',filters=4,epoch=5,lr=0.01)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcntry.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=5,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(20)\nfig.set_figheight(20)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-05-05-ITSTGCN_LOADER.html#iterative-thresholded-stgcn-it-stgcn-with-block-missing-1",
    "href": "posts/GCN/2023-05-05-ITSTGCN_LOADER.html#iterative-thresholded-stgcn-it-stgcn-with-block-missing-1",
    "title": "ITSTGCN LOADERS Version",
    "section": "iterative thresholded STGCN (IT-STGCN) with block missing",
    "text": "iterative thresholded STGCN (IT-STGCN) with block missing\n- data\n\nloader = MTMDatasetLoader()\n\n\ndataset = loader.get_dataset(lags=1)\n\n\ntrain_dataset, test_dataset = itstgcntry.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmy_list = [[] for _ in range(20)] #chickenpox\nanother_list = list(range(100,350))\nmy_list[1] = another_list\nmy_list[2] = another_list\nmy_list[5] = another_list\nmindex = my_list\n\n\ntrain_dataset_miss = itstgcntry.miss(train_dataset,mindex,mtype='block',Datatyp='StaticGraphTemporalSignal_lags')\n\n\ntrain_dataset_padded = itstgcntry.padding(train_dataset_miss,Datatyp='StaticGraphTemporalSignal_lags') # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcntry.ITStgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(Datatyp='StaticGraphTemporalSignal_lags',filters=4,epoch=5,lr=0.01)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcntry.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=5,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(20)\nfig.set_figheight(20)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-04-29-pedalme_GSO_st.html",
    "href": "posts/GCN/2023-04-29-pedalme_GSO_st.html",
    "title": "Padalme GSO_st",
    "section": "",
    "text": "edit"
  },
  {
    "objectID": "posts/GCN/2023-04-29-pedalme_GSO_st.html#random",
    "href": "posts/GCN/2023-04-29-pedalme_GSO_st.html#random",
    "title": "Padalme GSO_st",
    "section": "random",
    "text": "random\n\nplans_stgcn_rand = {\n    'max_iteration': 30, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mrate': [0.7],\n    'lags': [8], \n    'nof_filters': [12], \n    'inter_method': ['cubic','linear','nearest'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnsnd.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader2,dataset_name='pedalme')\n\n\nplnr.simulate()\n\n1/30 is done\n47/50"
  },
  {
    "objectID": "posts/GCN/2023-04-29-pedalme_GSO_st.html#block",
    "href": "posts/GCN/2023-04-29-pedalme_GSO_st.html#block",
    "title": "Padalme GSO_st",
    "section": "block",
    "text": "block\n\nmy_list = [[] for _ in range(15)] #pedalme\nanother_list = list(range(10,25))\nmy_list[1] = another_list\nmy_list[3] = another_list\nmy_list[4] = another_list\nmy_list[5] = another_list\nanother_list = list(range(5,20))\nmy_list[7] = another_list\nmy_list[9] = another_list\nmy_list[10] = another_list\nmy_list[11] = another_list\nmindex = my_list\n\n\n# mindex= [[],[],[],list(range(50,150)),[]]  # node 1\n# mindex= [list(range(10,100)),[],list(range(50,80)),[],[]] # node 2\n# mindex= [list(range(10,100)),[],list(range(50,80)),list(range(50,150)),[]] # node3\nplans_stgcn_block = {\n    'max_iteration': 30, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [4], \n    'nof_filters': [12], \n    'inter_method': ['linear','nearest','cubic'],\n    'epoch': [50]\n}\n\n\nplnr = itstgcnsnd.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader2,dataset_name='pedalme')\nplnr.simulate(mindex=mindex,mtype='block')\n\n\n# df1 = pd.read_csv('./simulation_results/2023-04-13_20-37-59.csv')\n\n\n# data = pd.concat([df1],axis=0);data"
  },
  {
    "objectID": "posts/GCN/2023-05-13-Other Method.html",
    "href": "posts/GCN/2023-05-13-Other Method.html",
    "title": "ITSTGCN DCRNN",
    "section": "",
    "text": "summerizing it"
  },
  {
    "objectID": "posts/GCN/2023-05-13-Other Method.html#stgcn",
    "href": "posts/GCN/2023-05-13-Other Method.html#stgcn",
    "title": "ITSTGCN DCRNN",
    "section": "STGCN",
    "text": "STGCN\n- 학습\n\nlrnr = itstgcnadd.StgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(epoch=5)\n\n- 모형 평가 및 시각화\n\nevtor = itstgcnadd.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(20)\nfig.set_figheight(15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-05-13-Other Method.html#itstgcn",
    "href": "posts/GCN/2023-05-13-Other Method.html#itstgcn",
    "title": "ITSTGCN DCRNN",
    "section": "ITSTGCN",
    "text": "ITSTGCN\n- 학습\n\nlrnr2 = itstgcnadd.ITStgcnLearner(train_dataset_padded)\n\n\nlrnr2.learn(epoch=5)\n\n- 모형 평가 및 시각화\n\nevtor2 = itstgcnadd.Evaluator(lrnr2,train_dataset_padded,test_dataset)\n\n\nfig = evtor2.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(20)\nfig.set_figheight(15)\nfig.tight_layout()\nfig\n\n\n\nimport itstgcnadd\nimport torch\nimport itstgcnadd.planner"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "",
    "text": "GNAR fiveNet,fivenodes lag 1"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-일정",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-일정",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "missing 일정",
    "text": "missing 일정\n\nstgcn_train1 = []\nstgcn_test1 = []\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nfor i in range(100):\n    net = RecurrentGCN(node_features=1, filters=4)\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n    net.train()\n    for epoch in range(50):\n        for time, (xt,yt) in enumerate(zip(X,y)):\n            yt_hat = net(xt, edge_index, edge_attr)\n            cost = torch.mean((yt_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n    yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n    yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n    \n    train_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\n    test_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n    \n    stgcn_train1.append(train_mse_total_stgcn.tolist())\n    stgcn_test1.append(test_mse_total_stgcn.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(stgcn_train1);\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(stgcn_test1);"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-다르게",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-다르게",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "missing 다르게",
    "text": "missing 다르게\n\nstgcn_train2 = []\nstgcn_test2 = []\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nfor i in range(100):\n    \n    _zero = Missing(fiveVTS_train)\n    _zero.miss(percent = 0.8)\n    _zero.second_linear()\n\n    missing_index = _zero.number\n    interpolated_signal = _zero.train_linear\n\n\n    X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n    net = RecurrentGCN(node_features=1, filters=4)\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n    net.train()\n    for epoch in range(50):\n        for time, (xt,yt) in enumerate(zip(X,y)):\n            yt_hat = net(xt, edge_index, edge_attr)\n            cost = torch.mean((yt_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n    yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n    yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n    \n    train_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\n    test_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean() \n    \n    stgcn_train2.append(train_mse_total_stgcn.tolist())\n    stgcn_test2.append(test_mse_total_stgcn.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(stgcn_train2);\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(stgcn_test2);"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-일정-1",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-일정-1",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "missing 일정",
    "text": "missing 일정\n\nestgcn_train1 = []\nestgcn_test1 = []\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nfor i in range(100):\n    net = RecurrentGCN(node_features=1, filters=4)\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n    net.train()\n    signal = interpolated_signal.copy()\n    for epoch in range(50):\n        signal = update_from_freq_domain(signal,missing_index)\n        X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n        y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for time, (xt,yt) in enumerate(zip(X,y)):        \n            yt_hat = net(xt, edge_index, edge_attr)\n            cost = torch.mean((yt_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n    yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n    yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n    train_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\n    test_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n    \n    estgcn_train1.append(train_mse_total_estgcn.tolist())\n    estgcn_test1.append(test_mse_total_estgcn.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(estgcn_train1); \n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(estgcn_test1);"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-매번-다르게",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-매번-다르게",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "missing 매번 다르게",
    "text": "missing 매번 다르게\n\nestgcn_train2 = []\nestgcn_test2 = []\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nfor i in range(100):\n    \n    _zero = Missing(fiveVTS_train)\n    _zero.miss(percent = 0.8)\n    _zero.second_linear()\n\n    missing_index = _zero.number\n    interpolated_signal = _zero.train_linear\n\n    X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\n    net = RecurrentGCN(node_features=1, filters=4)\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n    net.train()\n    signal = interpolated_signal.copy()\n    for epoch in range(50):\n        signal = update_from_freq_domain(signal,missing_index)\n        X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n        y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for time, (xt,yt) in enumerate(zip(X,y)):        \n            yt_hat = net(xt, edge_index, edge_attr)\n            cost = torch.mean((yt_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])               \n\n    yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n    yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n    train_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\n    test_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n    \n    estgcn_train2.append(train_mse_total_estgcn.tolist())\n    estgcn_test2.append(test_mse_total_estgcn.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(estgcn_train2);\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(estgcn_test2);"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-일정-2",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-일정-2",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "missing 일정",
    "text": "missing 일정\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\nX = np.array(torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:].squeeze())\ny = np.array(torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:].squeeze())\n\nXX = np.array(torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float().squeeze())\nyy = np.array(torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float().squeeze())\n\nreal_y = np.array(torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:])\n\n\n%R -i X\n%R -i XX\n\n\n%%R\ngnar_train1 <- matrix(ncol=1,nrow=100)\ngnar_test1 <- matrix(ncol=1,nrow=100)\nfor(i in 1:100){\n  answer <- GNARfit(vts = X, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\n  prediction <- predict(answer,n.ahead=40)\n  \n  train_mse_total_gnar <- mean(residuals(answer)**2)\n  test_mse_total_gnar <- mean((XX - prediction[1:40])**2)\n  \n  gnar_train1[i] <- train_mse_total_gnar\n  gnar_test1[i] <- train_mse_total_gnar\n}\n\n\n%R -o gnar_train1\n%R -o gnar_test1\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(gnar_train1);\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(gnar_test1);"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-다르게-1",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-다르게-1",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "missing 다르게",
    "text": "missing 다르게\n\nm = robjects.r.matrix(FloatVector([0,0,0,1,1,0,0,1,1,0,0,1,0,1,0,1,1,1,0,0,1,0,0,0,0]), nrow = 5, ncol = 5)\nprint(m)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    1    1\n[2,]    0    0    1    1    0\n[3,]    0    1    0    1    0\n[4,]    1    1    1    0    0\n[5,]    1    0    0    0    0\n\n\n\n\ngnar_train2 = []\ngnar_test2 = []\n\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nfor i in range(100):\n    \n    _zero = Missing(fiveVTS_train)\n    _zero.miss(percent = 0.8)\n    _zero.second_linear()\n\n    missing_index = _zero.number\n    interpolated_signal = _zero.train_linear\n\n    X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-2),:,:]\n\n    answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = 160, ncol = 5),net = GNAR.matrixtoGNAR(m), alphaOrder = 2, betaOrder = FloatVector([1, 1]))             \n    predict = GNAR.predict_GNARfit(answer,n_ahead=40)\n\n    \n    train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,5))**2).mean()\n    test_mse_total_gnar = ((yy.squeeze() - pd.DataFrame(predict).values.reshape(-1,5)[:-1,:])**2).mean()\n    \n    gnar_train2.append(train_mse_total_gnar.tolist())\n    gnar_test2.append(test_mse_total_gnar.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(gnar_train2);\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(gnar_test2);"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation_boxplot.html",
    "href": "posts/GCN/2023-04-05-Simulation_boxplot.html",
    "title": "Simulation",
    "section": "",
    "text": "Simulation Study"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation_boxplot.html#baseline",
    "href": "posts/GCN/2023-04-05-Simulation_boxplot.html#baseline",
    "title": "Simulation",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='epoch',color='method',y='mse',facet_col='lags',height=800)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation_boxplot.html#random",
    "href": "posts/GCN/2023-04-05-Simulation_boxplot.html#random",
    "title": "Simulation",
    "section": "random",
    "text": "random\n\ndata.query(\"method!='GNAR' and mtype =='rand' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation_boxplot.html#block",
    "href": "posts/GCN/2023-04-05-Simulation_boxplot.html#block",
    "title": "Simulation",
    "section": "block",
    "text": "block\n\ndata.query(\"method!='GNAR' and mtype =='block' and inter_method=='cubic' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=600)\n\n\n                                                \n\n\n\ndata.query(\"method!='GNAR' and mtype =='block' and inter_method!='cubic' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=600)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation_boxplot.html#weight-matrix-time-node-고려한-결과",
    "href": "posts/GCN/2023-04-05-Simulation_boxplot.html#weight-matrix-time-node-고려한-결과",
    "title": "Simulation",
    "section": "weight matrix time, node 고려한 결과",
    "text": "weight matrix time, node 고려한 결과\n\ndf1 = pd.read_csv('./simulation_results/2023-04-30_13-00-12.csv')\ndf2 = pd.read_csv('./simulation_results/2023-04-30_13-31-32.csv')\ndf3 = pd.read_csv('./simulation_results/2023-04-30_14-01-49.csv')\ndf4 = pd.read_csv('./simulation_results/2023-04-30_14-31-56.csv')\ndf5 = pd.read_csv('./simulation_results/2023-04-30_15-02-23.csv')\ndf6 = pd.read_csv('./simulation_results/2023-04-30_15-33-03.csv')\ndf7 = pd.read_csv('./simulation_results/2023-04-30_16-07-43.csv')\ndf8 = pd.read_csv('./simulation_results/2023-04-30_16-41-35.csv')\ndf9 = pd.read_csv('./simulation_results/2023-04-30_17-14-51.csv')\ndf10 = pd.read_csv('./simulation_results/2023-04-30_17-49-34.csv')\ndf11 = pd.read_csv('./simulation_results/2023-04-30_18-21-29.csv')\ndf12 = pd.read_csv('./simulation_results/2023-04-30_18-50-24.csv')\ndf13 = pd.read_csv('./simulation_results/2023-04-30_20-33-28.csv')\ndf14 = pd.read_csv('./simulation_results/2023-05-04_16-40-05.csv')\ndf15 = pd.read_csv('./simulation_results/2023-05-04_17-34-00.csv')\n\n\ndata2 = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15],axis=0)\n\n\ndata2.to_csv('./simulation_results/Real_simulation/pedalme_Simulation_itstgcnsnd.csv',index=False)\n\n\ndata2 = pd.read_csv('./simulation_results/Real_simulation/pedalme_Simulation_itstgcnsnd.csv')\n\n\ndata2.query(\"mtype!='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=1000)\n\n\n                                                \n\n\n\ndata2.query(\"mtype=='block'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=1200)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation_boxplot.html#baseline-1",
    "href": "posts/GCN/2023-04-05-Simulation_boxplot.html#baseline-1",
    "title": "Simulation",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation_boxplot.html#random-1",
    "href": "posts/GCN/2023-04-05-Simulation_boxplot.html#random-1",
    "title": "Simulation",
    "section": "random",
    "text": "random\n\ndata.query(\"method=='GNAR'\").groupby('mrate')['mse'].unique()\n\nmrate\n0.0    [1.2959295511245728, 1.2547194957733154]\n0.3    [1.2959295511245728, 1.2547194957733154]\n0.5    [1.2959295511245728, 1.2547194957733154]\nName: mse, dtype: object\n\n\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation_boxplot.html#block-1",
    "href": "posts/GCN/2023-04-05-Simulation_boxplot.html#block-1",
    "title": "Simulation",
    "section": "block",
    "text": "block\n\ndf1 = pd.read_csv('./simulation_results/2023-04-27_07-50-11.csv')\ndf2 = pd.read_csv('./simulation_results/2023-04-27_22-09-07.csv')\ndf3 = pd.read_csv('./simulation_results/2023-04-28_14-40-59.csv')\ndf4 = pd.read_csv('./simulation_results/2023-05-14_19-46-46.csv')\n# df5 = pd.read_csv('./simulation_results/2023-05-14_19-46-46.csv')\n\n\ndata = pd.concat([df1,df2,df3,df4],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation/wikimath_block.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation/wikimath_block.csv')\n\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation_boxplot.html#missing-values-on-the-same-nodes",
    "href": "posts/GCN/2023-04-05-Simulation_boxplot.html#missing-values-on-the-same-nodes",
    "title": "Simulation",
    "section": "missing values on the same nodes",
    "text": "missing values on the same nodes\n\n# 10%\ndf1 = pd.read_csv('./simulation_results/2023-04-29_03-57-07.csv') # STGCN IT-STGCN block\ndf2 = pd.read_csv('./simulation_results/2023-04-29_20-15-46.csv') # STGCN IT-STGCN\ndf3 = pd.read_csv('./simulation_results/2023-04-30_16-19-58.csv') # STGCN IT-STGCN\n# 60% 확인하고 다시 돌리기\ndf4 = pd.read_csv('./simulation_results/2023-05-05_04-21-57.csv') # STGCN IT-STGCN 60%\ndf5 = pd.read_csv('./simulation_results/2023-05-06_11-34-46.csv') # STGCN IT-STGCN\ndf6 = pd.read_csv('./simulation_results/2023-05-06_23-43-35.csv') # STGCN IT-STGCN\ndf7 = pd.read_csv('./simulation_results/2023-05-07_14-06-44.csv') # STGCN IT-STGCN\n\n\ndata = pd.concat([df1,df2,df3,df4,df5,df6,df7],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation/wikimath_GSO_st.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation/wikimath_GSO_st.csv')\n\n\ndata.query(\"method=='GNAR'\")['mse'].unique()\n\narray([], dtype=float64)\n\n\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='inter_method',facet_row='lags',height=800)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation_boxplot.html#baseline-2",
    "href": "posts/GCN/2023-04-05-Simulation_boxplot.html#baseline-2",
    "title": "Simulation",
    "section": "Baseline",
    "text": "Baseline\n\ndf1 = pd.read_csv('./simulation_results/2023-04-17_06-05-37.csv') # STGCN IT-STGCN 70%\ndf2 = pd.read_csv('./simulation_results/2023-04-17_08-05-26.csv') # STGCN IT-STGCN\ndf3 = pd.read_csv('./simulation_results/2023-04-17_13-41-19.csv') # STGCN IT-STGCN\ndf4 = pd.read_csv('./simulation_results/2023-04-17_15-44-21.csv') # STGCN IT-STGCN\ndf5 = pd.read_csv('./simulation_results/2023-04-17_21-27-38.csv') # STGCN IT-STGCN\n# df6 = pd.read_csv('./simulation_results/2023-04-15_15-00-32.csv') # GNAR 30%, 50%, 70% # 뭔가 일단 필요없어서 데이터셋에서 뺌\ndf7 = pd.read_csv('./simulation_results/2023-04-18_05-01-55.csv') # STGCN IT-STGCN\ndf8 = pd.read_csv('./simulation_results/2023-04-18_06-14-06.csv') # STGCN IT-STGCN\ndf9 = pd.read_csv('./simulation_results/2023-04-18_17-32-30.csv') # STGCN IT-STGCN\ndf10 = pd.read_csv('./simulation_results/2023-04-19_01-52-24.csv') # STGCN IT-STGCN\ndf11 = pd.read_csv('./simulation_results/2023-04-19_07-50-52.csv') # STGCN IT-STGCN\ndf12 = pd.read_csv('./simulation_results/2023-04-19_09-30-25.csv') # STGCN IT-STGCN\ndf13 = pd.read_csv('./simulation_results/2023-04-19_15-32-55.csv') # STGCN IT-STGCN\ndf14 = pd.read_csv('./simulation_results/2023-04-19_17-12-06.csv') # STGCN IT-STGCN\ndf15 = pd.read_csv('./simulation_results/2023-04-19_23-07-36.csv') # STGCN IT-STGCN\ndf16 = pd.read_csv('./simulation_results/2023-04-20_00-46-43.csv') # STGCN IT-STGCN\ndf17 = pd.read_csv('./simulation_results/2023-04-20_06-51-34.csv') # STGCN IT-STGCN\ndf18 = pd.read_csv('./simulation_results/2023-04-20_08-30-27.csv') # STGCN IT-STGCN\ndf19 = pd.read_csv('./simulation_results/2023-04-20_14-28-35.csv') # STGCN IT-STGCN\ndf20 = pd.read_csv('./simulation_results/2023-04-20_16-08-39.csv') # STGCN IT-STGCN\ndf21 = pd.read_csv('./simulation_results/2023-04-20_22-09-37.csv') # STGCN IT-STGCN\ndf22 = pd.read_csv('./simulation_results/2023-04-20_23-48-26.csv') # STGCN IT-STGCN\ndf23 = pd.read_csv('./simulation_results/2023-04-21_05-36-47.csv') # STGCN IT-STGCN\ndf24 = pd.read_csv('./simulation_results/2023-04-21_15-26-00.csv') # STGCN IT-STGCN\ndf25 = pd.read_csv('./simulation_results/2023-04-21_23-27-11.csv') # STGCN IT-STGCN\ndf26 = pd.read_csv('./simulation_results/2023-04-22_07-46-08.csv') # STGCN IT-STGCN\ndf27 = pd.read_csv('./simulation_results/2023-04-22_15-45-20.csv') # STGCN IT-STGCN\ndf28 = pd.read_csv('./simulation_results/2023-04-22_22-57-31.csv') # STGCN IT-STGCN\ndf29 = pd.read_csv('./simulation_results/2023-04-23_07-00-15.csv') # STGCN IT-STGCN\ndf30 = pd.read_csv('./simulation_results/2023-04-23_15-18-02.csv') # STGCN IT-STGCN\ndf31 = pd.read_csv('./simulation_results/2023-04-23_15-22-36.csv') # GNAR 70%\n# baseline\ndf32 = pd.read_csv('./simulation_results/2023-04-29_06-54-40.csv') # GNAR \ndf33 = pd.read_csv('./simulation_results/2023-04-30_18-55-12.csv')\ndf34 = pd.read_csv('./simulation_results/2023-05-01_02-55-33.csv')\ndf35 = pd.read_csv('./simulation_results/2023-05-01_10-21-15.csv')\ndf36 = pd.read_csv('./simulation_results/2023-05-01_19-23-57.csv')\ndf37 = pd.read_csv('./simulation_results/2023-05-02_01-10-53.csv')\ndf38 = pd.read_csv('./simulation_results/2023-05-02_08-26-53.csv')\ndf39 = pd.read_csv('./simulation_results/2023-05-02_16-00-40.csv')\ndf40 = pd.read_csv('./simulation_results/2023-05-03_00-34-09.csv')\ndf41 = pd.read_csv('./simulation_results/2023-05-03_08-04-42.csv')\ndf42 = pd.read_csv('./simulation_results/2023-05-03_15-50-50.csv')\ndf43 = pd.read_csv('./simulation_results/2023-05-03_23-46-56.csv')\ndf44 = pd.read_csv('./simulation_results/2023-05-04_05-22-59.csv')\ndf45 = pd.read_csv('./simulation_results/2023-05-04_09-22-37.csv')\ndf46 = pd.read_csv('./simulation_results/2023-05-04_15-00-57.csv')\ndf47 = pd.read_csv('./simulation_results/2023-05-04_23-41-21.csv')\ndf48 = pd.read_csv('./simulation_results/2023-05-05_07-23-04.csv')\ndf49 = pd.read_csv('./simulation_results/2023-05-05_15-03-17.csv')\ndf50 = pd.read_csv('./simulation_results/2023-05-06_05-18-07.csv')\ndf51 = pd.read_csv('./simulation_results/2023-05-06_12-57-14.csv')\ndf52 = pd.read_csv('./simulation_results/2023-05-06_19-10-23.csv')\ndf53 = pd.read_csv('./simulation_results/2023-05-07_03-20-10.csv')\ndf54 = pd.read_csv('./simulation_results/2023-05-07_11-26-24.csv')\ndf55 = pd.read_csv('./simulation_results/2023-05-08_00-04-56.csv')\ndf56 = pd.read_csv('./simulation_results/2023-05-08_04-27-01.csv')\ndf57 = pd.read_csv('./simulation_results/2023-05-08_10-06-55.csv')\ndf58 = pd.read_csv('./simulation_results/2023-05-08_17-50-36.csv')\ndf59 = pd.read_csv('./simulation_results/2023-05-09_03-28-08.csv')\ndf60 = pd.read_csv('./simulation_results/2023-05-09_11-08-10.csv')\ndf61 = pd.read_csv('./simulation_results/2023-05-09_20-11-45.csv')\n\n\ndata = pd.concat([df1,df2,df3,df4,df5,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17,df18,\n                 df19,df20,df21,df22,df23,df24,df25,df26,df27,df28,df29,df30,df31,df32,df33,df34,\n                 df35,df36,df37,df38,df39,df40,df41,df42,df43,df44,df45,df46,df47,df48,df49,df50,\n                 df51,df52,df53,df54,df55,df56,df57,df58,df59,df60,df61],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation/windmillsmall.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation/windmillsmall.csv')\n\n\ndata.query(\"method=='GNAR' and mrate ==0\")['mse'].unique()\n\narray([1.64923024])\n\n\n\ndata.query(\"method!='GNAR' and mrate ==0 \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',height=800)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation_boxplot.html#random-2",
    "href": "posts/GCN/2023-04-05-Simulation_boxplot.html#random-2",
    "title": "Simulation",
    "section": "random",
    "text": "random\n\ndata.query(\"method=='GNAR' and mrate !=0\")['mse'].unique()\n\narray([1.64923024])\n\n\n\ndata.query(\"method!='GNAR' and mtype =='rand' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation_boxplot.html#block-2",
    "href": "posts/GCN/2023-04-05-Simulation_boxplot.html#block-2",
    "title": "Simulation",
    "section": "block",
    "text": "block\n\ndf1 = pd.read_csv('./simulation_results/2023-04-24_02-48-08.csv') # STGCN IT-STGCN block\ndf2 = pd.read_csv('./simulation_results/2023-04-24_10-57-10.csv') # STGCN IT-STGCN\ndf3 = pd.read_csv('./simulation_results/2023-04-24_18-53-34.csv') # STGCN IT-STGCN\ndf4 = pd.read_csv('./simulation_results/2023-04-25_02-30-27.csv') # STGCN IT-STGCN\ndf5 = pd.read_csv('./simulation_results/2023-04-25_10-48-46.csv') # STGCN IT-STGCN\ndf6 = pd.read_csv('./simulation_results/2023-04-25_10-53-14.csv') # GNAR \ndf7 = pd.read_csv('./simulation_results/2023-04-25_18-40-53.csv') # STGCN IT-STGCN\ndf8 = pd.read_csv('./simulation_results/2023-04-25_23-30-08.csv') # STGCN IT-STGCN\ndf9 = pd.read_csv('./simulation_results/2023-04-26_04-15-00.csv') # STGCN IT-STGCN\ndf10 = pd.read_csv('./simulation_results/2023-04-27_07-59-36.csv') # STGCN IT-STGCN\ndf11 = pd.read_csv('./simulation_results/2023-04-27_15-29-00.csv') # STGCN IT-STGCN\ndf12 = pd.read_csv('./simulation_results/2023-04-27_23-37-18.csv') # STGCN IT-STGCN\ndf13 = pd.read_csv('./simulation_results/2023-04-28_08-21-54.csv') # STGCN IT-STGCN\ndf14 = pd.read_csv('./simulation_results/2023-04-28_16-06-55.csv') # STGCN IT-STGCN\ndf15 = pd.read_csv('./simulation_results/2023-04-28_21-19-37.csv') # STGCN IT-STGCN\ndf16 = pd.read_csv('./simulation_results/2023-04-29_03-07-03.csv') # STGCN IT-STGCN\ndf17 = pd.read_csv('./simulation_results/2023-04-29_09-00-42.csv') # STGCN IT-STGCN\ndf18 = pd.read_csv('./simulation_results/2023-04-29_19-07-49.csv') # STGCN IT-STGCN\ndf19 = pd.read_csv('./simulation_results/2023-04-30_05-14-07.csv') # STGCN IT-STGCN\ndf20 = pd.read_csv('./simulation_results/2023-04-30_15-23-16.csv') # STGCN IT-STGCN\ndf21 = pd.read_csv('./simulation_results/2023-05-01_00-16-37.csv') # STGCN IT-STGCN\ndf22 = pd.read_csv('./simulation_results/2023-05-01_07-41-52.csv') # STGCN IT-STGCN\ndf23 = pd.read_csv('./simulation_results/2023-05-01_16-21-41.csv') # STGCN IT-STGCN\ndf24 = pd.read_csv('./simulation_results/2023-05-01_23-38-23.csv') # STGCN IT-STGCN\ndf25 = pd.read_csv('./simulation_results/2023-05-02_13-51-13.csv') # STGCN IT-STGCN\ndf26 = pd.read_csv('./simulation_results/2023-05-02_21-43-26.csv') # STGCN IT-STGCN\ndf27 = pd.read_csv('./simulation_results/2023-05-03_06-04-32.csv') # STGCN IT-STGCN\ndf28 = pd.read_csv('./simulation_results/2023-05-03_13-43-11.csv') # STGCN IT-STGCN\ndf29 = pd.read_csv('./simulation_results/2023-05-03_21-58-04.csv') # STGCN IT-STGCN\ndf30 = pd.read_csv('./simulation_results/2023-05-04_04-39-00.csv') # STGCN IT-STGCN\n\n\ndata = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,\n                 df17,df18,df19,df20,df21,df22,df23,df24,df25,df26,df27,df28,df29,df30],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation/windmillsmall_block.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation/windmillsmall_block.csv')\n\n\ndata.query(\"method=='GNAR'\")['mse'].unique()\n\narray([1.64923024])\n\n\n\ndata.query(\"method!='GNAR' and mtype =='block' \").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation_boxplot.html#baseline-3",
    "href": "posts/GCN/2023-04-05-Simulation_boxplot.html#baseline-3",
    "title": "Simulation",
    "section": "Baseline",
    "text": "Baseline\n\ndata.query(\"mrate==0 and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=600)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation_boxplot.html#random-3",
    "href": "posts/GCN/2023-04-05-Simulation_boxplot.html#random-3",
    "title": "Simulation",
    "section": "random",
    "text": "random\n\ndata.query(\"method=='GNAR'\").groupby('mrate')['mse'].unique()\n\nmrate\n0.0    [1.0619367361068726, 1.068463921546936]\n0.3    [1.0619367361068726, 1.068463921546936]\n0.4    [1.0619367361068726, 1.068463921546936]\n0.8    [1.0619367361068726, 1.068463921546936]\n0.9    [1.0619367361068726, 1.068463921546936]\nName: mse, dtype: object\n\n\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR' and mrate!=0.8 and mrate!=0.9\").sort_values('lags').plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)\n\n\n                                                \n\n\n\ndata.query(\"mtype=='rand' and mrate !=0 and method!='GNAR' and mrate!=0.3 and mrate!=0.4\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=800)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation_boxplot.html#block-3",
    "href": "posts/GCN/2023-04-05-Simulation_boxplot.html#block-3",
    "title": "Simulation",
    "section": "block",
    "text": "block\n\ndf1 = pd.read_csv('./simulation_results/2023-05-04_21-03-21.csv')\ndf2 = pd.read_csv('./simulation_results/2023-05-05_12-10-44.csv')\ndf3 = pd.read_csv('./simulation_results/2023-05-06_12-42-22.csv')\ndf4 = pd.read_csv('./simulation_results/2023-05-06_15-40-47.csv')\n\n\ndata = pd.concat([df1,df2,df3,df4],axis=0)\n\n\ndata.to_csv('./simulation_results/Real_simulation/monte_block.csv',index=False)\n\n\ndata = pd.read_csv('./simulation_results/Real_simulation/monte_block.csv')\n\n\ndata.query(\"mtype=='block' and method=='GNAR'\")['mse'].mean()\n\n1.0652003288269043\n\n\n\ndata.query(\"mtype=='block' and method!='GNAR'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)"
  },
  {
    "objectID": "posts/GCN/2023-04-25-note_matrix.html",
    "href": "posts/GCN/2023-04-25-note_matrix.html",
    "title": "Note_weight amatrix",
    "section": "",
    "text": "weight matrix\n- 하이퍼파라메터\n- wt,ws,f\n- f를 펼침\n- 펼쳐진 f에 대응하는 W 생성\n- trim\n임의로 ftrimed_flatten이 f_flatten과 같다고 생각하자.\n- ftrimed"
  },
  {
    "objectID": "posts/GCN/2023-04-25-note_matrix.html#chickenpox",
    "href": "posts/GCN/2023-04-25-note_matrix.html#chickenpox",
    "title": "Note_weight amatrix",
    "section": "Chickenpox",
    "text": "Chickenpox\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nloader1 = ChickenpoxDatasetLoader()\n\n\na = loader1.get_dataset(lags=1)\n\ntime,number of nodes\n\nT,N,_ = np.array(a.features).shape\n\n- wt,ws,f\n\nwt = np.zeros((T,T))\nfor i in range(T):\n    for j in range(T):\n        if i==j :\n            wt[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            wt[i,j] = 1\n\n\nmtr = a.edge_index\nmtr2 = a.edge_weight\n\n\nws = np.zeros((N,N))\nfor i in range(N):\n    for j in range(mtr2.shape[0]):\n        if mtr[0][j] == i :\n            ws[i,mtr[1][j]] = mtr2[j]\n\n\nnp.array(ws).shape\n\n(20, 20)\n\n\n\nf = np.array(a.features).reshape(T,N)\n\n- f를 펼침\n\nf_flatten = f.reshape(-1,1)\nf_flatten\n\narray([[-1.08135724e-03],\n       [-7.11136085e-01],\n       [-3.22808515e+00],\n       ...,\n       [ 4.71099041e-02],\n       [ 2.45684924e+00],\n       [-3.44296107e-01]])\n\n\n- 펼쳐진 f에 대응하는 W 생성\n\ndef flatten_weight(ws,wt):\n  N = len(ws)\n  T = len(wt)\n  Is = np.eye(N,N)\n  lst = [[0]*T for t in range(T)]\n  for i in range(T):\n    for j in range(T):\n      if i==j: \n        lst[i][j] = ws \n      elif abs(i-j)==1:\n        lst[i][j] = Is\n      else:\n        lst[i][j] = Is*0\n  return np.concatenate([np.concatenate(l,axis=1) for l in lst],axis=0)\n\n\nW_flatten = flatten_weight(ws,wt)\nW_flatten\n\narray([[1., 1., 0., ..., 0., 0., 0.],\n       [1., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 1., 1., 1.],\n       [0., 0., 0., ..., 1., 1., 1.],\n       [0., 0., 0., ..., 1., 1., 1.]])\n\n\n\nnp.save('./weight_st/W_chickenpox.npy', W_flatten)\n\n\nnp.load('./weight_st/W_chickenpox.npy')\n\narray([[1., 1., 0., ..., 0., 0., 0.],\n       [1., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 1., 1., 1.],\n       [0., 0., 0., ..., 1., 1., 1.],\n       [0., 0., 0., ..., 1., 1., 1.]])\n\n\n\nd = np.array(W_flatten.sum(axis=1))\n\n\nD = np.diag(d)\n\n\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-W_flatten) @ np.diag(1/np.sqrt(d)))\n\n\nlamb, Psi = np.linalg.eigh(L)\n\n\nnp.save('./weight_st/Psi_chickenpox.npy', Psi)\n\n\nnp.load('./weight_st/Psi_chickenpox.npy')\n\narray([[ 1.04115841e-02, -1.47242159e-02,  1.47242532e-02, ...,\n         6.41186713e-04, -4.27546925e-04,  2.13800213e-04],\n       [ 8.23107997e-03, -1.16404883e-02,  1.16404382e-02, ...,\n        -4.77715858e-04,  3.18549731e-04, -1.59296626e-04],\n       [ 8.23107997e-03, -1.16404502e-02,  1.16402861e-02, ...,\n         2.85275946e-04, -1.90235274e-04,  9.51330388e-05],\n       ...,\n       [ 8.23107997e-03,  1.16404565e-02,  1.16403112e-02, ...,\n        -6.65738542e-04, -4.43946869e-04, -2.22009806e-04],\n       [ 1.04115841e-02,  1.47242028e-02,  1.47242009e-02, ...,\n         1.35543431e-04,  9.03781585e-05,  4.51938438e-05],\n       [ 8.23107997e-03,  1.16404680e-02,  1.16403570e-02, ...,\n         5.65995233e-04,  3.77431091e-04,  1.88745843e-04]])\n\n\n- trim\nftrimed_flatten = trim(f_flatten,W_flatten)"
  },
  {
    "objectID": "posts/GCN/2023-04-25-note_matrix.html#pedalme",
    "href": "posts/GCN/2023-04-25-note_matrix.html#pedalme",
    "title": "Note_weight amatrix",
    "section": "Pedalme",
    "text": "Pedalme\n\nfrom torch_geometric_temporal.dataset import PedalMeDatasetLoader\nloader2 = PedalMeDatasetLoader()\n\n\na = loader2.get_dataset(lags=1)\n\ntime,number of nodes\n\nT,N,_ = np.array(a.features).shape\n\n- wt,ws,f\n\nwt = np.zeros((T,T))\nfor i in range(T):\n    for j in range(T):\n        if i==j :\n            wt[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            wt[i,j] = 1\n\n\nmtr = a.edge_index\nmtr2 = a.edge_weight\n\n\nws = np.zeros((N,N))\nfor i in range(N):\n    for j in range(mtr2.shape[0]):\n        if mtr[0][j] == i :\n            ws[i,mtr[1][j]] = mtr2[j]\n\n\nnp.array(wt).shape\n\n(34, 34)\n\n\n\nnp.array(ws).shape\n\n(15, 15)\n\n\n\n34*15\n\n510\n\n\n\nf = np.array(a.features).reshape(T,N)\n\n- f를 펼침\n\nf_flatten = f.reshape(-1,1)\n# f_flatten\n\n- 펼쳐진 f에 대응하는 W 생성\n\ndef flatten_weight(ws,wt):\n  N = len(ws)\n  T = len(wt)\n  Is = np.eye(N,N)\n  lst = [[0]*T for t in range(T)]\n  for i in range(T):\n    for j in range(T):\n      if i==j: \n        lst[i][j] = ws \n      elif abs(i-j)==1:\n        lst[i][j] = Is\n      else:\n        lst[i][j] = Is*0\n  return np.concatenate([np.concatenate(l,axis=1) for l in lst],axis=0)\n\n\nW_flatten = flatten_weight(ws,wt)\nW_flatten\n\narray([[1.        , 0.42545896, 0.15735536, ..., 0.        , 0.        ,\n        0.        ],\n       [0.42545896, 1.        , 0.06751402, ..., 0.        , 0.        ,\n        0.        ],\n       [0.15735536, 0.06751402, 1.        , ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 1.        , 0.07069877,\n        0.06899971],\n       [0.        , 0.        , 0.        , ..., 0.07069877, 1.        ,\n        0.32983841],\n       [0.        , 0.        , 0.        , ..., 0.06899971, 0.32983841,\n        1.        ]])\n\n\n- trim\nftrimed_flatten = trim(f_flatten,W_flatten)\n\nnp.save('./weight_st/W_pedalme.npy', W_flatten)\n\n\nnp.load('./weight_st/W_pedalme.npy')\n\narray([[1.        , 0.42545896, 0.15735536, ..., 0.        , 0.        ,\n        0.        ],\n       [0.42545896, 1.        , 0.06751402, ..., 0.        , 0.        ,\n        0.        ],\n       [0.15735536, 0.06751402, 1.        , ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 1.        , 0.07069877,\n        0.06899971],\n       [0.        , 0.        , 0.        , ..., 0.07069877, 1.        ,\n        0.32983841],\n       [0.        , 0.        , 0.        , ..., 0.06899971, 0.32983841,\n        1.        ]])\n\n\n\nd = np.array(W_flatten.sum(axis=1))\n\n\nD = np.diag(d)\n\n\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-W_flatten) @ np.diag(1/np.sqrt(d)))\n\n\nlamb, Psi = np.linalg.eigh(L)\n\n\nPsi.T.shape\n\n(510, 510)\n\n\n\nPsi.shape\n\n(510, 510)\n\n\n\nnp.save('./weight_st/Psi_pedalme.npy', Psi)\n\n\nnp.load('./weight_st/Psi_pedalme.npy')\n\narray([[ 4.61219573e-02, -6.52404719e-02,  6.52791904e-02, ...,\n        -4.33862149e-04, -2.17206920e-04,  8.97548015e-05],\n       [ 4.44297451e-02, -6.28451968e-02,  6.28773329e-02, ...,\n        -2.56832039e-05, -1.49624707e-05,  7.00873447e-06],\n       [ 3.51291880e-02, -4.95907787e-02,  4.93238061e-02, ...,\n        -2.71803598e-02, -1.77647577e-02,  8.79413561e-03],\n       ...,\n       [ 3.75563137e-02,  5.30576275e-02,  5.28917644e-02, ...,\n         5.85830960e-03, -4.16888716e-03, -2.15565030e-03],\n       [ 3.87057680e-02,  5.47153694e-02,  5.46437177e-02, ...,\n         1.08555123e-03, -6.10976014e-04, -2.76608545e-04],\n       [ 4.03127107e-02,  5.70025038e-02,  5.69740769e-02, ...,\n        -2.05662084e-04,  9.91534370e-05,  4.05213281e-05]])"
  },
  {
    "objectID": "posts/GCN/2023-04-25-note_matrix.html#wikimath",
    "href": "posts/GCN/2023-04-25-note_matrix.html#wikimath",
    "title": "Note_weight amatrix",
    "section": "Wikimath",
    "text": "Wikimath\n\nfrom torch_geometric_temporal.dataset import WikiMathsDatasetLoader\nloader3 = WikiMathsDatasetLoader()\n\n\na = loader3.get_dataset(lags=1)\n\ntime,number of nodes\n\nT,N,_ = np.array(a.features).shape\n\n- wt,ws,f\n\nwt = np.zeros((T,T))\nfor i in range(T):\n    for j in range(T):\n        if i==j :\n            wt[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            wt[i,j] = 1\n\n\nmtr = a.edge_index\nmtr2 = a.edge_weight\n\n\nnp.array(mtr).shape\n\n(2, 27079)\n\n\n\nnp.array(mtr2).shape\n\n(27079,)\n\n\n\nmtr\n\narray([[   0,    0,    0, ..., 1056, 1063, 1065],\n       [   1,    2,    3, ..., 1059, 1064, 1066]])\n\n\n\npd.DataFrame(mtr2).iloc[:,0].unique()\n\narray([ 1,  4,  2,  5,  3,  6,  7,  9,  8, 12, 10, 13, 16, 11])\n\n\n\nws = np.zeros((N,N))\nfor i in range(N):\n    for j in range(mtr2.shape[0]):\n        if mtr[0][j] == i :\n            ws[i,mtr[1][j]] = mtr2[j]\n\n\nnp.array(ws).shape\n\n(1068, 1068)\n\n\n\nf = np.array(a.features).reshape(T,N)\n\n- f를 펼침\n\nf_flatten = f.reshape(-1,1)\n# f_flatten\n\n- 펼쳐진 f에 대응하는 W 생성\n\nN = len(ws)\nT = len(wt)\nIs = np.eye(N,N)\nlst = [[0]*T for t in range(T)]\n\n\ndef flatten_weight(ws,wt):\n  N = len(ws)\n  T = len(wt)\n  Is = np.eye(N,N)\n  lst = [[0]*T for t in range(T)]\n  for i in range(T):\n    for j in range(T):\n      if i==j: \n        lst[i][j] = ws \n      elif abs(i-j)==1:\n        lst[i][j] = Is\n      else:\n        lst[i][j] = Is*0\n  return np.concatenate([np.concatenate(l,axis=1) for l in lst],axis=0)\n\n\nW_flatten = flatten_weight(ws,wt)\nW_flatten\n\n- trim\nftrimed_flatten = trim(f_flatten,W_flatten)\n\nnp.save('./weight_st/W_wikimath.npy', W_flatten)\n\n\nnp.load('./weight_st/W_wikimath.npy')"
  },
  {
    "objectID": "posts/GCN/2023-04-25-note_matrix.html#windmillsmall",
    "href": "posts/GCN/2023-04-25-note_matrix.html#windmillsmall",
    "title": "Note_weight amatrix",
    "section": "Windmillsmall",
    "text": "Windmillsmall\n\nfrom torch_geometric_temporal.dataset import WindmillOutputSmallDatasetLoader\nloader6 = WindmillOutputSmallDatasetLoader()\n\n\na = loader6.get_dataset(lags=1)\n\ntime,number of nodes\n\nT,N,_ = np.array(a.features).shape\n\n- wt,ws,f\n\nwt = np.zeros((T,T))\nfor i in range(T):\n    for j in range(T):\n        if i==j :\n            wt[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            wt[i,j] = 1\n\n\nmtr = a.edge_index\nmtr2 = a.edge_weight\n\n\nws = np.zeros((N,N))\nfor i in range(N):\n    for j in range(mtr2.shape[0]):\n        if mtr[0][j] == i :\n            ws[i,mtr[1][j]] = mtr2[j]\n\n\nnp.array(ws).shape\n\n(11, 11)\n\n\n\nf = np.array(a.features).reshape(T,N)\n\n- f를 펼침\n\nf_flatten = f.reshape(-1,1)\n# f_flatten\n\n- 펼쳐진 f에 대응하는 W 생성\n\ndef flatten_weight(ws,wt):\n  N = len(ws)\n  T = len(wt)\n  Is = np.eye(N,N)\n  lst = [[0]*T for t in range(T)]\n  for i in range(T):\n    for j in range(T):\n      if i==j: \n        lst[i][j] = ws \n      elif abs(i-j)==1:\n        lst[i][j] = Is\n      else:\n        lst[i][j] = Is*0\n  return np.concatenate([np.concatenate(l,axis=1) for l in lst],axis=0)\n\n\nW_flatten = flatten_weight(ws,wt)\nW_flatten\n\n- trim\nftrimed_flatten = trim(f_flatten,W_flatten)\n\nnp.save('./weight_st/W_windmillsmall.npy', W_flatten)\n\n\nnp.load('./weight_st/W_windmillsmall.npy')"
  },
  {
    "objectID": "posts/GCN/2023-05-04-questions of pytorch geometric temporal.html",
    "href": "posts/GCN/2023-05-04-questions of pytorch geometric temporal.html",
    "title": "Questions of PyTorch Geometric Temporal",
    "section": "",
    "text": "PyTorch Geometric Temporal"
  },
  {
    "objectID": "posts/GCN/2023-05-04-questions of pytorch geometric temporal.html#applications",
    "href": "posts/GCN/2023-05-04-questions of pytorch geometric temporal.html#applications",
    "title": "Questions of PyTorch Geometric Temporal",
    "section": "Applications",
    "text": "Applications\n\nEpidemiological Forecasting\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import DCRNN\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = DCRNN(node_features, 32, 1)\n        self.linear = torch.nn.Linear(32, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features = 4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(200)):\n    cost = 0\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    \n    \n###########################################################\n# I added this to check the shape.\nprint(y_hat.shape,snapshot.y.shape,(y_hat-snapshot.y).shape)\n\n100%|██████████| 200/200 [02:40<00:00,  1.24it/s]\n\n\ntorch.Size([20, 1]) torch.Size([20]) torch.Size([20, 20])\n\n\n\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n# >>> MSE: 1.0232\n\nMSE: 1.0247\n\n\n\n\nShape Check (1)\n\na = torch.randn(20, 1)\n\n\nb = torch.randn(20)\n\n\nc = a-b\n\n\nprint(a.size(),b.size(),c.size())\n\ntorch.Size([20, 1]) torch.Size([20]) torch.Size([20, 20])\n\n\n\n\n\nDoesn’t it have to ‘y_hat’ be the same shape as snapshot.y?\n\nIf we want to compare the y_hat from the model with the values y, the same shape is appropriate to evaluate.\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features = 4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(200)):\n    cost = 0\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr).reshape(-1)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    \n    \n###########################################################\n# I added this to check the shape.\nprint(y_hat.shape,snapshot.y.shape,(y_hat-snapshot.y).shape)\n\n100%|██████████| 200/200 [01:27<00:00,  2.30it/s]\n\n\ntorch.Size([20]) torch.Size([20]) torch.Size([20])\n\n\n\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n# >>> MSE: 1.0232\n\nMSE: 1.2844\n\n\n\n\n\nShape Check (2)\n\na = torch.randn(20, 1).reshape(-1)\n\n\nb = torch.randn(20)\n\n\nc = a-b\n\n\nprint(a.size(),b.size(),c.size())\n\ntorch.Size([20]) torch.Size([20]) torch.Size([20])"
  },
  {
    "objectID": "posts/GCN/2023-05-04-questions of pytorch geometric temporal.html#web-traffic-prediction",
    "href": "posts/GCN/2023-05-04-questions of pytorch geometric temporal.html#web-traffic-prediction",
    "title": "Questions of PyTorch Geometric Temporal",
    "section": "Web Traffic Prediction",
    "text": "Web Traffic Prediction\n\nfrom torch_geometric_temporal.dataset import WikiMathsDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset(lags=14)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvGRU\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvGRU(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=14, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    \n    \n###########################################################\n# I added this to check the shape.\nprint(y_hat.shape,snapshot.y.shape,(y_hat-snapshot.y).shape)\n\n100%|██████████| 50/50 [31:26<00:00, 37.73s/it]\n\n\ntorch.Size([1068, 1]) torch.Size([1068]) torch.Size([1068, 1068])\n\n\n\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n# >>> MSE: 0.7760\n\nMSE: 0.7939\n\n\n\n\nShape Check (1)\n\na = torch.randn(1068, 1)\n\n\nb = torch.randn(1068)\n\n\nc = a-b\n\n\nprint(a.size(),b.size(),c.size())\n\ntorch.Size([1068, 1]) torch.Size([1068]) torch.Size([1068, 1068])\n\n\n\n\n\nIf the code changes the shape of y_hat?\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=14, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    \n    \n###########################################################\n# I added this to check the shape.\nprint(y_hat.shape,snapshot.y.shape,(y_hat-snapshot.y).shape)\n\n100%|██████████| 50/50 [36:39<00:00, 43.99s/it]\n\n\ntorch.Size([1068, 1]) torch.Size([1068]) torch.Size([1068, 1068])\n\n\n\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n# >>> MSE: 0.7760\n\nMSE: 0.7807\n\n\n\n\n\nShape Check (2)\n\na = torch.randn(1068, 1).reshape(-1)\n\n\nb = torch.randn(1068)\n\n\nc = a-b\n\n\nprint(a.size(),b.size(),c.size())\n\ntorch.Size([1068]) torch.Size([1068]) torch.Size([1068])\n\n\n\n\n\nReferences\n\n@inproceedings{rozemberczki2021pytorch, author = {Benedek Rozemberczki and Paul Scherer and Yixuan He and George Panagopoulos and Alexander Riedel and Maria Astefanoaei and Oliver Kiss and Ferenc Beres and and Guzman Lopez and Nicolas Collignon and Rik Sarkar}, title = {{PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models}}, year = {2021}, booktitle={Proceedings of the 30th ACM International Conference on Information and Knowledge Management}, pages = {4564–4573}, }"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html",
    "title": "2nd ITSTGCN",
    "section": "",
    "text": "GNAR fiveNet,fivenodes lag 1"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#stgcn",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#stgcn",
    "title": "2nd ITSTGCN",
    "section": "STGCN",
    "text": "STGCN\n\nclass STGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            for epoch in range(50):\n                for time, (xt,yt) in enumerate(zip(X,y)):\n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_stgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_stgcn = (((self.yy-yyhat).squeeze())**2).mean() \n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset, \n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_stgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_stgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#enhencement-of-stgcn",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#enhencement-of-stgcn",
    "title": "2nd ITSTGCN",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nclass ESTGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n    \n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            signal = interpolated_signal.copy()\n            for epoch in range(50):\n                signal = update_from_freq_domain(signal,missing_index)\n                X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n                y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n                for time, (xt,yt) in enumerate(zip(X,y)):        \n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])               \n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_estgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_estgcn = (((self.yy-yyhat).squeeze())**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_estgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_estgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#gnar",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#gnar",
    "title": "2nd ITSTGCN",
    "section": "GNAR",
    "text": "GNAR\n\nm = robjects.r.matrix(FloatVector([0,0,0,1,1,0,0,1,1,0,0,1,0,1,0,1,1,1,0,0,1,0,0,0,0]), nrow = 5, ncol = 5)\n\n\nclass GNAR_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-2),:,:]\n\n            answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = 160, ncol = 5),net = GNAR.matrixtoGNAR(m), alphaOrder = 2, betaOrder = FloatVector([1, 1]))             \n            predict = GNAR.predict_GNARfit(answer,n_ahead=40)\n\n\n            train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,5))**2).mean()\n            test_mse_total_gnar = ((self.yy.squeeze() - pd.DataFrame(predict).values.reshape(-1,5)[:-1,:])**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_gnar.tolist()\n            df_row['MSE_test'] = test_mse_total_gnar.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#stgcn-1",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#stgcn-1",
    "title": "2nd ITSTGCN",
    "section": "STGCN",
    "text": "STGCN\n\nDataset = 'fivenodes'\nMethod = 'stgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_stgcn= pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    stgcn = STGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    stgcn.iter()\n    df_add = stgcn.df.copy()\n    df_stgcn = pd.concat([df_stgcn,df_add],axis=0)\n\n\nsave_data(df_stgcn, './data/GNAR_stgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#enhencement-of-stgcn-1",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#enhencement-of-stgcn-1",
    "title": "2nd ITSTGCN",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nDataset = 'fivenodes'\nMethod = 'estgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_estgcn = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    estgcn = ESTGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    estgcn.iter()\n    df_add = estgcn.df.copy()\n    df_estgcn = pd.concat([df_estgcn,df_add],axis=0)\n\n\nsave_data(df_estgcn, './data/GNAR_estgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#gnar-1",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#gnar-1",
    "title": "2nd ITSTGCN",
    "section": "GNAR",
    "text": "GNAR\n\nDataset = 'fivenodes'\nMethod = 'gnar' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = None # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_gnar = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    gnar = GNAR_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    gnar.iter()\n    df_add = gnar.df.copy()\n    df_gnar = pd.concat([df_gnar,df_add],axis=0)\n\n\nsave_data(df_gnar, './data/GANR_gnar_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html",
    "href": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "",
    "text": "ST-GCN Dataset WikiMathsDatasetLoader"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#train",
    "href": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#train",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "Train",
    "text": "Train\n\ndata_train=[]\nfor time, snapshot in enumerate(train_dataset):\n    data_train.append([time,snapshot])\n\n\ndata_train[0][1].x.shape,data_train[0][1].y.shape,data_train[0][1].edge_index.shape,data_train[0][1].edge_attr.shape\n\n(torch.Size([1068, 4]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n580\n\n\n\nT_train = time\nN = len(data_train[0][1].x)\n\n\nedge_index = data_train[0][1].edge_index\nedge_attr = data_train[0][1].edge_attr\n\n\nx_train = []\nfor i in range(time):\n    x_train.append(data_train[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_train = data_tensor.reshape(time,1068,-1)\nx_train.shape\n\ntorch.Size([580, 1068, 4])\n\n\n\ny_train = []\nfor i in range(time):\n    y_train.append(data_train[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_train = data_tensor.reshape(time,1068)\ny_train.shape\n\ntorch.Size([580, 1068])\n\n\n\nx_train.shape, y_train.shape\n\n(torch.Size([580, 1068, 4]), torch.Size([580, 1068]))"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#test",
    "href": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#test",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "Test",
    "text": "Test\n\ndata_test=[]\nfor time, snapshot in enumerate(test_dataset):\n    data_test.append([time,snapshot])\n\n\ndata_test[0][1].x.shape,data_test[0][1].y.shape,data_test[0][1].edge_index.shape,data_test[0][1].edge_attr.shape\n\n(torch.Size([1068, 4]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n145\n\n\n\nT_test = time\n\n\nx_test = []\nfor i in range(time):\n    x_test.append(data_test[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_test = data_tensor.reshape(time,1068,-1)\nx_test.shape\n\ntorch.Size([145, 1068, 4])\n\n\n\ny_test = []\nfor i in range(time):\n    y_test.append(data_test[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_test = data_tensor.reshape(time,1068)\ny_test.shape\n\ntorch.Size([145, 1068])\n\n\n\nx_test.shape, y_test.shape\n\n(torch.Size([145, 1068, 4]), torch.Size([145, 1068]))"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#시나리오1-baseline",
    "href": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#시나리오1-baseline",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "시나리오1 (Baseline)",
    "text": "시나리오1 (Baseline)\n시나리오1\n\nmissing rate: 0%\n보간방법: None\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([x_train_f[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = x_train_f[4:T_train,:].reshape(T_train-4,N,-1).float()\n\n\nXX = x_test\nyy = y_test\n\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [05:47<00:00,  6.96s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y.squeeze()-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y.squeeze()-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\n\nGNAR 으로 적합 + 예측\n-\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\n\nR[write to console]: Loading required package: igraph\n\nR[write to console]: \nAttaching package: ‘igraph’\n\n\nR[write to console]: The following objects are masked from ‘package:stats’:\n\n    decompose, spectrum\n\n\nR[write to console]: The following object is masked from ‘package:base’:\n\n    union\n\n\nR[write to console]: Loading required package: wordcloud\n\nR[write to console]: Loading required package: RColorBrewer\n\n\n\n\nEdge = np.array(edge_index)\nX_gnar = np.array(x_train_f)\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet <- matrixtoGNAR(w)\n\n\n%%R\nanswer <- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 4, betaOrder = c(1,1,1,1))\nprediction <- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((yy-gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((yy-gnar_test)**2).mean()\n\n\n\n결과시각화\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,h=5)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n GNAR: mse(train) = {3:.2f}, mse(test) = {4:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(range(4,580),stgcn_train[:,i],label='STCGCN (train)',color='C0')\n    a.plot(range(583,728),stgcn_test[:,i],label='STCGCN (test)',color='C0')\n    a.plot(range(4,583),gnar_train[:,i],label='GNAR (train)',color='C1')\n    a.plot(range(583,728),gnar_test[:,i],label='GNAR (test)',color='C1')\n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario1: STGCN \\n missing=0% \\n interpolation=None \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n GNAR: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#시나리오2",
    "href": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#시나리오2",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "시나리오2",
    "text": "시나리오2\n시나리오2\n\nmissing rate: 50%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(x_train_f)\n_zero.miss(percent = 0.5)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],x_train[:,:,0][:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN 으로 적합 + 예측\nimport numpy as np\n\nT= 100\nN= 5\nlag =4 \n\nsignal=np.arange(T*N).reshape(T,N)\n\nX= np.stack([signal[i:(T-lag+i),:] for i in range(lag)],axis=-1)\nX.shape\n\ny=signal[lag:].reshape(T-lag,N,1)\ny.shape\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\n\nXX = x_test\nyy = y_test\n\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [06:23<00:00,  7.67s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train[4:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\n\nXX = x_test\nyy = y_test\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(np.stack([signal[i:(T_train+i),:] for i in range(4)],axis = -1)).reshape(T_train,N,4).float()\n    y = torch.tensor(signal).reshape(-1,N,1).float()[4:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().reshape(1,-1)]).squeeze()\n\n100%|██████████| 50/50 [07:11<00:00,  8.63s/it]\n\n\n- ESTGCN\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\ny_train.shape,T_train\n\n(torch.Size([580, 1068]), 580)\n\n\n\nreal_y = y_train\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nX_gnar = np.array(torch.concat([X[:-1,:,0], x_train[-1,:,:].T]))\nEdge = np.array(edge_index)\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet <- matrixtoGNAR(w)\n\n\n%%R\nanswer <- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 4, betaOrder = c(1,1,1,1))\nprediction <- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((y_test-gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((y_test-gnar_test)**2).mean()\n\n\n\n결과시각화\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,h=5)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],x_train[missing_index[i],i,0],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(4,580),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(583,728),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(4,584),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(582,727),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(4,583),gnar_train.squeeze()[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(583,728),gnar_test.squeeze()[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#시나리오3",
    "href": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#시나리오3",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "시나리오3",
    "text": "시나리오3\n시나리오3\n\nmissing rate: 80%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(x_train_f)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,h=5)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],x_train_f[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\n\nXX = x_test\nyy = y_test\n\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [06:40<00:00,  8.01s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train[4:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\n\nXX = x_test\nyy = y_test\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(np.stack([signal[i:(T_train+i),:] for i in range(4)],axis = -1)).reshape(T_train,N,4).float()\n    y = torch.tensor(signal).reshape(-1,N,1).float()[4:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().reshape(1,-1)]).squeeze()\n\n100%|██████████| 50/50 [07:18<00:00,  8.77s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nX_gnar = np.array(torch.concat([X[:-1,:,0], x_train[-1,:,:].T]))\nEdge = np.array(edge_index)\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet <- matrixtoGNAR(w)\n\n\n%%R\nanswer <- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 4, betaOrder = c(1,1,1,1))\nprediction <- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((y_test-gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((y_test-gnar_test)**2).mean()\n\n\n\n결과시각화\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,h=5)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],x_train[missing_index[i],i,0],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(4,580),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(583,728),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(4,584),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(582,727),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(4,583),gnar_train.squeeze()[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(583,728),gnar_test.squeeze()[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#시나리오4",
    "href": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#시나리오4",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "시나리오4",
    "text": "시나리오4\n시나리오4\n\nmissing rate: 30%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(x_train_f)\n_zero.miss(percent = 0.3)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,h=5)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],x_train_f[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\n\nXX = x_test\nyy = y_test\n\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [06:32<00:00,  7.86s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train[4:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\n\nXX = x_test\nyy = y_test\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(np.stack([signal[i:(T_train+i),:] for i in range(4)],axis = -1)).reshape(T_train,N,4).float()\n    y = torch.tensor(signal).reshape(-1,N,1).float()[4:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().reshape(1,-1)]).squeeze()\n\n100%|██████████| 50/50 [07:13<00:00,  8.66s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nX_gnar = np.array(torch.concat([X[:-1,:,0], x_train[-1,:,:].T]))\nEdge = np.array(edge_index)\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet <- matrixtoGNAR(w)\n\n\n%%R\nanswer <- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 4, betaOrder = c(1,1,1,1))\nprediction <- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((y_test-gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((y_test-gnar_test)**2).mean()\n\n\n\n결과시각화\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,h=5)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],x_train[missing_index[i],i,0],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(4,580),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(583,728),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(4,584),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(582,727),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(4,583),gnar_train.squeeze()[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(583,728),gnar_test.squeeze()[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario4: \\n missing=30% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html",
    "title": "SY 1st ITSTGCN",
    "section": "",
    "text": "edit"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#plnr_stgcn_rand",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#plnr_stgcn_rand",
    "title": "SY 1st ITSTGCN",
    "section": "PLNR_STGCN_RAND",
    "text": "PLNR_STGCN_RAND\n\n# _data = load_data('./data/fivenodes.pkl')\n\n# _edges = torch.tensor(_data['edges']).nonzero().tolist()\n# _FX = _data['f'].tolist()\n# _node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \n\ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = DatasetLoader(data_dict)\n\n\nplans_stgcn_rand = {\n    'max_iteration': 3, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mrate': [0.0, 0.2, 0.4],\n    'lags': [2, 4], \n    'nof_filters': [8,16], \n    'inter_method': ['nearest','linear'],\n    'epoch': [1]\n}\n\n\nclass PLNR_STGCN_RAND:\n    def __init__(self,plans,loader,dataset_name=None,simulation_results=None):\n        self.plans = plans\n        col = ['dataset', 'method', 'mrate', 'mtype', 'lags', 'nof_filters', 'inter_method', 'epoch', 'mse']\n        self.loader = loader\n        self.dataset_name = dataset_name\n        self.simulation_results = pd.DataFrame(columns=col) if simulation_results is None else simulation_results \n    def simulate(self):\n        for _ in range(self.plans['max_iteration']):  \n            product_iterator = itertools.product(\n                self.plans['method'], \n                self.plans['mrate'], \n                self.plans['lags'], \n                self.plans['nof_filters'], \n                self.plans['inter_method'],\n                self.plans['epoch']\n            )\n            for prod_iter in product_iterator:\n                method,mrate,lags,nof_filters,inter_method,epoch = prod_iter\n                self.dataset = self.loader.get_dataset(lags=lags)\n                train_dataset, test_dataset = torch_geometric_temporal.signal.temporal_signal_split(self.dataset, train_ratio=0.8)\n                if mrate > 0: \n                    mtype = 'rand'\n                    mindex = rand_mindex(train_dataset,mrate=mrate)\n                    train_dataset = padding(train_dataset_miss = miss(train_dataset,mindex=mindex,mtype=mtype),interpolation_method=inter_method)\n                elif mrate ==0: \n                    mtype = None\n                    inter_method = None \n                if method == 'STGCN':\n                    lrnr = StgcnLearner(train_dataset,dataset_name=self.dataset_name)\n                elif method == 'IT-STGCN':\n                    lrnr = ITStgcnLearner(train_dataset,dataset_name=self.dataset_name)\n                lrnr.learn(filters=nof_filters,epoch=epoch)\n                evtor = Evaluator(lrnr,train_dataset,test_dataset)\n                evtor.calculate_mse()\n                mse = evtor.mse['test']['total']\n                self._record(method,mrate,mtype,lags,nof_filters,inter_method,epoch,mse)\n            print('{}/{} is done'.format(_+1,self.plans['max_iteration']))\n    def _record(self,method,mrate,mtype,lags,nof_filters,inter_method,epoch,mse):\n        dct = {'dataset': self.dataset_name,\n               'method': method,\n               'mrate': mrate,\n               'mtype': mtype, \n               'lags': lags,\n               'nof_filters': nof_filters,\n               'inter_method': inter_method,\n               'epoch': epoch,\n               'mse': mse\n              }\n        simulation_result_new = pd.Series(dct).to_frame().transpose()\n        self.simulation_results = pd.concat([self.simulation_results,simulation_result_new]).reset_index(drop=True)\n\n\nplnr = PLNR_STGCN_RAND(plans,loader,dataset_name='five_nodes')\n\n\nplnr.simulate()\n\n1/3 is done\n2/3 is done\n3/3 is done\n\n\n\ndf = plnr.simulation_results\ndf\n\n\n\n\n\n  \n    \n      \n      dataset\n      method\n      mrate\n      mtype\n      lags\n      nof_filters\n      inter_method\n      epoch\n      mse\n    \n  \n  \n    \n      0\n      five_nodes\n      STGCN\n      0.0\n      None\n      2\n      8\n      None\n      1\n      1.202111\n    \n    \n      1\n      five_nodes\n      STGCN\n      0.0\n      None\n      2\n      8\n      None\n      1\n      1.173311\n    \n    \n      2\n      five_nodes\n      STGCN\n      0.0\n      None\n      2\n      16\n      None\n      1\n      1.170123\n    \n    \n      3\n      five_nodes\n      STGCN\n      0.0\n      None\n      2\n      16\n      None\n      1\n      1.18629\n    \n    \n      4\n      five_nodes\n      STGCN\n      0.0\n      None\n      4\n      8\n      None\n      1\n      1.238957\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      139\n      five_nodes\n      IT-STGCN\n      0.4\n      rand\n      2\n      16\n      linear\n      1\n      1.195191\n    \n    \n      140\n      five_nodes\n      IT-STGCN\n      0.4\n      rand\n      4\n      8\n      nearest\n      1\n      1.208371\n    \n    \n      141\n      five_nodes\n      IT-STGCN\n      0.4\n      rand\n      4\n      8\n      linear\n      1\n      1.160624\n    \n    \n      142\n      five_nodes\n      IT-STGCN\n      0.4\n      rand\n      4\n      16\n      nearest\n      1\n      1.15774\n    \n    \n      143\n      five_nodes\n      IT-STGCN\n      0.4\n      rand\n      4\n      16\n      linear\n      1\n      1.217873\n    \n  \n\n144 rows × 9 columns"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#plnr_stgcn_block",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#plnr_stgcn_block",
    "title": "SY 1st ITSTGCN",
    "section": "PLNR_STGCN_BLOCK",
    "text": "PLNR_STGCN_BLOCK\n\n# _data = load_data('./data/fivenodes.pkl')\n\n# _edges = torch.tensor(_data['edges']).nonzero().tolist()\n# _FX = _data['f'].tolist()\n# _node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \n\ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = DatasetLoader(data_dict)\n\n\nmindex_block = [list(range(10,100)),[],list(range(50,80)),[],[]]\nplans_stgcn_block = {\n    'max_iteration': 3, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex_block],\n    'lags': [2, 4], \n    'nof_filters': [8,16], \n    'inter_method': ['nearest','linear'],\n    'epoch': [1]\n}\n\n\nclass PLNR_STGCN_BLOCK:\n    def __init__(self,plans,loader,dataset_name=None,simulation_results=None):\n        self.plans = plans\n        col = ['dataset', 'method', 'mrate', 'mtype', 'lags', 'nof_filters', 'inter_method', 'epoch', 'mse']\n        self.loader = loader\n        self.dataset_name = dataset_name\n        self.simulation_results = pd.DataFrame(columns=col) if simulation_results is None else simulation_results \n    def simulate(self):\n        for _ in range(self.plans['max_iteration']):\n            product_iterator = itertools.product(\n                self.plans['method'], \n                self.plans['mindex'],\n                self.plans['lags'],\n                self.plans['nof_filters'],\n                self.plans['inter_method'],\n                self.plans['epoch']\n            )\n            for prod_iter in product_iterator:\n                method,mrate,lags,nof_filters,inter_method,epoch = prod_iter\n                self.dataset = self.loader.get_dataset(lags=lags)\n                train_dataset, test_dataset = torch_geometric_temporal.signal.temporal_signal_split(self.dataset, train_ratio=0.8)\n                mtype = 'block'\n                train_dataset = padding(train_dataset_miss = miss(train_dataset,mindex=mindex,mtype=mtype),interpolation_method=inter_method)\n                if method == 'STGCN':\n                    lrnr = StgcnLearner(train_dataset,dataset_name=self.dataset_name)\n                elif method == 'IT-STGCN':\n                    lrnr = ITStgcnLearner(train_dataset,dataset_name=self.dataset_name)\n                lrnr.learn(filters=nof_filters,epoch=epoch)\n                evtor = Evaluator(lrnr,train_dataset,test_dataset)\n                evtor.calculate_mse()\n                mse = evtor.mse['test']['total']\n                mrate= lrnr.mrate_total\n                self._record(method,mrate,mtype,lags,nof_filters,inter_method,epoch,mse)\n    def _record(self,method,mrate,mtype,lags,nof_filters,inter_method,epoch,mse):\n        dct = {'dataset': self.dataset_name,\n               'method': method,\n               'mrate': mrate,\n               'mtype': mtype, \n               'lags': lags,\n               'nof_filters': nof_filters,\n               'inter_method': inter_method,\n               'epoch': epoch,\n               'mse': mse\n              }\n        simulation_result_new = pd.Series(dct).to_frame().transpose()\n        self.simulation_results = pd.concat([self.simulation_results,simulation_result_new]).reset_index(drop=True)\n\n\nplnr = PLNR_STGCN_BLOCK(plans_stgcn_block,loader,dataset_name='five_nodes')\n\n\nplnr.simulate()\n\n1/1\n\n\n\ndf = plnr.simulation_results\ndf\n\n\n\n\n\n  \n    \n      \n      dataset\n      method\n      mrate\n      mtype\n      lags\n      nof_filters\n      inter_method\n      epoch\n      mse\n    \n  \n  \n    \n      0\n      five_nodes\n      STGCN\n      0.5\n      block\n      2\n      8\n      nearest\n      1\n      1.162601\n    \n    \n      1\n      five_nodes\n      STGCN\n      0.5\n      block\n      2\n      8\n      linear\n      1\n      1.145895\n    \n    \n      2\n      five_nodes\n      STGCN\n      0.5\n      block\n      2\n      16\n      nearest\n      1\n      1.166197\n    \n    \n      3\n      five_nodes\n      STGCN\n      0.5\n      block\n      2\n      16\n      linear\n      1\n      1.165355\n    \n    \n      4\n      five_nodes\n      STGCN\n      0.5\n      block\n      4\n      8\n      nearest\n      1\n      1.157954\n    \n    \n      5\n      five_nodes\n      STGCN\n      0.5\n      block\n      4\n      8\n      linear\n      1\n      1.162674\n    \n    \n      6\n      five_nodes\n      STGCN\n      0.5\n      block\n      4\n      16\n      nearest\n      1\n      1.179143\n    \n    \n      7\n      five_nodes\n      STGCN\n      0.5\n      block\n      4\n      16\n      linear\n      1\n      1.175561\n    \n    \n      8\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      2\n      8\n      nearest\n      1\n      1.195364\n    \n    \n      9\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      2\n      8\n      linear\n      1\n      1.2184\n    \n    \n      10\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      2\n      16\n      nearest\n      1\n      1.210481\n    \n    \n      11\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      2\n      16\n      linear\n      1\n      1.169326\n    \n    \n      12\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      4\n      8\n      nearest\n      1\n      1.193523\n    \n    \n      13\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      4\n      8\n      linear\n      1\n      1.199567\n    \n    \n      14\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      4\n      16\n      nearest\n      1\n      1.201094\n    \n    \n      15\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      4\n      16\n      linear\n      1\n      1.210867\n    \n    \n      16\n      five_nodes\n      STGCN\n      0.5\n      block\n      2\n      8\n      nearest\n      1\n      1.169622\n    \n    \n      17\n      five_nodes\n      STGCN\n      0.5\n      block\n      2\n      8\n      linear\n      1\n      1.173848\n    \n    \n      18\n      five_nodes\n      STGCN\n      0.5\n      block\n      2\n      16\n      nearest\n      1\n      1.176841\n    \n    \n      19\n      five_nodes\n      STGCN\n      0.5\n      block\n      2\n      16\n      linear\n      1\n      1.15848\n    \n    \n      20\n      five_nodes\n      STGCN\n      0.5\n      block\n      4\n      8\n      nearest\n      1\n      1.191304\n    \n    \n      21\n      five_nodes\n      STGCN\n      0.5\n      block\n      4\n      8\n      linear\n      1\n      1.155874\n    \n    \n      22\n      five_nodes\n      STGCN\n      0.5\n      block\n      4\n      16\n      nearest\n      1\n      1.188419\n    \n    \n      23\n      five_nodes\n      STGCN\n      0.5\n      block\n      4\n      16\n      linear\n      1\n      1.197183\n    \n    \n      24\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      2\n      8\n      nearest\n      1\n      1.210021\n    \n    \n      25\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      2\n      8\n      linear\n      1\n      1.184674\n    \n    \n      26\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      2\n      16\n      nearest\n      1\n      1.274009\n    \n    \n      27\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      2\n      16\n      linear\n      1\n      1.188723\n    \n    \n      28\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      4\n      8\n      nearest\n      1\n      1.217735\n    \n    \n      29\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      4\n      8\n      linear\n      1\n      1.202317\n    \n    \n      30\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      4\n      16\n      nearest\n      1\n      1.219543\n    \n    \n      31\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      4\n      16\n      linear\n      1\n      1.202418\n    \n    \n      32\n      five_nodes\n      STGCN\n      0.5\n      block\n      2\n      8\n      nearest\n      1\n      1.158991\n    \n    \n      33\n      five_nodes\n      STGCN\n      0.5\n      block\n      2\n      8\n      linear\n      1\n      1.187762\n    \n    \n      34\n      five_nodes\n      STGCN\n      0.5\n      block\n      2\n      16\n      nearest\n      1\n      1.182213\n    \n    \n      35\n      five_nodes\n      STGCN\n      0.5\n      block\n      2\n      16\n      linear\n      1\n      1.161439\n    \n    \n      36\n      five_nodes\n      STGCN\n      0.5\n      block\n      4\n      8\n      nearest\n      1\n      1.188787\n    \n    \n      37\n      five_nodes\n      STGCN\n      0.5\n      block\n      4\n      8\n      linear\n      1\n      1.233327\n    \n    \n      38\n      five_nodes\n      STGCN\n      0.5\n      block\n      4\n      16\n      nearest\n      1\n      1.15206\n    \n    \n      39\n      five_nodes\n      STGCN\n      0.5\n      block\n      4\n      16\n      linear\n      1\n      1.161346\n    \n    \n      40\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      2\n      8\n      nearest\n      1\n      1.215097\n    \n    \n      41\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      2\n      8\n      linear\n      1\n      1.163064\n    \n    \n      42\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      2\n      16\n      nearest\n      1\n      1.206054\n    \n    \n      43\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      2\n      16\n      linear\n      1\n      1.177454\n    \n    \n      44\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      4\n      8\n      nearest\n      1\n      1.233471\n    \n    \n      45\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      4\n      8\n      linear\n      1\n      1.209842\n    \n    \n      46\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      4\n      16\n      nearest\n      1\n      1.221017\n    \n    \n      47\n      five_nodes\n      IT-STGCN\n      0.5\n      block\n      4\n      16\n      linear\n      1\n      1.218403"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#plnr_gnar_rand",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#plnr_gnar_rand",
    "title": "SY 1st ITSTGCN",
    "section": "PLNR_GNAR_RAND",
    "text": "PLNR_GNAR_RAND\n\n# _data = load_data('./data/fivenodes.pkl')\n\n# _edges = torch.tensor(_data['edges']).nonzero().tolist()\n# _FX = _data['f'].tolist()\n# _node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \n\ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = DatasetLoader(data_dict)\n\n\nplans_gnar_rand = {\n    'max_iteration': 3, \n#    'method': ['GNAR'], \n    'mrate': [0.0, 0.2, 0.4],\n    'lags': [2, 4], \n#    'nof_filters': [8,16], \n    'inter_method': ['nearest','linear'],\n#    'epoch': [1]\n}\n\n\nclass PLNR_GNAR_RAND:\n    def __init__(self,plans,loader,dataset_name=None,simulation_results=None):\n        self.plans = plans\n        col = ['dataset', 'method', 'mrate', 'mtype', 'lags', 'nof_filters', 'inter_method', 'epoch', 'mse']\n        self.loader = loader\n        self.dataset_name = dataset_name\n        self.simulation_results = pd.DataFrame(columns=col) if simulation_results is None else simulation_results \n    def simulate(self):\n        for _ in range(self.plans['max_iteration']):\n            product_iterator = itertools.product(\n                self.plans['mrate'],\n                self.plans['lags'],\n                self.plans['inter_method']\n            )\n            for prod_iter in product_iterator:\n                mrate,lags,inter_method = prod_iter\n                self.dataset = self.loader.get_dataset(lags=lags)\n                train_dataset, test_dataset = torch_geometric_temporal.signal.temporal_signal_split(self.dataset, train_ratio=0.8)\n                if mrate > 0: \n                    mtype = 'rand'\n                    mindex = rand_mindex(train_dataset,mrate=mrate)\n                    train_dataset = padding(train_dataset_miss = miss(train_dataset,mindex=mindex,mtype=mtype),interpolation_method=inter_method)\n                elif mrate ==0: \n                    mtype = None\n                    inter_method = None \n                method = 'GNAR'\n                lrnr = GNARLearner(train_dataset,dataset_name=self.dataset_name)\n                lrnr.learn()\n                evtor = Evaluator(lrnr,train_dataset,test_dataset)\n                evtor.calculate_mse()\n                mse = evtor.mse['test']['total']\n                nof_filters = None \n                epoch= None\n                self._record(method,mrate,mtype,lags,nof_filters,inter_method,epoch,mse)\n    def _record(self,method,mrate,mtype,lags,nof_filters,inter_method,epoch,mse):\n        dct = {'dataset': self.dataset_name,\n               'method': method,\n               'mrate': mrate,\n               'mtype': mtype, \n               'lags': lags,\n               'nof_filters': nof_filters,\n               'inter_method': inter_method,\n               'epoch': epoch,\n               'mse': mse\n              }\n        simulation_result_new = pd.Series(dct).to_frame().transpose()\n        self.simulation_results = pd.concat([self.simulation_results,simulation_result_new]).reset_index(drop=True)\n\n\nplnr = PLNR_GNAR_RAND(plans_gnar_rand,loader,dataset_name='five_nodes')\nplnr.simulate()\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nplnr.simulation_results\n\n\n\n\n\n  \n    \n      \n      dataset\n      method\n      mrate\n      mtype\n      lags\n      nof_filters\n      inter_method\n      epoch\n      mse\n    \n  \n  \n    \n      0\n      five_nodes\n      GNAR\n      0.0\n      None\n      2\n      None\n      None\n      None\n      1.40683\n    \n    \n      1\n      five_nodes\n      GNAR\n      0.0\n      None\n      2\n      None\n      None\n      None\n      1.40683\n    \n    \n      2\n      five_nodes\n      GNAR\n      0.0\n      None\n      4\n      None\n      None\n      None\n      1.469004\n    \n    \n      3\n      five_nodes\n      GNAR\n      0.0\n      None\n      4\n      None\n      None\n      None\n      1.469004\n    \n    \n      4\n      five_nodes\n      GNAR\n      0.2\n      rand\n      2\n      None\n      nearest\n      None\n      1.40683\n    \n    \n      5\n      five_nodes\n      GNAR\n      0.2\n      rand\n      2\n      None\n      linear\n      None\n      1.40683\n    \n    \n      6\n      five_nodes\n      GNAR\n      0.2\n      rand\n      4\n      None\n      nearest\n      None\n      1.469004\n    \n    \n      7\n      five_nodes\n      GNAR\n      0.2\n      rand\n      4\n      None\n      linear\n      None\n      1.469004\n    \n    \n      8\n      five_nodes\n      GNAR\n      0.4\n      rand\n      2\n      None\n      nearest\n      None\n      1.40683\n    \n    \n      9\n      five_nodes\n      GNAR\n      0.4\n      rand\n      2\n      None\n      linear\n      None\n      1.40683\n    \n    \n      10\n      five_nodes\n      GNAR\n      0.4\n      rand\n      4\n      None\n      nearest\n      None\n      1.469004\n    \n    \n      11\n      five_nodes\n      GNAR\n      0.4\n      rand\n      4\n      None\n      linear\n      None\n      1.469004\n    \n    \n      12\n      five_nodes\n      GNAR\n      0.0\n      None\n      2\n      None\n      None\n      None\n      1.40683\n    \n    \n      13\n      five_nodes\n      GNAR\n      0.0\n      None\n      2\n      None\n      None\n      None\n      1.40683\n    \n    \n      14\n      five_nodes\n      GNAR\n      0.0\n      None\n      4\n      None\n      None\n      None\n      1.469004\n    \n    \n      15\n      five_nodes\n      GNAR\n      0.0\n      None\n      4\n      None\n      None\n      None\n      1.469004\n    \n    \n      16\n      five_nodes\n      GNAR\n      0.2\n      rand\n      2\n      None\n      nearest\n      None\n      1.40683\n    \n    \n      17\n      five_nodes\n      GNAR\n      0.2\n      rand\n      2\n      None\n      linear\n      None\n      1.40683\n    \n    \n      18\n      five_nodes\n      GNAR\n      0.2\n      rand\n      4\n      None\n      nearest\n      None\n      1.469004\n    \n    \n      19\n      five_nodes\n      GNAR\n      0.2\n      rand\n      4\n      None\n      linear\n      None\n      1.469004\n    \n    \n      20\n      five_nodes\n      GNAR\n      0.4\n      rand\n      2\n      None\n      nearest\n      None\n      1.40683\n    \n    \n      21\n      five_nodes\n      GNAR\n      0.4\n      rand\n      2\n      None\n      linear\n      None\n      1.40683\n    \n    \n      22\n      five_nodes\n      GNAR\n      0.4\n      rand\n      4\n      None\n      nearest\n      None\n      1.469004\n    \n    \n      23\n      five_nodes\n      GNAR\n      0.4\n      rand\n      4\n      None\n      linear\n      None\n      1.469004\n    \n    \n      24\n      five_nodes\n      GNAR\n      0.0\n      None\n      2\n      None\n      None\n      None\n      1.40683\n    \n    \n      25\n      five_nodes\n      GNAR\n      0.0\n      None\n      2\n      None\n      None\n      None\n      1.40683\n    \n    \n      26\n      five_nodes\n      GNAR\n      0.0\n      None\n      4\n      None\n      None\n      None\n      1.469004\n    \n    \n      27\n      five_nodes\n      GNAR\n      0.0\n      None\n      4\n      None\n      None\n      None\n      1.469004\n    \n    \n      28\n      five_nodes\n      GNAR\n      0.2\n      rand\n      2\n      None\n      nearest\n      None\n      1.40683\n    \n    \n      29\n      five_nodes\n      GNAR\n      0.2\n      rand\n      2\n      None\n      linear\n      None\n      1.40683\n    \n    \n      30\n      five_nodes\n      GNAR\n      0.2\n      rand\n      4\n      None\n      nearest\n      None\n      1.469004\n    \n    \n      31\n      five_nodes\n      GNAR\n      0.2\n      rand\n      4\n      None\n      linear\n      None\n      1.469004\n    \n    \n      32\n      five_nodes\n      GNAR\n      0.4\n      rand\n      2\n      None\n      nearest\n      None\n      1.40683\n    \n    \n      33\n      five_nodes\n      GNAR\n      0.4\n      rand\n      2\n      None\n      linear\n      None\n      1.40683\n    \n    \n      34\n      five_nodes\n      GNAR\n      0.4\n      rand\n      4\n      None\n      nearest\n      None\n      1.469004\n    \n    \n      35\n      five_nodes\n      GNAR\n      0.4\n      rand\n      4\n      None\n      linear\n      None\n      1.469004"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#plnr_gnar_block",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#plnr_gnar_block",
    "title": "SY 1st ITSTGCN",
    "section": "PLNR_GNAR_BLOCK",
    "text": "PLNR_GNAR_BLOCK\n\n# _data = load_data('./data/fivenodes.pkl')\n\n# _edges = torch.tensor(_data['edges']).nonzero().tolist()\n# _FX = _data['f'].tolist()\n# _node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \n\ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = DatasetLoader(data_dict)\n\n\nmindex_block = [list(range(10,100)),[],list(range(50,80)),[],[]]\nplans_gnar_block = {\n    'max_iteration': 3, \n    'method': ['GNAR'], \n    'mindex': [mindex_block],\n    'lags': [2, 4], \n    'inter_method': ['nearest','linear'],\n}\n\n\nclass PLNR_GNAR_BLOCK:\n    def __init__(self,plans,loader,dataset_name=None,simulation_results=None):\n        self.plans = plans\n        col = ['dataset', 'method', 'mrate', 'mtype', 'lags', 'nof_filters', 'inter_method', 'epoch', 'mse']\n        self.loader = loader\n        self.dataset_name = dataset_name\n        self.simulation_results = pd.DataFrame(columns=col) if simulation_results is None else simulation_results \n    def simulate(self):\n        for _ in range(self.plans['max_iteration']):\n            product_iterator = itertools.product(\n                self.plans['mindex'],\n                self.plans['lags'],\n                self.plans['inter_method']\n            )\n            for prod_iter in product_iterator:\n                mrate,lags,inter_method = prod_iter\n                self.dataset = self.loader.get_dataset(lags=lags)\n                train_dataset, test_dataset = torch_geometric_temporal.signal.temporal_signal_split(self.dataset, train_ratio=0.8)\n                mtype = 'block'\n                train_dataset = padding(train_dataset_miss = miss(train_dataset,mindex=mindex,mtype=mtype),interpolation_method=inter_method)\n                method = 'GNAR'\n                lrnr = GNARLearner(train_dataset,dataset_name=self.dataset_name)\n                lrnr.learn()\n                evtor = Evaluator(lrnr,train_dataset,test_dataset)\n                evtor.calculate_mse()\n                mse = evtor.mse['test']['total']\n                nof_filters = None \n                epoch= None\n                mrate= lrnr.mrate_total\n                self._record(method,mrate,mtype,lags,nof_filters,inter_method,epoch,mse)\n    def _record(self,method,mrate,mtype,lags,nof_filters,inter_method,epoch,mse):\n        dct = {'dataset': self.dataset_name,\n               'method': method,\n               'mrate': mrate,\n               'mtype': mtype, \n               'lags': lags,\n               'nof_filters': nof_filters,\n               'inter_method': inter_method,\n               'epoch': epoch,\n               'mse': mse\n              }\n        simulation_result_new = pd.Series(dct).to_frame().transpose()\n        self.simulation_results = pd.concat([self.simulation_results,simulation_result_new]).reset_index(drop=True)\n\n\nplnr = PLNR_GNAR_BLOCK(plans_gnar_block,loader,dataset_name='five_nodes')\nplnr.simulate()\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nplnr.simulation_results\n\n\n\n\n\n  \n    \n      \n      dataset\n      method\n      mrate\n      mtype\n      lags\n      nof_filters\n      inter_method\n      epoch\n      mse\n    \n  \n  \n    \n      0\n      five_nodes\n      GNAR\n      0.5\n      block\n      2\n      None\n      nearest\n      None\n      1.40683\n    \n    \n      1\n      five_nodes\n      GNAR\n      0.5\n      block\n      2\n      None\n      linear\n      None\n      1.40683\n    \n    \n      2\n      five_nodes\n      GNAR\n      0.5\n      block\n      4\n      None\n      nearest\n      None\n      1.469004\n    \n    \n      3\n      five_nodes\n      GNAR\n      0.5\n      block\n      4\n      None\n      linear\n      None\n      1.469004\n    \n    \n      4\n      five_nodes\n      GNAR\n      0.5\n      block\n      2\n      None\n      nearest\n      None\n      1.40683\n    \n    \n      5\n      five_nodes\n      GNAR\n      0.5\n      block\n      2\n      None\n      linear\n      None\n      1.40683\n    \n    \n      6\n      five_nodes\n      GNAR\n      0.5\n      block\n      4\n      None\n      nearest\n      None\n      1.469004\n    \n    \n      7\n      five_nodes\n      GNAR\n      0.5\n      block\n      4\n      None\n      linear\n      None\n      1.469004\n    \n    \n      8\n      five_nodes\n      GNAR\n      0.5\n      block\n      2\n      None\n      nearest\n      None\n      1.40683\n    \n    \n      9\n      five_nodes\n      GNAR\n      0.5\n      block\n      2\n      None\n      linear\n      None\n      1.40683\n    \n    \n      10\n      five_nodes\n      GNAR\n      0.5\n      block\n      4\n      None\n      nearest\n      None\n      1.469004\n    \n    \n      11\n      five_nodes\n      GNAR\n      0.5\n      block\n      4\n      None\n      linear\n      None\n      1.469004\n    \n  \n\n\n\n\n\n여기부터 서연이코드\n\nedges_tensor = torch.tensor(data['edges'])\nfiveVTS = np.array(data['f'])\nnonzero_indices = edges_tensor.nonzero()\nfiveNet_edge = np.array(nonzero_indices).T\nT = 200\nN = 5 # number of Nodes\nE = fiveNet_edge\nV = np.array([1,2,3,4,5])\nt = np.arange(0,T)\nnode_features = 1\nedge_index = torch.tensor(E)\nedge_attr = torch.tensor(np.array([1,1,1,1,1,1,1,1,1,1]),dtype=torch.float32)\n\n\nedge_index\n\nNameError: name 'edge_index' is not defined\n\n\n- train / test\n\nfiveVTS_train = fiveVTS[:int(len(fiveVTS)*0.8)]\nfiveVTS_test = fiveVTS[int(len(fiveVTS)*0.8):]"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#random-missing-values",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#random-missing-values",
    "title": "SY 1st ITSTGCN",
    "section": "Random Missing Values",
    "text": "Random Missing Values\n\nclass Missing:\n    def __init__(self,df):\n        self.df = df\n        self.N = N\n        self.number = []\n    def miss(self,percent=0.5):\n        self.missing = self.df.copy()\n        self.percent = percent\n        for i in range(self.N):\n            #self.seed = np.random.choice(1000,1,replace=False)\n            #np.random.seed(self.seed)\n            self.number.append(np.random.choice(int(len(self.df))-1,int(len(self.df)*self.percent),replace=False))\n            self.missing[self.number[i],i] = float('nan')\n    def first_mean(self):\n        self.train_mean = self.missing.copy()\n        for i in range(self.N):\n            self.train_mean[self.number[i],i] = np.nanmean(self.missing[:,i])\n    def second_linear(self):\n        self.train_linear = pd.DataFrame(self.missing)\n        self.train_linear.interpolate(method='linear', inplace=True)\n        self.train_linear = self.train_linear.fillna(0)\n        self.train_linear = np.array(self.train_linear).reshape(int(len(self.df)),N)\n\n\ncol = ['Dataset','iteration', 'method', 'missingrate', 'missingtype', 'lag', 'number_of_filters', 'interpolation','MSE_train', 'MSE_test']\n\nrate = [i/10 for i in range(10)]"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#class-code-by-method",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#class-code-by-method",
    "title": "SY 1st ITSTGCN",
    "section": "Class code by Method",
    "text": "Class code by Method"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#stgcn",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#stgcn",
    "title": "SY 1st ITSTGCN",
    "section": "STGCN",
    "text": "STGCN\n\nclass STGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            for epoch in range(50):\n                for time, (xt,yt) in enumerate(zip(X,y)):\n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_stgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_stgcn = (((self.yy-yyhat).squeeze())**2).mean() \n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset, \n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_stgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_stgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#enhencement-of-stgcn",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#enhencement-of-stgcn",
    "title": "SY 1st ITSTGCN",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nclass ESTGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n    \n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            signal = interpolated_signal.copy()\n            for epoch in range(50):\n                signal = update_from_freq_domain(signal,missing_index)\n                X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n                y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n                for time, (xt,yt) in enumerate(zip(X,y)):        \n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])               \n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_estgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_estgcn = (((self.yy-yyhat).squeeze())**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_estgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_estgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#gnar",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#gnar",
    "title": "SY 1st ITSTGCN",
    "section": "GNAR",
    "text": "GNAR\n\nm = robjects.r.matrix(FloatVector([0,0,0,1,1,0,0,1,1,0,0,1,0,1,0,1,1,1,0,0,1,0,0,0,0]), nrow = 5, ncol = 5)\n\n\nclass GNAR_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-2),:,:]\n\n            answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = 160, ncol = 5),net = GNAR.matrixtoGNAR(m), alphaOrder = 2, betaOrder = FloatVector([1, 1]))             \n            predict = GNAR.predict_GNARfit(answer,n_ahead=40)\n\n\n            train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,5))**2).mean()\n            test_mse_total_gnar = ((self.yy.squeeze() - pd.DataFrame(predict).values.reshape(-1,5)[:-1,:])**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_gnar.tolist()\n            df_row['MSE_test'] = test_mse_total_gnar.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#stgcn-1",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#stgcn-1",
    "title": "SY 1st ITSTGCN",
    "section": "STGCN",
    "text": "STGCN\n\nDataset = 'fivenodes'\nMethod = 'stgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_stgcn= pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    stgcn = STGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    stgcn.iter()\n    df_add = stgcn.df.copy()\n    df_stgcn = pd.concat([df_stgcn,df_add],axis=0)\n\n\nsave_data(df_stgcn, './data/GNAR_stgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#enhencement-of-stgcn-1",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#enhencement-of-stgcn-1",
    "title": "SY 1st ITSTGCN",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nDataset = 'fivenodes'\nMethod = 'estgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_estgcn = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    estgcn = ESTGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    estgcn.iter()\n    df_add = estgcn.df.copy()\n    df_estgcn = pd.concat([df_estgcn,df_add],axis=0)\n\n\nsave_data(df_estgcn, './data/GNAR_estgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#gnar-1",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#gnar-1",
    "title": "SY 1st ITSTGCN",
    "section": "GNAR",
    "text": "GNAR\n\nDataset = 'fivenodes'\nMethod = 'gnar' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = None # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_gnar = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    gnar = GNAR_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    gnar.iter()\n    df_add = gnar.df.copy()\n    df_gnar = pd.concat([df_gnar,df_add],axis=0)\n\n\nsave_data(df_gnar, './data/GANR_gnar_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html",
    "title": "Class of Method(GNAR) lag 2",
    "section": "",
    "text": "GNAR fiveNet,fivenodes lag 2"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html#시나리오1-baseline",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html#시나리오1-baseline",
    "title": "Class of Method(GNAR) lag 2",
    "section": "시나리오1 (Baseline)",
    "text": "시나리오1 (Baseline)\n시나리오1\n\nmissing rate: 0%\n보간방법: None\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([fiveVTS_train[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(fiveVTS_train[2:int(T*0.8),:].reshape(int(T*0.8)-2,N,-1)).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:26<00:00,  1.87it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nGNAR 으로 적합 + 예측\n-\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\nlibrary(tidyverse)\n\n\n%R -i fiveVTS_train\n\n\n%%R\nanswer <- GNARfit(vts = fiveVTS_train, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nprediction <- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((fiveVTS_test - gnar_test.reshape(-1,5))**2).mean(axis=0)\ntest_mse_total_gnar = ((fiveVTS_test - gnar_test.reshape(-1,5))**2).mean()\n\n\n\n결과시각화\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.995256618187614, 1.2577286248028454)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n mse(train) = {3:.2f}, mse(test) = {4:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(range(2,160),stgcn_train[:,i],label='STCGCN (train)',color='C0')\n    a.plot(range(162,200),stgcn_test[:,i],label='STCGCN (test)',color='C0')\n    a.plot(range(2,160),gnar_train.reshape(-1,5)[:,i],label='GNAR (train)',color='C1')\n    a.plot(range(160,200),gnar_test.reshape(-1,5)[:,i],label='GNAR (test)',color='C1')\n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario1: STGCN \\n missing=0% \\n interpolation=None \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n GNAR: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html#시나리오2",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html#시나리오2",
    "title": "Class of Method(GNAR) lag 2",
    "section": "시나리오2",
    "text": "시나리오2\n시나리오2\n\nmissing rate: 50%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.5)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[2:,:]).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:26<00:00,  1.85it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[2:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[2:,:]).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(np.stack([signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\n    y = torch.tensor(signal[2:,:]).float()\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [00:28<00:00,  1.78it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[2:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nX_train1 = np.array(interpolated_signal).squeeze()\nX_test1 =  np.array(fiveVTS_test).squeeze()\n\n\n%R -i X_train1\n\n\n%%R\nanswer <- GNARfit(vts = X_train1, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nprediction <- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((X_test1 - gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((X_test1 - gnar_test)**2).mean()\n\n\n\n결과시각화\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.6280648350096797, 1.3222499750457097)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(2,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(162,200),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(2,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(162,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(2,160),gnar_train[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(162,202),gnar_test[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html#시나리오3",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html#시나리오3",
    "title": "Class of Method(GNAR) lag 2",
    "section": "시나리오3",
    "text": "시나리오3\n시나리오3\n\nmissing rate: 80%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[2:,:]).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:27<00:00,  1.85it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[2:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[2:,:]).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(np.stack([signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\n    y = torch.tensor(signal[2:,:]).float()\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [00:28<00:00,  1.77it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[2:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nX_train1 = np.array(interpolated_signal).squeeze()\nX_test1 =  np.array(fiveVTS_test).squeeze()\n\n\n%R -i X_train1\n\n\n%%R\nanswer <- GNARfit(vts = X_train1, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nprediction <- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((X_test1 - gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((X_test1 - gnar_test)**2).mean()\n\n\n\n결과시각화\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.2092691948436627, 1.5191113001100904)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(2,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(162,200),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(2,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(162,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(2,160),gnar_train[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(162,202),gnar_test[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario3: \\n missing=80% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html#시나리오4",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html#시나리오4",
    "title": "Class of Method(GNAR) lag 2",
    "section": "시나리오4",
    "text": "시나리오4\n시나리오4\n\nmissing rate: 30%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.3)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[2:,:]).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:26<00:00,  1.86it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[2:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN 으로 적합 + 예측\n\nX = torch.tensor(np.stack([interpolated_signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[2:,:]).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(np.stack([signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\n    y = torch.tensor(signal[2:,:]).float()\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [00:27<00:00,  1.79it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[2:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nX_train1 = np.array(interpolated_signal).squeeze()\nX_test1 =  np.array(fiveVTS_test).squeeze()\n\n\n%R -i X_train1\n\n\n%%R\nanswer <- GNARfit(vts = X_train1, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nprediction <- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((X_test1 - gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((X_test1 - gnar_test)**2).mean()\n\n\n\n결과시각화\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.7594163080873634, 1.2656937545324825)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(2,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(162,200),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(2,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(162,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(2,160),gnar_train[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(162,202),gnar_test[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario3: \\n missing=80% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2022-12-07-torchgcn.html",
    "href": "posts/GCN/2022-12-07-torchgcn.html",
    "title": "TORCH_GEOMETRIC.NN",
    "section": "",
    "text": "221207\nhttps://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html\n\nimport torch\nfrom torch_geometric.data import Data\n\n\nedge_index = torch.tensor([[0, 1, 1, 2],\n                           [1, 0, 2, 1]], dtype=torch.long)\nx = torch.tensor([[-1], [0], [1]], dtype=torch.float)\ndata = Data(x=x, edge_index=edge_index)\n\n\ndata\n\nData(x=[3, 1], edge_index=[2, 4])\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n\nG=nx.Graph()\nG.add_node('0')\nG.add_node('1')\nG.add_node('2')\nG.add_edge('0','1')\nG.add_edge('1','2')\npos = {}\npos['0'] = (0,0)\npos['1'] = (1,1)\npos['2'] = (2,0)\nnx.draw(G,pos,with_labels=True)\nplt.show()\n\n\n\n\n\nfrom torch.nn import Linear, ReLU\nfrom torch_geometric.nn import Sequential, GCNConv\n\nex\nmodel = Sequential('x, edge_index', [\n    (GCNConv(in_channels, 64), 'x, edge_index -> x'),\n    ReLU(inplace=True),\n    (GCNConv(64, 64), 'x, edge_index -> x'),\n    ReLU(inplace=True),\n    Linear(64, out_channels),\n])\n\nmodel = Sequential('x, edge_index', [\n    (GCNConv(3, 64), 'x, edge_index -> x'),\n    ReLU(inplace=True),\n    (GCNConv(64, 64), 'x, edge_index -> x'),\n    ReLU(inplace=True),\n    Linear(64, 3),\n])\n\n\nmodel(x,edge_index)\n\n\nfrom torch.nn import Linear, ReLU, Dropout\nfrom torch_geometric.nn import Sequential, GCNConv, JumpingKnowledge\nfrom torch_geometric.nn import global_mean_pool\n\nmodel = Sequential('x, edge_index, batch', [\n    (Dropout(p=0.5), 'x -> x'),\n    (GCNConv(dataset.num_features, 64), 'x, edge_index -> x1'),\n    ReLU(inplace=True),\n    (GCNConv(64, 64), 'x1, edge_index -> x2'),\n    ReLU(inplace=True),\n    (lambda x1, x2: [x1, x2], 'x1, x2 -> xs'),\n    (JumpingKnowledge(\"cat\", 64, num_layers=2), 'xs -> x'),\n    (global_mean_pool, 'x, batch -> x'),\n    Linear(2 * 64, dataset.num_classes),\n])\n\nmodel = Sequential('x, edge_index, batch', [\n    (Dropout(p=0.5), 'x -> x'),\n    (GCNConv(dataset.num_features, 64), 'x, edge_index -> x1'),\n    ReLU(inplace=True),\n    (GCNConv(64, 64), 'x1, edge_index -> x2'),\n    ReLU(inplace=True),\n    (lambda x1, x2: [x1, x2], 'x1, x2 -> xs'),\n    (JumpingKnowledge(\"cat\", 64, num_layers=2), 'xs -> x'),\n    (global_mean_pool, 'x, batch -> x'),\n    Linear(2 * 64, dataset.num_classes),\n])\n\n\ntorch_geometric.nn.Linear()"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html",
    "href": "posts/GCN/2023-01-21-Class.html",
    "title": "Class of Method",
    "section": "",
    "text": "Class"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#mean",
    "href": "posts/GCN/2023-01-21-Class.html#mean",
    "title": "Class of Method",
    "section": "Mean",
    "text": "Mean\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero.train_mean\nc = ___zero.number\nd = train_X_mean\nf = train_y_mean\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n\n\n3282.4832243919373\n\n\n\nmean_mse100_10 = pd.DataFrame(_mse)\n\n\nmean_mae100_10 = pd.DataFrame(_mae)\n\n\n_train_result_mean10 = _train_result.copy()\n\n\n_test_result_mean10 = _test_result.copy()\n\n\nplt.plot(mean_mse100_10.T);\n\n\n\n\n\nplt.plot(mean_mae100_10.T);\n\n\n\n\n\nvis2(_zero.train_mean,_train_result[59]);\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result[0]);"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#linear",
    "href": "posts/GCN/2023-01-21-Class.html#linear",
    "title": "Class of Method",
    "section": "Linear",
    "text": "Linear\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero.second_linear\nc = ___zero.number\nd = train_X_linear\nf = train_y_linear\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n\n\n3295.478456020355\n\n\n\nlinear_mse100_10 = pd.DataFrame(_mse)\n\n\nlinear_mae100_10 = pd.DataFrame(_mae)\n\n\n_train_result_linear10 = _train_result.copy()\n\n\n_test_result_linear10 = _test_result.copy()\n\n\nplt.plot(linear_mse100_10.T);\n\n\n\n\n\nplt.plot(linear_mae100_10.T);\n\n\n\n\n\nvis2(_zero.train_mean,_train_result_linear10[59]);\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_linear10[0]);"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#mean-1",
    "href": "posts/GCN/2023-01-21-Class.html#mean-1",
    "title": "Class of Method",
    "section": "Mean",
    "text": "Mean\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero20.train_mean\nc = ___zero20.number\nd = train_X_mean\nf = train_y_mean\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:52<00:00,  1.89it/s]\n100%|██████████| 100/100 [00:52<00:00,  1.89it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n\n\n3277.0478909015656\n\n\n\nmean_mse100_20 = pd.DataFrame(_mse)\n\n\nmean_mae100_20 = pd.DataFrame(_mae)\n\n\n_train_result_mean20 = _train_result.copy()\n\n\n_test_result_mean20 = _test_result.copy()\n\n\nplt.plot(mean_mse100_20.T);\n\n\n\n\n\nplt.plot(mean_mae100_20.T);\n\n\n\n\n\nvis2(___zero20.train_mean,_train_result_mean20[59]);\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_mean20[0]);\n\n\n\n\n\nshow_lrpr(fiveVTS_test[1:,0],fiveVTS_test[1:,1],fiveVTS_test[1:,2],fiveVTS_test[1:,3],fiveVTS_test[1:,4],_test_result_mean20)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#linear-1",
    "href": "posts/GCN/2023-01-21-Class.html#linear-1",
    "title": "Class of Method",
    "section": "Linear",
    "text": "Linear\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero20.second_linear\nc = ___zero20.number\nd = train_X_linear\nf = train_y_linear\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n\n\n3298.6988050937653\n\n\n\nlinear_mse100_20 = pd.DataFrame(_mse)\n\n\nlinear_mae100_20 = pd.DataFrame(_mae)\n\n\n_train_result_linear20 = _train_result.copy()\n\n\n_test_result_linear20 = _test_result.copy()\n\n\nplt.plot(linear_mse100_20.T);\n\n\n\n\n\nplt.plot(linear_mae100_20.T);\n\n\n\n\n\nvis2(___zero20.train_mean,_train_result_linear20[59]);\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_linear20[0]);\n\n\n\n\n\nshow_lrpr(fiveVTS_test[1:,0],fiveVTS_test[1:,1],fiveVTS_test[1:,2],fiveVTS_test[1:,3],fiveVTS_test[1:,4],_test_result_linear20)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#mean-2",
    "href": "posts/GCN/2023-01-21-Class.html#mean-2",
    "title": "Class of Method",
    "section": "Mean",
    "text": "Mean\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero30.train_mean\nc = ___zero30.number\nd = train_X_mean\nf = train_y_mean\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n\n\n3280.9767186641693\n\n\n\nmean_mse_30 = pd.DataFrame(_mse)\n\n\nmean_mae_30 = pd.DataFrame(_mae)\n\n\n_train_result_mean30 = _train_result.copy()\n\n\n_test_result_mean30 = _test_result.copy()\n\n\nplt.plot(mean_mse_30.T);\n\n\n\n\n\nplt.plot(mean_mae_30.T);\n\n\n\n\n\nvis2(___zero30.train_mean,_train_result_mean30[59]);\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_mean30[0]);\n\n\n\n\n\nshow_lrpr(fiveVTS_test[1:,0],fiveVTS_test[1:,1],fiveVTS_test[1:,2],fiveVTS_test[1:,3],fiveVTS_test[1:,4],_test_result_mean30)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#linear-2",
    "href": "posts/GCN/2023-01-21-Class.html#linear-2",
    "title": "Class of Method",
    "section": "Linear",
    "text": "Linear\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero30.second_linear\nc = ___zero30.number\nd = train_X_linear\nf = train_y_linear\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n\n\n3305.375289440155\n\n\n\nlinear_mse_30 = pd.DataFrame(_mse)\n\n\nlinear_mae_30 = pd.DataFrame(_mae)\n\n\n_train_result_linear30 = _train_result.copy()\n\n\n_test_result_linear30 = _test_result.copy()\n\n\nplt.plot(linear_mse_30.T);\n\n\n\n\n\nplt.plot(linear_mae_30.T);\n\n\n\n\n\nvis2(___zero30.train_mean,_train_result_linear30[59]);\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_linear30[0]);\n\n\n\n\n\nshow_lrpr(fiveVTS_test[1:,0],fiveVTS_test[1:,1],fiveVTS_test[1:,2],fiveVTS_test[1:,3],fiveVTS_test[1:,4],_test_result_linear30)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#mean-3",
    "href": "posts/GCN/2023-01-21-Class.html#mean-3",
    "title": "Class of Method",
    "section": "Mean",
    "text": "Mean\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero40.train_mean\nc = ___zero40.number\nd = train_X_mean\nf = train_y_mean\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.88it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n\n\n3287.529237985611\n\n\n\nmean_mse_40 = pd.DataFrame(_mse)\n\n\nmean_mae_40 = pd.DataFrame(_mae)\n\n\n_train_result_mean40 = _train_result.copy()\n\n\n_test_result_mean40 = _test_result.copy()\n\n\nplt.plot(mean_mse_40.T);\n\n\n\n\n\nplt.plot(mean_mae_40.T);\n\n\n\n\n\nvis2(___zero40.train_mean,_train_result_mean40[59]);\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_mean40[0]);\n\n\n\n\n\nshow_lrpr(fiveVTS_test[1:,0],fiveVTS_test[1:,1],fiveVTS_test[1:,2],fiveVTS_test[1:,3],fiveVTS_test[1:,4],_test_result_mean40)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#linear-3",
    "href": "posts/GCN/2023-01-21-Class.html#linear-3",
    "title": "Class of Method",
    "section": "Linear",
    "text": "Linear\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero40.second_linear\nc = ___zero40.number\nd = train_X_linear\nf = train_y_linear\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.82it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.85it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.87it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:53<00:00,  1.86it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.83it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n100%|██████████| 100/100 [00:54<00:00,  1.84it/s]\n\n\n3303.96302652359\n\n\n\nlinear_mse_40 = pd.DataFrame(_mse)\n\n\nlinear_mae_40 = pd.DataFrame(_mae)\n\n\n_train_result_linear40 = _train_result.copy()\n\n\n_test_result_linear40 = _test_result.copy()\n\n\nplt.plot(linear_mse_40.T);\n\n\n\n\n\nplt.plot(linear_mae_40.T);\n\n\n\n\n\nvis2(___zero40.train_mean,_train_result_linear40[59]);\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_linear40[0]);\n\n\n\n\n\nshow_lrpr(fiveVTS_test[1:,0],fiveVTS_test[1:,1],fiveVTS_test[1:,2],fiveVTS_test[1:,3],fiveVTS_test[1:,4],_test_result_linear40)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#mean-4",
    "href": "posts/GCN/2023-01-21-Class.html#mean-4",
    "title": "Class of Method",
    "section": "Mean",
    "text": "Mean\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero50.train_mean\nc = ___zero50.number\nd = train_X_mean\nf = train_y_mean\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n\nmean_mse_50 = pd.DataFrame(_mse)\n\n\nmean_mae_50 = pd.DataFrame(_mae)\n\n\n_train_result_mean50 = _train_result.copy()\n\n\n_test_result_mean50 = _test_result.copy()\n\n\nplt.plot(mean_mse_50.T);\n\n\nplt.plot(mean_mae_50.T);\n\n\nvis2(_zero.train_mean,_train_result_mean50[59]);\n\n\nvis2(fiveVTS_test[1:],_test_result_mean50[0]);"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#linear-4",
    "href": "posts/GCN/2023-01-21-Class.html#linear-4",
    "title": "Class of Method",
    "section": "Linear",
    "text": "Linear\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero50.second_linear\nc = ___zero50.number\nd = train_X_linear\nf = train_y_linear\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n\nlinear_mse_50 = pd.DataFrame(_mse)\n\n\nlinear_mae_50 = pd.DataFrame(_mae)\n\n\n_train_result_linear50 = _train_result.copy()\n\n\n_test_result_linear50 = _test_result.copy()\n\n\nplt.plot(linear_mse_50.T);\n\n\nplt.plot(linear_mae_50.T);\n\n\nvis2(_zero.train_mean,_train_result_linear50[59]);\n\n\nvis2(fiveVTS_test[1:],_test_result_linear50[0]);"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html",
    "title": "1st ITSTGCN",
    "section": "",
    "text": "GNAR fiveNet,fivenodes lag 1"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#stgcn",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#stgcn",
    "title": "1st ITSTGCN",
    "section": "STGCN",
    "text": "STGCN\n\nclass STGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            for epoch in range(50):\n                for time, (xt,yt) in enumerate(zip(X,y)):\n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_stgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_stgcn = (((self.yy-yyhat).squeeze())**2).mean() \n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset, \n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_stgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_stgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#enhencement-of-stgcn",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#enhencement-of-stgcn",
    "title": "1st ITSTGCN",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nclass ESTGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n    \n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            signal = interpolated_signal.copy()\n            for epoch in range(50):\n                signal = update_from_freq_domain(signal,missing_index)\n                X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n                y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n                for time, (xt,yt) in enumerate(zip(X,y)):        \n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])               \n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_estgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_estgcn = (((self.yy-yyhat).squeeze())**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_estgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_estgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#gnar",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#gnar",
    "title": "1st ITSTGCN",
    "section": "GNAR",
    "text": "GNAR\n\nm = robjects.r.matrix(FloatVector([0,0,0,1,1,0,0,1,1,0,0,1,0,1,0,1,1,1,0,0,1,0,0,0,0]), nrow = 5, ncol = 5)\n\n\nclass GNAR_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-2),:,:]\n\n            answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = 160, ncol = 5),net = GNAR.matrixtoGNAR(m), alphaOrder = 2, betaOrder = FloatVector([1, 1]))             \n            predict = GNAR.predict_GNARfit(answer,n_ahead=40)\n\n\n            train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,5))**2).mean()\n            test_mse_total_gnar = ((self.yy.squeeze() - pd.DataFrame(predict).values.reshape(-1,5)[:-1,:])**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_gnar.tolist()\n            df_row['MSE_test'] = test_mse_total_gnar.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#stgcn-1",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#stgcn-1",
    "title": "1st ITSTGCN",
    "section": "STGCN",
    "text": "STGCN\n\nDataset = 'fivenodes'\nMethod = 'stgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_stgcn= pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    stgcn = STGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    stgcn.iter()\n    df_add = stgcn.df.copy()\n    df_stgcn = pd.concat([df_stgcn,df_add],axis=0)\n\n\nsave_data(df_stgcn, './data/GNAR_stgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#enhencement-of-stgcn-1",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#enhencement-of-stgcn-1",
    "title": "1st ITSTGCN",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nDataset = 'fivenodes'\nMethod = 'estgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_estgcn = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    estgcn = ESTGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    estgcn.iter()\n    df_add = estgcn.df.copy()\n    df_estgcn = pd.concat([df_estgcn,df_add],axis=0)\n\n\nsave_data(df_estgcn, './data/GNAR_estgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#gnar-1",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#gnar-1",
    "title": "1st ITSTGCN",
    "section": "GNAR",
    "text": "GNAR\n\nDataset = 'fivenodes'\nMethod = 'gnar' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = None # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_gnar = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    gnar = GNAR_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    gnar.iter()\n    df_add = gnar.df.copy()\n    df_gnar = pd.concat([df_gnar,df_add],axis=0)\n\n\nsave_data(df_gnar, './data/GANR_gnar_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-04-27-toy_example_notes.html",
    "href": "posts/GCN/2023-04-27-toy_example_notes.html",
    "title": "Toy Example Note",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nimport pandas as pd\nimport pickle\n\n\nT = 50\nt = np.arange(T)/T * 10 \n\n\ndef load_data(fname):\n    with open(fname, 'rb') as outfile:\n        data_dict = pickle.load(outfile)\n    return data_dict\n\n\ndef save_data(data_dict,fname):\n    with open(fname,'wb') as outfile:\n        pickle.dump(data_dict,outfile)\n\n\nimport torch\n\n\nx = 50*np.sin(2*t)#+30*np.sin(5*t)\neps_x  = np.random.normal(size=T)\ny = x.copy()\nfor i in range(2,T):\n    y[i] = 0.35*x[i-1] - 0.15*x[i-2] + 50*np.cos(0.5*t[i]) \neps_y  = np.random.normal(size=T)\n\n\nplt.plot(t,x,color='C0',lw=5)\nplt.plot(t,x+eps_x,alpha=0.5,color='C0')\nplt.plot(t,y,color='C1',lw=5)\nplt.plot(t,y+eps_y,alpha=0.5,color='C1')\n_node_ids = {'node1':0, 'node2':1}\n\n\n\n_node_ids = {'node1':0, 'node2':1}\n\n_FX1 = np.stack([x,y],axis=1).tolist()\n\n_edges1 = torch.tensor([[1,0]]).tolist()\n\ndata_dict1 = {'edges':_edges1, 'node_ids':_node_ids, 'FX':_FX1}\n#data_dict = itstgcn.load_data('./data/fivenodes.pkl')\n\nsave_data(data_dict1, './data/toy_example1.pkl')\n\ndata1 = pd.DataFrame({'x':x,'y':y,'xer':x,'yer':y})\n\nsave_data(data1, './data/toy_example_true1.csv')\n\n\n\n\n\nnp.stack([x+eps_x,y+eps_y],axis=1).shape\n\n(800, 2)\n\n\n\n_node_ids = {'node1':0, 'node2':1}\n\n\n_FX = np.stack([x+eps_x,y+eps_y],axis=1).tolist()\n\n\n_edges = torch.tensor([[0,0],[0,1],[1,0],[1,1]]).tolist()\n\n\ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\ndata_dict['edges']\n\n[[0, 0], [0, 1], [1, 0], [1, 1]]\n\n\n\nnp.array(data_dict['edges']).shape\n\n(4, 2)\n\n\n\nsave_data(data_dict, './data/toy_example.pkl')\n\n\ndata_dict = load_data('./data/toy_example.pkl')\n\n\ndata_dict.keys()\n\ndict_keys(['edges', 'node_ids', 'FX'])\n\n\n\ndata_dict['edges']\n\n[[0, 0], [0, 1], [1, 0], [1, 1]]\n\n\n\ndata_dict['node_ids']\n\n{'node1': 0, 'node2': 1}\n\n\n\nnp.array(data_dict['FX']).shape\n\n(800, 2)\n\n\n\ndata = pd.DataFrame({'x':x,'y':y,'xer':x+eps_x,'yer':y+eps_y})\n\n\nsave_data(data, './data/toy_example_true.csv')\n\n\ndata = load_data('./data/toy_example_true.csv')\n\n\n_node_ids = {'node1':0, 'node2':1}\n\n\n_FX1 = np.stack([x,y],axis=1).tolist()\n\n\n_edges1 = torch.tensor([[1,0]]).tolist()\n\n\ndata_dict1 = {'edges':_edges1, 'node_ids':_node_ids, 'FX':_FX1}\n#data_dict = itstgcn.load_data('./data/fivenodes.pkl')\n\n\nsave_data(data_dict1, './data/toy_example1.pkl')\n\n\ndata1 = pd.DataFrame({'x':x,'y':y,'xer':x,'yer':y})\n\n\nsave_data(data1, './data/toy_example_true1.csv')"
  },
  {
    "objectID": "posts/GCN/2023-03-20-data load, data save as pickle.html",
    "href": "posts/GCN/2023-03-20-data load, data save as pickle.html",
    "title": "data load, data save as pickle",
    "section": "",
    "text": "data load, data save as pickle\nhttps://pytorch-geometric-temporal.readthedocs.io/en/latest/_modules/index.html\n1st\n2nd\n3rd\n4rd\n5th - TwitterTennisDatasetLoader(원핫 인코딩 함수도 별도로 있음) - get_dataset(self) -> DynamicGraphTemporalSignal: - dataset = DynamicGraphTemporalSignal(self.edges, self.edge_weights, self.features, self.target"
  },
  {
    "objectID": "posts/GCN/2023-03-20-data load, data save as pickle.html#chickenpoxdatasetloader",
    "href": "posts/GCN/2023-03-20-data load, data save as pickle.html#chickenpoxdatasetloader",
    "title": "data load, data save as pickle",
    "section": "ChickenpoxDatasetLoader",
    "text": "ChickenpoxDatasetLoader\nChickenpox Hungary\n\nA dataset of county level chicken pox cases in Hungary between 2004 and 2014. We made it public during the development of PyTorch Geometric Temporal. The underlying graph is static - vertices are counties and edges are neighbourhoods. Vertex features are lagged weekly counts of the chickenpox cases (we included 4 lags). The target is the weekly number of cases for the upcoming week (signed integers). Our dataset consist of more than 500 snapshots (weeks).\n2004년부터 2014년 사이 헝가리의 지역별 수두증 발생 데이터셋\n그래프는 정적\nnode 지역\nedge 이웃 관계\nnode 특성은 수두증 발생의 지연된 주간 횟수(4주의 지연이 포함되어 있음)\ntarget는 다음 주에 대한 주간 사례 수\n500개 이상의 스냅샷(주간)\n\n데이터정리\n\nT = 519\nN = 20 # number of nodes\nE = 102 # edges\n\\(f(v,t)\\)의 차원? (1,)\n시간에 따라서 Number of nodes가 변하는지? False\n시간에 따라서 Number of nodes가 변하는지? False\nX: (20,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (20,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 20\n\nvertices are counties\n\n-Edges : 102\n\nedges are neighbourhoods\n\n- Time : 517\n\nbetween 2004 and 2014\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nloader1 = ChickenpoxDatasetLoader()\n\n\na = loader1.get_dataset()\n\n\nnp.array(a.features).shape\n\n(517, 20, 4)\n\n\n\nnp.array(a.edge_index).shape\n\n(2, 102)\n\n\n\nnp.array(a.edge_weight).shape\n\n(102,)\n\n\n\na.edge_weight\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\na.edge_index\n\narray([[ 0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,\n         3,  3,  3,  3,  3,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,\n         6,  6,  7,  7,  7,  7,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,\n        10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 12, 12, 12,\n        12, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15,\n        15, 15, 16, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 18,\n        18, 18, 19, 19, 19, 19],\n       [10,  6, 13,  1,  0,  5, 16,  0, 16,  1, 14, 10,  8,  2,  5,  8,\n        15, 12,  9, 10,  3,  4, 13,  0, 10,  2,  5,  0, 16,  6, 14, 13,\n        11, 18,  7, 17, 11, 18,  3,  2, 15,  8, 10,  9, 13,  3, 12, 10,\n         5,  9,  8,  3, 10,  2, 13,  0,  6, 11,  7, 13, 18,  3,  9, 13,\n        12, 13,  9,  6,  4, 12,  0, 11, 10, 18, 19,  1, 14,  6, 16,  3,\n        15,  8, 16, 14,  1,  0,  6,  7, 19, 17, 18, 14, 18, 17,  7,  6,\n        19, 11, 18, 14, 19, 17]])\n\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(a.edge_index.T)\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\nnode_list = []\nfor i in range(0, 20):\n    node_list.append(i)\n\n\nfig,ax = plt.subplots(20,1,figsize=(30,70))\nfor k in range(20):\n    ax[k].plot(np.array(loader1.targets).reshape(20,-1)[k][:],alpha=1,label='observed')\n    ax[k].set_title('node: {}'.format(node_list[k]))\n    ax[k].legend()\nfig.tight_layout()"
  },
  {
    "objectID": "posts/GCN/2023-03-20-data load, data save as pickle.html#pedalmedatasetloader",
    "href": "posts/GCN/2023-03-20-data load, data save as pickle.html#pedalmedatasetloader",
    "title": "data load, data save as pickle",
    "section": "PedalMeDatasetLoader",
    "text": "PedalMeDatasetLoader\nPedal Me Deliveries\n\nA dataset of PedalMe Bicycle deliver orders in London between 2020 and 2021. We made it public during the development of PyTorch Geometric Temporal. The underlying graph is static - vertices are localities and edges are spatial_connections. Vertex features are lagged weekly counts of the delivery demands (we included 4 lags). The target is the weekly number of deliveries the upcoming week. Our dataset consist of more than 30 snapshots (weeks)\n2020년과 2021년 사이 런던에서 PedalMe 자전거 배송 주문 데이터셋\n그래프는 정적\nnode 장소\nedge 공간 연결\nnode 특성은 배송 수요의 지연된 주간 횟수(4주의 지연이 포함되어 있음)\ntarget 다음 주에 대한 주간 배송 횟수\n30개 이상의 스냅샷(주간)\n\n데이터정리\n\nT = 31\nV = 지역의 집합\nN = 15 # number of nodes\nE = 225 # edges\n\\(f(v,t)\\)의 차원? (1,) # number of deliveries\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (15,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (15,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 15\n\nvertices are localities\n\n-Edges : 225\n\nedges are spatial_connections\n\n- Time : 31\n\nbetween 2020 and 2021\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import PedalMeDatasetLoader\nloader2 = PedalMeDatasetLoader()\n\n\na = loader2.get_dataset()\n\n\nnp.array(a.features).shape\n\n(31, 15, 4)\n\n\n\nnp.array(a.edge_index).shape\n\n(2, 225)\n\n\n\nnp.array(a.edge_weight).shape\n\n(225,)\n\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(a.edge_index.T)\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\nnode_list = []\nfor i in range(0, 15):\n    node_list.append(i)\n\n\nfig,ax = plt.subplots(15,1,figsize=(20,50))\nfor k in range(15):\n    ax[k].plot(np.array(loader2.targets).reshape(15,-1)[k][:],label='observed')\n    ax[k].set_title('node: {}'.format(node_list[k]))\n    ax[k].legend()\nfig.tight_layout()"
  },
  {
    "objectID": "posts/GCN/2023-03-20-data load, data save as pickle.html#wikimathsdatasetloader",
    "href": "posts/GCN/2023-03-20-data load, data save as pickle.html#wikimathsdatasetloader",
    "title": "data load, data save as pickle",
    "section": "WikiMathsDatasetLoader",
    "text": "WikiMathsDatasetLoader\nWikipedia Math\n\nA dataset of vital mathematics articles from Wikipedia. We made it public during the development of PyTorch Geometric Temporal. The underlying graph is static - vertices are Wikipedia pages and edges are links between them. The graph is directed and weighted. Weights represent the number of links found at the source Wikipedia page linking to the target Wikipedia page. The target is the daily user visits to the Wikipedia pages between March 16th 2019 and March 15th 2021 which results in 731 periods.\n위키피디아에서 중요한 수학 기사들로 이루어진 데이터셋\n그래프는 정적\nnode 위키피디아 페이지\nedge 페이지 간의 링크\n그래프는 방향성이 있으며 가중치가 있음\n가중치는 소스 위키피디아 페이지에서 대상 위키피디아 페이지로 연결된 링크 수\ntarget 2019년 3월 16일부터 2021년 3월 15일까지 위키피디아 페이지의 일일 사용자 방문 수\n기간은 731개의 기간으로 구성됩니다.\n\n데이터정리\n\nT = 723\nV = 위키피디아 페이지\nN = 1068 # number of nodes\nE = 27079 # edges\n\\(f(v,t)\\)의 차원? (1,) # 해당페이지를 유저가 방문한 횟수\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (1068,8) (N,8), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3),f(v,t_4),f(v,t_5),f(v,t_6),f(v,t_7)\\)\ny: (1068,) (N,), \\(f(v,t_8)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 1068\n\nvertices are Wikipedia pages\n\n-Edges : 27079\n\nedges are links between them\n\n- Time : 723\n\nWikipedia pages between March 16th 2019 and March 15th 2021\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import WikiMathsDatasetLoader\nloader3 = WikiMathsDatasetLoader()\n\n\na = loader3.get_dataset()\n\n\nnp.array(a.features).shape\n\n(723, 1068, 8)\n\n\n\nnp.array(a.edge_index).shape\n\n(2, 27079)\n\n\n\nnp.array(a.edge_weight).shape\n\n(27079,)\n\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(a.edge_index.T)\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\nnode_list = []\nfor i in range(0, 1068):\n    node_list.append(i)\n\n\nfig, ax = plt.subplots(20, 1, figsize=(20, 70))\nindices = random.sample(range(0, 1068), 20)\nfor k, idx in enumerate(indices):\n    ax[k].plot(np.array(loader3.targets).reshape(1068,-1)[idx][:], label='observed')\n    ax[k].set_title('node: {}'.format(node_list[idx]))\n    ax[k].legend()\nfig.tight_layout()"
  },
  {
    "objectID": "posts/GCN/2023-03-20-data load, data save as pickle.html#windmilloutputlargedatasetloader",
    "href": "posts/GCN/2023-03-20-data load, data save as pickle.html#windmilloutputlargedatasetloader",
    "title": "data load, data save as pickle",
    "section": "WindmillOutputLargeDatasetLoader",
    "text": "WindmillOutputLargeDatasetLoader\n\nHourly energy output of windmills from a European country for more than 2 years. Vertices represent 319 windmills and weighted edges describe the strength of relationships. The target variable allows for regression tasks\n유럽 국가의 풍력 발전소에서 2년 이상에 걸쳐 발생한 시간별 에너지 출력 데이터\nnode 319개의 풍력 발전소\n가중치가 있는 edge는 관계의 강도\n회귀 분석 작업에 적합한 목표 변수를 제공\n\n데이터정리\n\nT = 17464\nV = 풍력발전소\nN = 319 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (319,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (319,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 319\n\nvertices represent 319 windmills\n\n-Edges : 101761\n\nweighted edges describe the strength of relationships.\n\n- Time : 17464\n\nmore than 2 years\n\n\nfrom torch_geometric_temporal.dataset import WindmillOutputLargeDatasetLoader\nloader4 = WindmillOutputLargeDatasetLoader()\n\n\na = loader4.get_dataset()\n\n\nnp.array(a.features).shape\n\n(17464, 319, 8)\n\n\n\nnp.array(a.edge_index).shape\n\n(2, 101761)\n\n\n\nnp.array(a.edge_weight).shape\n\n(101761,)\n\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(a.edge_index.T)\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\na= np.arange(1,100)\na\n\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n\n\n\na[::10]\n\narray([ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91])\n\n\n\nnp.array(loader4.targets).reshape(319,-1)[idx][::100].shape\n\n(175,)\n\n\n\nnode_list = []\nfor i in range(0, 319):\n    node_list.append(i)\n\n\nfig, ax = plt.subplots(20, 1, figsize=(30, 50))\nindices = random.sample(range(0, 319), 20)\nfor k, idx in enumerate(indices):\n    ax[k].plot(np.array(loader4.targets).reshape(319,-1)[idx][:1000], label='observed')\n    ax[k].set_title('node: {}'.format(node_list[idx]))\n    ax[k].legend()\nfig.tight_layout()\n\n\n\n\n\nfig, ax = plt.subplots(20, 1, figsize=(30, 50))\nindices = random.sample(range(0, 319), 20)\nfor k, idx in enumerate(indices):\n    ax[k].plot(np.array(loader4.targets).reshape(319,-1)[idx][:], label='observed')\n    ax[k].set_title('node: {}'.format(node_list[idx]))\n    ax[k].legend()\nfig.tight_layout()"
  },
  {
    "objectID": "posts/GCN/2023-03-20-data load, data save as pickle.html#windmilloutputmediumdatasetloader",
    "href": "posts/GCN/2023-03-20-data load, data save as pickle.html#windmilloutputmediumdatasetloader",
    "title": "data load, data save as pickle",
    "section": "WindmillOutputMediumDatasetLoader",
    "text": "WindmillOutputMediumDatasetLoader\n\nHourly energy output of windmills from a European country for more than 2 years. Vertices represent 26 windmills and weighted edges describe the strength of relationships. The target variable allows for regression tasks.\n유럽 국가의 풍력 발전소에서 2년 이상에 걸쳐 발생한 시간별 에너지 출력 데이터셋\nnode 26개의 풍력 발전소\n가중치가 있는 edge는 관계의 강도\n회귀 분석 작업에 적합한 목표 변수를 제공\n\n데이터정리\n\nT = 17464\nV = 풍력발전소\nN = 26 # number of nodes\nE = 676 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (26,4) (N,8), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (26,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 26\n\nvertices represent 26 windmills\n\n-Edges : 676\n\nweighted edges describe the strength of relationships\n\n- Time : 17464\n\nmore than 2 years\n\n\nfrom torch_geometric_temporal.dataset import WindmillOutputMediumDatasetLoader\nloader5 = WindmillOutputMediumDatasetLoader()\n\n\na = loader5.get_dataset()\n\n\nnp.array(a.features).shape\n\n(17464, 26, 8)\n\n\n\nnp.array(a.edge_index).shape\n\n(2, 676)\n\n\n\nnp.array(a.edge_weight).shape\n\n(676,)\n\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(a.edge_index.T)\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\nnode_list = []\nfor i in range(0, 26):\n    node_list.append(i)\n\n\nfig,ax = plt.subplots(26,1,figsize=(30,70))\nfor k in range(26):\n    ax[k].plot(np.array(loader5.targets).reshape(26,-1)[k],label='observed')\n    ax[k].set_title('node: {}'.format(node_list[k]))\n    ax[k].legend()\nfig.tight_layout()\n\n\n\n\n\nfig,ax = plt.subplots(26,1,figsize=(30,70))\nfor k in range(26):\n    ax[k].plot(np.array(loader5.targets).reshape(26,-1)[k][:1000],label='observed')\n    ax[k].set_title('node: {}'.format(node_list[k]))\n    ax[k].legend()\nfig.tight_layout()"
  },
  {
    "objectID": "posts/GCN/2023-03-20-data load, data save as pickle.html#windmilloutputsmalldatasetloader",
    "href": "posts/GCN/2023-03-20-data load, data save as pickle.html#windmilloutputsmalldatasetloader",
    "title": "data load, data save as pickle",
    "section": "WindmillOutputSmallDatasetLoader",
    "text": "WindmillOutputSmallDatasetLoader\n\nHourly energy output of windmills from a European country for more than 2 years. Vertices represent 11 windmills and weighted edges describe the strength of relationships. The target variable allows for regression tasks.\n유럽 국가의 풍력 발전소에서 2년 이상에 걸쳐 발생한 시간별 에너지 출력 데이터셋\nnode 11개의 풍력 발전소\n가중치가 있는 edge는 관계의 강도\n회귀 분석 작업에 적합한 목표 변수를 제공합니다.\n\n데이터정리\n\nT = 17464\nV = 풍력발전소\nN = 11 # number of nodes\nE = 121 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (11,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (11,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 11\n\nvertices represent 11 windmills\n\n-Edges : 121\n\nweighted edges describe the strength of relationships\n\n- Time : 17464\n\nmore than 2 years\n\n\nfrom torch_geometric_temporal.dataset import WindmillOutputSmallDatasetLoader\nloader6 = WindmillOutputSmallDatasetLoader()\n\n\na = loader6.get_dataset()\n\n\nnp.array(a.features).shape\n\n(17464, 11, 8)\n\n\n\nnp.array(a.edge_index).shape\n\n(2, 121)\n\n\n\nnp.array(a.edge_weight).shape\n\n(121,)\n\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(a.edge_index.T)\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\nnode_list = []\nfor i in range(0, 11):\n    node_list.append(i)\n\n\nfig,ax = plt.subplots(11,1,figsize=(30,50))\nfor k in range(11):\n    ax[k].plot(np.array(loader6.targets).reshape(11,-1)[k][:],label='observed')\n    ax[k].set_title('node: {}'.format(node_list[k]))\n    ax[k].legend()\nfig.tight_layout()"
  },
  {
    "objectID": "posts/GCN/2023-03-20-data load, data save as pickle.html#metrladatasetloader_real-world-traffic-dataset",
    "href": "posts/GCN/2023-03-20-data load, data save as pickle.html#metrladatasetloader_real-world-traffic-dataset",
    "title": "data load, data save as pickle",
    "section": "METRLADatasetLoader_real world traffic dataset",
    "text": "METRLADatasetLoader_real world traffic dataset\nA traffic forecasting dataset based on Los Angeles Metropolitan traffic conditions. The dataset contains traffic readings collected from 207 loop detectors on highways in Los Angeles County in aggregated 5 minute intervals for 4 months between March 2012 to June 2012.\n데이터정리\n\nT = 33\nV = 구역\nN = 207 # number of nodes\nE = 225\n\\(f(v,t)\\)의 차원? (3,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? False\nX: (207,4) (N,2,12), \\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11}\\)\ny: (207,) (N,), \\((x_{12})\\)\n예제코드적용가능여부: No\n\nhttps://arxiv.org/pdf/1707.01926.pdf\n- Nodes : 207\n\nvertices are localities\n\n-Edges : 225\n\nedges are spatial_connections\n\n- Time : 33\n\nbetween 2020 and 2021\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import METRLADatasetLoader\nloader7 = METRLADatasetLoader()\n\n\na = loader7.get_dataset(num_timesteps_in=1,num_timesteps_out=1)\n\n\nnp.array(a.edge_index).shape\n\n(2, 1722)\n\n\n\nnp.array(a.edge_weight).shape\n\n(1722,)\n\n\n\nnp.array(a.targets).shape\n\n(34271, 207, 1)\n\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(a.edge_index.T)\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\nnode_list = []\nfor i in range(0, 207):\n    node_list.append(i)\n\n\nfig, ax = plt.subplots(20, 1, figsize=(30, 50))\nindices = random.sample(range(0, 207), 20)\nfor k, idx in enumerate(indices):\n    ax[k].plot(np.array(loader7.targets).reshape(207,-1)[idx], label='observed')\n    ax[k].set_title('node: {}'.format(node_list[idx]))\n    ax[k].legend()\nfig.tight_layout()"
  },
  {
    "objectID": "posts/GCN/2023-03-20-data load, data save as pickle.html#pemsbaydatasetloader",
    "href": "posts/GCN/2023-03-20-data load, data save as pickle.html#pemsbaydatasetloader",
    "title": "data load, data save as pickle",
    "section": "PemsBayDatasetLoader",
    "text": "PemsBayDatasetLoader\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1111/tgis.12644\n\nA traffic forecasting dataset as described in Diffusion Convolution Layer Paper.\nA traffic forecasting dataset as described in Diffusion Convolution Layer Paper.\nThis traffic dataset is collected by California Transportation Agencies (CalTrans) Performance Measurement System (PeMS). It is represented by a network of 325 traffic sensors in the Bay Area with 6 months of traffic readings ranging from Jan 1st 2017 to May 31th 2017 in 5 minute intervals.\nFor details see: \"Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting\" <https://arxiv.org/abs/1707.01926>\n캘리포니아 교통국(CalTrans) 성능 측정 시스템(PeMS)에서 수집\n2017년 1월 1일부터 2017년 5월 31일까지 6개월 동안 5분 간격으로 트래픽 판독치가 포함된 베이 지역의 325개 교통 센서 네트워크로 표시\n\n데이터정리\n\nT = 17470\nV = 교통센서\nN = 325 # number of nodes\nE = 2694 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # Hourly energy output\n시간에 따라서 N이 변하는지? False\n시간에 따라서 E가 변하는지? No\nX: (325,2,12) (N,2,12),\n\n\\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11}\\)\n\\(z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11}\\)\n\ny: (325,) (N,2,12),\n\n\\(x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23},x_{24}\\)\n\\(z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23},z_{24}\\)\n\n예제코드적용가능여부: No\n\n- Nodes : 325\n\nvertices are sensors\n\n-Edges : 2694\n\nweighted edges are between seonsor paris measured by the road nretwork distance\n\n- Time : 52081\n\n6 months of traffic readings ranging from Jan 1st 2017 to May 31th 2017 in 5 minute intervals\n\n\nfrom torch_geometric_temporal.dataset import PemsBayDatasetLoader\nloader8 = PemsBayDatasetLoader()\n\n\na = loader8.get_dataset(num_timesteps_in=1,num_timesteps_out=1)\n\n\nnp.array(a.edge_index).shape\n\n(2, 2694)\n\n\n\na.edge_index\n\narray([[  0,   1,   2, ..., 324, 324, 324],\n       [  0,   1,   2, ..., 257, 310, 324]])\n\n\n\nnp.array(a.edge_weight).shape\n\n(2694,)\n\n\n\nnp.array(a.targets).shape\n\n(52104, 325, 2, 1)\n\n\n\na.targets[0][0][0]\n\narray([0.99281436], dtype=float32)\n\n\n\nnode_list = []\nfor i in range(0, 325):\n    node_list.append(i)\n\n\nfig, ax = plt.subplots(20, 1, figsize=(30, 50))\nindices = random.sample(range(0, 325), 20)\nfor k, idx in enumerate(indices):\n    ax[k].plot(np.array(loader8.targets).reshape(325,-1)[idx], label='observed')\n    ax[k].set_title('node: {}'.format(node_list[idx]))\n    ax[k].legend()\nfig.tight_layout()"
  },
  {
    "objectID": "posts/GCN/2023-03-20-data load, data save as pickle.html#englandcoviddatasetloader",
    "href": "posts/GCN/2023-03-20-data load, data save as pickle.html#englandcoviddatasetloader",
    "title": "data load, data save as pickle",
    "section": "EnglandCovidDatasetLoader",
    "text": "EnglandCovidDatasetLoader\nCovid19 England\n\nA dataset about mass mobility between regions in England and the number of confirmed COVID-19 cases from March to May 2020 [38]. Each day contains a different mobility graph and node features corresponding to the number of cases in the previous days. Mobility stems from Facebook Data For Good 1 and cases from gov.uk 2\n\nhttps://arxiv.org/pdf/2009.08388.pdf\n데이터정리\n\nT = 60\nV = 지역\nN = 129 # number of nodes\nE = 2158\n\\(f(v,t)\\)의 차원? (1,) # 코로나확진자수\n시간에 따라서 Number of nodes가 변하는지? False\n시간에 따라서 Number of edge가 변하는지? TRUE\nX: (20,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (20,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 129\n\nvertices are correspond to the number of COVID-19 cases in the region in the past window days.\n\n-Edges : 2158\n\nthe spatial edges capture county-to-county movement at a specific date, and a county is connected to a number of past instances of itself with temporal edges.\n\n- Time : 61\n\nfrom 3 March to 12 of May\n\n\nfrom torch_geometric_temporal.dataset import EnglandCovidDatasetLoader\nloader9 = EnglandCovidDatasetLoader()\n\n\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\n\na = loader9.get_dataset(lags=1)\n\n\na, _a = temporal_signal_split(a, train_ratio=0.8)\n\n\na.snapshot_count,_a.snapshot_count\n\n(60, 12)\n\n\n원래 60\n\na.edge_indices[59]\n\nIndexError: list index out of range\n\n\n\nnp.array(a.edge_indices[59]).shape\n\n(2, 1476)\n\n\n\na.edge_weights[59]\n\narray([2.96600e+03, 1.88595e+05, 1.30000e+02, ..., 1.10000e+01,\n       1.80000e+01, 1.00000e+01])\n\n\n\nnp.array(a.edge_weights[59]).shape\n\n(1476,)\n\n\n\na.snapshot_count\n\n60\n\n\n\nnp.array(a.features).shape\n\n(60, 129, 1)\n\n\n\nnp.array(a.targets).shape\n\n(60, 129)\n\n\n\nG = nx.Graph()\nG.add_edges_from(a.edge_indices[0].T)\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\nG = nx.Graph()\nG.add_edges_from(a.edge_indices[58].T)\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\nnode_list = []\nfor i in range(0, 129):\n    node_list.append(i)\n\n\nfig, ax = plt.subplots(20, 1, figsize=(30, 50))\nindices = random.sample(range(0, 129), 20)\nfor k, idx in enumerate(indices):\n    ax[k].plot(np.array(loader9.targets).reshape(129,-1)[idx], label='observed')\n    ax[k].set_title('node: {}'.format(node_list[idx]))\n    ax[k].legend()\nfig.tight_layout()"
  },
  {
    "objectID": "posts/GCN/2023-03-20-data load, data save as pickle.html#montevideobusdatasetloader",
    "href": "posts/GCN/2023-03-20-data load, data save as pickle.html#montevideobusdatasetloader",
    "title": "data load, data save as pickle",
    "section": "MontevideoBusDatasetLoader",
    "text": "MontevideoBusDatasetLoader\nMontevideo Buses\n\nA dataset of inflow passenger at bus stop level from Montevideo city. This dataset comprises hourly inflow passenger data at bus stop level for 11 bus lines during October 2020 from Montevideo city (Uruguay). The bus lines selected are the ones that carry people to the center of the city and they load more than 25% of the total daily inflow traffic. Vertices are bus stops, edges are links between bus stops when a bus line connects them and the weight represent the road distance. The target is the passenger inflow. This is a curated dataset made from different data sources of the Metropolitan Transportation System (STM) of Montevideo. These datasets are freely available to anyone in the National Catalog of Open Data from the government of Uruguay (https://catalogodatos.gub.uy/)\n몬테비데오 시티에서 버스 정류장 층으로 유입된 승객의 데이터 셋\n2020년 10월 동안 몬테비데오 시티(우루과이)에서 11개 버스 노선에 대한 버스 정류장 수준의 시간당 유입 승객 데이터\n선정된 버스 노선은 도심까지 사람을 실어 나르는 노선으로 하루 총 유입량의 25% 이상을 적재\nnode는 버스 정류장\nedge는 버스 노선이 버스 정류장을 연결할 때 버스 정류장 사이의 링크\nweight는 도로 거리\ntarget은 승객 유입\n몬테비데오의 메트로폴리탄 교통 시스템(STM)의 서로 다른 데이터 소스로 만들어진 큐레이션된 데이터 셋\n\n데이터정리\n\nT = 743\nV = 버스정류장\nN = 675 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # passenger inflow\n시간에 따라서 Number of nodes가 변하는지? False\n시간에 따라서 Number of nodes가 변하는지? False\nX: (675,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (675,,) (N,), \\(f(v,t_4)\\)\n예제코드적용가능여부: Yes\n\n- Nodes : 675\n\nvertices are bus stops\n\n-Edges : 690\n\nedges are links between bus stops when a bus line connects them and the weight represent the road distance\n\n- Time : 743\n\nhourly inflow passenger data at bus stop level for 11 bus lines during October 2020 from Montevideo city (Uruguay).\n\n\nfrom torch_geometric_temporal.dataset import MontevideoBusDatasetLoader\nloader10 = MontevideoBusDatasetLoader()\n\n\na = loader10.get_dataset(lags=1)\n\n\nnp.array(a.edge_index).shape\n\n(2, 690)\n\n\n\nnp.array(a.edge_weight).shape\n\n(690,)\n\n\n\nnp.array(a.features).shape\n\n(743, 675, 1)\n\n\n\nG = nx.Graph()\n\n\nG.add_edges_from(a.edge_index.T)\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\nnode_list = []\nfor i in range(0, 675):\n    node_list.append(i)\n\n\nfig, ax = plt.subplots(20, 1, figsize=(30, 50))\nindices = random.sample(range(0, 319), 20)\nfor k, idx in enumerate(indices):\n    ax[k].plot(np.array(loader10.targets).reshape(675,-1)[idx], label='observed')\n    ax[k].set_title('node: {}'.format(node_list[idx]))\n    ax[k].legend()\nfig.tight_layout()"
  },
  {
    "objectID": "posts/GCN/2023-03-20-data load, data save as pickle.html#twittertennisdatasetloader",
    "href": "posts/GCN/2023-03-20-data load, data save as pickle.html#twittertennisdatasetloader",
    "title": "data load, data save as pickle",
    "section": "TwitterTennisDatasetLoader",
    "text": "TwitterTennisDatasetLoader\nhttps://appliednetsci.springeropen.com/articles/10.1007/s41109-018-0080-5?ref=https://githubhelp.com\nTwitter Tennis RG and UO\n\nTwitter mention graphs of major tennis tournaments from 2017. Each snapshot contains the graph of popular player or sport news accounts and mentions between them [5, 6]. Node labels encode the number of mentions received and vertex features are structural properties\n\n데이터정리\n\nT = 52081\nV = 트위터계정\n\nN = 1000 # number of nodes\nE = 119 = N^2 # edges\n\\(f(v,t)\\)의 차원? (1,) # passenger inflow\n시간에 따라서 N이 변하는지? ??\n시간에 따라서 E가 변하는지? True\nX: ?\ny: ?\n예제코드적용가능여부: No\n\n- Nodes : 1000\n\nvertices are Twitter accounts\n\n-Edges : 119\n\nedges are mentions between them\n\n- Time : 52081\n\nTwitter mention graphs related to major tennis tournaments from 2017\n\n\nfrom torch_geometric_temporal.dataset import TwitterTennisDatasetLoader\nloader11 = TwitterTennisDatasetLoader()\n\n\na = loader11.get_dataset()\n\n\na.edge_indices[119]\n\narray([[900, 347, 347, 407, 407, 407, 407, 407, 407,   0,  35, 448, 407,\n        396, 396, 370, 233, 233, 357,  15,  15, 135, 135, 358, 233, 233,\n        243, 115, 115, 667, 667, 667, 667, 440, 440, 440, 101, 650, 309,\n        309, 309, 233, 347, 347, 974, 161, 218, 309, 309,  93,  93, 813,\n        101, 417,  69,  69, 480, 480, 416, 272, 813, 813, 813, 379, 903,\n        903, 903,  95, 309, 309, 309, 144, 890, 890, 484, 484, 484, 653,\n        653, 234, 234, 234, 253, 253, 630, 769, 769, 156, 156, 156, 892,\n        912, 912,   0, 278, 896, 896, 896, 233,  92,  69, 802, 324, 324,\n        574,  87,  87,  87, 365, 365,  87, 522, 611, 427, 427, 427, 110,\n        246, 246, 445, 156, 156, 133, 133, 217, 217,   4,   4, 105, 105,\n        105, 105, 630, 630, 630, 792,  84, 452,  51,  84, 445, 613, 985,\n        290, 290, 290, 290, 290, 290, 420, 420, 420, 420, 420, 420, 420,\n        420, 420,   0, 626, 626, 519, 519, 519, 519, 519, 519, 519, 519,\n        519, 217, 437, 437, 437, 309, 309, 217, 666,   4, 637, 540,  38,\n         38, 605, 605, 605, 605, 605, 215],\n       [  0,   0, 152,  41, 590,   0, 133, 116, 731,   1,   1,   1,   2,\n          0,   1,   0,   0, 982,   2,   1,   0,   0,   1,  51,   1, 427,\n          2,  74,   0,   1,  97,   0,   5,   0,   1, 614,  99,   0, 427,\n          0,   1, 203,   2,   1, 731,   1,   1,   2,  15,   0, 152,   1,\n          1,   0,   0,   1,   1,   0,   0,   0,  81,  97,   0,   0,   0,\n        416,  69,   1, 317,  67, 438,   0,   0,   1,   0,   1, 234,  15,\n          0,   1, 484,   0,   0, 141,   0,   1,  97, 221,   0, 540, 445,\n          1,   0,   2,   0,   0, 132, 438, 745,  92,   2,  80,   0, 152,\n          0,   0,   1,   4, 129, 338, 170,   0,   1, 123,   1,   0,   1,\n          1,   0,   1,   1, 427,   0,   1,   1, 217,   0,   1,   1,  97,\n          0, 226, 123,   1, 427,   0,   0,   1,   1,   1,   0,   1,   1,\n          5, 121,   0,  99,  15, 226,   5,   2, 221,  97,   6,  99, 226,\n        121,   0, 706,   0, 211,  99,   0,   5,  97, 226, 121,   6, 221,\n          2,   0,   1,   0, 217,  69, 416, 210,   0, 634, 291, 236,   1,\n          0, 234, 437, 286,   1, 745,  56]])\n\n\n\na.edge_weights[119]\n\narray([1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 6, 2, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 2, 2, 1, 1, 1, 1, 1, 1, 3, 3, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 3, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 3, 3, 1,\n       3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\n\n\na.features[119]\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [1., 0., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.]])\n\n\n\na.snapshot_count\n\n120\n\n\n\nnp.array(a.targets).shape\n\n(120, 1000)\n\n\n\nnode_list = []\nfor i in range(0, 1000):\n    node_list.append(i)\n\n\nfig, ax = plt.subplots(20, 1, figsize=(30, 50))\nindices = random.sample(range(0, 1000), 20)\nfor k, idx in enumerate(indices):\n    ax[k].plot(np.array(loader11.targets).reshape(1000,-1)[idx], label='observed')\n    ax[k].set_title('node: {}'.format(node_list[idx]))\n    ax[k].legend()\nfig.tight_layout()\n\n\n\n\n\ndef transform_degree(x, cutoff=4):\n    log_deg = np.ceil(np.log(x + 1.0))\n    return np.minimum(log_deg, cutoff)\n\n\ndef transform_transitivity(x):\n    trans = x * 10\n    return np.floor(trans)\n\n\ndef onehot_encoding(x, unique_vals):\n    E = np.zeros((len(x), len(unique_vals)))\n    for i, val in enumerate(x):\n        E[i, unique_vals.index(val)] = 1.0\n    return E\n\n\ndef encode_features(X, log_degree_cutoff=4):\n    X_arr = np.array(X)\n    a = transform_degree(X_arr[:, 0], log_degree_cutoff)\n    b = transform_transitivity(X_arr[:, 1])\n    A = onehot_encoding(a, range(log_degree_cutoff + 1))\n    B = onehot_encoding(b, range(11))\n    return np.concatenate((A, B), axis=1)"
  },
  {
    "objectID": "posts/GCN/2023-03-20-data load, data save as pickle.html#mtmdatasetloader",
    "href": "posts/GCN/2023-03-20-data load, data save as pickle.html#mtmdatasetloader",
    "title": "data load, data save as pickle",
    "section": "MTMDatasetLoader",
    "text": "MTMDatasetLoader\nMTM-1 Hand Motions\n\nA dataset of Methods-Time Measurement-1 <https://en.wikipedia.org/wiki/Methods-time_measurement>(MTM-1) motions, signalled as consecutive video frames of 21 3D hand keypoints, acquired via MediaPipe Hands <https://google.github.io/mediapipe/solutions/hands.html> from RGB-Video material. Vertices are the finger joints of the human hand and edges are the bones connecting them. The targets are manually labeled for each frame, according to one of the five MTM-1 motions (classes :math:C): Grasp, Release, Move, Reach, Position plus a negative class for frames without graph signals (no hand present). This is a classification task where :math:T consecutive frames need to be assigned to the corresponding class :math:C. The data x is returned in shape :obj:(3, 21, T), the target is returned one-hot-encoded in shape :obj:(T, 6).\n\n데이터정리\n\nT = 14452\nV = 손의 shape에 대응하는 dot\n\nN = 325 # number of nodes\nE = 19 = N^2 # edges\n\\(f(v,t)\\)의 차원? (Grasp, Release, Move, Reach, Poision, -1)\n시간에 따라서 N이 변하는지? ??\n시간에 따라서 E가 변하는지? ??\nX: ?\ny: ?\n예제코드적용가능여부: No\n\n- Nodes : 325\n\nvertices are are the finger joints of the human hand\n\n-Edges : 19\n\nedges are the bones connecting them\n\n- Time : 14452\n\n # target eoncoding: {0 : 'Grasp', 1 : 'Move', 2 : 'Negative',\n        #                   3 : 'Position', 4 : 'Reach', 5 : 'Release'}\n\n\nfrom torch_geometric_temporal.dataset import MTMDatasetLoader\nloader12 = MTMDatasetLoader()\n\n\na = loader12.get_dataset(frames=16)\n\n\nnp.array(a.edge_index).shape\n\n(2, 19)\n\n\n\nnp.shape(a.edge_weight)\n\n(19,)\n\n\n\nnp.array(a.features).shape\n\n(14453, 3, 21, 16)\n\n\n\na.snapshot_count\n\n14453\n\n\n\nnp.array(a.targets).shape\n\n(14453, 16, 6)\n\n\n\na.targets[0]\n\narray([[0., 0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0.]])\n\n\n\nb=[]\nfor i in range(a.snapshot_count):\n    b.append(np.argmax(a.targets[i]))\n\n\npd.DataFrame(b)[0].unique()\n\narray([2, 4, 0, 1, 3, 5])\n\n\n\npd.DataFrame(b).value_counts()\n\n0    3910\n1    3189\n3    3069\n4    2480\n2    1266\n5     539\ndtype: int64"
  },
  {
    "objectID": "posts/GCN/2023-04-27-toy_example_figure.html",
    "href": "posts/GCN/2023-04-27-toy_example_figure.html",
    "title": "Toy Example Figure(Intro)",
    "section": "",
    "text": "edit\n\n\nimport\n\nimport itstgcn \nimport torch\nimport numpy as np\n\n\nimport matplotlib.pyplot as plt\n\n\nimport random\n\n\nclass Eval_csy:\n    def __init__(self,learner,train_dataset):\n        self.learner = learner\n        # self.learner.model.eval()\n        try:self.learner.model.eval()\n        except:pass\n        self.train_dataset = train_dataset\n        self.lags = self.learner.lags\n        rslt_tr = self.learner(self.train_dataset) \n        self.X_tr = rslt_tr['X']\n        self.y_tr = rslt_tr['y']\n        self.f_tr = torch.concat([self.train_dataset[0].x.T,self.y_tr],axis=0).float()\n        self.yhat_tr = rslt_tr['yhat']\n        self.fhat_tr = torch.concat([self.train_dataset[0].x.T,self.yhat_tr],axis=0).float()\n\n\nimport pickle\nimport pandas as pd\n\n\ndef load_data(fname):\n    with open(fname, 'rb') as outfile:\n        data_dict = pickle.load(outfile)\n    return data_dict\n\ndef save_data(data_dict,fname):\n    with open(fname,'wb') as outfile:\n        pickle.dump(data_dict,outfile)\n\n\nfrom plotnine import *\n\n\n\nExample\n\nT = 200\nt = np.arange(T)/T * 10\nx = 0.1*np.sin(2*t)+0.2*np.sin(4*t)+0.1*np.sin(8*t)+0.2*np.sin(16*t)\neps_x  = np.random.normal(size=T)*0\ny = x.copy()\n# for i in range(2,T):\n#     y[i] = 0.35*x[i-1] - 0.15*x[i-2] + 0.5*np.cos(0.5*t[i]) \neps_y  = np.random.normal(size=T)*0\nx = x*0.35\ny = y*0.3\nplt.plot(t,x,color='C0',lw=5)\nplt.plot(t,x+eps_x,alpha=0.5,color='C0')\nplt.plot(t,y,color='C1',lw=5)\nplt.plot(t,y+eps_y,alpha=0.5,color='C1')\n_node_ids = {'node1':0, 'node2':1}\n\n# _FX1 = np.stack([x,y],axis=1).tolist()\n_FX1 = np.stack([x+eps_x,y+eps_y],axis=1).tolist()\n\n_edges1 = torch.tensor([[0,1],[1,0]]).tolist()\n\ndata_dict1 = {'edges':_edges1, 'node_ids':_node_ids, 'FX':_FX1}\n#data_dict = itstgcn.load_data('./data/fivenodes.pkl')\n\nsave_data(data_dict1, './data/toy_example1.pkl')\n\ndata1 = pd.DataFrame({'x':x,'y':y,'xer':x,'yer':y})\n\nsave_data(data1, './data/toy_example_true1.csv')\n\n\n\n\n\ndata_dict1 = itstgcn.load_data('./data/toy_example1.pkl')\nloader1 = itstgcn.DatasetLoader(data_dict1)\n\n\ndataset05031 = loader1.get_dataset(lags=1)\n\n\n# mindex05031 = itstgcn.rand_mindex(dataset05031,mrate=0)\n# dataset_miss05031 = itstgcn.miss(dataset05031,mindex05031,mtype='rand')\n\n\nmindex = [random.sample(range(0, T), int(T*0.5)),[np.array(list(range(100,120)))]]\ndataset_miss05031 = itstgcn.miss(dataset05031,mindex,mtype='block')\n\n/home/csy/Dropbox/blog/posts/GCN/itstgcn/utils.py:71: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343998658/work/torch/csrc/utils/tensor_new.cpp:245.)\n\n\n\ndataset_padded_cubic05031 = itstgcn.padding(dataset_miss05031,imputation_method='cubic')\n\n- 학습\n\nlrnr05031 = itstgcn.StgcnLearner(dataset_padded_cubic05031)\n\n\nlrnr05031.learn(filters=8,epoch=50)\n\n50/50\n\n\n\ndf1 = itstgcn.load_data('./data/toy_example_true1.csv')\n\n\nevtor05031 = Eval_csy(lrnr05031,dataset_padded_cubic05031)\n\n\nlrnr05032 = itstgcn.ITStgcnLearner(dataset_padded_cubic05031)\n\n\nlrnr05032.learn(filters=8,epoch=50)\n\n50/50\n\n\n\nevtor05032 = Eval_csy(lrnr05032,dataset_padded_cubic05031)\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,figsize=(40,15))\n# fig.suptitle('Figure 1')\nax1.plot(df1['x'][:],'--.',color='C1')\nax1.plot(evtor05031.f_tr[:,0],'--.',color='C2')\nax1.plot(evtor05031.fhat_tr[:,0],color='C3',lw=3)\nax1.set_title('STGCN')\nax2.plot(df1['y'][:],'--.',color='C1')\nax2.plot(evtor05031.fhat_tr[:,1],color='C3',lw=3)\nax2.plot(evtor05031.f_tr[:,1],'--.',color='C2')\nax2.set_title('STGCN')\nax3.plot(df1['x'][:],'--.',color='C1')\nax3.plot(evtor05032.f_tr[:,0],'--.',color='C2')\nax3.plot(evtor05032.fhat_tr[:,0],color='C4',lw=3)\nax3.set_title('ITSTGCN')\nax4.plot(df1['y'][:],'--.',color='C1')\nax4.plot(evtor05032.f_tr[:,1],'--.',color='C2')\nax4.plot(evtor05032.fhat_tr[:,1],color='C4',lw=3)\nax4.set_title('ITSTGCN')\n\nfor ax in fig.get_axes():\n    ax.label_outer()\n\nwith plt.style.context('seaborn-white'):\n    # plt.rcParams['font.family'] = 'xkcd'\n    # plt.xkcd(scale=0,length=200)\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,figsize=(40,15))\n    fig.suptitle('Figure 1(node 1)',fontsize=40)\n    ax1.plot(df1['x'][:],'-',color='C3',label='Complete Data')\n    ax1.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=20)\n    ax1.tick_params(axis='x', labelsize=20)\n    ax2.plot(df1['x'][:],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax2.plot(torch.tensor(dataset_miss05031.features).reshape(-1,2)[:,0],'--o',color='C3',label='Missing')\n    ax2.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax2.tick_params(axis='y', labelsize=20)\n    ax2.tick_params(axis='x', labelsize=20)\n    ax3.plot(df1['x'][:],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax3.plot(evtor05032.f_tr[:,0],'--o',color='C3',alpha=0.8,label='Imputation')\n    ax3.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax3.tick_params(axis='y', labelsize=20)\n    ax3.tick_params(axis='x', labelsize=20)\n    ax4.plot(df1['x'][:],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax4.plot(evtor05031.fhat_tr[:,0],color='brown',lw=3,label='STGCN')\n    ax4.plot(evtor05032.fhat_tr[:,0],color='blue',lw=3,label='ITSTGCN')\n    ax4.plot(55, 0, 'o', markersize=100, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    ax4.plot(150, 0, 'o', markersize=80, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    ax4.plot(185, 0, 'o', markersize=80, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    ax4.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax4.tick_params(axis='y', labelsize=20)\n    ax4.tick_params(axis='x', labelsize=20)\n\n\n\n\n\nwith plt.style.context('seaborn-white'):\n    # plt.rcParams['font.family'] = 'xkcd'\n    # plt.xkcd(scale=0,length=200)\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,figsize=(40,15))\n    fig.suptitle('Figure 1(node 2)',fontsize=40)\n    ax1.plot(df1['y'][:],'-',color='C3',label='Complete Data')\n    ax1.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax1.tick_params(axis='y', labelsize=20)\n    ax1.tick_params(axis='x', labelsize=20)\n    ax2.plot(df1['y'][:],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax2.plot(torch.tensor(dataset_miss05031.features).reshape(-1,2)[:,1],'--o',color='C3',label='Missing')\n    ax2.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax2.tick_params(axis='y', labelsize=20)\n    ax2.tick_params(axis='x', labelsize=20)\n    ax3.plot(df1['y'][:],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax3.plot(evtor05032.f_tr[:,1],'--o',color='C3',alpha=0.8,label='Imputation')\n    ax3.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax3.tick_params(axis='y', labelsize=20)\n    ax3.tick_params(axis='x', labelsize=20)\n    ax4.plot(df1['x'][:],'--',color='C5',alpha=0.5,label='Complete Data')\n    ax4.plot(evtor05031.fhat_tr[:,1],color='brown',lw=3,label='STGCN')\n    ax4.plot(evtor05032.fhat_tr[:,1],color='blue',lw=3,label='ITSTGCN')\n    ax4.plot(120, 0.01,'s', markersize=110, markerfacecolor='none', markeredgecolor='red',markeredgewidth=3)\n    ax4.legend(fontsize=20,loc='lower left',facecolor='white', frameon=True)\n    ax4.tick_params(axis='y', labelsize=20)\n    ax4.tick_params(axis='x', labelsize=20)"
  },
  {
    "objectID": "posts/GCN/2023-05-11-CPUvsGPU.html",
    "href": "posts/GCN/2023-05-11-CPUvsGPU.html",
    "title": "PyG Geometric Temporal CPU vs GPU",
    "section": "",
    "text": "CPU vs GPU\n\n\n!nvidia-smi\n\nFri May 12 06:42:19 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 495.46       Driver Version: 495.46       CUDA Version: 11.5     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:65:00.0 Off |                  N/A |\n|  0%   37C    P8    35W / 420W |     19MiB / 24268MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      1062      G   /usr/lib/xorg/Xorg                  9MiB |\n|    0   N/A  N/A      1309      G   /usr/bin/gnome-shell                8MiB |\n+-----------------------------------------------------------------------------+\n\n\n\nimport time\n\n\nimport torch\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import DCRNN\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = DCRNN(node_features, 32, 1)\n        self.linear = torch.nn.Linear(32, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\nfrom tqdm import tqdm\n\n\nGPU\n\nmodel = RecurrentGCN(node_features = 4)\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n\ntime.time()\n\n1683806883.6646736\n\n\n\nt1=time.time()\nfor epoch in tqdm(range(200)):\n    cost = 0\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr).to(\"cuda:0\")\n        cost = cost + torch.mean((y_hat-snapshot.y.to(\"cuda:0\"))**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 200/200 [03:05<00:00,  1.08it/s]\n\n\n\nimport time\n\n\nt2=time.time()\n\n\nt2-t1\n\n185.09801506996155\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 1.0507\n\n\n\n\nCPU\n\nmodel = RecurrentGCN(node_features = 4)\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n\nimport time\n\n\nt1=time.time()\nfor epoch in tqdm(range(200)):\n    cost = 0\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 200/200 [05:08<00:00,  1.54s/it]\n\n\n\nimport time\n\n\nt2=time.time()\n\n\nt2-t1\n\n308.58231496810913\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 1.0542"
  },
  {
    "objectID": "posts/GCN/2022-12-28-gcn_simulation.html",
    "href": "posts/GCN/2022-12-28-gcn_simulation.html",
    "title": "Simulation of geometric-temporal",
    "section": "",
    "text": "Simulation\n\nhttps://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/dataset.html#module-torch_geometric_temporal.dataset.chickenpox\n\nimport\n\nimport networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\n\n\n\n공식 홈페이지 예제\n\ndata\n\nfrom torch_geometric_temporal.dataset import WikiMathsDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset(lags=14)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\n\nRecurrentGCN\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvGRU\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvGRU(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\n\nLearn\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=14, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(1)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 1/1 [00:11<00:00, 11.93s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([1068, 14])\n\n\n\nsnapshot.y.shape\n\ntorch.Size([1068])\n\n\n\n1068개의 nodes\n한 개의 node에 mapping된 차원의 수\n\n\n_edge_index.shape\n\ntorch.Size([2, 27079])\n\n\n\n_edge_attr.shape\n\ntorch.Size([27079])\n\n\n\n_y.shape\n\ntorch.Size([1068])\n\n\n\n\n\n우리 예제\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\n\n\nT = 100\nN = 4 # number of Nodes\nE = np.array([[0,1],[1,2],[2,3],[3,0]]).T\nV = np.array([1,2,3,4])\nAMP = np.array([3,2,1,2.2])\nt = np.arange(0,T)\nnode_features = 1\n\n\nf = np.stack([a*np.sin(2*t**2/1000)+np.random.normal(loc=0,scale=0.2,size=T) for a in AMP],axis=1).reshape(T,N,node_features)\nf = torch.tensor(f).float()\n\n\nf.shape\n\ntorch.Size([100, 4, 1])\n\n\n\nX = f[:99,:,:]\ny = f[1:,:,:]\n\n\nplt.plot(y[:,0,0],label=\"v1\")\nplt.plot(y[:,1,0],label=\"v2\")\nplt.plot(y[:,2,0],label=\"v3\")\nplt.plot(y[:,3,0],label=\"v4\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efc48673490>\n\n\n\n\n\n\nedge_index = torch.tensor(E)\nedge_attr = torch.tensor(np.array([1,1,1,1]),dtype=torch.float32)\n\n\n_ee = enumerate(zip(X,y))\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:16<00:00,  3.01it/s]\n\n\n\nyhat = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nplt.plot(y[:,0,0],label=\"y in V1\")\nplt.plot(yhat[:,0,0],label=\"yhat in V1\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efc48524730>\n\n\n\n\n\n\nplt.plot(y[:,1,0],label=\"y in V2\")\nplt.plot(yhat[:,1,0],label=\"yhat in V2\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efc4849c730>\n\n\n\n\n\n\nplt.plot(y[:,2,0],label=\"y in V3\")\nplt.plot(yhat[:,2,0],label=\"yhat in V3\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efc484098e0>\n\n\n\n\n\n\nplt.plot(y[:,3,0],label=\"y in V4\")\nplt.plot(yhat[:,3,0],label=\"yhat in V4\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efc483f5880>\n\n\n\n\n\n\n\nGNAR\n\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr\n\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\n\nR[write to console]: Loading required package: igraph\n\nR[write to console]: \nAttaching package: ‘igraph’\n\n\nR[write to console]: The following objects are masked from ‘package:stats’:\n\n    decompose, spectrum\n\n\nR[write to console]: The following object is masked from ‘package:base’:\n\n    union\n\n\nR[write to console]: Loading required package: wordcloud\n\nR[write to console]: Loading required package: RColorBrewer\n\n\n\n\n%%R\nsummary(fiveNet)\n\nGNARnet with 5 nodes and 10 edges\n of equal length  1\n\n\n\n%%R\nedges <- as.matrix(fiveNet)\nedges\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    1    1\n[2,]    0    0    1    1    0\n[3,]    0    1    0    1    0\n[4,]    1    1    1    0    0\n[5,]    1    0    0    0    0\n\n\n\n%%R\nprint(fiveNet)\n\nGNARnet with 5 nodes \nedges:1--4 1--5 2--3 2--4 3--2 3--4 4--1 4--2 4--3 5--1 \n     \n edges of each of length  1 \n\n\n\n%%R\ndata(\"fiveNode\")\nanswer <- GNARfit(vts = fiveVTS, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nanswer\n\nModel: \nGNAR(2,[1,1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\n dmatalpha1  dmatbeta1.1   dmatalpha2  dmatbeta2.1  \n    0.20624      0.50277      0.02124     -0.09523  \n\n\n\n\n%%R\nlayout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))\nplot(fiveVTS[, 1], ylab = \"Node A Time Series\")\nlines(fitted(answer)[, 1], col = 2)\nplot(fiveVTS[, 2], ylab = \"Node B Time Series\")\nlines(fitted(answer)[, 2], col = 2)\nplot(fiveVTS[, 3], ylab = \"Node C Time Series\")\nlines(fitted(answer)[, 3], col = 2)\nplot(fiveVTS[, 4], ylab = \"Node D Time Series\")\nlines(fitted(answer)[, 4], col = 2)\n\n\n\n\n\n%R -o fiveVTS\n%R -o edges\n\n\nnode: 5\ntime 200\n\n\nedges_tensor = torch.tensor(edges)\n\n\nnonzero_indices = edges_tensor.nonzero()\n\n\nfiveNet_edge = np.array(nonzero_indices).T\nfiveNet_edge\n\narray([[0, 0, 1, 1, 2, 2, 3, 3, 3, 4],\n       [3, 4, 2, 3, 1, 3, 0, 1, 2, 0]])\n\n\n\nfiveVTS.shape\n\n(200, 5)\n\n\n\nT = 200\nN = 5 # number of Nodes\nE = fiveNet_edge\nV = np.array([1,2,3,4,5])\nt = np.arange(0,T)\nnode_features = 1\n\n\nf = torch.tensor(fiveVTS).reshape(200,5,1).float()\n\n\nX = f[:199,:,:]\ny = f[1:,:,:]\n\n\nedge_index = torch.tensor(E)\nedge_attr = torch.tensor(np.array([1,1,1,1,1,1,1,1,1,1]),dtype=torch.float32)\n\n\n_ee = enumerate(zip(X,y))\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=1, filters=8)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|█| 50/50 [00:34<00:00,  1.45it/\n\n\n\nyhat = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nyhat.shape\n\n(199, 5, 1)\n\n\n\nplt.plot(y[:,1])\nplt.plot(yhat[:,1].data)\n\n\n\n\n\n\nWind network time series\nthe data suite vswind that contains a number of R objects pertaining to 721 wind speeds taken at each of 102 weather stations in England and Wales. The suite contains the vector time series vswindts, the associated network vswindnet, a character vector of the weather station location names in vswindnames and coordinates of the stations in the two column matrix vswindcoords. The data originate from the UK Met Office site http://wow.metoffice.gov.uk and full details can be found in the vswind help file in the GNAR package.\n\n%%R\noldpar <- par(cex = 0.75)\nwindnetplot()\npar(oldpar)\n\n\n\n\n\n%%R\nedges_wind <- as.matrix(vswindnet)\n\n\n%R -o vswindts\n%R -o edges_wind\n\n\nnodes : 102\ntime step : 721\n\n\nvswindts.shape\n\n(721, 102)\n\n\n\nedges_wind.shape\n\n(102, 102)\n\n\n\nedges_winds = torch.tensor(edges_wind)\n\n\nnonzero_indices_wind = edges_winds.nonzero()\n\n\nvswindnet_edge = np.array(nonzero_indices_wind).T\nvswindnet_edge.shape\n\n(2, 202)\n\n\n\nT = 721\nN = 102 # number of Nodes\nE = vswindnet_edge\nV = np.array(range(101))\nt = np.arange(0,T)\nnode_features = 1\n\n\nf = torch.tensor(vswindts).reshape(721,102,1).float()\n\n\nX = f[:720,:,:]\ny = f[1:,:,:]\n\n\nedge_index = torch.tensor(E)\nedge_attr = torch.tensor(np.array([1]*202),dtype=torch.float32)\n\n\n_ee = enumerate(zip(X,y))\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [02:16<00:00,  2.73s/it]\n\n\n\nyhat = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nyhat.shape\n\n(720, 102, 1)\n\n\n\nplt.plot(y[:,1])\nplt.plot(yhat[:,1].data)\n\n\n\n\n\n\nOECD GDP\n해당예제는 GNAR 패키지에서 네트워크(엣지)를 맞추는 예제로서 나옴, 그렇기에 네트워크 존재하지 않아 연구 예제로서 사용하지 않을 예정\n이 데이터는 네트워크를 추정하여 fit 및 predict함\nGOP growth rate time series\n\n35 countries from the OECD website\ntime series : 1961 - 2013\nT = 52\nNodes = 35\nIn this data set 20.8% (379 out of 1820) of the observations were missing due to some nodes not being included from the start.\n\n\n%%R\nlibrary(\"fields\")\n\n\n%R -o gdpVTS\n\n\ngdpVTS.shape\n\n(52, 35)\n\n\n\nplt.plot(gdpVTS[:,1])"
  },
  {
    "objectID": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html",
    "href": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "",
    "text": "ST-GCN Dataset WikiMathsDatasetLoader"
  },
  {
    "objectID": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#train",
    "href": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#train",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "Train",
    "text": "Train\n\ndata_train=[]\nfor time, snapshot in enumerate(train_dataset):\n    data_train.append([time,snapshot])\n\n\ndata_train[0][1].x.shape,data_train[0][1].y.shape,data_train[0][1].edge_index.shape,data_train[0][1].edge_attr.shape\n\n(torch.Size([1068, 1]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n583\n\n\n\nT_train = time\nN = len(data_train[0][1].x)\n\n\nedge_index = data_train[0][1].edge_index\nedge_attr = data_train[0][1].edge_attr\n\n\nx_train = []\nfor i in range(time):\n    x_train.append(data_train[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_train = data_tensor.reshape(time,1068,-1)\nx_train.shape\n\ntorch.Size([583, 1068, 1])\n\n\n\ny_train = []\nfor i in range(time):\n    y_train.append(data_train[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_train = data_tensor.reshape(time,1068)\ny_train.shape\n\ntorch.Size([583, 1068])\n\n\n\nx_train.shape, y_train.shape\n\n(torch.Size([583, 1068, 1]), torch.Size([583, 1068]))"
  },
  {
    "objectID": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#test",
    "href": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#test",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "Test",
    "text": "Test\n\ndata_test=[]\nfor time, snapshot in enumerate(test_dataset):\n    data_test.append([time,snapshot])\n\n\ndata_test[0][1].x.shape,data_test[0][1].y.shape,data_test[0][1].edge_index.shape,data_test[0][1].edge_attr.shape\n\n(torch.Size([1068, 1]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n145\n\n\n\nT_test = time\n\n\nx_test = []\nfor i in range(time):\n    x_test.append(data_test[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_test = data_tensor.reshape(time,1068,-1)\nx_test.shape\n\ntorch.Size([145, 1068, 1])\n\n\n\ny_test = []\nfor i in range(time):\n    y_test.append(data_test[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_test = data_tensor.reshape(time,1068)\ny_test.shape\n\ntorch.Size([145, 1068])\n\n\n\nx_test.shape, y_test.shape\n\n(torch.Size([145, 1068, 1]), torch.Size([145, 1068]))"
  },
  {
    "objectID": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#시나리오1-baseline",
    "href": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#시나리오1-baseline",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "시나리오1 (Baseline)",
    "text": "시나리오1 (Baseline)\n시나리오1\n\nmissing rate: 0%\n보간방법: None\n\n\nSTGCN 으로 적합 + 예측\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [05:31<00:00,  6.62s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nGNAR 으로 적합 + 예측\n-\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\n\n\nEdge = np.array(edge_index)\nX_gnar = np.array(x_train.squeeze())\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet <- matrixtoGNAR(w)\n\n\n%%R\nanswer <- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 1, betaOrder = c(1))\nprediction <- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean(axis=0)\ntest_mse_total_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean()\n\n\n\n결과시각화\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n GNAR: mse(train) = {3:.2f}, mse(test) = {4:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(range(1,583),stgcn_train[:,i],label='STCGCN (train)',color='C0')\n    a.plot(range(584,728),stgcn_test[:,i],label='STCGCN (test)',color='C0')\n    a.plot(range(1,583),gnar_train[:,i],label='GNAR (train)',color='C1')\n    a.plot(range(583,728),gnar_test[:,i],label='GNAR (test)',color='C1')\n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario1: STGCN \\n missing=0% \\n interpolation=None \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n GNAR: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#시나리오2",
    "href": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#시나리오2",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "시나리오2",
    "text": "시나리오2\n시나리오2\n\nmissing rate: 50%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(x_train.squeeze())\n_zero.miss(percent = 0.5)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],x_train[:,:,0][:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [05:34<00:00,  6.68s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nESTGCN 으로 적합 + 예측\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(T_train,N,1).float()[:-1,:,:]\n    y = torch.tensor(signal).reshape(T_train,N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [06:56<00:00,  8.33s/it]\n\n\n- ESTGCN\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nx_test.shape,y_test.shape\n\n(torch.Size([145, 1068, 1]), torch.Size([145, 1068]))\n\n\n\nreal_y = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nEdge = np.array(edge_index)\nX_gnar = np.array(x_train.squeeze())\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet <- matrixtoGNAR(w)\n\n\n%%R\nanswer <- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 1, betaOrder = c(1))\nprediction <- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean(axis=0)\ntest_mse_total_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean()\n\n\n\n결과시각화\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],x_train[missing_index[i],i,0],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,583),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(584,728),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,583),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(584,728),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(1,583),gnar_train.squeeze()[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(584,729),gnar_test.squeeze()[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#시나리오3",
    "href": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#시나리오3",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "시나리오3",
    "text": "시나리오3\n시나리오3\n\nmissing rate: 80%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(x_train.squeeze())\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],x_train.squeeze()[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [05:51<00:00,  7.04s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nESTGCN 으로 적합 + 예측\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(T_train,N,1).float()[:-1,:,:]\n    y = torch.tensor(signal).reshape(T_train,N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [07:00<00:00,  8.40s/it]\n\n\n- ESTGCN\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nEdge = np.array(edge_index)\nX_gnar = np.array(x_train.squeeze())\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet <- matrixtoGNAR(w)\n\n\n%%R\nanswer <- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 1, betaOrder = c(1))\nprediction <- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean(axis=0)\ntest_mse_total_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean()\n\n\n\n결과시각화\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],x_train[missing_index[i],i,0],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,583),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(584,728),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,583),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(584,728),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(1,583),gnar_train.squeeze()[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(584,729),gnar_test.squeeze()[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#시나리오4",
    "href": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#시나리오4",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "시나리오4",
    "text": "시나리오4\n시나리오4\n\nmissing rate: 30%\n보간방법: linear\n\n- 결측치생성 + 보간\n\n_zero = Missing(x_train.squeeze())\n_zero.miss(percent = 0.3)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],x_train.squeeze()[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN 으로 적합 + 예측\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [05:54<00:00,  7.09s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nESTGCN 으로 적합 + 예측\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(T_train,N,1).float()[:-1,:,:]\n    y = torch.tensor(signal).reshape(T_train,N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|██████████| 50/50 [07:00<00:00,  8.40s/it]\n\n\n- ESTGCN\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcn은 stgcn에 의한 적합결과를 의미함\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR 으로 적합 + 예측\n-\n\nEdge = np.array(edge_index)\nX_gnar = np.array(x_train.squeeze())\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet <- matrixtoGNAR(w)\n\n\n%%R\nanswer <- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 1, betaOrder = c(1))\nprediction <- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean(axis=0)\ntest_mse_total_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean()\n\n\n\n결과시각화\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],x_train[missing_index[i],i,0],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,583),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(584,728),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,583),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(584,728),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(1,583),gnar_train.squeeze()[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(584,729),gnar_test.squeeze()[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation.html",
    "href": "posts/GCN/2023-04-05-Simulation.html",
    "title": "Simulation",
    "section": "",
    "text": "Simulation Study"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation.html#random",
    "href": "posts/GCN/2023-04-05-Simulation.html#random",
    "title": "Simulation",
    "section": "random",
    "text": "random\n\ndf1 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_random_epoch50.csv')\ndf2 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_random_epoch100.csv')\ndf3 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_random_epoch150.csv')\ndf4 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_random_epoch200.csv')\n\n\ndf_gnar = pd.read_csv('./simulation_results/fivenodes/fivenodes_GNAR_random.csv')\n\n\ndata = pd.concat([df1,df2,df3,df4,df_gnar],axis=0)\n\n\ndata.query(\"method!='GNAR' and inter_method=='linear' and lags==2\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='epoch',facet_row='nof_filters',height=1200)\n\n\n                                                \n\n\n\n시뮬 예정(평균 시간, 평균mse)\n0.7,0.75,0.8,0.85\n12,16\n150\n\n# 1. mrate = 0.8, filter = 12, epoch = 150\ndata.query(\"mrate==0.8 and inter_method=='linear' and nof_filters==12 and epoch==150 and lags==2\")['calculation_time'].mean(),data.query(\"mrate==0.8 and inter_method=='linear' and nof_filters==12 and epoch==150 and lags==2\")['mse'].mean()\n\n(109.59549897114435, 1.2304790377616883)\n\n\n\ndata.query(\"mrate==0.8 and inter_method=='linear' and nof_filters==12 and epoch==150 and lags==2\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='epoch',facet_row='nof_filters',height=400)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation.html#block",
    "href": "posts/GCN/2023-04-05-Simulation.html#block",
    "title": "Simulation",
    "section": "block",
    "text": "block\n\ndf1 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_block_node1_epoch50.csv')\ndf2 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_block_node1_epoch100.csv')\ndf3 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_block_node1_epoch150.csv')\ndf4 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_block_node1_epoch200.csv')\ndf5 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_block_node2_epoch50.csv')\ndf6 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_block_node2_epoch100.csv')\ndf7 = pd.read_csv('./simulation_results/fivenodes/fivenodes_STGCN_ITSTGCN_block_node2_epoch150.csv')\ndf8 = pd.read_csv('./simulation_results/fivenodes/fivenodes_GNAR_block_node1.csv')\ndf9 = pd.read_csv('./simulation_results/fivenodes/fivenodes_GNAR_block_node2.csv')\n\n\ndf1['block']=1\ndf2['block']=1\ndf3['block']=1\ndf4['block']=1\ndf5['block']=2\ndf6['block']=2\ndf7['block']=2\ndf8['block']=1\ndf9['block']=2\n\n\ndata2 = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9],axis=0)\n\n\ndata2.query(\"method=='GNAR' and block == 1\")['mse'].mean(),data2.query(\"method=='GNAR' and block == 2\")['mse'].mean()\n\n(1.455923080444336, 1.5004450678825378)\n\n\n\ndata2.query(\"method=='GNAR' and inter_method == 'linear'\")['mse'].mean(),data2.query(\"method=='GNAR' and inter_method == 'nearest'\")['mse'].mean() # 차이 없음\n\n(1.4813642161233085, 1.4813642161233085)\n\n\n\ndata2.query(\"epoch==50\")['calculation_time'].mean(),data2.query(\"epoch==50\")['calculation_time'].max()\n\n(39.11611335332747, 56.8712797164917)\n\n\n\ndata2.query(\"epoch==150\")['calculation_time'].mean(),data2.query(\"epoch==150\")['calculation_time'].max()\n\n(102.26520284502594, 152.8869686126709)\n\n\n\ndata2.query(\"method!='GNAR' and lags == 2 and inter_method=='nearest'\").plot.box(backend='plotly',x='block',color='method',y='mse',facet_col='epoch',facet_row='nof_filters',height=800)\n\n\n                                                \n\n\n\ndata2.query(\"inter_method=='linear' and epoch==150\").plot.box(backend='plotly',x='block',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)\n\n\n                                                \n\n\n\n시뮬 예정(평균 시간, 평균mse)\nblock 1,2 위 세팅 그대로\n랜덤ㅁ 말고 block만\n\n# 1. block = 2 interpolation = linear, filter = 12, epoch = 150\ndata2.query(\"block==1 and inter_method=='linear' and nof_filters==12 and epoch==50 and lags==2\")['calculation_time'].mean(),data2.query(\"block==2 and inter_method=='linear' and nof_filters==12 and epoch==50 and lags==2\")['mse'].mean()\n\n(40.18422634204229, 1.2096982955932618)\n\n\n\ndata2.query(\"block==1 and inter_method=='linear' and nof_filters==12 and epoch==50 and lags==2\").plot.box(backend='plotly',x='block',color='method',y='mse',facet_col='epoch',facet_row='nof_filters',height=400)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation.html#random-1",
    "href": "posts/GCN/2023-04-05-Simulation.html#random-1",
    "title": "Simulation",
    "section": "random",
    "text": "random\n\n공식 패키지: lags 4 지정\nmrate = 0.3\n\n결측값 비율 크니까 오차 많이 커지는 경향 있어서\n\nnof_filters = 4\n\n차이 없어서\n\nlags = 4, 8\n\n클 수록 작아지는 경향 있어서\n\nGNAR보다 MSE는 낮음\ncal_time\n\nmean = 10\nmax = 21\n\nblock 은 임의로 한 노드만 해 본 결과임\n\n\ndata = pd.read_csv('./simulation_results/chickenpox_random.csv').sort_values(by='lags')\n\n\ndata.query(\"method!='GNAR'\")['calculation_time'].mean(),data.query(\"method!='GNAR'\")['calculation_time'].max(),data.query(\"method!='GNAR'\")['calculation_time'].min()\n\n(10.42619569649299, 21.886654376983643, 7.567165851593018)\n\n\n\ndata.query(\"method!='GNAR' and inter_method=='cubic' and mrate==0.3\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)\n\n\n                                                \n\n\n\ndata.query(\"method=='GNAR' and inter_method=='linear'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',height=600)\n\n\n                                                \n\n\n\n시뮬 예정(평균 시간, 평균mse)\nepoch = 50\nmrate = 0.3~0.5\nfilter 32 공식예제로 가기 하고 샆으면 3개 정도 추가로\n\n# 1. mrate = 0.3, filter = 4, epoch = 50, lags = 4\ndata.query(\"method !='GNAR' and mrate==0.3 and inter_method=='cubic' and nof_filters==4 and lags==2\")['calculation_time'].mean(),data.query(\"method != 'GNAR' and mrate==0.3 and inter_method=='cubic' and nof_filters==4 and lags==2\")['mse'].mean()\n\n(10.115000387032827, 1.0320488701264063)\n\n\n\ndata.query(\"method !='GNAR' and mrate==0.3 and inter_method=='cubic' and nof_filters==4 and lags==2\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=400)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation.html#block-1",
    "href": "posts/GCN/2023-04-05-Simulation.html#block-1",
    "title": "Simulation",
    "section": "block",
    "text": "block\n\ndata = pd.read_csv('./simulation_results/chickenpox_block.csv')\n\n\ndata.query(\"method != 'GNAR' and lags!=4 and lags!=6 and inter_method !='linear'\").plot.box(backend='plotly',x='nof_filters',color='method',y='mse',facet_col='lags',facet_row='inter_method',height=600)\n\n\n                                                \n\n\n\ndata.query(\"method=='GNAR'\").plot.box(backend='plotly',x='mrate',color='inter_method',y='mse',facet_col='lags',height=600)\n\n\n                                                \n\n\n\n시뮬 예정(평균 시간, 평균mse)\nblock, rand 다\n공식예제 수 따라\nepoch 50\n나중에 시간 남으면 100\n\ndata.query(\"inter_method=='cubic' and nof_filters==4 and lags==8\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=400)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation.html#block-2",
    "href": "posts/GCN/2023-04-05-Simulation.html#block-2",
    "title": "Simulation",
    "section": "block",
    "text": "block\n\ndata = pd.read_csv('./simulation_results/pedalme_block.csv');data\n\n\n\n\n\n  \n    \n      \n      dataset\n      method\n      mrate\n      mtype\n      lags\n      nof_filters\n      inter_method\n      epoch\n      mse\n      calculation_time\n    \n  \n  \n    \n      0\n      pedalme\n      IT-STGCN\n      0.047619\n      block\n      2\n      4.0\n      cubic\n      5.0\n      1.229210\n      0.758090\n    \n    \n      1\n      pedalme\n      STGCN\n      0.047619\n      block\n      2\n      12.0\n      linear\n      5.0\n      1.223644\n      0.681700\n    \n    \n      2\n      pedalme\n      STGCN\n      0.047619\n      block\n      2\n      12.0\n      cubic\n      5.0\n      1.237086\n      0.684113\n    \n    \n      3\n      pedalme\n      STGCN\n      0.047619\n      block\n      2\n      4.0\n      linear\n      5.0\n      1.225114\n      0.659210\n    \n    \n      4\n      pedalme\n      STGCN\n      0.047619\n      block\n      2\n      4.0\n      cubic\n      5.0\n      1.216191\n      0.664208\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      715\n      pedalme\n      IT-STGCN\n      0.045977\n      block\n      8\n      4.0\n      cubic\n      5.0\n      1.425474\n      0.640063\n    \n    \n      716\n      pedalme\n      STGCN\n      0.045977\n      block\n      8\n      12.0\n      cubic\n      5.0\n      1.302402\n      0.718187\n    \n    \n      717\n      pedalme\n      STGCN\n      0.045977\n      block\n      8\n      12.0\n      linear\n      5.0\n      1.336038\n      0.719500\n    \n    \n      718\n      pedalme\n      IT-STGCN\n      0.045977\n      block\n      8\n      12.0\n      linear\n      5.0\n      1.311962\n      0.831888\n    \n    \n      719\n      pedalme\n      IT-STGCN\n      0.045977\n      block\n      8\n      12.0\n      cubic\n      5.0\n      1.315647\n      0.667004\n    \n  \n\n720 rows × 10 columns\n\n\n\nmissing rate 조정하기 30~50% 여러개 block 해서\n\ndata.query(\"method!='GNAR'\").plot.box(backend='plotly',x='inter_method',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)\n\n\n                                                \n\n\n\n시뮬 예정(평균 시간, 평균mse)\n\ndata.query(\"inter_method=='linear' and nof_filters==12 and lags==4\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=400)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation.html#random-3",
    "href": "posts/GCN/2023-04-05-Simulation.html#random-3",
    "title": "Simulation",
    "section": "random",
    "text": "random\n\ndf1 = pd.read_csv('./simulation_results/2023-04-15_16-58-03.csv')\ndf2 = pd.read_csv('./simulation_results/2023-04-15_17-01-39.csv')\ndf3 = pd.read_csv('./simulation_results/2023-04-15_17-07-23.csv')\ndf4 = pd.read_csv('./simulation_results/2023-04-15_17-13-13.csv')\ndf5 = pd.read_csv('./simulation_results/2023-04-15_17-29-49.csv')\n\n\ndata = pd.concat([df1,df2,df3,df4,df5],axis=0)\n\n\ndata.query(\"method=='STGCN'\").sort_values(['mrate','lags','nof_filters'])\n\n\n\n\n\n  \n    \n      \n      dataset\n      method\n      mrate\n      mtype\n      lags\n      nof_filters\n      inter_method\n      epoch\n      mse\n      calculation_time\n    \n  \n  \n    \n      0\n      wikimath\n      STGCN\n      0.3\n      rand\n      4\n      12\n      linear\n      1\n      0.863623\n      25.504817\n    \n    \n      0\n      wikimath\n      STGCN\n      0.3\n      rand\n      4\n      12\n      cubic\n      1\n      0.847675\n      27.086116\n    \n    \n      0\n      wikimath\n      STGCN\n      0.4\n      rand\n      2\n      12\n      linear\n      1\n      0.912734\n      30.048937\n    \n    \n      1\n      wikimath\n      STGCN\n      0.4\n      rand\n      2\n      12\n      cubic\n      1\n      0.916843\n      27.104823\n    \n    \n      0\n      wikimath\n      STGCN\n      0.4\n      rand\n      4\n      12\n      linear\n      1\n      0.907305\n      24.776503\n    \n    \n      1\n      wikimath\n      STGCN\n      0.4\n      rand\n      4\n      12\n      cubic\n      1\n      0.854127\n      24.608104\n    \n    \n      2\n      wikimath\n      STGCN\n      0.4\n      rand\n      8\n      12\n      linear\n      1\n      0.788011\n      24.233431\n    \n    \n      3\n      wikimath\n      STGCN\n      0.4\n      rand\n      8\n      12\n      cubic\n      1\n      0.795219\n      24.228026\n    \n    \n      0\n      wikimath\n      STGCN\n      0.5\n      rand\n      4\n      12\n      linear\n      1\n      0.914080\n      26.301605\n    \n    \n      1\n      wikimath\n      STGCN\n      0.5\n      rand\n      4\n      12\n      cubic\n      1\n      0.975948\n      27.855870\n    \n  \n\n\n\n\n\ndata.query(\"method!='STGCN'\").sort_values(['mrate','lags','nof_filters'])\n\n\n\n\n\n  \n    \n      \n      dataset\n      method\n      mrate\n      mtype\n      lags\n      nof_filters\n      inter_method\n      epoch\n      mse\n      calculation_time\n    \n  \n  \n    \n      1\n      wikimath\n      IT-STGCN\n      0.3\n      rand\n      4\n      12\n      linear\n      1\n      0.908916\n      28.928112\n    \n    \n      1\n      wikimath\n      IT-STGCN\n      0.3\n      rand\n      4\n      12\n      cubic\n      1\n      0.856639\n      29.759748\n    \n    \n      4\n      wikimath\n      IT-STGCN\n      0.4\n      rand\n      2\n      12\n      linear\n      1\n      0.864580\n      29.660712\n    \n    \n      5\n      wikimath\n      IT-STGCN\n      0.4\n      rand\n      2\n      12\n      cubic\n      1\n      0.926426\n      30.838968\n    \n    \n      2\n      wikimath\n      IT-STGCN\n      0.4\n      rand\n      4\n      12\n      linear\n      1\n      0.871146\n      29.008776\n    \n    \n      3\n      wikimath\n      IT-STGCN\n      0.4\n      rand\n      4\n      12\n      cubic\n      1\n      0.905354\n      30.405766\n    \n    \n      6\n      wikimath\n      IT-STGCN\n      0.4\n      rand\n      8\n      12\n      linear\n      1\n      0.822462\n      32.329447\n    \n    \n      7\n      wikimath\n      IT-STGCN\n      0.4\n      rand\n      8\n      12\n      cubic\n      1\n      0.817621\n      29.447260\n    \n    \n      2\n      wikimath\n      IT-STGCN\n      0.5\n      rand\n      4\n      12\n      linear\n      1\n      0.878943\n      31.140878\n    \n    \n      3\n      wikimath\n      IT-STGCN\n      0.5\n      rand\n      4\n      12\n      cubic\n      1\n      1.002361\n      28.461372\n    \n  \n\n\n\n\n\ndata.query(\"method!='GNAR'\")['calculation_time'].mean(),data.query(\"method!='GNAR'\")['calculation_time'].max(),data.query(\"method!='GNAR'\")['calculation_time'].min()\n\n\ndata.query(\"mtype=='rand' and mrate != 0.9 and method!='GNAR' and inter_method=='cubic'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)\n\n\ndata.query(\"mtype=='rand' and method!='GNAR' and inter_method=='linear'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)\n\n\n시뮬 예정(평균 시간, 평균mse)\n\ndata.query(\"method !='GNAR' and mrate==0.3 and inter_method=='cubic' and nof_filters==12 and lags==8\")['calculation_time'].mean(),data.query(\"method !='GNAR' and mrate==0.3 and inter_method=='cubic' and nof_filters==12 and lags==8\")['mse'].mean()\n\n\ndata.query(\"method !='GNAR' and mrate==0.3 and inter_method=='linear' and nof_filters==12 and lags==8\")['calculation_time'].mean(),data.query(\"method !='GNAR' and mrate==0.3 and inter_method=='linear' and nof_filters==12 and lags==8\")['mse'].mean()\n\n\ndata.query(\"method !='GNAR' and mrate==0.3 and nof_filters==12 and lags==8\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=400)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation.html#block-3",
    "href": "posts/GCN/2023-04-05-Simulation.html#block-3",
    "title": "Simulation",
    "section": "block",
    "text": "block\n\ndata = pd.read_csv('./simulation_results/wiki_block.csv');data\n\n\ndata.query(\"method!='GNAR'\")['calculation_time'].mean(),data.query(\"method!='GNAR'\")['calculation_time'].max(),data.query(\"method!='GNAR'\")['calculation_time'].min()\n\n\ndata.query(\"method!='GNAR' and inter_method=='cubic'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)\n\n\ndata.query(\"inter_method=='linear'\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=1200)\n\n\n시뮬 예정(평균 시간, 평균mse)\n\ndata.query(\"inter_method=='linear' and nof_filters==12 and lags==8\")['calculation_time'].mean(),data.query(\"inter_method=='linear' and nof_filters==12 and lags==8\")['mse'].mean()\n\n\ndata.query(\"inter_method=='linear' and nof_filters==12 and lags==8\").plot.box(backend='plotly',x='mrate',color='method',y='mse',facet_col='lags',facet_row='nof_filters',height=400)"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation.html#random-4",
    "href": "posts/GCN/2023-04-05-Simulation.html#random-4",
    "title": "Simulation",
    "section": "random",
    "text": "random"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation.html#block-4",
    "href": "posts/GCN/2023-04-05-Simulation.html#block-4",
    "title": "Simulation",
    "section": "block",
    "text": "block"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation.html#random-5",
    "href": "posts/GCN/2023-04-05-Simulation.html#random-5",
    "title": "Simulation",
    "section": "random",
    "text": "random"
  },
  {
    "objectID": "posts/GCN/2023-04-05-Simulation.html#block-5",
    "href": "posts/GCN/2023-04-05-Simulation.html#block-5",
    "title": "Simulation",
    "section": "block",
    "text": "block"
  },
  {
    "objectID": "posts/GCN/2023-05-11-PyGGeometricTemporalEx.html",
    "href": "posts/GCN/2023-05-11-PyGGeometricTemporalEx.html",
    "title": "PyG Geometric Temporal Examples",
    "section": "",
    "text": "Examples\n\nRefer: https://github.com/benedekrozemberczki/pytorch_geometric_temporal/tree/master/examples/recurrent\n\nA3GCN2\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric_temporal.nn.recurrent import A3TGCN2\n\n\n# GPU support\nDEVICE = torch.device('cuda') # cuda\nshuffle=True\nbatch_size = 32\n\n\n#Dataset\n#Traffic forecasting dataset based on Los Angeles Metropolitan traffic\n#207 loop detectors on highways\n#March 2012 - June 2012\n#From the paper: Diffusion Convolutional Recurrent Neural Network\n\n\nfrom torch_geometric_temporal.dataset import METRLADatasetLoader\nloader = METRLADatasetLoader()\ndataset = loader.get_dataset(num_timesteps_in=12, num_timesteps_out=12)\n\n\n# Visualize traffic over time\nsensor_number = 1\nhours = 24\nsensor_labels = [bucket.y[sensor_number][0].item() for bucket in list(dataset)[:hours]]\nplt.plot(sensor_labels)\n\n\n\n\n\n# Train test split \n\nfrom torch_geometric_temporal.signal import temporal_signal_split\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)\n\n\n# Creating Dataloaders\n\ntrain_input = np.array(train_dataset.features) # (27399, 207, 2, 12)\ntrain_target = np.array(train_dataset.targets) # (27399, 207, 12)\ntrain_x_tensor = torch.from_numpy(train_input).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\ntrain_target_tensor = torch.from_numpy(train_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\ntrain_dataset_new = torch.utils.data.TensorDataset(train_x_tensor, train_target_tensor)\ntrain_loader = torch.utils.data.DataLoader(train_dataset_new, batch_size=batch_size, shuffle=shuffle,drop_last=True)\n\n\ntest_input = np.array(test_dataset.features) # (, 207, 2, 12)\ntest_target = np.array(test_dataset.targets) # (, 207, 12)\ntest_x_tensor = torch.from_numpy(test_input).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\ntest_target_tensor = torch.from_numpy(test_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\ntest_dataset_new = torch.utils.data.TensorDataset(test_x_tensor, test_target_tensor)\ntest_loader = torch.utils.data.DataLoader(test_dataset_new, batch_size=batch_size, shuffle=shuffle,drop_last=True)\n\n\n# Making the model \nclass TemporalGNN(torch.nn.Module):\n    def __init__(self, node_features, periods, batch_size):\n        super(TemporalGNN, self).__init__()\n        # Attention Temporal Graph Convolutional Cell\n        self.tgnn = A3TGCN2(in_channels=node_features,  out_channels=32, periods=periods,batch_size=batch_size) # node_features=2, periods=12\n        # Equals single-shot prediction\n        self.linear = torch.nn.Linear(32, periods)\n\n    def forward(self, x, edge_index):\n        \"\"\"\n        x = Node features for T time steps\n        edge_index = Graph edge indices\n        \"\"\"\n        h = self.tgnn(x, edge_index) # x [b, 207, 2, 12]  returns h [b, 207, 12]\n        h = F.relu(h) \n        h = self.linear(h)\n        return h\n\n\nTemporalGNN(node_features=2, periods=12, batch_size=2)\n\nTemporalGNN(\n  (tgnn): A3TGCN2(\n    (_base_tgcn): TGCN2(\n      (conv_z): GCNConv(2, 32)\n      (linear_z): Linear(in_features=64, out_features=32, bias=True)\n      (conv_r): GCNConv(2, 32)\n      (linear_r): Linear(in_features=64, out_features=32, bias=True)\n      (conv_h): GCNConv(2, 32)\n      (linear_h): Linear(in_features=64, out_features=32, bias=True)\n    )\n  )\n  (linear): Linear(in_features=32, out_features=12, bias=True)\n)\n\n\n\n# Create model and optimizers\nmodel = TemporalGNN(node_features=2, periods=12, batch_size=batch_size).to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nloss_fn = torch.nn.MSELoss()\n\n\nprint('Net\\'s state_dict:')\ntotal_param = 0\nfor param_tensor in model.state_dict():\n    print(param_tensor, '\\t', model.state_dict()[param_tensor].size())\n    total_param += np.prod(model.state_dict()[param_tensor].size())\nprint('Net\\'s total params:', total_param)\n#--------------------------------------------------\nprint('Optimizer\\'s state_dict:')  # If you notice here the Attention is a trainable parameter\nfor var_name in optimizer.state_dict():\n    print(var_name, '\\t', optimizer.state_dict()[var_name])\n\nNet's state_dict:\ntgnn._attention      torch.Size([12])\ntgnn._base_tgcn.conv_z.bias      torch.Size([32])\ntgnn._base_tgcn.conv_z.lin.weight    torch.Size([32, 2])\ntgnn._base_tgcn.linear_z.weight      torch.Size([32, 64])\ntgnn._base_tgcn.linear_z.bias    torch.Size([32])\ntgnn._base_tgcn.conv_r.bias      torch.Size([32])\ntgnn._base_tgcn.conv_r.lin.weight    torch.Size([32, 2])\ntgnn._base_tgcn.linear_r.weight      torch.Size([32, 64])\ntgnn._base_tgcn.linear_r.bias    torch.Size([32])\ntgnn._base_tgcn.conv_h.bias      torch.Size([32])\ntgnn._base_tgcn.conv_h.lin.weight    torch.Size([32, 2])\ntgnn._base_tgcn.linear_h.weight      torch.Size([32, 64])\ntgnn._base_tgcn.linear_h.bias    torch.Size([32])\nlinear.weight    torch.Size([12, 32])\nlinear.bias      torch.Size([12])\nNet's total params: 6936\nOptimizer's state_dict:\nstate    {}\nparam_groups     [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]}]\n\n\n\n# Loading the graph once because it's a static graph\n\nfor snapshot in train_dataset:\n    static_edge_index = snapshot.edge_index.to(DEVICE)\n    break;\n\n\n# Training the model \nmodel.train()\n\nfor epoch in range(3): # 30\n    step = 0\n    loss_list = []\n    for encoder_inputs, labels in train_loader:\n        y_hat = model(encoder_inputs, static_edge_index)         # Get model predictions\n        loss = loss_fn(y_hat, labels) # Mean squared error #loss = torch.mean((y_hat-labels)**2)  sqrt to change it to rmse\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        step= step+ 1\n        loss_list.append(loss.item())\n        if step % 100 == 0 :\n            print(sum(loss_list)/len(loss_list))\n    print(\"Epoch {} train RMSE: {:.4f}\".format(epoch, sum(loss_list)/len(loss_list)))\n\n0.44013768196105957\n0.44626042172312735\n0.4474838371078173\n0.44511159002780915\n0.4446671283841133\n0.4492054430147012\n0.4518048374993461\n0.4486549423262477\nEpoch 0 train RMSE: 0.4478\n0.45916826501488683\n0.451208501085639\n0.4479588058094184\n0.4489660017192364\n0.4470058409571648\n0.44722517212231955\n0.4471735526408468\n0.4478337676823139\nEpoch 1 train RMSE: 0.4471\n0.45132808953523634\n0.4424465082585812\n0.44844810595115026\n0.44880970306694506\n0.44570656085014343\n0.4443529421339432\n0.44464062641773905\n0.4452343595586717\nEpoch 2 train RMSE: 0.4465\n\n\n\n## Evaluation\n\n#- Lets get some sample predictions for a specific horizon (e.g. 288/12 = 24 hours)\n#- The model always gets one hour and needs to predict the next hour\n\n\nmodel.eval()\nstep = 0\n# Store for analysis\ntotal_loss = []\nfor encoder_inputs, labels in test_loader:\n    # Get model predictions\n    y_hat = model(encoder_inputs, static_edge_index)\n    # Mean squared error\n    loss = loss_fn(y_hat, labels)\n    total_loss.append(loss.item())\n    # Store for analysis below\n    #test_labels.append(labels)\n    #predictions.append(y_hat)\n\n\nprint(\"Test MSE: {:.4f}\".format(sum(total_loss)/len(total_loss)))\n\nTest MSE: 0.5491\n\n\n\n### Visualization\n\n#- The further away the point in time is, the worse the predictions get\n#- Predictions shape: [num_data_points, num_sensors, num_timesteps]\n\n\nsensor = 123\ntimestep = 11 \npreds = np.asarray([pred[sensor][timestep].detach().cpu().numpy() for pred in y_hat])\nlabs  = np.asarray([label[sensor][timestep].cpu().numpy() for label in labels])\nprint(\"Data points:,\", preds.shape)\n\nData points:, (32,)\n\n\n\nplt.figure(figsize=(20,5))\nsns.lineplot(data=preds, label=\"pred\")\nsns.lineplot(data=labs, label=\"true\")\n\n<AxesSubplot:>\n\n\n\n\n\n\n\nA3GCN(cuda 문제)\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import A3TGCN\n\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, periods):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = A3TGCN(node_features, 32, periods)\n        self.linear = torch.nn.Linear(32, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x.to(\"cuda:0\").view(x.shape[0], 1, x.shape[1]), edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\nmodel = RecurrentGCN(node_features = 1, periods = 4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    cost = 0\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\n\n\nAGCRN\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import AGCRN\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset(lags=8)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = AGCRN(number_of_nodes = 20,\n                              in_channels = node_features,\n                              out_channels = 2,\n                              K = 2,\n                              embedding_dimensions = 4)\n        self.linear = torch.nn.Linear(2, 1)\n\n    def forward(self, x, e, h):\n        h_0 = self.recurrent(x, e, h)\n        y = F.relu(h_0)\n        y = self.linear(y)\n        return y, h_0\n\n\nmodel = RecurrentGCN(node_features = 8)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\ne = torch.empty(20, 4)\n\ntorch.nn.init.xavier_uniform_(e)\n\nfor epoch in tqdm(range(200)):\n    cost = 0\n    h = None\n    for time, snapshot in enumerate(train_dataset):\n        x = snapshot.x.view(1, 20, 8)\n        y_hat, h = model(x, e, h)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 200/200 [01:39<00:00,  2.00it/s]\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    x = snapshot.x.view(1, 20, 8)\n    y_hat, h = model(x, e, h)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 1.0500\n\n\n\n\nDCRNN(추가함)\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import DCRNN\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = DCRNN(node_features, 32, 1)\n        self.linear = torch.nn.Linear(32, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\nmodel = RecurrentGCN(node_features = 4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(200)):\n    cost = 0\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 200/200 [01:42<00:00,  1.95it/s]\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 1.0212\n\n\n\n\nDYGRENCODER\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import DyGrEncoder\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = DyGrEncoder(conv_out_channels=4, conv_num_layers=1, conv_aggr=\"mean\", lstm_out_channels=32, lstm_num_layers=1)\n        self.linear = torch.nn.Linear(32, 1)\n\n    def forward(self, x, edge_index, edge_weight, h_0, c_0):\n        h, h_0, c_0 = self.recurrent(x, edge_index, edge_weight, h_0, c_0)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h, h_0, c_0\n\n\nmodel = RecurrentGCN(node_features = 4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(200)):\n    cost = 0\n    h, c = None, None\n    for time, snapshot in enumerate(train_dataset):\n        y_hat, h, c = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, h, c)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 200/200 [00:41<00:00,  4.84it/s]\n\n\n\nmodel.eval()\ncost = 0\nh, c = None, None\nfor time, snapshot in enumerate(test_dataset):\n    y_hat, h, c = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, h, c)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 1.0133\n\n\n\n\nEVOLVGVNH\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import EvolveGCNH\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_count, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = EvolveGCNH(node_count, node_features)\n        self.linear = torch.nn.Linear(node_features, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\nmodel = RecurrentGCN(node_features = 4, node_count = 20)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(200)):\n    cost = 0\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 200/200 [00:39<00:00,  5.07it/s]\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 1.0187\n\n\n\n\nEVOLVEGCNO\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import EvolveGCNO\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = EvolveGCNO(node_features)\n        self.linear = torch.nn.Linear(node_features, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\nmodel = RecurrentGCN(node_features = 4)\nfor param in model.parameters():\n    param.retain_grad()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(200)):\n    cost = 0\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    cost = cost / (time+1)\n    cost.backward(retain_graph=True)\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 200/200 [00:22<00:00,  9.01it/s]\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    if time == 0:\n        model.recurrent.weight = None\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 0.9990\n\n\n\n\nGCLSTM\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GCLSTM\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GCLSTM(node_features, 32, 1)\n        self.linear = torch.nn.Linear(32, 1)\n\n    def forward(self, x, edge_index, edge_weight, h, c):\n        h_0, c_0 = self.recurrent(x, edge_index, edge_weight, h, c)\n        h = F.relu(h_0)\n        h = self.linear(h)\n        return h, h_0, c_0\n\n\nmodel = RecurrentGCN(node_features=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(200)):\n    cost = 0\n    h, c = None, None\n    for time, snapshot in enumerate(train_dataset):\n        y_hat, h, c = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, h, c)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 200/200 [00:45<00:00,  4.36it/s]\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat, h, c = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, h, c)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 1.0865\n\n\n\n\nGConvLSTM\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvLSTM\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvLSTM(node_features, 32, 1)\n        self.linear = torch.nn.Linear(32, 1)\n\n    def forward(self, x, edge_index, edge_weight, h, c):\n        h_0, c_0 = self.recurrent(x, edge_index, edge_weight, h, c)\n        h = F.relu(h_0)\n        h = self.linear(h)\n        return h, h_0, c_0\n\n\nmodel = RecurrentGCN(node_features=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(200)):\n    cost = 0\n    h, c = None, None\n    for time, snapshot in enumerate(train_dataset):\n        y_hat, h, c = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, h, c)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 200/200 [03:28<00:00,  1.04s/it]\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat, h, c = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, h, c)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 1.0877\n\n\n\n\nLightning(설치 안 됨)\n\nimport torch\nfrom torch.nn import functional as F\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\nfrom torch_geometric_temporal.nn.recurrent import DCRNN\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\n\nclass LitDiffConvModel(pl.LightningModule):\n\n    def __init__(self, node_features, filters):\n        super().__init__()\n        self.recurrent = DCRNN(node_features, filters, 1)\n        self.linear = torch.nn.Linear(filters, 1)\n\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)\n        return optimizer\n\n    def training_step(self, train_batch, batch_idx):\n        x = train_batch.x\n        y = train_batch.y.view(-1, 1)\n        edge_index = train_batch.edge_index\n        h = self.recurrent(x, edge_index)\n        h = F.relu(h)\n        h = self.linear(h)\n        loss = F.mse_loss(h, y)\n        return loss\n\n    def validation_step(self, val_batch, batch_idx):\n        x = val_batch.x\n        y = val_batch.y.view(-1, 1)\n        edge_index = val_batch.edge_index\n        h = self.recurrent(x, edge_index)\n        h = F.relu(h)\n        h = self.linear(h)\n        loss = F.mse_loss(h, y)\n        metrics = {'val_loss': loss}\n        self.log_dict(metrics)\n        return metrics\n\n\nloader = ChickenpoxDatasetLoader()\n\ndataset_loader = loader.get_dataset(lags=32)\n\ntrain_loader, val_loader = temporal_signal_split(dataset_loader,\n                                                 train_ratio=0.2)\n\n\nmodel = LitDiffConvModel(node_features=32,\n                         filters=16)\n\n\nearly_stop_callback = EarlyStopping(monitor='val_loss',\n                                    min_delta=0.00,\n                                    patience=10,\n                                    verbose=False,\n                                    mode='max')\n\n\ntrainer = pl.Trainer(callbacks=[early_stop_callback])\n\n\ntrainer.fit(model, train_loader, val_loader)\n\n\n\nLRGCN\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import LRGCN\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = LRGCN(node_features, 32, 1, 1)\n        self.linear = torch.nn.Linear(32, 1)\n\n    def forward(self, x, edge_index, edge_weight, h_0, c_0):\n        h_0, c_0 = self.recurrent(x, edge_index, edge_weight, h_0, c_0)\n        h = F.relu(h_0)\n        h = self.linear(h)\n        return h, h_0, c_0\n\n\nmodel = RecurrentGCN(node_features = 4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(200)):\n    cost = 0\n    h, c = None, None\n    for time, snapshot in enumerate(train_dataset):\n        y_hat, h, c = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, h, c)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 200/200 [01:01<00:00,  3.26it/s]\n\n\n\nmodel.eval()\ncost = 0\nh, c = None, None\nfor time, snapshot in enumerate(test_dataset):\n    y_hat, h, c = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, h, c)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 1.0741\n\n\n\n\nMPNNLSTM\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import MPNNLSTM\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = MPNNLSTM(node_features, 32,  20, 1, 0.5) # 32, 32, 20, 1, 0.5 이었는데 position 잘못되었다해서 32하나 뺌\n        self.linear = torch.nn.Linear(2*32 + node_features, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\nmodel = RecurrentGCN(node_features = 4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    cost = 0\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:53<00:00,  1.07s/it]\n\n\n\nmodel.eval()\ncost = 0\nfor time, snapshot in enumerate(test_dataset):\n    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 1.0783\n\n\n\n\nTGCN\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable):\n        return iterable\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import TGCN\n\nfrom torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = TGCN(node_features, 32)\n        self.linear = torch.nn.Linear(32, 1)\n\n    def forward(self, x, edge_index, edge_weight, prev_hidden_state):\n        h = self.recurrent(x, edge_index, edge_weight, prev_hidden_state)\n        y = F.relu(h)\n        y = self.linear(y)\n        return y, h\n\n\nmodel = RecurrentGCN(node_features = 4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    cost = 0\n    hidden_state = None\n    for time, snapshot in enumerate(train_dataset):\n        y_hat, hidden_state = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr,hidden_state)\n        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n    cost = cost / (time+1)\n    cost.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:15<00:00,  3.30it/s]\n\n\n\nmodel.eval()\ncost = 0\nhidden_state = None\nfor time, snapshot in enumerate(test_dataset):\n    y_hat, hidden_state = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, hidden_state)\n    cost = cost + torch.mean((y_hat-snapshot.y)**2)\ncost = cost / (time+1)\ncost = cost.item()\nprint(\"MSE: {:.4f}\".format(cost))\n\nMSE: 1.0087"
  },
  {
    "objectID": "posts/GCN/2022-12-29-STGCN-tutorial.html",
    "href": "posts/GCN/2022-12-29-STGCN-tutorial.html",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "",
    "text": "Simulation"
  },
  {
    "objectID": "posts/GCN/2022-12-29-STGCN-tutorial.html#pyg-의-data-자료형",
    "href": "posts/GCN/2022-12-29-STGCN-tutorial.html#pyg-의-data-자료형",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "PyG 의 Data 자료형",
    "text": "PyG 의 Data 자료형\n\nref: https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#data-handling-of-graphs\n\n- 자료는 PyG의 Data 오브젝트를 기반으로 한다.\n(예제) 아래와 같은 그래프자료를 고려하자.\n\n이러한 자료형은 아래와 같은 형식으로 저장한다.\n\nedge_index = torch.tensor([[0, 1, 1, 2],\n                           [1, 0, 2, 1]], dtype=torch.long)\nx = torch.tensor([[-1], [0], [1]], dtype=torch.float)\ndata = Data(x=x, edge_index=edge_index) # Data는 그래프자료형을 만드는 클래스\n\n\ntype(data)\n\ntorch_geometric.data.data.Data\n\n\n\ndata.x\n\ntensor([[-1.],\n        [ 0.],\n        [ 1.]])\n\n\n\ndata.edge_index\n\ntensor([[0, 1, 1, 2],\n        [1, 0, 2, 1]])"
  },
  {
    "objectID": "posts/GCN/2022-12-29-STGCN-tutorial.html#pytorch-geometric-temporal-의-자료형",
    "href": "posts/GCN/2022-12-29-STGCN-tutorial.html#pytorch-geometric-temporal-의-자료형",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "PyTorch Geometric Temporal 의 자료형",
    "text": "PyTorch Geometric Temporal 의 자료형\n\nref: PyTorch Geometric Temporal Signal\n\n아래의 클래스들중 하나를 이용하여 만든다.\n## Temporal Signal Iterators\ntorch_geometric_temporal.signal.StaticGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphStaticSignal\n## Heterogeneous Temporal Signal Iterators\ntorch_geometric_temporal.signal.StaticHeteroGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicHeteroGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicHeteroGraphStaticSignal\n이중 “Heterogeneous Temporal Signal” 은 우리가 관심이 있는 신호가 아니므로 사실상 아래의 3개만 고려하면 된다.\n\ntorch_geometric_temporal.signal.StaticGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphStaticSignal\n\n여기에서 StaticGraphTemporalSignal 는 시간에 따라서 그래프 구조가 일정한 경우, 즉 \\({\\cal G}_t=\\{{\\cal V},{\\cal E}\\}\\)와 같은 구조를 의미한다.\n(예제1) StaticGraphTemporalSignal 를 이용하여 데이터 셋 만들기\n- json data \\(\\to\\) dict\n\nimport json\nimport urllib\n\n\nurl = \"https://raw.githubusercontent.com/benedekrozemberczki/pytorch_geometric_temporal/master/dataset/chickenpox.json\"\ndata_dict = json.loads(urllib.request.urlopen(url).read())\n# data_dict 출력이 김\n\n\ndata_dict.keys()\n\ndict_keys(['edges', 'node_ids', 'FX'])\n\n\n- 살펴보기\n\nnp.array(data_dict['edges']).T\n\narray([[ 0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,\n         3,  3,  3,  3,  3,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,\n         6,  6,  7,  7,  7,  7,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,\n        10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 12, 12, 12,\n        12, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15,\n        15, 15, 16, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 18,\n        18, 18, 19, 19, 19, 19],\n       [10,  6, 13,  1,  0,  5, 16,  0, 16,  1, 14, 10,  8,  2,  5,  8,\n        15, 12,  9, 10,  3,  4, 13,  0, 10,  2,  5,  0, 16,  6, 14, 13,\n        11, 18,  7, 17, 11, 18,  3,  2, 15,  8, 10,  9, 13,  3, 12, 10,\n         5,  9,  8,  3, 10,  2, 13,  0,  6, 11,  7, 13, 18,  3,  9, 13,\n        12, 13,  9,  6,  4, 12,  0, 11, 10, 18, 19,  1, 14,  6, 16,  3,\n        15,  8, 16, 14,  1,  0,  6,  7, 19, 17, 18, 14, 18, 17,  7,  6,\n        19, 11, 18, 14, 19, 17]])\n\n\n\n\\({\\cal E} = \\{(0,10),(0,6), \\dots, (19,17)\\}\\)\n혹은 \\({\\cal E} = \\{(\\tt{BACS},\\tt{JASZ}), ({\\tt BACS},{\\tt FEJER}), \\dots, (\\tt{ZALA},\\tt{VAS})\\}\\)\n\n\ndata_dict['node_ids']\n\n{'BACS': 0,\n 'BARANYA': 1,\n 'BEKES': 2,\n 'BORSOD': 3,\n 'BUDAPEST': 4,\n 'CSONGRAD': 5,\n 'FEJER': 6,\n 'GYOR': 7,\n 'HAJDU': 8,\n 'HEVES': 9,\n 'JASZ': 10,\n 'KOMAROM': 11,\n 'NOGRAD': 12,\n 'PEST': 13,\n 'SOMOGY': 14,\n 'SZABOLCS': 15,\n 'TOLNA': 16,\n 'VAS': 17,\n 'VESZPREM': 18,\n 'ZALA': 19}\n\n\n\n\\({\\cal V}=\\{\\tt{BACS},\\tt{BARANYA} \\dots, \\tt{ZALA}\\}\\)\n\n\nnp.array(data_dict['FX']), np.array(data_dict['FX']).shape\n\n(array([[-1.08135724e-03, -7.11136085e-01, -3.22808515e+00, ...,\n          1.09445310e+00, -7.08747750e-01, -1.82280792e+00],\n        [ 2.85705967e-02, -5.98430173e-01, -2.29097341e-01, ...,\n         -1.59220988e+00, -2.24597623e-01,  7.86330575e-01],\n        [ 3.54742090e-01,  1.90511208e-01,  1.61028185e+00, ...,\n          1.38183225e-01, -7.08747750e-01, -5.61724314e-01],\n        ...,\n        [-4.75512620e-01, -1.19952837e+00, -3.89043358e-01, ...,\n         -1.00023329e+00, -1.71429032e+00,  4.70746677e-02],\n        [-2.08645035e-01,  6.03766218e-01,  1.08216835e-02, ...,\n          4.71099041e-02,  2.45684924e+00, -3.44296107e-01],\n        [ 1.21464875e+00,  7.16472130e-01,  1.29038982e+00, ...,\n          4.56939849e-01,  7.43702632e-01,  1.00375878e+00]]),\n (521, 20))\n\n\n\n\\({\\bf f}=\\begin{bmatrix} {\\bf f}_1\\\\ {\\bf f}_2\\\\ \\dots \\\\ {\\bf f}_{521} \\end{bmatrix}=\\begin{bmatrix} f(t=1,v=\\tt{BACS}) & \\dots & f(t=1,v=\\tt{ZALA}) \\\\ f(t=2,v=\\tt{BACS}) & \\dots & f(t=2,v=\\tt{ZALA}) \\\\ \\dots & \\dots & \\dots \\\\ f(t=521,v=\\tt{BACS}) & \\dots & f(t=521,v=\\tt{ZALA}) \\end{bmatrix}\\)\n\n즉 data_dict는 아래와 같이 구성되어 있음\n\n\n\n\n\n\n\n\n\n\n수학 기호\n코드에 저장된 변수\n자료형\n차원\n설명\n\n\n\n\n\\({\\cal V}\\)\ndata_dict['node_ids']\ndict\n20\n20개의 노드에 대한 설명이 있음\n\n\n\\({\\cal E}\\)\ndata_dict['edges']\nlist (double list)\n(102,2)\n노드들에 대한 102개의 연결을 정의함\n\n\n\\({\\bf f}\\)\ndata_dict['node_ids']\ndict\n(521,20)\n\\(f(t,v)\\) for \\(v \\in {\\cal V}\\) and \\(t = 1,\\dots, T\\)\n\n\n\n- 주어진 자료를 정리하여 그래프신호 \\(\\big(\\{{\\cal V},{\\cal E},{\\bf W}\\},{\\bf f}\\big)\\)를 만들면 아래와 같다.\n\nedges = np.array(data_dict[\"edges\"]).T\nedge_weight = np.ones(edges.shape[1])\nf = np.array(data_dict[\"FX\"])\n\n\n여기에서 edges는 \\({\\cal E}\\)에 대한 정보를\nedges_weight는 \\({\\bf W}\\)에 대한 정보를\nf는 \\({\\bf f}\\)에 대한 정보를 저장한다.\n\n\nNote: 이때 \\({\\bf W}={\\bf E}\\) 로 정의한다. (하지만 꼭 이래야 하는건 아니야)\n\n- data_dict \\(\\to\\) dl\n\nlags = 4\nfeatures = [f[i : i + lags, :].T for i in range(f.shape[0] - lags)]\ntargets = [f[i + lags, :].T for i in range(f.shape[0] - lags)]\n\n\nnp.array(features).shape, np.array(targets).shape\n\n((517, 20, 4), (517, 20))\n\n\n\n\n\n\n\n\n\n설명변수\n반응변수\n\n\n\n\n\\({\\bf X} = {\\tt features} = \\begin{bmatrix} {\\bf f}_1 & {\\bf f}_2 & {\\bf f}_3 & {\\bf f}_4 \\\\ {\\bf f}_2 & {\\bf f}_3 & {\\bf f}_4 & {\\bf f}_5 \\\\ \\dots & \\dots & \\dots & \\dots \\\\ {\\bf f}_{517} & {\\bf f}_{518} & {\\bf f}_{519} & {\\bf f}_{520} \\end{bmatrix}\\)\n\\({\\bf y}= {\\tt targets} = \\begin{bmatrix} {\\bf f}_5 \\\\ {\\bf f}_6 \\\\ \\dots \\\\ {\\bf f}_{521} \\end{bmatrix}\\)\n\n\n\n\nAR 느낌으로 표현하면 AR(4) 임\n\n\ndataset = torch_geometric_temporal.signal.StaticGraphTemporalSignal(\n    edge_index= edges,\n    edge_weight = edge_weight,\n    features = features,\n    targets = targets\n)\n\n\ndataset\n\n<torch_geometric_temporal.signal.static_graph_temporal_signal.StaticGraphTemporalSignal at 0x7f3423668bd0>\n\n\n- 그런데 이 과정을 아래와 같이 할 수도 있음\n# PyTorch Geometric Temporal 공식홈페이지에 소개된 코드\nloader = torch_geometric_temporal.dataset.ChickenpoxDatasetLoader()\ndataset=loader.get_dataset(lags=4)\n- dataset은 dataset[0], \\(\\dots\\) , dataset[516]과 같은 방식으로 각 시점별 자료에 접근가능\n\ndataset[0]\n\nData(x=[20, 4], edge_index=[2, 102], edge_attr=[102], y=[20])\n\n\n각 시점에 대한 자료형은 아까 살펴보았던 PyG의 Data 자료형과 같음\n\ntype(dataset[0])\n\ntorch_geometric.data.data.Data\n\n\n\ndataset[0].x \n\ntensor([[-1.0814e-03,  2.8571e-02,  3.5474e-01,  2.9544e-01],\n        [-7.1114e-01, -5.9843e-01,  1.9051e-01,  1.0922e+00],\n        [-3.2281e+00, -2.2910e-01,  1.6103e+00, -1.5487e+00],\n        [ 6.4750e-01, -2.2117e+00, -9.6858e-01,  1.1862e+00],\n        [-1.7302e-01, -9.4717e-01,  1.0347e+00, -6.3751e-01],\n        [ 3.6345e-01, -7.5468e-01,  2.9768e-01, -1.6273e-01],\n        [-3.4174e+00,  1.7031e+00, -1.6434e+00,  1.7434e+00],\n        [-1.9641e+00,  5.5208e-01,  1.1811e+00,  6.7002e-01],\n        [-2.2133e+00,  3.0492e+00, -2.3839e+00,  1.8545e+00],\n        [-3.3141e-01,  9.5218e-01, -3.7281e-01, -8.2971e-02],\n        [-1.8380e+00, -5.8728e-01, -3.5514e-02, -7.2298e-02],\n        [-3.4669e-01, -1.9827e-01,  3.9540e-01, -2.4774e-01],\n        [ 1.4219e+00, -1.3266e+00,  5.2338e-01, -1.6374e-01],\n        [-7.7044e-01,  3.2872e-01, -1.0400e+00,  3.4945e-01],\n        [-7.8061e-01, -6.5022e-01,  1.4361e+00, -1.2864e-01],\n        [-1.0993e+00,  1.2732e-01,  5.3621e-01,  1.9023e-01],\n        [ 2.4583e+00, -1.7811e+00,  5.0732e-02, -9.4371e-01],\n        [ 1.0945e+00, -1.5922e+00,  1.3818e-01,  1.1855e+00],\n        [-7.0875e-01, -2.2460e-01, -7.0875e-01,  1.5630e+00],\n        [-1.8228e+00,  7.8633e-01, -5.6172e-01,  1.2647e+00]])\n\n\n\n이 값들은 features[0]의 값들과 같음. 즉 \\([{\\bf f}_1~ {\\bf f}_2~ {\\bf f}_3~ {\\bf f}_4]\\)를 의미함\n\n\ndataset[0].y\n\ntensor([ 0.7106, -0.0725,  2.6099,  1.7870,  0.8024, -0.2614, -0.8370,  1.9674,\n        -0.4212,  0.1655,  1.2519,  2.3743,  0.7877,  0.4531, -0.1721, -0.0614,\n         1.0452,  0.3203, -1.3791,  0.0036])\n\n\n\n이 값들은 targets[0]의 값들과 같음. 즉 \\({\\bf f}_5\\)를 의미함"
  },
  {
    "objectID": "posts/GCN/2022-12-29-STGCN-tutorial.html#summary-of-data",
    "href": "posts/GCN/2022-12-29-STGCN-tutorial.html#summary-of-data",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "summary of data",
    "text": "summary of data\n\n\\(T\\) = 519\n\\(N\\) = 20 # number of nodes\n\\(|{\\cal E}|\\) = 102 # edges\n\\(f(t,v)\\)의 차원? (1,)\n시간에 따라서 Number of nodes가 변하는지? False\n시간에 따라서 Number of nodes가 변하는지? False\n\\({\\bf X}\\): (20,4)\n\\({\\bf y}\\): (20,)\n예제코드적용가능여부: Yes\n\n- Nodes : 20\n\nvertices are counties\n\n-Edges : 102\n\nedges are neighbourhoods\n\n- Time : 519\n\nbetween 2004 and 2014\nper weeks\n\n\nloader = torch_geometric_temporal.dataset.ChickenpoxDatasetLoader()\ndataset = loader.get_dataset(lags=4)\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)"
  },
  {
    "objectID": "posts/GCN/2022-12-29-STGCN-tutorial.html#learn",
    "href": "posts/GCN/2022-12-29-STGCN-tutorial.html#learn",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "learn",
    "text": "learn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for t, snapshot in enumerate(train_dataset):\n        yt_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((yt_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:57<00:00,  1.15s/it]"
  },
  {
    "objectID": "posts/GCN/2022-12-29-STGCN-tutorial.html#visualization",
    "href": "posts/GCN/2022-12-29-STGCN-tutorial.html#visualization",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "visualization",
    "text": "visualization\n\nmodel.eval()\n\nRecurrentGCN(\n  (recurrent): GConvGRU(\n    (conv_x_z): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_z): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_r): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_r): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_h): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_h): ChebConv(32, 32, K=2, normalization=sym)\n  )\n  (linear): Linear(in_features=32, out_features=1, bias=True)\n)\n\n\n\nyhat_train = torch.stack([model(snapshot.x,snapshot.edge_index, snapshot.edge_attr) for snapshot in train_dataset]).detach().numpy()\nyhat_test = torch.stack([model(snapshot.x,snapshot.edge_index, snapshot.edge_attr) for snapshot in test_dataset]).detach().numpy()\n\n\nV = list(data_dict['node_ids'].keys())\n\n\nfig,ax = plt.subplots(20,1,figsize=(10,50))\nfor k in range(20):\n    ax[k].plot(f[:,k],'--',alpha=0.5,label='observed')\n    ax[k].set_title('node: {}'.format(V[k]))\n    ax[k].plot(yhat_train[:,k],label='predicted (tr)')\n    ax[k].plot(range(yhat_train.shape[0],yhat_train.shape[0]+yhat_test.shape[0]),yhat_test[:,k],label='predicted (test)')\n    ax[k].legend()\nfig.tight_layout()"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html",
    "title": "GCN Algorithm Example 1",
    "section": "",
    "text": "Our method; GNAR Dataset Example(fiveVTS, fiveNet)"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#데이터-일부-missing-처리",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#데이터-일부-missing-처리",
    "title": "GCN Algorithm Example 1",
    "section": "데이터 일부 missing 처리",
    "text": "데이터 일부 missing 처리\n\n1) Block 처리\n\n[1] ST-GCN\n\n%%R\nfiveVTS0 <- fiveVTS\nfiveVTS0[50:150, 3] <- NA\n\n\nplt.plot(fiveVTS0[:,2])\n\n\n\n\n\nT = 200\nN = 5 # number of Nodes\nE = fiveNet_edge\nV = np.array([1,2,3,4,5])\nt = np.arange(0,T)\nnode_features = 1\n\n\nf = torch.tensor(fiveVTS0).reshape(200,5,1).float()\n\n\nX = f[:199,:,:]\ny = f[1:,:,:]\n\n\nedge_index = torch.tensor(E)\nedge_attr = torch.tensor(np.array([1,1,1,1,1,1,1,1,1,1]),dtype=torch.float32)\n\n\n_ee = enumerate(zip(X,y))\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32<00:00,  1.53it/s]\n\n\n\nyhat = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nplt.plot(yhat[:,0].data)\nplt.plot(yhat[:,1].data)\nplt.plot(yhat[:,2].data)\nplt.plot(yhat[:,3].data)\n\n\n\n\n\n\n\n2) Random missing values\n\n%%R\nset.seed(1)\nfiveVTSrandom <- fiveVTS\nsampleindex = sort(sample(1:200, 100))\nfiveVTSrandom[sampleindex,3] <- NA\n\n\n%R -o fiveVTSrandom\n%R -o sampleindex\n\n\nplt.plot(fiveVTSrandom[:,2],'o')\n\n\n\n\n\n\n3) By 2\n\n%%R\nfiveVTStwo <- fiveVTS\nindextwo <- rep(seq(1, by = 2, 200))\nfiveVTStwo[indextwo, 3] <- NA\n\n\n%R -o fiveVTStwo\n%R -o indextwo\n\n\nplt.plot(fiveVTStwo[:,2],'o')"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#mean",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#mean",
    "title": "GCN Algorithm Example 1",
    "section": "1.1. Mean",
    "text": "1.1. Mean\n\n1) Block\n\nfiveVTS0_mean = fiveVTS0.copy()\n\n\nfiveVTS0_mean[49:150,2] = np.mean(fiveVTS0[:49,2].tolist()+fiveVTS0[150:,2].tolist())\n\n\nplt.plot(fiveVTS0_mean[:,2])\n\n\n\n\n\n\n2) Random missing values\n\nfiveVTSrandom_mean = fiveVTSrandom.copy()\n\n\ndf = pd.DataFrame(fiveVTSrandom[:,2])\nmean_value = df.mean() # finds the mean value of the column A\ndf = df.fillna(mean_value) # replace missing values with the mean value\n\n\nfiveVTSrandom_mean[:,2] = np.array(df).reshape(200,)\n\n\nplt.plot(fiveVTSrandom_mean[:,2])\n\n\n\n\n\n\n3) By 2\n\nfiveVTStwo_mean = fiveVTStwo.copy()\n\n\ndf = pd.DataFrame(fiveVTStwo[:,2])\nmean_value = df.mean() # finds the mean value of the column A\ndf = df.fillna(mean_value) # replace missing values with the mean value\n\n\nfiveVTStwo_mean[:,2] = np.array(df).reshape(200,)\n\n\nplt.plot(fiveVTStwo_mean[:,2])"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#linear-interpolation",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#linear-interpolation",
    "title": "GCN Algorithm Example 1",
    "section": "1.2. linear interpolation",
    "text": "1.2. linear interpolation\n\n1) Block\n\nfiveVTS0_linearinterpolation = fiveVTS0.copy()\n\n\n# Sample data points\nx = np.array([48,150])\ny = np.array([fiveVTS0_linearinterpolation[48,2],fiveVTS0_linearinterpolation[150,2]])\n\n# Create interpolating function\nf = interp1d(x, y, kind='linear')\n\n# Estimate y value for x = 2.5\ny_interp = f(range(49,150))\n\n\nfiveVTS0_linearinterpolation[49:150,2] = y_interp\n\n\nplt.plot(fiveVTS0_linearinterpolation[:,2])\n\n\n\n\n\n\n2) Random missing values\n\nfiveVTSrandom_linearinterpolation = fiveVTSrandom.copy()\n\n\n_df = pd.DataFrame(fiveVTSrandom_linearinterpolation[:,2])\n_df.interpolate(method='linear', inplace=True)\n_df = _df.fillna(0)\n\n\nfiveVTSrandom_linearinterpolation[:,2] = np.array(_df).reshape(200,)\n\n\nplt.plot(fiveVTSrandom_linearinterpolation[:,2])\n\n\n\n\n\n\n3) By 2\n\nfiveVTStwo_linearinterpolation = fiveVTStwo.copy()\n\n\n_df = pd.Series(fiveVTStwo_linearinterpolation[:,2])\n_df.interpolate(method='linear', inplace=True)\n_df = _df.fillna(0)\n\n\nfiveVTStwo_linearinterpolation[:,2] = _df\n\n\nplt.plot(fiveVTStwo_linearinterpolation[:,2])"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#mean-1",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#mean-1",
    "title": "GCN Algorithm Example 1",
    "section": "2.1. Mean",
    "text": "2.1. Mean\n\n1) Block\n\nf_mean = torch.tensor(fiveVTS0_mean).reshape(200,5,1).float()\n\n\nX_mean = f_mean[:199,:,:]\ny_mean = f_mean[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_mean,y_mean)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32<00:00,  1.56it/s]\n\n\n\nfhat_mean = torch.stack([model(xt, edge_index, edge_attr) for xt in X_mean]).detach().numpy()\n\n\nplt.plot(fhat_mean[:,2].data)\n\n\n\n\n\n\n2) Random missing values\n\nf_fiveVTSrandom_mean = torch.tensor(fiveVTSrandom_mean).reshape(200,5,1).float()\n\n\nX_fiveVTSrandom_mean = f_fiveVTSrandom_mean[:199,:,:]\ny_fiveVTSrandom_mean = f_fiveVTSrandom_mean[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_fiveVTSrandom_mean,y_fiveVTSrandom_mean)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32<00:00,  1.55it/s]\n\n\n\nfhat_fiveVTSrandom_mean = torch.stack([model(xt, edge_index, edge_attr) for xt in X_fiveVTSrandom_mean]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTSrandom_mean[:,2].data)\n\n\n\n\n\n\n3) By 2\n\nf_fiveVTStwo_mean = torch.tensor(fiveVTStwo_mean).reshape(200,5,1).float()\n\n\nX_fiveVTStwo_mean = f_fiveVTStwo_mean[:199,:,:]\ny_fiveVTStwo_mean = f_fiveVTStwo_mean[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_fiveVTStwo_mean,y_fiveVTStwo_mean)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32<00:00,  1.54it/s]\n\n\n\nfhat_fiveVTStwo_mean = torch.stack([model(xt, edge_index, edge_attr) for xt in X_fiveVTStwo_mean]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTStwo_mean[:,2].data)"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#linear-interpolation-1",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#linear-interpolation-1",
    "title": "GCN Algorithm Example 1",
    "section": "2.2. linear interpolation",
    "text": "2.2. linear interpolation\n\n1) Block\n\nf_linearinterpolation = torch.tensor(fiveVTS0_linearinterpolation).reshape(200,5,1).float()\n\n\nX_linearinterpolation = f_linearinterpolation[:199,:,:]\ny_linearinterpolation = f_linearinterpolation[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_linearinterpolation,y_linearinterpolation)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32<00:00,  1.55it/s]\n\n\n\nfhat_linearinterpolation = torch.stack([model(xt, edge_index, edge_attr) for xt in X_linearinterpolation]).detach().numpy()\n\n\nplt.plot(fhat_linearinterpolation[:,2].data)\n\n\n\n\n\n\n2) Random missing values\n\nf_fiveVTSrandom_linearinterpolation = torch.tensor(fiveVTSrandom_linearinterpolation).reshape(200,5,1).float()\n\n\nX_fiveVTSrandom_linearinterpolation = f_fiveVTSrandom_linearinterpolation[:199,:,:]\ny_fiveVTSrandom_linearinterpolation = f_fiveVTSrandom_linearinterpolation[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_fiveVTSrandom_linearinterpolation,y_fiveVTSrandom_linearinterpolation)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32<00:00,  1.55it/s]\n\n\n\nfhat_fiveVTSrandom_linearinterpolation = torch.stack([model(xt, edge_index, edge_attr) for xt in X_fiveVTSrandom_linearinterpolation]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTSrandom_linearinterpolation[:,2].data)\n\n\n\n\n\n\n3) By 2\n\nf_fiveVTStwo_linearinterpolation = torch.tensor(fiveVTStwo_linearinterpolation).reshape(200,5,1).float()\n\n\nX_fiveVTStwo_linearinterpolation = f_fiveVTSrandom_linearinterpolation[:199,:,:]\ny_fiveVTStwo_linearinterpolation = f_fiveVTSrandom_linearinterpolation[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_fiveVTStwo_linearinterpolation,y_fiveVTStwo_linearinterpolation)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32<00:00,  1.55it/s]\n\n\n\nfhat_fiveVTStwo_linearinterpolation = torch.stack([model(xt, edge_index, edge_attr) for xt in X_fiveVTStwo_linearinterpolation]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTStwo_linearinterpolation[:,2].data)"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#원래-f",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#원래-f",
    "title": "GCN Algorithm Example 1",
    "section": "2.3. 원래 f",
    "text": "2.3. 원래 f\n\nf = torch.tensor(fiveVTS).reshape(200,5,1).float()\n\n\nX = f[:199,:,:]\ny = f[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [00:32<00:00,  1.55it/s]\n\n\n\nfhat_fiveVTS = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTS[:,2].data)"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#mean-2",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#mean-2",
    "title": "GCN Algorithm Example 1",
    "section": "3.1. Mean",
    "text": "3.1. Mean\n\n3.1.1. Temporal\n\n1) Block\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])    \nfhatbar = np.hstack([Psi[i] @ fhat_mean[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_mean_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_mean_temporal[:,0])\nplt.plot(fhatbarhat_mean_temporal[:,1])\nplt.plot(fhatbarhat_mean_temporal[:,2])\nplt.plot(fhatbarhat_mean_temporal[:,3])\nplt.plot(fhatbarhat_mean_temporal[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])    \nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTSrandom_mean[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_random_mean_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_random_mean_temporal[:,0])\nplt.plot(fhatbarhat_random_mean_temporal[:,1])\nplt.plot(fhatbarhat_random_mean_temporal[:,2])\nplt.plot(fhatbarhat_random_mean_temporal[:,3])\nplt.plot(fhatbarhat_random_mean_temporal[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])    \nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTStwo_mean[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_twomean_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_twomean_temporal[:,0])\nplt.plot(fhatbarhat_twomean_temporal[:,1])\nplt.plot(fhatbarhat_twomean_temporal[:,2])\nplt.plot(fhatbarhat_twomean_temporal[:,3])\nplt.plot(fhatbarhat_twomean_temporal[:,4])\n\n\n\n\n\n\n\n3.1.2. Spatio\n\n1) Block\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_mean.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_mean_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_mean_spatio[:,0])\nplt.plot(fhatbarhat_mean_spatio[:,1])\nplt.plot(fhatbarhat_mean_spatio[:,2])\nplt.plot(fhatbarhat_mean_spatio[:,3])\nplt.plot(fhatbarhat_mean_spatio[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTSrandom_mean.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_random_mean_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_random_mean_spatio[:,0])\nplt.plot(fhatbarhat_random_mean_spatio[:,1])\nplt.plot(fhatbarhat_random_mean_spatio[:,2])\nplt.plot(fhatbarhat_random_mean_spatio[:,3])\nplt.plot(fhatbarhat_random_mean_spatio[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTStwo_mean.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_two_mean_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_two_mean_spatio[:,0])\nplt.plot(fhatbarhat_two_mean_spatio[:,1])\nplt.plot(fhatbarhat_two_mean_spatio[:,2])\nplt.plot(fhatbarhat_two_mean_spatio[:,3])\nplt.plot(fhatbarhat_two_mean_spatio[:,4])\n\n\n\n\n\n\n\n3.1.3. Spatio-Temporal\n\n1) Block\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_mean.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_mean_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhatbarhat_mean_spatio_temporal[:,0])\nplt.plot(fhatbarhat_mean_spatio_temporal[:,1])\nplt.plot(fhatbarhat_mean_spatio_temporal[:,2])\nplt.plot(fhatbarhat_mean_spatio_temporal[:,3])\nplt.plot(fhatbarhat_mean_spatio_temporal[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTSrandom_mean.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_random_mean_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,0])\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,1])\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,2])\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,3])\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTStwo_mean.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_two_mean_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,0])\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,1])\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,2])\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,3])\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,4])"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#linear-interpolation-2",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#linear-interpolation-2",
    "title": "GCN Algorithm Example 1",
    "section": "3.2.linear interpolation",
    "text": "3.2.linear interpolation\n\n3.2.1. Temporal\n\n1) Block\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])\nfhatbar = np.hstack([Psi[i] @ fhat_linearinterpolation[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_linearinterpolation_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,0])\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,1])\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,2])\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,3])\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])\nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTSrandom_linearinterpolation[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_random_linearinterpolation_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,0])\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,1])\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,2])\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,3])\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])\nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTStwo_linearinterpolation[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_two_linearinterpolation_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,0])\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,1])\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,2])\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,3])\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,4])\n\n\n\n\n\n\n\n3.2.2. Spatio\n\n1) Block\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_linearinterpolation.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_linearinterpolation_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,0])\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,1])\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,2])\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,3])\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTSrandom_linearinterpolation.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_random_linearinterpolation_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,0])\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,1])\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,2])\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,3])\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTStwo_linearinterpolation.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_two_linearinterpolation_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,0])\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,1])\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,2])\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,3])\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,4])\n\n\n\n\n\n\n\n3.2.3. Spatio-Temporal\n\n1) Block\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_linearinterpolation.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhat_linearinterpolation_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,0])\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,1])\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,2])\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,3])\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTSrandom_linearinterpolation.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhat_random_linearinterpolation_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,0])\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,1])\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,2])\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,3])\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTStwo_linearinterpolation.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhat_two_linearinterpolation_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,0])\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,1])\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,2])\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,3])\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,4])"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#original",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#original",
    "title": "GCN Algorithm Example 1",
    "section": "3.3. original",
    "text": "3.3. original\n\n1) Temporal\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])    \nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTS[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_temporal[:,0])\nplt.plot(fhatbarhat_temporal[:,1])\nplt.plot(fhatbarhat_temporal[:,2])\nplt.plot(fhatbarhat_temporal[:,3])\nplt.plot(fhatbarhat_temporal[:,4])\n\n\n\n\n\n\n2) Spatio\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTS.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_spatio[:,0])\nplt.plot(fhatbarhat_spatio[:,1])\nplt.plot(fhatbarhat_spatio[:,2])\nplt.plot(fhatbarhat_spatio[:,3])\nplt.plot(fhatbarhat_spatio[:,4])\n\n\n\n\n\n\n3) Spatio-Temporal\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTS.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhat_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhat_spatio_temporal[:,0])\nplt.plot(fhat_spatio_temporal[:,1])\nplt.plot(fhat_spatio_temporal[:,2])\nplt.plot(fhat_spatio_temporal[:,3])\nplt.plot(fhat_spatio_temporal[:,4])"
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html",
    "title": "Graph code",
    "section": "",
    "text": "Poster"
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#linear1",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#linear1",
    "title": "Graph code",
    "section": "Linear(1)",
    "text": "Linear(1)\n\n_x = np.linspace(0,2,1000)\n_y1 = 5*_x\n_y = _y1 + x # x is epsilon\n\n\ndf1=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\nw=np.zeros((1000,1000))\n\n\nfor i in range(1000):\n    for j in range(1000):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nclass SIMUL:\n    def __init__(self,df):\n        self.df = df\n        self.y = df.y.to_numpy()\n        self.y1 = df.y1.to_numpy()\n        self.x = df.x.to_numpy()\n        self.n = len(self.y)\n        self.W = w\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)      \n    def fit(self,sd=5,ref=30,ymin=-5,ymax=20,cuts=0,cutf=995): # fit with ebayesthresh\n        self._eigen()\n        self.ybar = self.Psi.T @ self.y # fbar := graph fourier transform of f\n        self.power = self.ybar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.ybar**2),sd=sd))\n        self.ybar_threshed = np.where(self.power_threshed>0,self.ybar,0)\n        self.yhat = self.Psi@self.ybar_threshed\n        self.df = self.df.assign(yHat = self.yhat)\n        self.df = self.df.assign(Residual = self.df.y- self.df.yHat)\n        self.differ=(np.abs(self.y-self.yhat)-np.min(np.abs(self.y-self.yhat)))/(np.max(np.abs(self.y-self.yhat))-np.min(np.abs(self.y-self.yhat))) #color 표현은 위핸 표준화\n        self.df = self.df.assign(differ = self.differ)\n        \n        fig,ax = plt.subplots(figsize=(10,10))\n        ax.scatter(self.x,self.y,color='gray',s=50,alpha=0.7)\n        ax.scatter(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],color='red',s=50)\n        ax.plot(self.x[cuts:cutf],self.yhat[cuts:cutf], '--k',lw=3)\n        ax.scatter(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],color='red',s=550,facecolors='none', edgecolors='r')\n        fig.tight_layout()\n        fig.savefig('fig1.eps',format='eps')\n\n\n_simul = SIMUL(df1)\n\n\n_simul.fit(sd=20,ref=25,ymin=-10,ymax=15)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque."
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#linear2",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#linear2",
    "title": "Graph code",
    "section": "Linear(2)",
    "text": "Linear(2)\n\n_x = np.linspace(0,2,1000)\n_y1 = 5*_x**2\n_y = _y1 + x # x is epsilon\n\n\ndf2=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul2 = SIMUL(df2)\n\n\n_simul2.fit(sd=20,ref=20,ymin=-10,ymax=15)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque."
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#cos",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#cos",
    "title": "Graph code",
    "section": "COS",
    "text": "COS\n\n_x = np.linspace(0,2,1000)\n_y1 = -2+ 3*np.cos(_x) + 1*np.cos(2*_x) + 5*np.cos(5*_x)\n_y = _y1 + x\n\n\ndf4=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul4 = SIMUL(df4)\n\n\n_simul4.fit(sd=20,ref=20,ymin=-10,ymax=15)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque."
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#sin",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#sin",
    "title": "Graph code",
    "section": "SIN",
    "text": "SIN\n\n_x = np.linspace(0,2,1000)\n_y1 =  3*np.sin(_x) + 1*np.sin(_x**2) + 5*np.sin(5*_x) \n_y = _y1 + x # x is epsilon\n\n\ndf5=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul5 = SIMUL(df5)\n\n\n_simul5.fit(ref=15,ymin=-10,ymax=15,cuts=5)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque."
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#d-manifold",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#d-manifold",
    "title": "Graph code",
    "section": "1D manifold",
    "text": "1D manifold\n\nnp.random.seed(777)\npi=np.pi\nn=1000\nang=np.linspace(-pi,pi-2*pi/n,n)\nr=5+np.cos(np.linspace(0,12*pi,n))\nvx=r*np.cos(ang)\nvy=r*np.sin(ang)\nf1=10*np.sin(np.linspace(0,6*pi,n))\nf = f1 + x\n\n\ndf = pd.DataFrame({'x' : vx, 'y' : vy, 'f' : f, 'f1' : f1})\n\n\nclass SIMUL:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.f.to_numpy()\n        self.f1 = df.f1.to_numpy()\n        self.x = df.x.to_numpy()\n        self.y = df.y.to_numpy()\n        self.n = len(self.f)\n        self.theta= None\n    def get_distance(self):\n        self.D = np.zeros([self.n,self.n])\n        locations = np.stack([self.x, self.y],axis=1)\n        for i in tqdm.tqdm(range(self.n)):\n            for j in range(i,self.n):\n                self.D[i,j]=np.linalg.norm(locations[i]-locations[j])\n        self.D = self.D + self.D.T\n    def get_weightmatrix(self,theta=1,beta=0.5,kappa=4000):\n        self.theta = theta\n        dist = np.where(self.D < kappa,self.D,0)\n        self.W = np.exp(-(dist/self.theta)**2)\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)       \n    def fit(self,sd=5,ref=60): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.f # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2),sd=sd))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(fHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.f- self.df.fHat)\n        self.dif=(np.abs(self.f-self.fhat)-np.min(np.abs(self.f-self.fhat)))/(np.max(np.abs(self.f-self.fhat))-np.min(np.abs(self.f-self.fhat)))\n        self.df = self.df.assign(dif = self.dif)\n        self.bottom = np.zeros_like(self.f)\n        self.width=0.05\n        self.depth=0.05\n#         fig = plt.figure(figsize=(10,10))\n        # ax = fig.add_subplot(1,1,1, projection='3d')\n        #\n        fig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(30,15),subplot_kw={\"projection\":\"3d\"})\n        ax1.grid(False)\n        ax1.scatter3D(self.x,self.y,self.f,zdir='z',s=50,marker='.',color='gray')\n        ax1.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.f[index_of_trueoutlier_bool],zdir='z',s=50,marker='.',color='red')\n        ax1.scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['f'],edgecolors='red',zdir='z',s=50,facecolors='none')\n        ax1.plot3D(self.x,self.y,self.f1,'--k',lw=3)\n        ax2.view_init(elev=30., azim=60)\n        \n        ax2.grid(False)\n        ax2.scatter3D(self.x,self.y,self.f,zdir='z',s=50,marker='.',color='gray')\n        ax2.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.f[index_of_trueoutlier_bool],zdir='z',s=50,marker='.',color='red') \n        ax2.scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['f'],edgecolors='red',zdir='z',s=50,facecolors='none')\n        ax2.plot3D(self.x,self.y,self.f1,'--k',lw=3)\n        ax2.view_init(elev=30., azim=40)\n        \n        ax3.grid(False)\n        ax3.scatter3D(self.x,self.y,self.f,zdir='z',s=50,marker='.',color='gray')\n        ax3.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.f[index_of_trueoutlier_bool],zdir='z',s=50,marker='.',color='red') \n        ax3.scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['f'],edgecolors='red',zdir='z',s=50,facecolors='none')\n        ax3.plot3D(self.x,self.y,self.f1,'--k',lw=3)\n        ax3.view_init(elev=30., azim=10)\n        \n        fig.savefig('fig2.eps',format='eps')\n\n\n_simul3d = SIMUL(df)\n\n\n_simul3d.get_distance()\n\n100%|██████████| 1000/1000 [00:01<00:00, 562.21it/s]\n\n\n\n_simul3d.get_weightmatrix(theta=(_simul3d.D[_simul3d.D>0].mean()),kappa=2500) \n\n\n%%capture --no-display\n_simul3d.fit(sd=15,ref=20)"
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#bunny",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#bunny",
    "title": "Graph code",
    "section": "Bunny",
    "text": "Bunny\n\nG = graphs.Bunny()\nn = G.N\n\n\ng = filters.Heat(G, tau=75) # 꼬리부분의 빨간신호를 퍼지게하는 정도\n\n\nnormal = np.random.randn(n)\nunif = np.concatenate([np.random.uniform(low=3,high=7,size=60), np.random.uniform(low=-7,high=-3,size=60),np.zeros(n-120)]); np.random.shuffle(unif)\nnoise = normal + unif\n\n\nindex_of_trueoutlier_bool = (unif!=0)\n\n\nf = np.zeros(n)\nf[1000] = -3234\nf = g.filter(f, method='chebyshev') \n\n2022-11-10 21:12:29,879:[WARNING](pygsp.graphs.graph.lmax): The largest eigenvalue G.lmax is not available, we need to estimate it. Explicitly call G.estimate_lmax() or G.compute_fourier_basis() once beforehand to suppress the warning.\n\n\n\nW = G.W.toarray()\nx = G.coords[:,0]\ny = G.coords[:,1]\nz = -G.coords[:,2]\n\n\ndf = pd.DataFrame({'x' : x, 'y' : y, 'z' : z, 'f' : f, 'noise' : noise})\n\n\nclass SIMUL:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.f.to_numpy()\n        self.z = df.z.to_numpy()\n        self.x = df.x.to_numpy()\n        self.y = df.y.to_numpy()\n        self.noise = df.noise.to_numpy()\n        self.fnoise = self.f + self.noise\n        self.W = W\n        self.n = len(self.f)\n        self.theta= None\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)       \n    def fit(self,sd=2.5,ref=6): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.fnoise # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2),sd=sd))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(fnoise = self.fnoise)\n        self.df = self.df.assign(fHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.f + self.df.noise - self.df.fHat)\n        self.bottom = np.zeros_like(self.f)\n        self.width=0.05\n        self.depth=0.05\n        \n        fig = plt.figure(figsize=(30,12),dpi=400)\n        ax1 = fig.add_subplot(251, projection='3d')\n        ax1.grid(False)\n        ax1.scatter3D(self.x,self.y,self.z,c='gray',zdir='z',alpha=0.5,marker='.')\n        ax1.view_init(elev=60., azim=-90)\n\n        ax2= fig.add_subplot(252, projection='3d')\n        ax2.grid(False)\n        ax2.scatter3D(self.x,self.y,self.z,c=self.f,cmap='hsv',zdir='z',marker='.',alpha=0.5,vmin=-12,vmax=10)\n        ax2.view_init(elev=60., azim=-90)\n\n        ax3= fig.add_subplot(253, projection='3d')\n        ax3.grid(False)\n        ax3.scatter3D(self.x,self.y,self.z,c=self.fnoise,cmap='hsv',zdir='z',marker='.',alpha=0.5,vmin=-12,vmax=10)\n        ax3.view_init(elev=60., azim=-90)\n        \n        ax4= fig.add_subplot(254, projection='3d')\n        ax4.grid(False)\n        ax4.scatter3D(self.x,self.y,self.z,c=self.fnoise,cmap='hsv',zdir='z',marker='.',vmin=-12,vmax=10,s=1)\n        ax4.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.z[index_of_trueoutlier_bool],c=self.fnoise[index_of_trueoutlier_bool],cmap='hsv',zdir='z',marker='.',s=50)\n        ax4.view_init(elev=60., azim=-90)\n\n        ax5= fig.add_subplot(255, projection='3d')\n        ax5.grid(False)\n        ax5.scatter3D(self.x,self.y,self.z,c=self.fnoise,cmap='hsv',zdir='z',marker='.',vmin=-12,vmax=10,s=1)\n        ax5.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.z[index_of_trueoutlier_bool],c=self.fnoise[index_of_trueoutlier_bool],cmap='hsv',zdir='z',marker='.',s=50)\n        ax5.scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['z'],zdir='z',s=550,marker='.',edgecolors='red',facecolors='none')\n        ax5.view_init(elev=60., azim=-90)\n        \n        ax6 = fig.add_subplot(256, projection='3d')\n        ax6.grid(False)\n        ax6.scatter3D(self.x,self.y,self.z,c='gray',zdir='z',alpha=0.5,marker='.')\n        ax6.view_init(elev=-60., azim=-90)\n\n        ax7= fig.add_subplot(257, projection='3d')\n        ax7.grid(False)\n        ax7.scatter3D(self.x,self.y,self.z,c=self.f,cmap='hsv',zdir='z',marker='.',alpha=0.5,vmin=-12,vmax=10)\n        ax7.view_init(elev=-60., azim=-90)\n\n        ax8= fig.add_subplot(258, projection='3d')\n        ax8.grid(False)\n        ax8.scatter3D(self.x,self.y,self.z,c=self.fnoise,cmap='hsv',zdir='z',marker='.',alpha=0.5,vmin=-12,vmax=10)\n        ax8.view_init(elev=-60., azim=-90)\n        \n        ax9= fig.add_subplot(259, projection='3d')\n        ax9.grid(False)\n        ax9.scatter3D(self.x,self.y,self.z,c=self.fnoise,cmap='hsv',zdir='z',marker='.',vmin=-12,vmax=10,s=1)\n        ax9.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.z[index_of_trueoutlier_bool],c=self.fnoise[index_of_trueoutlier_bool],cmap='hsv',zdir='z',marker='.',s=50)\n        ax9.view_init(elev=-60., azim=-90)\n\n        ax10= fig.add_subplot(2,5,10, projection='3d')\n        ax10.grid(False)\n        ax10.scatter3D(self.x,self.y,self.z,c=self.fnoise,cmap='hsv',zdir='z',marker='.',vmin=-12,vmax=10,s=1)\n        ax10.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.z[index_of_trueoutlier_bool],c=self.fnoise[index_of_trueoutlier_bool],cmap='hsv',zdir='z',marker='.',s=50)\n        ax10.scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['z'],zdir='z',s=550,marker='.',edgecolors='red',facecolors='none')\n        ax10.view_init(elev=-60., azim=-90)        \n        fig.savefig('fig_bunny.eps',format='eps')\n\n\n_simul = SIMUL(df)\n\n\nmax(_simul.f),max(_simul.fnoise)\n\n(-0.010827167666814895, 8.453057038638512)\n\n\n\nmin(_simul.f),min(_simul.fnoise)\n\n(-4.74620052476489, -11.196627043702925)\n\n\n\n%%capture --no-display\n_simul.fit(sd=20,ref=10)"
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#earthquake",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#earthquake",
    "title": "Graph code",
    "section": "Earthquake",
    "text": "Earthquake\n\ndf= pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv')\n\n\ndf_global= pd.concat([pd.read_csv('00_05.csv'),pd.read_csv('05_10.csv'),pd.read_csv('10_15.csv'),pd.read_csv('15_20.csv')]).iloc[:,[0,1,2,4]].rename(columns={'latitude':'Latitude','longitude':'Longitude','mag':'Magnitude'}).reset_index().iloc[:,1:]\n\n\ndf_global = df_global.assign(Year=list(map(lambda x: x.split('-')[0], df_global.time))).iloc[:,1:]\n\n\ndf_global.Year = df_global.Year.astype(np.float64)\n\n\nclass MooYaHo:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.Magnitude.to_numpy()\n        self.year = df.Year.to_numpy()\n        self.lat = df.Latitude.to_numpy()\n        self.long = df.Longitude.to_numpy()\n        self.n = len(self.f)\n        \n        self.theta= None\n    def get_distance(self):\n        self.D = np.zeros([self.n,self.n])\n        locations = np.stack([self.lat, self.long],axis=1)\n        for i in tqdm.tqdm(range(self.n)):\n            for j in range(i,self.n): \n                self.D[i,j]=haversine(locations[i],locations[j])\n        self.D = self.D+self.D.T\n    def get_weightmatrix(self,theta=1,beta=0.5,kappa=4000):\n        self.theta = theta\n        dist = np.where(self.D<kappa,self.D,0)\n        self.W = np.exp(-(dist/self.theta)**2)\n\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)        \n    def fit(self,m):\n        self._eigen()\n        self.fhat = self.Psi[:,0:m]@self.Psi[:,0:m].T@self.f\n        self.df = self.df.assign(MagnitudeHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.Magnitude- self.df.MagnitudeHat)\n        plt.plot(self.f,'.')\n        plt.plot(self.fhat,'x')\n\n\nclass MooYaHo2(MooYaHo): # ebayesthresh 기능추가\n    def fit2(self,ref=0.5): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.f # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2)))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(MagnitudeHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.Magnitude- self.df.MagnitudeHat)\n        self.con = np.where(self.df.Residual>0.7,1,0)\n\n\nclass eachlocation(MooYaHo2):\n    def haiti(self,MagThresh=7,ResThresh=1,adjzoom=5,adjmarkersize = 40):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=15,\n                        center=dict(lat=18.4430, lon=-72.5710), \n                        zoom= adjzoom,\n                        height=900,\n                        opacity = 0.8,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-3,3])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 5,\n                      marker_color= 'blue',\n                      opacity = 0.1\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= adjmarkersize,\n                      marker_color= 'red',\n                      opacity = 0.8\n                      )\n        fig.add_trace(go.Scattermapbox(\n                    lat=self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                    lon=self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                    mode='markers',\n                    marker=go.scattermapbox.Marker(\n                        size=20,\n                        color='rgb(255, 255, 255)',\n                        opacity=0.4\n                    )\n                ))\n        return fig \n    def lquique(self,MagThresh=7,ResThresh=1,adjzoom=5, adjmarkersize= 40):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=15,\n                        center=dict(lat=-32.6953, lon=-71.4416), \n                        zoom=adjzoom,\n                        height=900,\n                        opacity = 0.8,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 5,\n                      marker_color= 'blue',\n                      opacity = 0.1\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= adjmarkersize,\n                      marker_color= 'red',\n                      opacity = 0.8\n                      )\n        fig.add_trace(go.Scattermapbox(\n                    lat=self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                    lon=self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                    mode='markers',\n                    marker=go.scattermapbox.Marker(\n                        size=20,\n                        color='rgb(255, 255, 255)',\n                        opacity=0.8\n                    )\n                ))\n        return fig \n    def sichuan(self,MagThresh=7,ResThresh=1,adjzoom=5,adjmarkersize=40):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=15,\n                        center=dict(lat=30.3080, lon=102.8880), \n                        zoom=adjzoom,\n                        height=900,\n                        opacity = 0.6,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 5,\n                      marker_color= 'blue',\n                      opacity = 0.1\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= adjmarkersize,\n                      marker_color= 'red',\n                      opacity = 0.8\n                      )\n        fig.add_trace(go.Scattermapbox(\n                    lat=self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                    lon=self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                    mode='markers',\n                    marker=go.scattermapbox.Marker(\n                        size=20,\n                        color='rgb(255, 255, 255)',\n                        opacity=0.8\n                    )\n                ))\n        return fig \n\n\neach_location=eachlocation(df_global.query(\"2010 <= Year < 2015\"))\n\n- get distance\n\neach_location.get_distance()\n\n100%|██████████| 12498/12498 [03:24<00:00, 61.15it/s] \n\n\n\neach_location.D[each_location.D>0].mean()\n\n8810.865423093777\n\n\n\nplt.hist(each_location.D[each_location.D>0])\n\n(array([14176290., 16005894., 21186674., 22331128., 19394182., 17548252.,\n        16668048., 13316436., 12973260.,  2582550.]),\n array([8.97930163e-02, 2.00141141e+03, 4.00273303e+03, 6.00405465e+03,\n        8.00537626e+03, 1.00066979e+04, 1.20080195e+04, 1.40093411e+04,\n        1.60106627e+04, 1.80119844e+04, 2.00133060e+04]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n- weight matrix\n\neach_location.get_weightmatrix(theta=(8810.865423093777),kappa=2500) \n\n- fit\n\neach_location.fit2()\n\n\neach_location.haiti(MagThresh=6.9,ResThresh=0.5,adjzoom=5,adjmarkersize=40)\nfig = each_location.haiti(MagThresh=6.9,ResThresh=0.5,adjzoom=5,adjmarkersize=40)\nfig.write_image('fig_haiti.png',scale=3)\n\n\neach_location.lquique(MagThresh=6.4,ResThresh=0.4,adjzoom=5,adjmarkersize=40)\n# fig = each_location.lquique(MagThresh=6.4,ResThresh=0.4,adjzoom=5,adjmarkersize=20)\n# fig.write_image('fig_lquique.svg',scale=3)\n\n\neach_location.sichuan(MagThresh=6.5,ResThresh=0.4,adjzoom=5,adjmarkersize=40)\n# fig = each_location.sichuan(MagThresh=6.5,ResThresh=0.4,adjzoom=5,adjmarkersize=20)\n# fig.write_image('fig_sichuan.svg',scale=3)"
  },
  {
    "objectID": "posts/GODE/index.html",
    "href": "posts/GODE/index.html",
    "title": "GODE",
    "section": "",
    "text": "About GODE paper"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html",
    "title": "Class code for Comparison Study",
    "section": "",
    "text": "Simulation\nex - The Stanford bunny data generated using the pygsp package is a common graphics 3D test model and NN-graph. It has 2503 data. We use filter.Heat in this package and it calculate data by \\(\\hat{g}(x) = \\exp(\\frac{-\\tau x}{\\lambda_{max}})\\) and \\(\\tau\\) is 75. We use Chebyshev polynomial approximation on this filter. We make zero vector whixh size is 2503, and put -3000 to one value to use a Lanczos approximation. A Lanczos approximation will resize signals by flattened.\nref: https://pygsp.readthedocs.io/en/v0.5.1/reference/filters.html\n\\(r = 5 + \\cos(\\frac{12\\pi - (-\\pi)}{n})\\times i , (i=1,2,\\dots , n)\\)\n\\(r \\cos(\\frac{\\pi - 2\\times \\pi /n - (-\\pi) }{n}\\times i)),(i=1,2,\\dots ,n)\\)\n\\(r \\sin((\\frac{\\pi - 2\\times \\pi /n - (-\\pi) }{n}\\times i)),(i=1,2,\\dots ,n)\\)\n\\(f = 10 \\times (\\frac{6 \\pi}{n} \\times i),(i=1,2, \\dots , n)\\)"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#import",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#import",
    "title": "Class code for Comparison Study",
    "section": "Import",
    "text": "Import\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.linear_model import SGDOneClassSVM\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.pipeline import make_pipeline\n\nimport pandas as pd\nfrom sklearn.neighbors import LocalOutlierFactor\n\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr\n\nfrom sklearn.datasets import fetch_kddcup99, fetch_covtype, fetch_openml\nfrom sklearn.preprocessing import LabelBinarizer\n\nimport tqdm\n\nfrom pygsp import graphs, filters, plotting, utils\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\nimport plotly.graph_objects as go\nfrom IPython.display import HTML\n\nimport plotly.express as px\n\nfrom sklearn.covariance import EmpiricalCovariance, MinCovDet\n\nfrom alibi_detect.od import IForest\n# from pyod.models.iforest import IForest\n\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nimport seaborn as sns\n\nfrom PyNomaly import loop\n\nfrom sklearn import svm\n\nfrom pyod.models.lscp import LSCP\nfrom pyod.models.hbos import HBOS\n\nfrom pyod.models.so_gaal import SO_GAAL\nfrom pyod.models.mcd import MCD\nfrom pyod.models.mo_gaal import MO_GAAL\nfrom pyod.models.knn import KNN\nfrom pyod.models.lof import LOF\nfrom pyod.models.ocsvm import OCSVM\n\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.sos import SOS"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#class-code",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#class-code",
    "title": "Class code for Comparison Study",
    "section": "Class Code",
    "text": "Class Code\n\ntab_linear = pd.DataFrame(columns=[\"Accuracy\",\"Precision\",\"Recall\",\"F1\"])\ntab_orbit = pd.DataFrame(columns=[\"Accuracy\",\"Precision\",\"Recall\",\"F1\"])\ntab_bunny = pd.DataFrame(columns=[\"Accuracy\",\"Precision\",\"Recall\",\"F1\"])\n\n\nclass Conf_matrx:\n    def __init__(self,original,compare,tab):\n        self.original = original\n        self.compare = compare\n        self.tab = tab\n    def conf(self,name):\n        self.conf_matrix = confusion_matrix(self.original, self.compare)\n        \n        fig, ax = plt.subplots(figsize=(5, 5))\n        ax.matshow(self.conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n        for i in range(self.conf_matrix.shape[0]):\n            for j in range(self.conf_matrix.shape[1]):\n                ax.text(x=j, y=i,s=self.conf_matrix[i, j], va='center', ha='center', size='xx-large')\n        plt.xlabel('Predictions', fontsize=18)\n        plt.ylabel('Actuals', fontsize=18)\n        plt.title('Confusion Matrix', fontsize=18)\n        plt.show()\n        \n        self.acc = accuracy_score(self.original, self.compare)\n        self.pre = precision_score(self.original, self.compare)\n        self.rec = recall_score(self.original, self.compare)\n        self.f1 = f1_score(self.original, self.compare)\n        \n        print('Accuracy: %.3f' % self.acc)\n        print('Precision: %.3f' % self.pre)\n        print('Recall: %.3f' % self.rec)\n        print('F1 Score: %.3f' % self.f1)\n        \n        self.tab = self.tab.append(pd.DataFrame({\"Accuracy\":[self.acc],\"Precision\":[self.pre],\"Recall\":[self.rec],\"F1\":[self.f1]},index = [name]))\n\n\nclass Linear:\n    def __init__(self,df):\n        self.df = df\n        self.y = df.y.to_numpy()\n        #self.y1 = df.y1.to_numpy()\n        self.x = df.x.to_numpy()\n        self.n = len(self.y)\n        self.W = w\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)      \n    def fit(self,sd=20): # fit with ebayesthresh\n        self._eigen()\n        self.ybar = self.Psi.T @ self.y # fbar := graph fourier transform of f\n        self.power = self.ybar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.ybar**2),sd=sd))\n        self.ybar_threshed = np.where(self.power_threshed>0,self.ybar,0)\n        self.yhat = self.Psi@self.ybar_threshed\n        self.df = self.df.assign(yHat = self.yhat)\n        self.df = self.df.assign(Residual = self.df.y- self.df.yHat)\n\n\nclass Orbit:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.f.to_numpy()\n        self.x = df.x.to_numpy()\n        self.y = df.y.to_numpy()\n        self.n = len(self.f)\n        self.theta= None\n    def get_distance(self):\n        self.D = np.zeros([self.n,self.n])\n        locations = np.stack([self.x, self.y],axis=1)\n        for i in tqdm.tqdm(range(self.n)):\n            for j in range(i,self.n):\n                self.D[i,j]=np.linalg.norm(locations[i]-locations[j])\n        self.D = self.D + self.D.T\n    def get_weightmatrix(self,theta=1,beta=0.5,kappa=4000):\n        self.theta = theta\n        dist = np.where(self.D < kappa,self.D,0)\n        self.W = np.exp(-(dist/self.theta)**2)\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)       \n    def fit(self,sd=5,ref=20): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.f # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2),sd=sd))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(fHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.f- self.df.fHat)\n        self.bottom = np.zeros_like(self.f)\n        self.width=0.05\n        self.depth=0.05\n\n\nclass BUNNY:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.f.to_numpy()\n        self.z = df.z.to_numpy()\n        self.x = df.x.to_numpy()\n        self.y = df.y.to_numpy()\n        self.noise = df.noise.to_numpy()\n        self.fnoise = self.f + self.noise\n        self.W = _W\n        self.n = len(self.f)\n        self.theta= None\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)       \n    def fit(self,sd=5,ref=6): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.fnoise # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2),sd=sd))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(fnoise = self.fnoise)\n        self.df = self.df.assign(fHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.f + self.df.noise - self.df.fHat)\n        self.bottom = np.zeros_like(self.f)\n        self.width=0.05\n        self.depth=0.05"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#linear-ebayesthresh",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#linear-ebayesthresh",
    "title": "Class code for Comparison Study",
    "section": "Linear EbayesThresh",
    "text": "Linear EbayesThresh\n\n%load_ext rpy2.ipython\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython\n\n\n\n%%R\nlibrary(EbayesThresh)\nset.seed(1)\nepsilon = rnorm(1000)\nsignal_1 = sample(c(runif(25,-2,-1.5), runif(25,1.5,2), rep(0,950)))\nindex_of_trueoutlier_1 = which(signal_1!=0)\nindex_of_trueoutlier_1\nx_1=signal_1+epsilon\n\n\n%R -o x_1\n%R -o index_of_trueoutlier_1\n%R -o signal_1\n\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\n\n\noutlier_true_index_1 = index_of_trueoutlier_1\n\n\noutlier_true_value_1 = x_1[index_of_trueoutlier_1]\n\n\noutlier_true_one_1 = signal_1.copy()\n\n\noutlier_true_one_1 = list(map(lambda x: -1 if x!=0 else 1,outlier_true_one_1))"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#linear",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#linear",
    "title": "Class code for Comparison Study",
    "section": "Linear",
    "text": "Linear\n\n_x_1 = np.linspace(0,2,1000)\n_y1_1 = 5*_x_1\n_y_1 = _y1_1 + x_1 # x is epsilon\n\n\n_df=pd.DataFrame({'x':_x_1, 'y':_y_1})\n\n\nX = np.array(_df)\n\n\nGODE\n\nw=np.zeros((1000,1000))\n\n\nfor i in range(1000):\n    for j in range(1000):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\n_Linear = Linear(_df)\n\n\n_Linear.fit(sd=5)\n\n\noutlier_simul_one = (_Linear.df['Residual']**2).tolist()\n\n\noutlier_simul_one = list(map(lambda x: -1 if x > 20 else 1,outlier_simul_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_simul_one,tab_linear)\n\n\n_conf.conf(\"GODE\")\n\n\n\n\nAccuracy: 0.950\nPrecision: 0.950\nRecall: 1.000\nF1 Score: 0.974\n\n\n\none = _conf.tab\n\n\n\nLOF\n\nclf = LocalOutlierFactor(n_neighbors=2)\n\n\n_conf = Conf_matrx(outlier_true_one_1,clf.fit_predict(X),tab_linear)\n\n\n_conf.conf(\"LOF (Breunig et al., 2000)\")\n\n\n\n\nAccuracy: 0.890\nPrecision: 0.973\nRecall: 0.909\nF1 Score: 0.940\n\n\n\ntwo = one.append(_conf.tab)\n\n\n\nKNN\n\nfrom pyod.models.knn import KNN\n\n\nclf = KNN()\nclf.fit(_df[['x', 'y']])\n_df['knn_Clf'] = clf.labels_\n\n\noutlier_KNN_one = list(clf.labels_)\n\n\noutlier_KNN_one = list(map(lambda x: 1 if x==0  else -1,outlier_KNN_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_KNN_one,tab_linear)\n\n\n_conf.conf(\"kNN (Ramaswamy et al., 2000)\")\n\n\n\n\nAccuracy: 0.912\nPrecision: 0.979\nRecall: 0.927\nF1 Score: 0.952\n\n\n\nthree = two.append(_conf.tab)\n\n\n\nCBLOF\n\nclf = CBLOF(contamination=0.05,check_estimator=False, random_state=77)\nclf.fit(_df[['x', 'y']])\n_df['CBLOF_Clf'] = clf.labels_\n\n\noutlier_CBLOF_one = list(clf.labels_)\n\n\noutlier_CBLOF_one = list(map(lambda x: 1 if x==0  else -1,outlier_CBLOF_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_CBLOF_one,tab_linear)\n\n\n_conf.conf(\"CBLOF (He et al., 2003)\")\n\n\n\n\nAccuracy: 0.920\nPrecision: 0.958\nRecall: 0.958\nF1 Score: 0.958\n\n\n\nfour = three.append(_conf.tab)\n\n\n\nOCSVM\n\nclf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n\n\nclf.fit(X)\n\nOneClassSVM(gamma=0.1, nu=0.1)\n\n\n\noutlier_OSVM_one = list(clf.predict(X))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_OSVM_one,tab_linear)\n\n\n_conf.conf(\"OCSVM (Sch ̈olkopf et al., 2001)\")\n\n\n\n\nAccuracy: 0.909\nPrecision: 0.978\nRecall: 0.925\nF1 Score: 0.951\n\n\n\nfive = four.append(_conf.tab)\n\n\n\nMCD\n\nclf = MCD()\nclf.fit(_df[['x', 'y']])\n_df['MCD_clf'] = clf.labels_\n\n\noutlier_MCD_one = list(clf.labels_)\n\n\noutlier_MCD_one = list(map(lambda x: 1 if x==0  else -1,outlier_MCD_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_MCD_one,tab_linear)\n\n\n_conf.conf(\"MCD (Hardin and Rocke, 2004)\")\n\n\n\n\nAccuracy: 0.918\nPrecision: 0.982\nRecall: 0.931\nF1 Score: 0.956\n\n\n\nsix = five.append(_conf.tab)\n\n\n\nFeature Bagging\n\nclf = FeatureBagging()\nclf.fit(_df[['x', 'y']])\n_df['FeatureBagging_clf'] = clf.labels_\n\n\noutlier_FeatureBagging_one = list(clf.labels_)\n\n\noutlier_FeatureBagging_one = list(map(lambda x: 1 if x==0  else -1,outlier_FeatureBagging_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_FeatureBagging_one,tab_linear)\n\n\n_conf.conf(\"Feature Bagging (Lazarevic and Kumar, 2005)\")\n\n\n\n\nAccuracy: 0.918\nPrecision: 0.982\nRecall: 0.931\nF1 Score: 0.956\n\n\n\nseven = six.append(_conf.tab)\n\n\n\nABOD\n\nclf = ABOD(contamination=0.05)\nclf.fit(_df[['x', 'y']])\n_df['ABOD_Clf'] = clf.labels_\n\n\noutlier_ABOD_one = list(clf.labels_)\n\n\noutlier_ABOD_one = list(map(lambda x: 1 if x==0  else -1,outlier_ABOD_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_ABOD_one,tab_linear)\n\n\n_conf.conf(\"ABOD (Kriegel et al., 2008)\")\n\n\n\n\nAccuracy: 0.946\nPrecision: 0.972\nRecall: 0.972\nF1 Score: 0.972\n\n\n\neight = seven.append(_conf.tab)\n\n\n\nIForest\n\nod = IForest(\n    threshold=0.,\n    n_estimators=100\n)\n\n\nod.fit(_df[['x', 'y']])\n\n\npreds = od.predict(\n    _df[['x', 'y']],\n    return_instance_score=True\n)\n\n\n_df['IF_alibi'] = preds['data']['is_outlier']\n\n\noutlier_alibi_one = _df['IF_alibi']\n\n\noutlier_alibi_one = list(map(lambda x: 1 if x==0  else -1,outlier_alibi_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_alibi_one,tab_linear)\n\n\n_conf.conf(\"Isolation Forest (Liu et al., 2008)\")\n\n\n\n\nAccuracy: 0.800\nPrecision: 0.984\nRecall: 0.802\nF1 Score: 0.884\n\n\n\nnine = eight.append(_conf.tab)\n\n\n\nHBOS\n\nclf = HBOS()\nclf.fit(_df[['x', 'y']])\n_df['HBOS_clf'] = clf.labels_\n\n\noutlier_HBOS_one = list(clf.labels_)\n\n\noutlier_HBOS_one = list(map(lambda x: 1 if x==0  else -1,outlier_HBOS_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_HBOS_one,tab_linear)\n\n\n_conf.conf(\"HBOS (Goldstein and Dengel, 2012)\")\n\n\n\n\nAccuracy: 0.889\nPrecision: 0.960\nRecall: 0.921\nF1 Score: 0.940\n\n\n\nten = nine.append(_conf.tab)\n\n\n\nSOS\n\noutlier_SOS_one = list(clf.labels_)\n\n\noutlier_SOS_one = list(map(lambda x: 1 if x==0  else -1,outlier_SOS_one))\n\n\nclf = SOS()\nclf.fit(_df[['x', 'y']])\n_df['SOS_clf'] = clf.labels_\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_SOS_one,tab_linear)\n\n\n_conf.conf(\"SOS (Janssens et al., 2012)\")\n\n\n\n\nAccuracy: 0.889\nPrecision: 0.960\nRecall: 0.921\nF1 Score: 0.940\n\n\n\neleven = ten.append(_conf.tab)\n\n\n\nSO_GAAL\n\nclf = SO_GAAL()\nclf.fit(_df[['x', 'y']])\n_df['SO_GAAL_clf'] = clf.labels_\n\nEpoch 1 of 60\n\nTesting for epoch 1 index 1:\n\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n\n\n\nTesting for epoch 1 index 2:\nEpoch 2 of 60\n\nTesting for epoch 2 index 1:\n\nTesting for epoch 2 index 2:\nEpoch 3 of 60\n\nTesting for epoch 3 index 1:\n\nTesting for epoch 3 index 2:\nEpoch 4 of 60\n\nTesting for epoch 4 index 1:\n\nTesting for epoch 4 index 2:\nEpoch 5 of 60\n\nTesting for epoch 5 index 1:\n\nTesting for epoch 5 index 2:\nEpoch 6 of 60\n\nTesting for epoch 6 index 1:\n\nTesting for epoch 6 index 2:\nEpoch 7 of 60\n\nTesting for epoch 7 index 1:\n\nTesting for epoch 7 index 2:\nEpoch 8 of 60\n\nTesting for epoch 8 index 1:\n\nTesting for epoch 8 index 2:\nEpoch 9 of 60\n\nTesting for epoch 9 index 1:\n\nTesting for epoch 9 index 2:\nEpoch 10 of 60\n\nTesting for epoch 10 index 1:\n\nTesting for epoch 10 index 2:\nEpoch 11 of 60\n\nTesting for epoch 11 index 1:\n\nTesting for epoch 11 index 2:\nEpoch 12 of 60\n\nTesting for epoch 12 index 1:\n\nTesting for epoch 12 index 2:\nEpoch 13 of 60\n\nTesting for epoch 13 index 1:\n\nTesting for epoch 13 index 2:\nEpoch 14 of 60\n\nTesting for epoch 14 index 1:\n\nTesting for epoch 14 index 2:\nEpoch 15 of 60\n\nTesting for epoch 15 index 1:\n\nTesting for epoch 15 index 2:\nEpoch 16 of 60\n\nTesting for epoch 16 index 1:\n\nTesting for epoch 16 index 2:\nEpoch 17 of 60\n\nTesting for epoch 17 index 1:\n\nTesting for epoch 17 index 2:\nEpoch 18 of 60\n\nTesting for epoch 18 index 1:\n\nTesting for epoch 18 index 2:\nEpoch 19 of 60\n\nTesting for epoch 19 index 1:\n\nTesting for epoch 19 index 2:\nEpoch 20 of 60\n\nTesting for epoch 20 index 1:\n\nTesting for epoch 20 index 2:\nEpoch 21 of 60\n\nTesting for epoch 21 index 1:\n\nTesting for epoch 21 index 2:\nEpoch 22 of 60\n\nTesting for epoch 22 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.3973\n\nTesting for epoch 22 index 2:\n16/16 [==============================] - 0s 679us/step - loss: 1.4180\nEpoch 23 of 60\n\nTesting for epoch 23 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4019\n\nTesting for epoch 23 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4210\nEpoch 24 of 60\n\nTesting for epoch 24 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4234\n\nTesting for epoch 24 index 2:\n16/16 [==============================] - 0s 691us/step - loss: 1.4552\nEpoch 25 of 60\n\nTesting for epoch 25 index 1:\n16/16 [==============================] - 0s 663us/step - loss: 1.4271\n\nTesting for epoch 25 index 2:\n16/16 [==============================] - 0s 767us/step - loss: 1.4613\nEpoch 26 of 60\n\nTesting for epoch 26 index 1:\n16/16 [==============================] - 0s 602us/step - loss: 1.4776\n\nTesting for epoch 26 index 2:\n16/16 [==============================] - 0s 715us/step - loss: 1.4349\nEpoch 27 of 60\n\nTesting for epoch 27 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4333\n\nTesting for epoch 27 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4840\nEpoch 28 of 60\n\nTesting for epoch 28 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5092\n\nTesting for epoch 28 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4956\nEpoch 29 of 60\n\nTesting for epoch 29 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5026\n\nTesting for epoch 29 index 2:\n16/16 [==============================] - 0s 831us/step - loss: 1.5576\nEpoch 30 of 60\n\nTesting for epoch 30 index 1:\n16/16 [==============================] - 0s 611us/step - loss: 1.5602\n\nTesting for epoch 30 index 2:\n16/16 [==============================] - 0s 638us/step - loss: 1.4791\nEpoch 31 of 60\n\nTesting for epoch 31 index 1:\n16/16 [==============================] - 0s 619us/step - loss: 1.5625\n\nTesting for epoch 31 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5635\nEpoch 32 of 60\n\nTesting for epoch 32 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5925\n\nTesting for epoch 32 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5807\nEpoch 33 of 60\n\nTesting for epoch 33 index 1:\n16/16 [==============================] - 0s 844us/step - loss: 1.5739\n\nTesting for epoch 33 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6122\nEpoch 34 of 60\n\nTesting for epoch 34 index 1:\n16/16 [==============================] - 0s 657us/step - loss: 1.6156\n\nTesting for epoch 34 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6021\nEpoch 35 of 60\n\nTesting for epoch 35 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6237\n\nTesting for epoch 35 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6302\nEpoch 36 of 60\n\nTesting for epoch 36 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6586\n\nTesting for epoch 36 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 1.6349\nEpoch 37 of 60\n\nTesting for epoch 37 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6708\n\nTesting for epoch 37 index 2:\n16/16 [==============================] - 0s 726us/step - loss: 1.7010\nEpoch 38 of 60\n\nTesting for epoch 38 index 1:\n16/16 [==============================] - 0s 826us/step - loss: 1.6865\n\nTesting for epoch 38 index 2:\n16/16 [==============================] - 0s 820us/step - loss: 1.6874\nEpoch 39 of 60\n\nTesting for epoch 39 index 1:\n16/16 [==============================] - 0s 680us/step - loss: 1.7410\n\nTesting for epoch 39 index 2:\n16/16 [==============================] - 0s 663us/step - loss: 1.7334\nEpoch 40 of 60\n\nTesting for epoch 40 index 1:\n16/16 [==============================] - 0s 637us/step - loss: 1.6871\n\nTesting for epoch 40 index 2:\n16/16 [==============================] - 0s 621us/step - loss: 1.7771\nEpoch 41 of 60\n\nTesting for epoch 41 index 1:\n16/16 [==============================] - 0s 629us/step - loss: 1.7724\n\nTesting for epoch 41 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.7815\nEpoch 42 of 60\n\nTesting for epoch 42 index 1:\n16/16 [==============================] - 0s 647us/step - loss: 1.7470\n\nTesting for epoch 42 index 2:\n16/16 [==============================] - 0s 612us/step - loss: 1.7897\nEpoch 43 of 60\n\nTesting for epoch 43 index 1:\n16/16 [==============================] - 0s 660us/step - loss: 1.8400\n\nTesting for epoch 43 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8351\nEpoch 44 of 60\n\nTesting for epoch 44 index 1:\n16/16 [==============================] - 0s 689us/step - loss: 1.8388\n\nTesting for epoch 44 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8174\nEpoch 45 of 60\n\nTesting for epoch 45 index 1:\n16/16 [==============================] - 0s 974us/step - loss: 1.8131\n\nTesting for epoch 45 index 2:\n16/16 [==============================] - 0s 712us/step - loss: 1.8391\nEpoch 46 of 60\n\nTesting for epoch 46 index 1:\n16/16 [==============================] - 0s 635us/step - loss: 1.7937\n\nTesting for epoch 46 index 2:\n16/16 [==============================] - 0s 971us/step - loss: 1.8550\nEpoch 47 of 60\n\nTesting for epoch 47 index 1:\n16/16 [==============================] - 0s 628us/step - loss: 1.8632\n\nTesting for epoch 47 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8457\nEpoch 48 of 60\n\nTesting for epoch 48 index 1:\n16/16 [==============================] - 0s 628us/step - loss: 1.8924\n\nTesting for epoch 48 index 2:\n16/16 [==============================] - 0s 640us/step - loss: 1.8481\nEpoch 49 of 60\n\nTesting for epoch 49 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8722\n\nTesting for epoch 49 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9405\nEpoch 50 of 60\n\nTesting for epoch 50 index 1:\n16/16 [==============================] - 0s 640us/step - loss: 1.9428\n\nTesting for epoch 50 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8585\nEpoch 51 of 60\n\nTesting for epoch 51 index 1:\n16/16 [==============================] - 0s 638us/step - loss: 1.8806\n\nTesting for epoch 51 index 2:\n16/16 [==============================] - 0s 608us/step - loss: 1.9145\nEpoch 52 of 60\n\nTesting for epoch 52 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9380\n\nTesting for epoch 52 index 2:\n16/16 [==============================] - 0s 615us/step - loss: 1.8934\nEpoch 53 of 60\n\nTesting for epoch 53 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9282\n\nTesting for epoch 53 index 2:\n16/16 [==============================] - 0s 651us/step - loss: 1.8956\nEpoch 54 of 60\n\nTesting for epoch 54 index 1:\n16/16 [==============================] - 0s 630us/step - loss: 1.8997\n\nTesting for epoch 54 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9230\nEpoch 55 of 60\n\nTesting for epoch 55 index 1:\n16/16 [==============================] - 0s 671us/step - loss: 1.9290\n\nTesting for epoch 55 index 2:\n16/16 [==============================] - 0s 885us/step - loss: 1.9631\nEpoch 56 of 60\n\nTesting for epoch 56 index 1:\n16/16 [==============================] - 0s 643us/step - loss: 1.9394\n\nTesting for epoch 56 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9368\nEpoch 57 of 60\n\nTesting for epoch 57 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9715\n\nTesting for epoch 57 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9327\nEpoch 58 of 60\n\nTesting for epoch 58 index 1:\n16/16 [==============================] - 0s 690us/step - loss: 1.9782\n\nTesting for epoch 58 index 2:\n16/16 [==============================] - 0s 612us/step - loss: 1.9637\nEpoch 59 of 60\n\nTesting for epoch 59 index 1:\n16/16 [==============================] - 0s 575us/step - loss: 1.9657\n\nTesting for epoch 59 index 2:\n16/16 [==============================] - 0s 890us/step - loss: 1.9923\nEpoch 60 of 60\n\nTesting for epoch 60 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9824\n\nTesting for epoch 60 index 2:\n16/16 [==============================] - 0s 658us/step - loss: 2.0536\n\n\n\noutlier_SO_GAAL_one = list(clf.labels_)\n\n\noutlier_SO_GAAL_one = list(map(lambda x: 1 if x==0  else -1,outlier_SO_GAAL_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_SO_GAAL_one,tab_linear)\n\n\n_conf.conf(\"SO-GAAL (Liu et al., 2019)\")\n\n\n\n\nAccuracy: 0.868\nPrecision: 0.954\nRecall: 0.904\nF1 Score: 0.929\n\n\n\ntwelve = eleven.append(_conf.tab)\n\n\n\nMO_GAAL\n\nclf = MO_GAAL()\nclf.fit(_df[['x', 'y']])\n_df['MO_GAAL_clf'] = clf.labels_\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n\n\nEpoch 1 of 60\n\nTesting for epoch 1 index 1:\n\nTesting for epoch 1 index 2:\nEpoch 2 of 60\n\nTesting for epoch 2 index 1:\n\nTesting for epoch 2 index 2:\nEpoch 3 of 60\n\nTesting for epoch 3 index 1:\n\nTesting for epoch 3 index 2:\nEpoch 4 of 60\n\nTesting for epoch 4 index 1:\n\nTesting for epoch 4 index 2:\nEpoch 5 of 60\n\nTesting for epoch 5 index 1:\n\nTesting for epoch 5 index 2:\nEpoch 6 of 60\n\nTesting for epoch 6 index 1:\n\nTesting for epoch 6 index 2:\nEpoch 7 of 60\n\nTesting for epoch 7 index 1:\n\nTesting for epoch 7 index 2:\nEpoch 8 of 60\n\nTesting for epoch 8 index 1:\n\nTesting for epoch 8 index 2:\nEpoch 9 of 60\n\nTesting for epoch 9 index 1:\n\nTesting for epoch 9 index 2:\nEpoch 10 of 60\n\nTesting for epoch 10 index 1:\n\nTesting for epoch 10 index 2:\nEpoch 11 of 60\n\nTesting for epoch 11 index 1:\n\nTesting for epoch 11 index 2:\nEpoch 12 of 60\n\nTesting for epoch 12 index 1:\n\nTesting for epoch 12 index 2:\nEpoch 13 of 60\n\nTesting for epoch 13 index 1:\n\nTesting for epoch 13 index 2:\nEpoch 14 of 60\n\nTesting for epoch 14 index 1:\n\nTesting for epoch 14 index 2:\nEpoch 15 of 60\n\nTesting for epoch 15 index 1:\n\nTesting for epoch 15 index 2:\nEpoch 16 of 60\n\nTesting for epoch 16 index 1:\n\nTesting for epoch 16 index 2:\nEpoch 17 of 60\n\nTesting for epoch 17 index 1:\n\nTesting for epoch 17 index 2:\nEpoch 18 of 60\n\nTesting for epoch 18 index 1:\n\nTesting for epoch 18 index 2:\nEpoch 19 of 60\n\nTesting for epoch 19 index 1:\n\nTesting for epoch 19 index 2:\nEpoch 20 of 60\n\nTesting for epoch 20 index 1:\n\nTesting for epoch 20 index 2:\nEpoch 21 of 60\n\nTesting for epoch 21 index 1:\n\nTesting for epoch 21 index 2:\n16/16 [==============================] - 0s 674us/step - loss: 0.5119\n16/16 [==============================] - 0s 1ms/step - loss: 0.8174\n16/16 [==============================] - 0s 1ms/step - loss: 1.0584\n16/16 [==============================] - 0s 1ms/step - loss: 1.2057\n16/16 [==============================] - 0s 1ms/step - loss: 1.2512\n16/16 [==============================] - 0s 653us/step - loss: 1.2690\n16/16 [==============================] - 0s 640us/step - loss: 1.2744\n16/16 [==============================] - 0s 846us/step - loss: 1.2761\n16/16 [==============================] - 0s 782us/step - loss: 1.2766\n16/16 [==============================] - 0s 1ms/step - loss: 1.2766\nEpoch 22 of 60\n\nTesting for epoch 22 index 1:\n16/16 [==============================] - 0s 665us/step - loss: 0.5016\n16/16 [==============================] - 0s 1ms/step - loss: 0.8168\n16/16 [==============================] - 0s 729us/step - loss: 1.0701\n16/16 [==============================] - 0s 619us/step - loss: 1.2239\n16/16 [==============================] - 0s 952us/step - loss: 1.2703\n16/16 [==============================] - 0s 680us/step - loss: 1.2884\n16/16 [==============================] - 0s 1ms/step - loss: 1.2938\n16/16 [==============================] - 0s 1ms/step - loss: 1.2955\n16/16 [==============================] - 0s 674us/step - loss: 1.2959\n16/16 [==============================] - 0s 680us/step - loss: 1.2959\n\nTesting for epoch 22 index 2:\n16/16 [==============================] - 0s 638us/step - loss: 0.4978\n16/16 [==============================] - 0s 1ms/step - loss: 0.8174\n16/16 [==============================] - 0s 988us/step - loss: 1.0777\n16/16 [==============================] - 0s 683us/step - loss: 1.2388\n16/16 [==============================] - 0s 1ms/step - loss: 1.2871\n16/16 [==============================] - 0s 1ms/step - loss: 1.3063\n16/16 [==============================] - 0s 899us/step - loss: 1.3121\n16/16 [==============================] - 0s 701us/step - loss: 1.3140\n16/16 [==============================] - 0s 674us/step - loss: 1.3144\n16/16 [==============================] - 0s 894us/step - loss: 1.3144\nEpoch 23 of 60\n\nTesting for epoch 23 index 1:\n16/16 [==============================] - 0s 629us/step - loss: 0.4856\n16/16 [==============================] - 0s 911us/step - loss: 0.8190\n16/16 [==============================] - 0s 1ms/step - loss: 1.0913\n16/16 [==============================] - 0s 1ms/step - loss: 1.2605\n16/16 [==============================] - 0s 1ms/step - loss: 1.3112\n16/16 [==============================] - 0s 1ms/step - loss: 1.3310\n16/16 [==============================] - 0s 2ms/step - loss: 1.3370\n16/16 [==============================] - 0s 1ms/step - loss: 1.3389\n16/16 [==============================] - 0s 745us/step - loss: 1.3393\n16/16 [==============================] - 0s 964us/step - loss: 1.3393\n\nTesting for epoch 23 index 2:\n16/16 [==============================] - 0s 640us/step - loss: 0.4780\n16/16 [==============================] - 0s 851us/step - loss: 0.8265\n16/16 [==============================] - 0s 1ms/step - loss: 1.1094\n16/16 [==============================] - 0s 1ms/step - loss: 1.2901\n16/16 [==============================] - 0s 702us/step - loss: 1.3448\n16/16 [==============================] - 0s 939us/step - loss: 1.3665\n16/16 [==============================] - 0s 854us/step - loss: 1.3731\n16/16 [==============================] - 0s 872us/step - loss: 1.3753\n16/16 [==============================] - 0s 633us/step - loss: 1.3759\n16/16 [==============================] - 0s 1ms/step - loss: 1.3759\nEpoch 24 of 60\n\nTesting for epoch 24 index 1:\n16/16 [==============================] - 0s 709us/step - loss: 0.4680\n16/16 [==============================] - 0s 964us/step - loss: 0.8342\n16/16 [==============================] - 0s 717us/step - loss: 1.1328\n16/16 [==============================] - 0s 631us/step - loss: 1.3245\n16/16 [==============================] - 0s 1ms/step - loss: 1.3825\n16/16 [==============================] - 0s 1ms/step - loss: 1.4056\n16/16 [==============================] - 0s 1ms/step - loss: 1.4125\n16/16 [==============================] - 0s 675us/step - loss: 1.4148\n16/16 [==============================] - 0s 1ms/step - loss: 1.4154\n16/16 [==============================] - 0s 1ms/step - loss: 1.4154\n\nTesting for epoch 24 index 2:\n16/16 [==============================] - 0s 683us/step - loss: 0.4700\n16/16 [==============================] - 0s 1ms/step - loss: 0.8212\n16/16 [==============================] - 0s 608us/step - loss: 1.1067\n16/16 [==============================] - 0s 1ms/step - loss: 1.2919\n16/16 [==============================] - 0s 645us/step - loss: 1.3484\n16/16 [==============================] - 0s 655us/step - loss: 1.3713\n16/16 [==============================] - 0s 1ms/step - loss: 1.3780\n16/16 [==============================] - 0s 707us/step - loss: 1.3802\n16/16 [==============================] - 0s 1ms/step - loss: 1.3807\n16/16 [==============================] - 0s 1ms/step - loss: 1.3807\nEpoch 25 of 60\n\nTesting for epoch 25 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4568\n16/16 [==============================] - 0s 1ms/step - loss: 0.8264\n16/16 [==============================] - 0s 866us/step - loss: 1.1304\n16/16 [==============================] - 0s 737us/step - loss: 1.3292\n16/16 [==============================] - 0s 1ms/step - loss: 1.3891\n16/16 [==============================] - 0s 859us/step - loss: 1.4139\n16/16 [==============================] - 0s 664us/step - loss: 1.4209\n16/16 [==============================] - 0s 1ms/step - loss: 1.4233\n16/16 [==============================] - 0s 632us/step - loss: 1.4239\n16/16 [==============================] - 0s 2ms/step - loss: 1.4239\n\nTesting for epoch 25 index 2:\n16/16 [==============================] - 0s 660us/step - loss: 0.4472\n16/16 [==============================] - 0s 613us/step - loss: 0.8308\n16/16 [==============================] - 0s 633us/step - loss: 1.1472\n16/16 [==============================] - 0s 640us/step - loss: 1.3580\n16/16 [==============================] - 0s 1ms/step - loss: 1.4212\n16/16 [==============================] - 0s 644us/step - loss: 1.4477\n16/16 [==============================] - 0s 621us/step - loss: 1.4553\n16/16 [==============================] - 0s 601us/step - loss: 1.4579\n16/16 [==============================] - 0s 799us/step - loss: 1.4585\n16/16 [==============================] - 0s 663us/step - loss: 1.4585\nEpoch 26 of 60\n\nTesting for epoch 26 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4458\n16/16 [==============================] - 0s 639us/step - loss: 0.8332\n16/16 [==============================] - 0s 622us/step - loss: 1.1594\n16/16 [==============================] - 0s 620us/step - loss: 1.3754\n16/16 [==============================] - 0s 987us/step - loss: 1.4394\n16/16 [==============================] - 0s 652us/step - loss: 1.4660\n16/16 [==============================] - 0s 641us/step - loss: 1.4735\n16/16 [==============================] - 0s 628us/step - loss: 1.4761\n16/16 [==============================] - 0s 1ms/step - loss: 1.4766\n16/16 [==============================] - 0s 597us/step - loss: 1.4766\n\nTesting for epoch 26 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4323\n16/16 [==============================] - 0s 1ms/step - loss: 0.8289\n16/16 [==============================] - 0s 1ms/step - loss: 1.1745\n16/16 [==============================] - 0s 2ms/step - loss: 1.4047\n16/16 [==============================] - 0s 1ms/step - loss: 1.4730\n16/16 [==============================] - 0s 835us/step - loss: 1.5017\n16/16 [==============================] - 0s 684us/step - loss: 1.5100\n16/16 [==============================] - 0s 643us/step - loss: 1.5128\n16/16 [==============================] - 0s 1ms/step - loss: 1.5135\n16/16 [==============================] - 0s 1ms/step - loss: 1.5135\nEpoch 27 of 60\n\nTesting for epoch 27 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4311\n16/16 [==============================] - 0s 693us/step - loss: 0.8327\n16/16 [==============================] - 0s 639us/step - loss: 1.1867\n16/16 [==============================] - 0s 606us/step - loss: 1.4217\n16/16 [==============================] - 0s 816us/step - loss: 1.4904\n16/16 [==============================] - 0s 828us/step - loss: 1.5189\n16/16 [==============================] - 0s 1ms/step - loss: 1.5270\n16/16 [==============================] - 0s 1ms/step - loss: 1.5298\n16/16 [==============================] - 0s 1ms/step - loss: 1.5303\n16/16 [==============================] - 0s 779us/step - loss: 1.5303\n\nTesting for epoch 27 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4224\n16/16 [==============================] - 0s 1ms/step - loss: 0.8208\n16/16 [==============================] - 0s 998us/step - loss: 1.1776\n16/16 [==============================] - 0s 1ms/step - loss: 1.4157\n16/16 [==============================] - 0s 635us/step - loss: 1.4850\n16/16 [==============================] - 0s 1ms/step - loss: 1.5136\n16/16 [==============================] - 0s 1ms/step - loss: 1.5217\n16/16 [==============================] - 0s 640us/step - loss: 1.5245\n16/16 [==============================] - 0s 590us/step - loss: 1.5251\n16/16 [==============================] - 0s 1ms/step - loss: 1.5251\nEpoch 28 of 60\n\nTesting for epoch 28 index 1:\n16/16 [==============================] - 0s 707us/step - loss: 0.4220\n16/16 [==============================] - 0s 790us/step - loss: 0.8241\n16/16 [==============================] - 0s 1ms/step - loss: 1.1871\n16/16 [==============================] - 0s 829us/step - loss: 1.4277\n16/16 [==============================] - 0s 796us/step - loss: 1.4965\n16/16 [==============================] - 0s 1ms/step - loss: 1.5243\n16/16 [==============================] - 0s 1ms/step - loss: 1.5321\n16/16 [==============================] - 0s 1ms/step - loss: 1.5347\n16/16 [==============================] - 0s 611us/step - loss: 1.5352\n16/16 [==============================] - 0s 607us/step - loss: 1.5351\n\nTesting for epoch 28 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4162\n16/16 [==============================] - 0s 765us/step - loss: 0.8240\n16/16 [==============================] - 0s 1ms/step - loss: 1.1967\n16/16 [==============================] - 0s 1ms/step - loss: 1.4447\n16/16 [==============================] - 0s 1ms/step - loss: 1.5154\n16/16 [==============================] - 0s 718us/step - loss: 1.5437\n16/16 [==============================] - 0s 1ms/step - loss: 1.5517\n16/16 [==============================] - 0s 1ms/step - loss: 1.5543\n16/16 [==============================] - 0s 659us/step - loss: 1.5548\n16/16 [==============================] - 0s 2ms/step - loss: 1.5547\nEpoch 29 of 60\n\nTesting for epoch 29 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4156\n16/16 [==============================] - 0s 843us/step - loss: 0.8177\n16/16 [==============================] - 0s 623us/step - loss: 1.1876\n16/16 [==============================] - 0s 1ms/step - loss: 1.4313\n16/16 [==============================] - 0s 1ms/step - loss: 1.4993\n16/16 [==============================] - 0s 1ms/step - loss: 1.5259\n16/16 [==============================] - 0s 723us/step - loss: 1.5332\n16/16 [==============================] - 0s 640us/step - loss: 1.5355\n16/16 [==============================] - 0s 625us/step - loss: 1.5358\n16/16 [==============================] - 0s 634us/step - loss: 1.5357\n\nTesting for epoch 29 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4090\n16/16 [==============================] - 0s 1ms/step - loss: 0.8233\n16/16 [==============================] - 0s 1ms/step - loss: 1.2093\n16/16 [==============================] - 0s 643us/step - loss: 1.4641\n16/16 [==============================] - 0s 627us/step - loss: 1.5348\n16/16 [==============================] - 0s 668us/step - loss: 1.5623\n16/16 [==============================] - 0s 885us/step - loss: 1.5697\n16/16 [==============================] - 0s 887us/step - loss: 1.5721\n16/16 [==============================] - 0s 640us/step - loss: 1.5724\n16/16 [==============================] - 0s 1ms/step - loss: 1.5724\nEpoch 30 of 60\n\nTesting for epoch 30 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4023\n16/16 [==============================] - 0s 847us/step - loss: 0.8249\n16/16 [==============================] - 0s 1ms/step - loss: 1.2211\n16/16 [==============================] - 0s 669us/step - loss: 1.4795\n16/16 [==============================] - 0s 837us/step - loss: 1.5497\n16/16 [==============================] - 0s 1ms/step - loss: 1.5763\n16/16 [==============================] - 0s 792us/step - loss: 1.5833\n16/16 [==============================] - 0s 1ms/step - loss: 1.5854\n16/16 [==============================] - 0s 821us/step - loss: 1.5856\n16/16 [==============================] - 0s 654us/step - loss: 1.5855\n\nTesting for epoch 30 index 2:\n16/16 [==============================] - 0s 815us/step - loss: 0.4039\n16/16 [==============================] - 0s 621us/step - loss: 0.8273\n16/16 [==============================] - 0s 636us/step - loss: 1.2286\n16/16 [==============================] - 0s 1ms/step - loss: 1.4900\n16/16 [==============================] - 0s 1ms/step - loss: 1.5605\n16/16 [==============================] - 0s 1ms/step - loss: 1.5869\n16/16 [==============================] - 0s 1ms/step - loss: 1.5938\n16/16 [==============================] - 0s 2ms/step - loss: 1.5958\n16/16 [==============================] - 0s 1ms/step - loss: 1.5960\n16/16 [==============================] - 0s 721us/step - loss: 1.5958\nEpoch 31 of 60\n\nTesting for epoch 31 index 1:\n16/16 [==============================] - 0s 822us/step - loss: 0.3922\n16/16 [==============================] - 0s 618us/step - loss: 0.8303\n16/16 [==============================] - 0s 979us/step - loss: 1.2484\n16/16 [==============================] - 0s 594us/step - loss: 1.5177\n16/16 [==============================] - 0s 584us/step - loss: 1.5887\n16/16 [==============================] - 0s 886us/step - loss: 1.6148\n16/16 [==============================] - 0s 616us/step - loss: 1.6214\n16/16 [==============================] - 0s 986us/step - loss: 1.6232\n16/16 [==============================] - 0s 634us/step - loss: 1.6234\n16/16 [==============================] - 0s 647us/step - loss: 1.6232\n\nTesting for epoch 31 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3917\n16/16 [==============================] - 0s 638us/step - loss: 0.8412\n16/16 [==============================] - 0s 1ms/step - loss: 1.2745\n16/16 [==============================] - 0s 1ms/step - loss: 1.5525\n16/16 [==============================] - 0s 597us/step - loss: 1.6251\n16/16 [==============================] - 0s 1ms/step - loss: 1.6514\n16/16 [==============================] - 0s 1ms/step - loss: 1.6580\n16/16 [==============================] - 0s 1ms/step - loss: 1.6598\n16/16 [==============================] - 0s 1ms/step - loss: 1.6598\n16/16 [==============================] - 0s 875us/step - loss: 1.6597\nEpoch 32 of 60\n\nTesting for epoch 32 index 1:\n16/16 [==============================] - 0s 643us/step - loss: 0.3861\n16/16 [==============================] - 0s 1ms/step - loss: 0.8410\n16/16 [==============================] - 0s 1ms/step - loss: 1.2819\n16/16 [==============================] - 0s 747us/step - loss: 1.5604\n16/16 [==============================] - 0s 1ms/step - loss: 1.6314\n16/16 [==============================] - 0s 2ms/step - loss: 1.6566\n16/16 [==============================] - 0s 1ms/step - loss: 1.6625\n16/16 [==============================] - 0s 1ms/step - loss: 1.6641\n16/16 [==============================] - 0s 682us/step - loss: 1.6641\n16/16 [==============================] - 0s 868us/step - loss: 1.6639\n\nTesting for epoch 32 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3785\n16/16 [==============================] - 0s 640us/step - loss: 0.8366\n16/16 [==============================] - 0s 620us/step - loss: 1.2831\n16/16 [==============================] - 0s 630us/step - loss: 1.5628\n16/16 [==============================] - 0s 569us/step - loss: 1.6327\n16/16 [==============================] - 0s 1ms/step - loss: 1.6570\n16/16 [==============================] - 0s 1ms/step - loss: 1.6626\n16/16 [==============================] - 0s 671us/step - loss: 1.6639\n16/16 [==============================] - 0s 1ms/step - loss: 1.6638\n16/16 [==============================] - 0s 1ms/step - loss: 1.6636\nEpoch 33 of 60\n\nTesting for epoch 33 index 1:\n16/16 [==============================] - 0s 686us/step - loss: 0.3794\n16/16 [==============================] - 0s 1ms/step - loss: 0.8368\n16/16 [==============================] - 0s 590us/step - loss: 1.2836\n16/16 [==============================] - 0s 1ms/step - loss: 1.5578\n16/16 [==============================] - 0s 1ms/step - loss: 1.6242\n16/16 [==============================] - 0s 1ms/step - loss: 1.6467\n16/16 [==============================] - 0s 1ms/step - loss: 1.6516\n16/16 [==============================] - 0s 880us/step - loss: 1.6526\n16/16 [==============================] - 0s 1ms/step - loss: 1.6524\n16/16 [==============================] - 0s 744us/step - loss: 1.6521\n\nTesting for epoch 33 index 2:\n16/16 [==============================] - 0s 639us/step - loss: 0.3767\n16/16 [==============================] - 0s 675us/step - loss: 0.8386\n16/16 [==============================] - 0s 636us/step - loss: 1.2925\n16/16 [==============================] - 0s 667us/step - loss: 1.5686\n16/16 [==============================] - 0s 570us/step - loss: 1.6342\n16/16 [==============================] - 0s 650us/step - loss: 1.6560\n16/16 [==============================] - 0s 1ms/step - loss: 1.6605\n16/16 [==============================] - 0s 1ms/step - loss: 1.6614\n16/16 [==============================] - 0s 828us/step - loss: 1.6611\n16/16 [==============================] - 0s 754us/step - loss: 1.6608\nEpoch 34 of 60\n\nTesting for epoch 34 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3785\n16/16 [==============================] - 0s 606us/step - loss: 0.8568\n16/16 [==============================] - 0s 1ms/step - loss: 1.3267\n16/16 [==============================] - 0s 1ms/step - loss: 1.6075\n16/16 [==============================] - 0s 632us/step - loss: 1.6723\n16/16 [==============================] - 0s 700us/step - loss: 1.6932\n16/16 [==============================] - 0s 814us/step - loss: 1.6974\n16/16 [==============================] - 0s 1ms/step - loss: 1.6980\n16/16 [==============================] - 0s 2ms/step - loss: 1.6977\n16/16 [==============================] - 0s 1ms/step - loss: 1.6974\n\nTesting for epoch 34 index 2:\n16/16 [==============================] - 0s 611us/step - loss: 0.3640\n16/16 [==============================] - 0s 586us/step - loss: 0.8516\n16/16 [==============================] - 0s 613us/step - loss: 1.3334\n16/16 [==============================] - 0s 631us/step - loss: 1.6188\n16/16 [==============================] - 0s 635us/step - loss: 1.6834\n16/16 [==============================] - 0s 1ms/step - loss: 1.7039\n16/16 [==============================] - 0s 600us/step - loss: 1.7078\n16/16 [==============================] - 0s 784us/step - loss: 1.7083\n16/16 [==============================] - 0s 1ms/step - loss: 1.7080\n16/16 [==============================] - 0s 654us/step - loss: 1.7076\nEpoch 35 of 60\n\nTesting for epoch 35 index 1:\n16/16 [==============================] - 0s 642us/step - loss: 0.3623\n16/16 [==============================] - 0s 870us/step - loss: 0.8627\n16/16 [==============================] - 0s 1ms/step - loss: 1.3550\n16/16 [==============================] - 0s 935us/step - loss: 1.6411\n16/16 [==============================] - 0s 631us/step - loss: 1.7039\n16/16 [==============================] - 0s 1ms/step - loss: 1.7231\n16/16 [==============================] - 0s 659us/step - loss: 1.7264\n16/16 [==============================] - 0s 1ms/step - loss: 1.7267\n16/16 [==============================] - 0s 1ms/step - loss: 1.7262\n16/16 [==============================] - 0s 706us/step - loss: 1.7259\n\nTesting for epoch 35 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3604\n16/16 [==============================] - 0s 635us/step - loss: 0.8668\n16/16 [==============================] - 0s 889us/step - loss: 1.3676\n16/16 [==============================] - 0s 1ms/step - loss: 1.6572\n16/16 [==============================] - 0s 947us/step - loss: 1.7200\n16/16 [==============================] - 0s 2ms/step - loss: 1.7389\n16/16 [==============================] - 0s 772us/step - loss: 1.7421\n16/16 [==============================] - 0s 1ms/step - loss: 1.7424\n16/16 [==============================] - 0s 1ms/step - loss: 1.7419\n16/16 [==============================] - 0s 744us/step - loss: 1.7415\nEpoch 36 of 60\n\nTesting for epoch 36 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3571\n16/16 [==============================] - 0s 680us/step - loss: 0.8755\n16/16 [==============================] - 0s 751us/step - loss: 1.3899\n16/16 [==============================] - 0s 1ms/step - loss: 1.6814\n16/16 [==============================] - 0s 1ms/step - loss: 1.7429\n16/16 [==============================] - 0s 1ms/step - loss: 1.7609\n16/16 [==============================] - 0s 681us/step - loss: 1.7637\n16/16 [==============================] - 0s 677us/step - loss: 1.7638\n16/16 [==============================] - 0s 646us/step - loss: 1.7633\n16/16 [==============================] - 0s 1ms/step - loss: 1.7629\n\nTesting for epoch 36 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3680\n16/16 [==============================] - 0s 623us/step - loss: 0.8560\n16/16 [==============================] - 0s 652us/step - loss: 1.3413\n16/16 [==============================] - 0s 605us/step - loss: 1.6123\n16/16 [==============================] - 0s 622us/step - loss: 1.6680\n16/16 [==============================] - 0s 808us/step - loss: 1.6837\n16/16 [==============================] - 0s 1ms/step - loss: 1.6859\n16/16 [==============================] - 0s 889us/step - loss: 1.6858\n16/16 [==============================] - 0s 633us/step - loss: 1.6852\n16/16 [==============================] - 0s 626us/step - loss: 1.6848\nEpoch 37 of 60\n\nTesting for epoch 37 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3582\n16/16 [==============================] - 0s 660us/step - loss: 0.8667\n16/16 [==============================] - 0s 649us/step - loss: 1.3768\n16/16 [==============================] - 0s 1000us/step - loss: 1.6562\n16/16 [==============================] - 0s 1ms/step - loss: 1.7123\n16/16 [==============================] - 0s 634us/step - loss: 1.7277\n16/16 [==============================] - 0s 685us/step - loss: 1.7297\n16/16 [==============================] - 0s 1ms/step - loss: 1.7295\n16/16 [==============================] - 0s 1ms/step - loss: 1.7288\n16/16 [==============================] - 0s 628us/step - loss: 1.7284\n\nTesting for epoch 37 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3583\n16/16 [==============================] - 0s 1ms/step - loss: 0.8671\n16/16 [==============================] - 0s 1ms/step - loss: 1.3803\n16/16 [==============================] - 0s 1ms/step - loss: 1.6596\n16/16 [==============================] - 0s 1ms/step - loss: 1.7150\n16/16 [==============================] - 0s 692us/step - loss: 1.7298\n16/16 [==============================] - 0s 979us/step - loss: 1.7317\n16/16 [==============================] - 0s 1ms/step - loss: 1.7314\n16/16 [==============================] - 0s 1ms/step - loss: 1.7308\n16/16 [==============================] - 0s 1ms/step - loss: 1.7304\nEpoch 38 of 60\n\nTesting for epoch 38 index 1:\n16/16 [==============================] - 0s 863us/step - loss: 0.3471\n16/16 [==============================] - 0s 661us/step - loss: 0.8780\n16/16 [==============================] - 0s 659us/step - loss: 1.4219\n16/16 [==============================] - 0s 1ms/step - loss: 1.7117\n16/16 [==============================] - 0s 1ms/step - loss: 1.7680\n16/16 [==============================] - 0s 1ms/step - loss: 1.7827\n16/16 [==============================] - 0s 824us/step - loss: 1.7845\n16/16 [==============================] - 0s 1ms/step - loss: 1.7841\n16/16 [==============================] - 0s 1ms/step - loss: 1.7834\n16/16 [==============================] - 0s 2ms/step - loss: 1.7830\n\nTesting for epoch 38 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3466\n16/16 [==============================] - 0s 680us/step - loss: 0.8801\n16/16 [==============================] - 0s 1ms/step - loss: 1.4285\n16/16 [==============================] - 0s 615us/step - loss: 1.7186\n16/16 [==============================] - 0s 1ms/step - loss: 1.7739\n16/16 [==============================] - 0s 1ms/step - loss: 1.7880\n16/16 [==============================] - 0s 1ms/step - loss: 1.7895\n16/16 [==============================] - 0s 619us/step - loss: 1.7890\n16/16 [==============================] - 0s 1ms/step - loss: 1.7882\n16/16 [==============================] - 0s 587us/step - loss: 1.7878\nEpoch 39 of 60\n\nTesting for epoch 39 index 1:\n16/16 [==============================] - 0s 861us/step - loss: 0.3492\n16/16 [==============================] - 0s 879us/step - loss: 0.8719\n16/16 [==============================] - 0s 664us/step - loss: 1.4147\n16/16 [==============================] - 0s 643us/step - loss: 1.6946\n16/16 [==============================] - 0s 1ms/step - loss: 1.7465\n16/16 [==============================] - 0s 621us/step - loss: 1.7591\n16/16 [==============================] - 0s 594us/step - loss: 1.7602\n16/16 [==============================] - 0s 612us/step - loss: 1.7596\n16/16 [==============================] - 0s 594us/step - loss: 1.7588\n16/16 [==============================] - 0s 660us/step - loss: 1.7584\n\nTesting for epoch 39 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3448\n16/16 [==============================] - 0s 891us/step - loss: 0.8840\n16/16 [==============================] - 0s 1ms/step - loss: 1.4475\n16/16 [==============================] - 0s 634us/step - loss: 1.7374\n16/16 [==============================] - 0s 1ms/step - loss: 1.7907\n16/16 [==============================] - 0s 1ms/step - loss: 1.8035\n16/16 [==============================] - 0s 698us/step - loss: 1.8046\n16/16 [==============================] - 0s 660us/step - loss: 1.8040\n16/16 [==============================] - 0s 828us/step - loss: 1.8032\n16/16 [==============================] - 0s 1ms/step - loss: 1.8028\nEpoch 40 of 60\n\nTesting for epoch 40 index 1:\n16/16 [==============================] - 0s 604us/step - loss: 0.3427\n16/16 [==============================] - 0s 629us/step - loss: 0.8792\n16/16 [==============================] - 0s 613us/step - loss: 1.4449\n16/16 [==============================] - 0s 605us/step - loss: 1.7294\n16/16 [==============================] - 0s 1ms/step - loss: 1.7803\n16/16 [==============================] - 0s 1ms/step - loss: 1.7920\n16/16 [==============================] - 0s 670us/step - loss: 1.7928\n16/16 [==============================] - 0s 1ms/step - loss: 1.7921\n16/16 [==============================] - 0s 702us/step - loss: 1.7912\n16/16 [==============================] - 0s 978us/step - loss: 1.7908\n\nTesting for epoch 40 index 2:\n16/16 [==============================] - 0s 884us/step - loss: 0.3356\n16/16 [==============================] - 0s 1ms/step - loss: 0.8885\n16/16 [==============================] - 0s 850us/step - loss: 1.4743\n16/16 [==============================] - 0s 730us/step - loss: 1.7694\n16/16 [==============================] - 0s 1ms/step - loss: 1.8221\n16/16 [==============================] - 0s 944us/step - loss: 1.8343\n16/16 [==============================] - 0s 932us/step - loss: 1.8352\n16/16 [==============================] - 0s 696us/step - loss: 1.8345\n16/16 [==============================] - 0s 1ms/step - loss: 1.8337\n16/16 [==============================] - 0s 1ms/step - loss: 1.8333\nEpoch 41 of 60\n\nTesting for epoch 41 index 1:\n16/16 [==============================] - 0s 612us/step - loss: 0.3393\n16/16 [==============================] - 0s 595us/step - loss: 0.8775\n16/16 [==============================] - 0s 1ms/step - loss: 1.4502\n16/16 [==============================] - 0s 1ms/step - loss: 1.7321\n16/16 [==============================] - 0s 1ms/step - loss: 1.7808\n16/16 [==============================] - 0s 1ms/step - loss: 1.7913\n16/16 [==============================] - 0s 1ms/step - loss: 1.7917\n16/16 [==============================] - 0s 663us/step - loss: 1.7909\n16/16 [==============================] - 0s 688us/step - loss: 1.7900\n16/16 [==============================] - 0s 659us/step - loss: 1.7895\n\nTesting for epoch 41 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3349\n16/16 [==============================] - 0s 913us/step - loss: 0.8782\n16/16 [==============================] - 0s 683us/step - loss: 1.4573\n16/16 [==============================] - 0s 2ms/step - loss: 1.7431\n16/16 [==============================] - 0s 1ms/step - loss: 1.7923\n16/16 [==============================] - 0s 1ms/step - loss: 1.8028\n16/16 [==============================] - 0s 647us/step - loss: 1.8033\n16/16 [==============================] - 0s 633us/step - loss: 1.8024\n16/16 [==============================] - 0s 573us/step - loss: 1.8015\n16/16 [==============================] - 0s 2ms/step - loss: 1.8011\nEpoch 42 of 60\n\nTesting for epoch 42 index 1:\n16/16 [==============================] - 0s 712us/step - loss: 0.3296\n16/16 [==============================] - 0s 757us/step - loss: 0.8910\n16/16 [==============================] - 0s 1ms/step - loss: 1.4933\n16/16 [==============================] - 0s 676us/step - loss: 1.7869\n16/16 [==============================] - 0s 639us/step - loss: 1.8366\n16/16 [==============================] - 0s 622us/step - loss: 1.8470\n16/16 [==============================] - 0s 605us/step - loss: 1.8474\n16/16 [==============================] - 0s 1ms/step - loss: 1.8464\n16/16 [==============================] - 0s 1ms/step - loss: 1.8456\n16/16 [==============================] - 0s 1ms/step - loss: 1.8451\n\nTesting for epoch 42 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3304\n16/16 [==============================] - 0s 1ms/step - loss: 0.8862\n16/16 [==============================] - 0s 704us/step - loss: 1.4818\n16/16 [==============================] - 0s 1ms/step - loss: 1.7733\n16/16 [==============================] - 0s 679us/step - loss: 1.8222\n16/16 [==============================] - 0s 644us/step - loss: 1.8324\n16/16 [==============================] - 0s 1ms/step - loss: 1.8327\n16/16 [==============================] - 0s 1ms/step - loss: 1.8318\n16/16 [==============================] - 0s 653us/step - loss: 1.8309\n16/16 [==============================] - 0s 1ms/step - loss: 1.8304\nEpoch 43 of 60\n\nTesting for epoch 43 index 1:\n16/16 [==============================] - 0s 769us/step - loss: 0.3289\n16/16 [==============================] - 0s 1ms/step - loss: 0.8923\n16/16 [==============================] - 0s 889us/step - loss: 1.4981\n16/16 [==============================] - 0s 676us/step - loss: 1.7912\n16/16 [==============================] - 0s 576us/step - loss: 1.8395\n16/16 [==============================] - 0s 594us/step - loss: 1.8492\n16/16 [==============================] - 0s 614us/step - loss: 1.8493\n16/16 [==============================] - 0s 754us/step - loss: 1.8483\n16/16 [==============================] - 0s 1ms/step - loss: 1.8474\n16/16 [==============================] - 0s 620us/step - loss: 1.8469\n\nTesting for epoch 43 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3236\n16/16 [==============================] - 0s 669us/step - loss: 0.8897\n16/16 [==============================] - 0s 1ms/step - loss: 1.4962\n16/16 [==============================] - 0s 622us/step - loss: 1.7918\n16/16 [==============================] - 0s 636us/step - loss: 1.8402\n16/16 [==============================] - 0s 609us/step - loss: 1.8499\n16/16 [==============================] - 0s 589us/step - loss: 1.8499\n16/16 [==============================] - 0s 1ms/step - loss: 1.8488\n16/16 [==============================] - 0s 1ms/step - loss: 1.8479\n16/16 [==============================] - 0s 1ms/step - loss: 1.8474\nEpoch 44 of 60\n\nTesting for epoch 44 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3240\n16/16 [==============================] - 0s 1ms/step - loss: 0.9065\n16/16 [==============================] - 0s 1ms/step - loss: 1.5340\n16/16 [==============================] - 0s 699us/step - loss: 1.8380\n16/16 [==============================] - 0s 644us/step - loss: 1.8875\n16/16 [==============================] - 0s 603us/step - loss: 1.8973\n16/16 [==============================] - 0s 1ms/step - loss: 1.8974\n16/16 [==============================] - 0s 1ms/step - loss: 1.8964\n16/16 [==============================] - 0s 1ms/step - loss: 1.8954\n16/16 [==============================] - 0s 773us/step - loss: 1.8950\n\nTesting for epoch 44 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3120\n16/16 [==============================] - 0s 980us/step - loss: 0.9140\n16/16 [==============================] - 0s 1ms/step - loss: 1.5655\n16/16 [==============================] - 0s 1ms/step - loss: 1.8840\n16/16 [==============================] - 0s 1ms/step - loss: 1.9360\n16/16 [==============================] - 0s 677us/step - loss: 1.9465\n16/16 [==============================] - 0s 1ms/step - loss: 1.9468\n16/16 [==============================] - 0s 736us/step - loss: 1.9458\n16/16 [==============================] - 0s 645us/step - loss: 1.9449\n16/16 [==============================] - 0s 648us/step - loss: 1.9444\nEpoch 45 of 60\n\nTesting for epoch 45 index 1:\n16/16 [==============================] - 0s 673us/step - loss: 0.3280\n16/16 [==============================] - 0s 651us/step - loss: 0.8989\n16/16 [==============================] - 0s 1ms/step - loss: 1.5171\n16/16 [==============================] - 0s 626us/step - loss: 1.8154\n16/16 [==============================] - 0s 1ms/step - loss: 1.8629\n16/16 [==============================] - 0s 629us/step - loss: 1.8720\n16/16 [==============================] - 0s 1ms/step - loss: 1.8720\n16/16 [==============================] - 0s 1ms/step - loss: 1.8709\n16/16 [==============================] - 0s 1ms/step - loss: 1.8700\n16/16 [==============================] - 0s 632us/step - loss: 1.8696\n\nTesting for epoch 45 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3253\n16/16 [==============================] - 0s 816us/step - loss: 0.8820\n16/16 [==============================] - 0s 782us/step - loss: 1.4859\n16/16 [==============================] - 0s 1ms/step - loss: 1.7760\n16/16 [==============================] - 0s 1ms/step - loss: 1.8211\n16/16 [==============================] - 0s 672us/step - loss: 1.8292\n16/16 [==============================] - 0s 2ms/step - loss: 1.8287\n16/16 [==============================] - 0s 1ms/step - loss: 1.8275\n16/16 [==============================] - 0s 2ms/step - loss: 1.8265\n16/16 [==============================] - 0s 705us/step - loss: 1.8259\nEpoch 46 of 60\n\nTesting for epoch 46 index 1:\n16/16 [==============================] - 0s 629us/step - loss: 0.3018\n16/16 [==============================] - 0s 609us/step - loss: 0.9113\n16/16 [==============================] - 0s 1ms/step - loss: 1.5796\n16/16 [==============================] - 0s 1ms/step - loss: 1.8997\n16/16 [==============================] - 0s 1ms/step - loss: 1.9496\n16/16 [==============================] - 0s 657us/step - loss: 1.9589\n16/16 [==============================] - 0s 591us/step - loss: 1.9587\n16/16 [==============================] - 0s 1ms/step - loss: 1.9575\n16/16 [==============================] - 0s 2ms/step - loss: 1.9565\n16/16 [==============================] - 0s 893us/step - loss: 1.9560\n\nTesting for epoch 46 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3160\n16/16 [==============================] - 0s 649us/step - loss: 0.8929\n16/16 [==============================] - 0s 663us/step - loss: 1.5280\n16/16 [==============================] - 0s 668us/step - loss: 1.8325\n16/16 [==============================] - 0s 1ms/step - loss: 1.8797\n16/16 [==============================] - 0s 698us/step - loss: 1.8884\n16/16 [==============================] - 0s 700us/step - loss: 1.8881\n16/16 [==============================] - 0s 631us/step - loss: 1.8869\n16/16 [==============================] - 0s 1ms/step - loss: 1.8859\n16/16 [==============================] - 0s 649us/step - loss: 1.8854\nEpoch 47 of 60\n\nTesting for epoch 47 index 1:\n16/16 [==============================] - 0s 684us/step - loss: 0.3158\n16/16 [==============================] - 0s 627us/step - loss: 0.9064\n16/16 [==============================] - 0s 1ms/step - loss: 1.5588\n16/16 [==============================] - 0s 632us/step - loss: 1.8678\n16/16 [==============================] - 0s 618us/step - loss: 1.9149\n16/16 [==============================] - 0s 585us/step - loss: 1.9232\n16/16 [==============================] - 0s 1ms/step - loss: 1.9228\n16/16 [==============================] - 0s 1ms/step - loss: 1.9215\n16/16 [==============================] - 0s 1ms/step - loss: 1.9205\n16/16 [==============================] - 0s 694us/step - loss: 1.9200\n\nTesting for epoch 47 index 2:\n16/16 [==============================] - 0s 695us/step - loss: 0.3114\n16/16 [==============================] - 0s 805us/step - loss: 0.8972\n16/16 [==============================] - 0s 899us/step - loss: 1.5487\n16/16 [==============================] - 0s 1ms/step - loss: 1.8577\n16/16 [==============================] - 0s 705us/step - loss: 1.9042\n16/16 [==============================] - 0s 597us/step - loss: 1.9123\n16/16 [==============================] - 0s 630us/step - loss: 1.9118\n16/16 [==============================] - 0s 706us/step - loss: 1.9105\n16/16 [==============================] - 0s 1ms/step - loss: 1.9094\n16/16 [==============================] - 0s 790us/step - loss: 1.9089\nEpoch 48 of 60\n\nTesting for epoch 48 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3079\n16/16 [==============================] - 0s 1ms/step - loss: 0.9138\n16/16 [==============================] - 0s 610us/step - loss: 1.5910\n16/16 [==============================] - 0s 1ms/step - loss: 1.9092\n16/16 [==============================] - 0s 680us/step - loss: 1.9566\n16/16 [==============================] - 0s 609us/step - loss: 1.9647\n16/16 [==============================] - 0s 617us/step - loss: 1.9641\n16/16 [==============================] - 0s 620us/step - loss: 1.9628\n16/16 [==============================] - 0s 621us/step - loss: 1.9617\n16/16 [==============================] - 0s 639us/step - loss: 1.9612\n\nTesting for epoch 48 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2996\n16/16 [==============================] - 0s 668us/step - loss: 0.9044\n16/16 [==============================] - 0s 1ms/step - loss: 1.5879\n16/16 [==============================] - 0s 1ms/step - loss: 1.9109\n16/16 [==============================] - 0s 1ms/step - loss: 1.9594\n16/16 [==============================] - 0s 1ms/step - loss: 1.9680\n16/16 [==============================] - 0s 677us/step - loss: 1.9676\n16/16 [==============================] - 0s 640us/step - loss: 1.9664\n16/16 [==============================] - 0s 630us/step - loss: 1.9654\n16/16 [==============================] - 0s 612us/step - loss: 1.9649\nEpoch 49 of 60\n\nTesting for epoch 49 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3126\n16/16 [==============================] - 0s 1ms/step - loss: 0.8881\n16/16 [==============================] - 0s 985us/step - loss: 1.5372\n16/16 [==============================] - 0s 2ms/step - loss: 1.8393\n16/16 [==============================] - 0s 621us/step - loss: 1.8833\n16/16 [==============================] - 0s 753us/step - loss: 1.8904\n16/16 [==============================] - 0s 1ms/step - loss: 1.8896\n16/16 [==============================] - 0s 602us/step - loss: 1.8883\n16/16 [==============================] - 0s 689us/step - loss: 1.8872\n16/16 [==============================] - 0s 1ms/step - loss: 1.8867\n\nTesting for epoch 49 index 2:\n16/16 [==============================] - 0s 658us/step - loss: 0.3055\n16/16 [==============================] - 0s 641us/step - loss: 0.8975\n16/16 [==============================] - 0s 577us/step - loss: 1.5715\n16/16 [==============================] - 0s 612us/step - loss: 1.8861\n16/16 [==============================] - 0s 605us/step - loss: 1.9320\n16/16 [==============================] - 0s 997us/step - loss: 1.9395\n16/16 [==============================] - 0s 1ms/step - loss: 1.9388\n16/16 [==============================] - 0s 644us/step - loss: 1.9374\n16/16 [==============================] - 0s 952us/step - loss: 1.9364\n16/16 [==============================] - 0s 628us/step - loss: 1.9358\nEpoch 50 of 60\n\nTesting for epoch 50 index 1:\n16/16 [==============================] - 0s 796us/step - loss: 0.2940\n16/16 [==============================] - 0s 649us/step - loss: 0.9002\n16/16 [==============================] - 0s 1ms/step - loss: 1.5930\n16/16 [==============================] - 0s 964us/step - loss: 1.9135\n16/16 [==============================] - 0s 1ms/step - loss: 1.9596\n16/16 [==============================] - 0s 674us/step - loss: 1.9670\n16/16 [==============================] - 0s 652us/step - loss: 1.9662\n16/16 [==============================] - 0s 642us/step - loss: 1.9648\n16/16 [==============================] - 0s 1ms/step - loss: 1.9638\n16/16 [==============================] - 0s 1ms/step - loss: 1.9633\n\nTesting for epoch 50 index 2:\n16/16 [==============================] - 0s 665us/step - loss: 0.3044\n16/16 [==============================] - 0s 665us/step - loss: 0.9019\n16/16 [==============================] - 0s 1ms/step - loss: 1.5888\n16/16 [==============================] - 0s 665us/step - loss: 1.9064\n16/16 [==============================] - 0s 1ms/step - loss: 1.9517\n16/16 [==============================] - 0s 660us/step - loss: 1.9588\n16/16 [==============================] - 0s 691us/step - loss: 1.9579\n16/16 [==============================] - 0s 1ms/step - loss: 1.9565\n16/16 [==============================] - 0s 1ms/step - loss: 1.9554\n16/16 [==============================] - 0s 693us/step - loss: 1.9549\nEpoch 51 of 60\n\nTesting for epoch 51 index 1:\n16/16 [==============================] - 0s 939us/step - loss: 0.3020\n16/16 [==============================] - 0s 739us/step - loss: 0.8987\n16/16 [==============================] - 0s 698us/step - loss: 1.5860\n16/16 [==============================] - 0s 1ms/step - loss: 1.9003\n16/16 [==============================] - 0s 652us/step - loss: 1.9445\n16/16 [==============================] - 0s 807us/step - loss: 1.9511\n16/16 [==============================] - 0s 1ms/step - loss: 1.9502\n16/16 [==============================] - 0s 1ms/step - loss: 1.9488\n16/16 [==============================] - 0s 669us/step - loss: 1.9477\n16/16 [==============================] - 0s 642us/step - loss: 1.9472\n\nTesting for epoch 51 index 2:\n16/16 [==============================] - 0s 919us/step - loss: 0.2888\n16/16 [==============================] - 0s 799us/step - loss: 0.9191\n16/16 [==============================] - 0s 639us/step - loss: 1.6532\n16/16 [==============================] - 0s 1ms/step - loss: 1.9907\n16/16 [==============================] - 0s 617us/step - loss: 2.0383\n16/16 [==============================] - 0s 1000us/step - loss: 2.0458\n16/16 [==============================] - 0s 1ms/step - loss: 2.0449\n16/16 [==============================] - 0s 617us/step - loss: 2.0435\n16/16 [==============================] - 0s 1ms/step - loss: 2.0424\n16/16 [==============================] - 0s 617us/step - loss: 2.0419\nEpoch 52 of 60\n\nTesting for epoch 52 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2896\n16/16 [==============================] - 0s 692us/step - loss: 0.9198\n16/16 [==============================] - 0s 662us/step - loss: 1.6543\n16/16 [==============================] - 0s 644us/step - loss: 1.9879\n16/16 [==============================] - 0s 1ms/step - loss: 2.0342\n16/16 [==============================] - 0s 642us/step - loss: 2.0410\n16/16 [==============================] - 0s 1ms/step - loss: 2.0401\n16/16 [==============================] - 0s 1ms/step - loss: 2.0387\n16/16 [==============================] - 0s 1ms/step - loss: 2.0376\n16/16 [==============================] - 0s 1ms/step - loss: 2.0371\n\nTesting for epoch 52 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2887\n16/16 [==============================] - 0s 655us/step - loss: 0.9164\n16/16 [==============================] - 0s 625us/step - loss: 1.6524\n16/16 [==============================] - 0s 664us/step - loss: 1.9863\n16/16 [==============================] - 0s 1ms/step - loss: 2.0320\n16/16 [==============================] - 0s 1ms/step - loss: 2.0386\n16/16 [==============================] - 0s 695us/step - loss: 2.0375\n16/16 [==============================] - 0s 671us/step - loss: 2.0360\n16/16 [==============================] - 0s 593us/step - loss: 2.0348\n16/16 [==============================] - 0s 592us/step - loss: 2.0343\nEpoch 53 of 60\n\nTesting for epoch 53 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2903\n16/16 [==============================] - 0s 652us/step - loss: 0.9111\n16/16 [==============================] - 0s 634us/step - loss: 1.6388\n16/16 [==============================] - 0s 820us/step - loss: 1.9646\n16/16 [==============================] - 0s 1ms/step - loss: 2.0082\n16/16 [==============================] - 0s 730us/step - loss: 2.0139\n16/16 [==============================] - 0s 592us/step - loss: 2.0126\n16/16 [==============================] - 0s 590us/step - loss: 2.0110\n16/16 [==============================] - 0s 854us/step - loss: 2.0098\n16/16 [==============================] - 0s 639us/step - loss: 2.0093\n\nTesting for epoch 53 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2907\n16/16 [==============================] - 0s 1ms/step - loss: 0.9147\n16/16 [==============================] - 0s 719us/step - loss: 1.6530\n16/16 [==============================] - 0s 658us/step - loss: 1.9850\n16/16 [==============================] - 0s 634us/step - loss: 2.0298\n16/16 [==============================] - 0s 653us/step - loss: 2.0361\n16/16 [==============================] - 0s 2ms/step - loss: 2.0350\n16/16 [==============================] - 0s 1ms/step - loss: 2.0335\n16/16 [==============================] - 0s 625us/step - loss: 2.0324\n16/16 [==============================] - 0s 2ms/step - loss: 2.0319\nEpoch 54 of 60\n\nTesting for epoch 54 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2956\n16/16 [==============================] - 0s 1ms/step - loss: 0.9136\n16/16 [==============================] - 0s 1ms/step - loss: 1.6438\n16/16 [==============================] - 0s 1ms/step - loss: 1.9674\n16/16 [==============================] - 0s 1ms/step - loss: 2.0101\n16/16 [==============================] - 0s 1ms/step - loss: 2.0156\n16/16 [==============================] - 0s 1ms/step - loss: 2.0143\n16/16 [==============================] - 0s 1ms/step - loss: 2.0127\n16/16 [==============================] - 0s 1ms/step - loss: 2.0116\n16/16 [==============================] - 0s 1ms/step - loss: 2.0111\n\nTesting for epoch 54 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2939\n16/16 [==============================] - 0s 1ms/step - loss: 0.9110\n16/16 [==============================] - 0s 1ms/step - loss: 1.6463\n16/16 [==============================] - 0s 1ms/step - loss: 1.9729\n16/16 [==============================] - 0s 2ms/step - loss: 2.0161\n16/16 [==============================] - 0s 2ms/step - loss: 2.0218\n16/16 [==============================] - 0s 1ms/step - loss: 2.0206\n16/16 [==============================] - 0s 1ms/step - loss: 2.0191\n16/16 [==============================] - 0s 1ms/step - loss: 2.0180\n16/16 [==============================] - 0s 679us/step - loss: 2.0175\nEpoch 55 of 60\n\nTesting for epoch 55 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2919\n16/16 [==============================] - 0s 2ms/step - loss: 0.9017\n16/16 [==============================] - 0s 1ms/step - loss: 1.6263\n16/16 [==============================] - 0s 1ms/step - loss: 1.9428\n16/16 [==============================] - 0s 1ms/step - loss: 1.9832\n16/16 [==============================] - 0s 1ms/step - loss: 1.9878\n16/16 [==============================] - 0s 694us/step - loss: 1.9863\n16/16 [==============================] - 0s 642us/step - loss: 1.9846\n16/16 [==============================] - 0s 593us/step - loss: 1.9834\n16/16 [==============================] - 0s 607us/step - loss: 1.9829\n\nTesting for epoch 55 index 2:\n16/16 [==============================] - 0s 627us/step - loss: 0.2932\n16/16 [==============================] - 0s 619us/step - loss: 0.8949\n16/16 [==============================] - 0s 621us/step - loss: 1.6136\n16/16 [==============================] - 0s 623us/step - loss: 1.9271\n16/16 [==============================] - 0s 610us/step - loss: 1.9666\n16/16 [==============================] - 0s 1ms/step - loss: 1.9708\n16/16 [==============================] - 0s 1ms/step - loss: 1.9692\n16/16 [==============================] - 0s 629us/step - loss: 1.9675\n16/16 [==============================] - 0s 1ms/step - loss: 1.9663\n16/16 [==============================] - 0s 580us/step - loss: 1.9658\nEpoch 56 of 60\n\nTesting for epoch 56 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2877\n16/16 [==============================] - 0s 1ms/step - loss: 0.9139\n16/16 [==============================] - 0s 1ms/step - loss: 1.6633\n16/16 [==============================] - 0s 1ms/step - loss: 1.9868\n16/16 [==============================] - 0s 1ms/step - loss: 2.0271\n16/16 [==============================] - 0s 654us/step - loss: 2.0314\n16/16 [==============================] - 0s 683us/step - loss: 2.0298\n16/16 [==============================] - 0s 615us/step - loss: 2.0281\n16/16 [==============================] - 0s 1ms/step - loss: 2.0269\n16/16 [==============================] - 0s 663us/step - loss: 2.0263\n\nTesting for epoch 56 index 2:\n16/16 [==============================] - 0s 742us/step - loss: 0.2883\n16/16 [==============================] - 0s 641us/step - loss: 0.8940\n16/16 [==============================] - 0s 1ms/step - loss: 1.6224\n16/16 [==============================] - 0s 634us/step - loss: 1.9361\n16/16 [==============================] - 0s 617us/step - loss: 1.9749\n16/16 [==============================] - 0s 1ms/step - loss: 1.9788\n16/16 [==============================] - 0s 628us/step - loss: 1.9771\n16/16 [==============================] - 0s 628us/step - loss: 1.9755\n16/16 [==============================] - 0s 1ms/step - loss: 1.9743\n16/16 [==============================] - 0s 802us/step - loss: 1.9737\nEpoch 57 of 60\n\nTesting for epoch 57 index 1:\n16/16 [==============================] - 0s 624us/step - loss: 0.2843\n16/16 [==============================] - 0s 1ms/step - loss: 0.9090\n16/16 [==============================] - 0s 1ms/step - loss: 1.6600\n16/16 [==============================] - 0s 712us/step - loss: 1.9791\n16/16 [==============================] - 0s 648us/step - loss: 2.0175\n16/16 [==============================] - 0s 647us/step - loss: 2.0210\n16/16 [==============================] - 0s 613us/step - loss: 2.0192\n16/16 [==============================] - 0s 629us/step - loss: 2.0174\n16/16 [==============================] - 0s 1ms/step - loss: 2.0162\n16/16 [==============================] - 0s 859us/step - loss: 2.0156\n\nTesting for epoch 57 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2836\n16/16 [==============================] - 0s 1ms/step - loss: 0.9146\n16/16 [==============================] - 0s 2ms/step - loss: 1.6782\n16/16 [==============================] - 0s 1ms/step - loss: 2.0028\n16/16 [==============================] - 0s 1ms/step - loss: 2.0419\n16/16 [==============================] - 0s 1ms/step - loss: 2.0457\n16/16 [==============================] - 0s 1ms/step - loss: 2.0439\n16/16 [==============================] - 0s 1ms/step - loss: 2.0422\n16/16 [==============================] - 0s 676us/step - loss: 2.0410\n16/16 [==============================] - 0s 2ms/step - loss: 2.0405\nEpoch 58 of 60\n\nTesting for epoch 58 index 1:\n16/16 [==============================] - 0s 684us/step - loss: 0.2791\n16/16 [==============================] - 0s 1ms/step - loss: 0.9413\n16/16 [==============================] - 0s 1ms/step - loss: 1.7429\n16/16 [==============================] - 0s 1ms/step - loss: 2.0793\n16/16 [==============================] - 0s 748us/step - loss: 2.1192\n16/16 [==============================] - 0s 1ms/step - loss: 2.1229\n16/16 [==============================] - 0s 657us/step - loss: 2.1211\n16/16 [==============================] - 0s 826us/step - loss: 2.1193\n16/16 [==============================] - 0s 654us/step - loss: 2.1181\n16/16 [==============================] - 0s 662us/step - loss: 2.1176\n\nTesting for epoch 58 index 2:\n16/16 [==============================] - 0s 623us/step - loss: 0.2745\n16/16 [==============================] - 0s 636us/step - loss: 0.9399\n16/16 [==============================] - 0s 604us/step - loss: 1.7502\n16/16 [==============================] - 0s 1ms/step - loss: 2.0900\n16/16 [==============================] - 0s 1ms/step - loss: 2.1304\n16/16 [==============================] - 0s 1ms/step - loss: 2.1343\n16/16 [==============================] - 0s 2ms/step - loss: 2.1327\n16/16 [==============================] - 0s 1ms/step - loss: 2.1310\n16/16 [==============================] - 0s 1ms/step - loss: 2.1299\n16/16 [==============================] - 0s 1ms/step - loss: 2.1293\nEpoch 59 of 60\n\nTesting for epoch 59 index 1:\n16/16 [==============================] - 0s 850us/step - loss: 0.2705\n16/16 [==============================] - 0s 624us/step - loss: 0.9431\n16/16 [==============================] - 0s 626us/step - loss: 1.7595\n16/16 [==============================] - 0s 946us/step - loss: 2.0960\n16/16 [==============================] - 0s 631us/step - loss: 2.1341\n16/16 [==============================] - 0s 653us/step - loss: 2.1372\n16/16 [==============================] - 0s 1ms/step - loss: 2.1353\n16/16 [==============================] - 0s 745us/step - loss: 2.1335\n16/16 [==============================] - 0s 1ms/step - loss: 2.1322\n16/16 [==============================] - 0s 1ms/step - loss: 2.1317\n\nTesting for epoch 59 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 0.2706\n16/16 [==============================] - 0s 1ms/step - loss: 0.9454\n16/16 [==============================] - 0s 1ms/step - loss: 1.7683\n16/16 [==============================] - 0s 1ms/step - loss: 2.1066\n16/16 [==============================] - 0s 1ms/step - loss: 2.1448\n16/16 [==============================] - 0s 1ms/step - loss: 2.1478\n16/16 [==============================] - 0s 1ms/step - loss: 2.1459\n16/16 [==============================] - 0s 1ms/step - loss: 2.1441\n16/16 [==============================] - 0s 676us/step - loss: 2.1429\n16/16 [==============================] - 0s 1ms/step - loss: 2.1423\nEpoch 60 of 60\n\nTesting for epoch 60 index 1:\n16/16 [==============================] - 0s 648us/step - loss: 0.2763\n16/16 [==============================] - 0s 1ms/step - loss: 0.9427\n16/16 [==============================] - 0s 690us/step - loss: 1.7522\n16/16 [==============================] - 0s 1ms/step - loss: 2.0797\n16/16 [==============================] - 0s 717us/step - loss: 2.1151\n16/16 [==============================] - 0s 650us/step - loss: 2.1173\n16/16 [==============================] - 0s 1ms/step - loss: 2.1151\n16/16 [==============================] - 0s 1ms/step - loss: 2.1133\n16/16 [==============================] - 0s 674us/step - loss: 2.1120\n16/16 [==============================] - 0s 1ms/step - loss: 2.1114\n\nTesting for epoch 60 index 2:\n16/16 [==============================] - 0s 660us/step - loss: 0.2810\n16/16 [==============================] - 0s 668us/step - loss: 0.9270\n16/16 [==============================] - 0s 615us/step - loss: 1.7161\n16/16 [==============================] - 0s 637us/step - loss: 2.0351\n16/16 [==============================] - 0s 1ms/step - loss: 2.0696\n16/16 [==============================] - 0s 694us/step - loss: 2.0717\n16/16 [==============================] - 0s 1ms/step - loss: 2.0696\n16/16 [==============================] - 0s 641us/step - loss: 2.0678\n16/16 [==============================] - 0s 599us/step - loss: 2.0666\n16/16 [==============================] - 0s 624us/step - loss: 2.0660\n\n\n\noutlier_MO_GAAL_one = list(clf.labels_)\n\n\noutlier_MO_GAAL_one = list(map(lambda x: 1 if x==0  else -1,outlier_MO_GAAL_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_MO_GAAL_one,tab_linear)\n\n\n_conf.conf(\"MO-GAAL (Liu et al., 2019)\")\n\n\n\n\nAccuracy: 0.879\nPrecision: 0.955\nRecall: 0.916\nF1 Score: 0.935\n\n\n\nthirteen = twelve.append(_conf.tab)\n\n\n\nLSCP\n\ndetectors = [KNN(), LOF(), OCSVM()]\nclf = LSCP(detectors)\nclf.fit(_df[['x', 'y']])\n_df['LSCP_clf'] = clf.labels_\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/pyod/models/lscp.py:382: UserWarning: The number of histogram bins is greater than the number of classifiers, reducing n_bins to n_clf.\n  warnings.warn(\n\n\n\noutlier_LSCP_one = list(clf.labels_)\n\n\noutlier_LSCP_one = list(map(lambda x: 1 if x==0  else -1,outlier_LSCP_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_LSCP_one,tab_linear)\n\n\n_conf.conf(\"LSCP (Zhao et al., 2019)\")\n\n\n\n\nAccuracy: 0.908\nPrecision: 0.977\nRecall: 0.925\nF1 Score: 0.950\n\n\n\nfourteen_linear = thirteen.append(_conf.tab)"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#linear-result",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#linear-result",
    "title": "Class code for Comparison Study",
    "section": "Linear Result",
    "text": "Linear Result\n\nround(fourteen_linear,3)\n\n\n\n\n\n  \n    \n      \n      Accuracy\n      Precision\n      Recall\n      F1\n    \n  \n  \n    \n      GODE\n      0.959\n      0.960\n      0.999\n      0.979\n    \n    \n      LOF (Breunig et al., 2000)\n      0.890\n      0.973\n      0.909\n      0.940\n    \n    \n      kNN (Ramaswamy et al., 2000)\n      0.912\n      0.979\n      0.927\n      0.952\n    \n    \n      CBLOF (He et al., 2003)\n      0.920\n      0.958\n      0.958\n      0.958\n    \n    \n      OCSVM (Sch ̈olkopf et al., 2001)\n      0.909\n      0.978\n      0.925\n      0.951\n    \n    \n      MCD (Hardin and Rocke, 2004)\n      0.918\n      0.982\n      0.931\n      0.956\n    \n    \n      Feature Bagging (Lazarevic and Kumar, 2005)\n      0.918\n      0.982\n      0.931\n      0.956\n    \n    \n      ABOD (Kriegel et al., 2008)\n      0.946\n      0.972\n      0.972\n      0.972\n    \n    \n      Isolation Forest (Liu et al., 2008)\n      0.800\n      0.984\n      0.802\n      0.884\n    \n    \n      HBOS (Goldstein and Dengel, 2012)\n      0.889\n      0.960\n      0.921\n      0.940\n    \n    \n      SOS (Janssens et al., 2012)\n      0.889\n      0.960\n      0.921\n      0.940\n    \n    \n      SO-GAAL (Liu et al., 2019)\n      0.868\n      0.954\n      0.904\n      0.929\n    \n    \n      MO-GAAL (Liu et al., 2019)\n      0.879\n      0.955\n      0.916\n      0.935\n    \n    \n      LSCP (Zhao et al., 2019)\n      0.908\n      0.977\n      0.925\n      0.950"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#orbit-ebayesthresh",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#orbit-ebayesthresh",
    "title": "Class code for Comparison Study",
    "section": "Orbit EbayesThresh",
    "text": "Orbit EbayesThresh\n\n%load_ext rpy2.ipython\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython\n\n\n\n%%R\nlibrary(EbayesThresh)\nset.seed(1)\nepsilon = rnorm(1000)\nsignal = sample(c(runif(25,-7,-5), runif(25,5,7), rep(0,950)))\nindex_of_trueoutlier = which(signal!=0)\nindex_of_trueoutlier\nx=signal+epsilon\nplot(1:1000,x)\npoints(index_of_trueoutlier,x[index_of_trueoutlier],col=2,cex=4)\n\n#plot(x,type='l')\n#mu <- EbayesThresh::ebayesthresh(x,sdev=2)\n#lines(mu,col=2,lty=2,lwd=2)\n\n\n\n\n\n%R -o x\n%R -o index_of_trueoutlier\n%R -o signal\n\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\n\n\nxhat = np.array(ebayesthresh(FloatVector(x)))\n\n\n# plt.plot(x)\n# plt.plot(xhat)\n\n\noutlier_true_index = index_of_trueoutlier\n\n\noutlier_true_value = x[index_of_trueoutlier]\n\npackage와 비교를 위해 outlier는 -1, inlier는 1로 표시\n\noutlier_true_one = signal.copy()\n\n\noutlier_true_one = list(map(lambda x: -1 if x!=0 else 1,outlier_true_one))"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#orbit",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#orbit",
    "title": "Class code for Comparison Study",
    "section": "Orbit",
    "text": "Orbit\n\nnp.random.seed(777)\npi=np.pi\nn=1000\nang=np.linspace(-pi,pi-2*pi/n,n)\nr=5+np.cos(np.linspace(0,12*pi,n))\nvx=r*np.cos(ang)\nvy=r*np.sin(ang)\nf1=10*np.sin(np.linspace(0,6*pi,n))\nf = f1 + x\n\n\n_df = pd.DataFrame({'x' : vx, 'y' : vy, 'f' : f})\n\n\nX = np.array(_df)\n\n\nGODE\n\n_Orbit = Orbit(_df)\n\n\n_Orbit.get_distance()\n\n100%|██████████| 1000/1000 [00:02<00:00, 497.54it/s]\n\n\n\n_Orbit.get_weightmatrix(theta=(_Orbit.D[_Orbit.D>0].mean()),kappa=2500) \n\n\n_Orbit.fit(sd=15,ref=20)\n\n\noutlier_simul_one = (_Orbit.df['Residual']**2).tolist()\n\n\noutlier_simul_one = list(map(lambda x: -1 if x > 20 else 1,outlier_simul_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_simul_one,tab_orbit)\n\n\n_conf.conf(\"GODE\")\n\n\n\n\nAccuracy: 0.997\nPrecision: 0.997\nRecall: 1.000\nF1 Score: 0.998\n\n\n\none = _conf.tab\n\n\n\nLOF\n\nclf = LocalOutlierFactor(n_neighbors=2)\n\n\n_conf = Conf_matrx(outlier_true_one,clf.fit_predict(X),tab_orbit)\n\n\n_conf.conf(\"LOF (Breunig et al., 2000)\")\n\n\n\n\nAccuracy: 0.886\nPrecision: 0.987\nRecall: 0.892\nF1 Score: 0.937\n\n\n\ntwo = one.append(_conf.tab)\n\n\n\nKNN\n\nclf = KNN()\nclf.fit(_df[['x', 'y','f']])\n_df['knn_clf'] = clf.labels_\n\n\noutlier_KNN_one = list(clf.labels_)\n\n\noutlier_KNN_one = list(map(lambda x: 1 if x==0  else -1,outlier_KNN_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_KNN_one,tab_orbit)\n\n\n_conf.conf(\"kNN (Ramaswamy et al., 2000)\")\n\n\n\n\nAccuracy: 0.948\nPrecision: 0.999\nRecall: 0.946\nF1 Score: 0.972\n\n\n\nthree = two.append(_conf.tab)\n\n\n\nCBLOF\n\nclf = CBLOF(contamination=0.05,check_estimator=False, random_state=77)\nclf.fit(_df[['x', 'y','f']])\n_df['CBLOF_Clf'] = clf.labels_\n\n\noutlier_CBLOF_one = list(clf.labels_)\n\n\noutlier_CBLOF_one = list(map(lambda x: 1 if x==0  else -1,outlier_CBLOF_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_CBLOF_one,tab_orbit)\n\n\n_conf.conf(\"CBLOF (He et al., 2003)\")\n\n\n\n\nAccuracy: 0.918\nPrecision: 0.957\nRecall: 0.957\nF1 Score: 0.957\n\n\n\nfour = three.append(_conf.tab)\n\n\n\nOCSVM\n\nclf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n\n\nclf.fit(X)\n\nOneClassSVM(gamma=0.1, nu=0.1)\n\n\n\noutlier_OSVM_one = list(clf.predict(X))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_OSVM_one,tab_orbit)\n\n\n_conf.conf(\"OCSVM (Sch ̈olkopf et al., 2001)\")\n\n\n\n\nAccuracy: 0.923\nPrecision: 0.988\nRecall: 0.931\nF1 Score: 0.958\n\n\n\nfive = four.append(_conf.tab)\n\n\n\nMCD\n\nclf = MCD()\nclf.fit(_df[['x', 'y','f']])\n_df['MCD_clf'] = clf.labels_\n\n\noutlier_MCD_one = list(clf.labels_)\n\n\noutlier_MCD_one = list(map(lambda x: 1 if x==0  else -1,outlier_MCD_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_MCD_one,tab_orbit)\n\n\n_conf.conf(\"MCD (Hardin and Rocke, 2004)\")\n\n\n\n\nAccuracy: 0.866\nPrecision: 0.953\nRecall: 0.903\nF1 Score: 0.928\n\n\n\nsix = five.append(_conf.tab)\n\n\n\nFeature Bagging\n\nclf = FeatureBagging()\nclf.fit(_df[['x', 'y','f']])\n_df['FeatureBagging_clf'] = clf.labels_\n\n\noutlier_FeatureBagging_one = list(clf.labels_)\n\n\noutlier_FeatureBagging_one = list(map(lambda x: 1 if x==0  else -1,outlier_FeatureBagging_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_FeatureBagging_one,tab_orbit)\n\n\n_conf.conf(\"Feature Bagging (Lazarevic and Kumar, 2005)\")\n\n\n\n\nAccuracy: 0.912\nPrecision: 0.979\nRecall: 0.927\nF1 Score: 0.952\n\n\n\nseven = six.append(_conf.tab)\n\n\n\nABOD\n\nclf = ABOD(contamination=0.05)\nclf.fit(_df[['x', 'y','f']])\n_df['ABOD_Clf'] = clf.labels_\n\n\noutlier_ABOD_one = list(clf.labels_)\n\n\noutlier_ABOD_one = list(map(lambda x: 1 if x==0  else -1,outlier_ABOD_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_ABOD_one,tab_orbit)\n\n\n_conf.conf(\"ABOD (Kriegel et al., 2008)\")\n\n\n\n\nAccuracy: 0.988\nPrecision: 0.994\nRecall: 0.994\nF1 Score: 0.994\n\n\n\neight = seven.append(_conf.tab)\n\n\n\nIForest\n\nod = IForest(\n    threshold=0.,\n    n_estimators=100\n)\n\n\nod.fit(_df[['x', 'y','f']])\n\n\npreds = od.predict(\n    _df[['x', 'y','f']],\n    return_instance_score=True\n)\n\n\n_df['IF_alibi'] = preds['data']['is_outlier']\n\n\noutlier_alibi_one = _df['IF_alibi']\n\n\noutlier_alibi_one = list(map(lambda x: 1 if x==0  else -1,outlier_alibi_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_alibi_one,tab_orbit)\n\n\n_conf.conf(\"Isolation Forest (Liu et al., 2008)\")\n\n\n\n\nAccuracy: 0.378\nPrecision: 0.997\nRecall: 0.346\nF1 Score: 0.514\n\n\n\nnine = eight.append(_conf.tab)\n\n\n\nHBOS\n\nclf = HBOS()\nclf.fit(_df[['x', 'y','f']])\n_df['HBOS_clf'] = clf.labels_\n\n\noutlier_HBOS_one = list(clf.labels_)\n\n\noutlier_HBOS_one = list(map(lambda x: 1 if x==0  else -1,outlier_HBOS_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_HBOS_one,tab_orbit)\n\n\n_conf.conf(\"HBOS (Goldstein and Dengel, 2012)\")\n\n\n\n\nAccuracy: 0.881\nPrecision: 0.961\nRecall: 0.912\nF1 Score: 0.936\n\n\n\nten = nine.append(_conf.tab)\n\n\n\nSOS\n\noutlier_SOS_one = list(clf.labels_)\n\n\noutlier_SOS_one = list(map(lambda x: 1 if x==0  else -1,outlier_SOS_one))\n\n\nclf = SOS()\nclf.fit(_df[['x', 'y','f']])\n_df['SOS_clf'] = clf.labels_\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_SOS_one,tab_orbit)\n\n\n_conf.conf(\"SOS (Janssens et al., 2012)\")\n\n\n\n\nAccuracy: 0.881\nPrecision: 0.961\nRecall: 0.912\nF1 Score: 0.936\n\n\n\neleven = ten.append(_conf.tab)\n\n\n\nSO_GAAL\n\nclf = SO_GAAL()\nclf.fit(_df[['x', 'y','f']])\n_df['SO_GAAL_clf'] = clf.labels_\n\nEpoch 1 of 60\n\nTesting for epoch 1 index 1:\n\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n\n\n\nTesting for epoch 1 index 2:\nEpoch 2 of 60\n\nTesting for epoch 2 index 1:\n\nTesting for epoch 2 index 2:\nEpoch 3 of 60\n\nTesting for epoch 3 index 1:\n\nTesting for epoch 3 index 2:\nEpoch 4 of 60\n\nTesting for epoch 4 index 1:\n\nTesting for epoch 4 index 2:\nEpoch 5 of 60\n\nTesting for epoch 5 index 1:\n\nTesting for epoch 5 index 2:\nEpoch 6 of 60\n\nTesting for epoch 6 index 1:\n\nTesting for epoch 6 index 2:\nEpoch 7 of 60\n\nTesting for epoch 7 index 1:\n\nTesting for epoch 7 index 2:\nEpoch 8 of 60\n\nTesting for epoch 8 index 1:\n\nTesting for epoch 8 index 2:\nEpoch 9 of 60\n\nTesting for epoch 9 index 1:\n\nTesting for epoch 9 index 2:\nEpoch 10 of 60\n\nTesting for epoch 10 index 1:\n\nTesting for epoch 10 index 2:\nEpoch 11 of 60\n\nTesting for epoch 11 index 1:\n\nTesting for epoch 11 index 2:\nEpoch 12 of 60\n\nTesting for epoch 12 index 1:\n\nTesting for epoch 12 index 2:\nEpoch 13 of 60\n\nTesting for epoch 13 index 1:\n\nTesting for epoch 13 index 2:\nEpoch 14 of 60\n\nTesting for epoch 14 index 1:\n\nTesting for epoch 14 index 2:\nEpoch 15 of 60\n\nTesting for epoch 15 index 1:\n\nTesting for epoch 15 index 2:\nEpoch 16 of 60\n\nTesting for epoch 16 index 1:\n\nTesting for epoch 16 index 2:\nEpoch 17 of 60\n\nTesting for epoch 17 index 1:\n\nTesting for epoch 17 index 2:\nEpoch 18 of 60\n\nTesting for epoch 18 index 1:\n\nTesting for epoch 18 index 2:\nEpoch 19 of 60\n\nTesting for epoch 19 index 1:\n\nTesting for epoch 19 index 2:\nEpoch 20 of 60\n\nTesting for epoch 20 index 1:\n\nTesting for epoch 20 index 2:\nEpoch 21 of 60\n\nTesting for epoch 21 index 1:\n\nTesting for epoch 21 index 2:\nEpoch 22 of 60\n\nTesting for epoch 22 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2135\n\nTesting for epoch 22 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2178\nEpoch 23 of 60\n\nTesting for epoch 23 index 1:\n16/16 [==============================] - 0s 891us/step - loss: 1.2227\n\nTesting for epoch 23 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2138\nEpoch 24 of 60\n\nTesting for epoch 24 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2244\n\nTesting for epoch 24 index 2:\n16/16 [==============================] - 0s 917us/step - loss: 1.2068\nEpoch 25 of 60\n\nTesting for epoch 25 index 1:\n16/16 [==============================] - 0s 624us/step - loss: 1.2319\n\nTesting for epoch 25 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2260\nEpoch 26 of 60\n\nTesting for epoch 26 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2357\n\nTesting for epoch 26 index 2:\n16/16 [==============================] - 0s 985us/step - loss: 1.2294\nEpoch 27 of 60\n\nTesting for epoch 27 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2426\n\nTesting for epoch 27 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2583\nEpoch 28 of 60\n\nTesting for epoch 28 index 1:\n16/16 [==============================] - 0s 815us/step - loss: 1.2599\n\nTesting for epoch 28 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2752\nEpoch 29 of 60\n\nTesting for epoch 29 index 1:\n16/16 [==============================] - 0s 639us/step - loss: 1.3019\n\nTesting for epoch 29 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2905\nEpoch 30 of 60\n\nTesting for epoch 30 index 1:\n16/16 [==============================] - 0s 635us/step - loss: 1.3191\n\nTesting for epoch 30 index 2:\n16/16 [==============================] - 0s 781us/step - loss: 1.3229\nEpoch 31 of 60\n\nTesting for epoch 31 index 1:\n16/16 [==============================] - 0s 604us/step - loss: 1.3371\n\nTesting for epoch 31 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.3418\nEpoch 32 of 60\n\nTesting for epoch 32 index 1:\n16/16 [==============================] - 0s 803us/step - loss: 1.3589\n\nTesting for epoch 32 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.3819\nEpoch 33 of 60\n\nTesting for epoch 33 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.3966\n\nTesting for epoch 33 index 2:\n16/16 [==============================] - 0s 956us/step - loss: 1.3947\nEpoch 34 of 60\n\nTesting for epoch 34 index 1:\n16/16 [==============================] - 0s 609us/step - loss: 1.4201\n\nTesting for epoch 34 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4322\nEpoch 35 of 60\n\nTesting for epoch 35 index 1:\n16/16 [==============================] - 0s 617us/step - loss: 1.4333\n\nTesting for epoch 35 index 2:\n16/16 [==============================] - 0s 626us/step - loss: 1.4465\nEpoch 36 of 60\n\nTesting for epoch 36 index 1:\n16/16 [==============================] - 0s 640us/step - loss: 1.4560\n\nTesting for epoch 36 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4823\nEpoch 37 of 60\n\nTesting for epoch 37 index 1:\n16/16 [==============================] - 0s 932us/step - loss: 1.4888\n\nTesting for epoch 37 index 2:\n16/16 [==============================] - 0s 782us/step - loss: 1.5030\nEpoch 38 of 60\n\nTesting for epoch 38 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5161\n\nTesting for epoch 38 index 2:\n16/16 [==============================] - 0s 599us/step - loss: 1.5196\nEpoch 39 of 60\n\nTesting for epoch 39 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5412\n\nTesting for epoch 39 index 2:\n16/16 [==============================] - 0s 877us/step - loss: 1.5368\nEpoch 40 of 60\n\nTesting for epoch 40 index 1:\n16/16 [==============================] - 0s 579us/step - loss: 1.5523\n\nTesting for epoch 40 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5574\nEpoch 41 of 60\n\nTesting for epoch 41 index 1:\n16/16 [==============================] - 0s 981us/step - loss: 1.5684\n\nTesting for epoch 41 index 2:\n16/16 [==============================] - 0s 643us/step - loss: 1.5748\nEpoch 42 of 60\n\nTesting for epoch 42 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5725\n\nTesting for epoch 42 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5772\nEpoch 43 of 60\n\nTesting for epoch 43 index 1:\n16/16 [==============================] - 0s 648us/step - loss: 1.5934\n\nTesting for epoch 43 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6053\nEpoch 44 of 60\n\nTesting for epoch 44 index 1:\n16/16 [==============================] - 0s 907us/step - loss: 1.6078\n\nTesting for epoch 44 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6025\nEpoch 45 of 60\n\nTesting for epoch 45 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6277\n\nTesting for epoch 45 index 2:\n16/16 [==============================] - 0s 615us/step - loss: 1.6348\nEpoch 46 of 60\n\nTesting for epoch 46 index 1:\n16/16 [==============================] - 0s 815us/step - loss: 1.6427\n\nTesting for epoch 46 index 2:\n16/16 [==============================] - 0s 606us/step - loss: 1.6405\nEpoch 47 of 60\n\nTesting for epoch 47 index 1:\n16/16 [==============================] - 0s 619us/step - loss: 1.6498\n\nTesting for epoch 47 index 2:\n16/16 [==============================] - 0s 812us/step - loss: 1.6603\nEpoch 48 of 60\n\nTesting for epoch 48 index 1:\n16/16 [==============================] - 0s 614us/step - loss: 1.6775\n\nTesting for epoch 48 index 2:\n16/16 [==============================] - 0s 650us/step - loss: 1.6890\nEpoch 49 of 60\n\nTesting for epoch 49 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6979\n\nTesting for epoch 49 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6971\nEpoch 50 of 60\n\nTesting for epoch 50 index 1:\n16/16 [==============================] - 0s 624us/step - loss: 1.7076\n\nTesting for epoch 50 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.7120\nEpoch 51 of 60\n\nTesting for epoch 51 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.7271\n\nTesting for epoch 51 index 2:\n16/16 [==============================] - 0s 863us/step - loss: 1.7406\nEpoch 52 of 60\n\nTesting for epoch 52 index 1:\n16/16 [==============================] - 0s 623us/step - loss: 1.7534\n\nTesting for epoch 52 index 2:\n16/16 [==============================] - 0s 677us/step - loss: 1.7597\nEpoch 53 of 60\n\nTesting for epoch 53 index 1:\n16/16 [==============================] - 0s 741us/step - loss: 1.7555\n\nTesting for epoch 53 index 2:\n16/16 [==============================] - 0s 678us/step - loss: 1.7716\nEpoch 54 of 60\n\nTesting for epoch 54 index 1:\n16/16 [==============================] - 0s 827us/step - loss: 1.7776\n\nTesting for epoch 54 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.7776\nEpoch 55 of 60\n\nTesting for epoch 55 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8009\n\nTesting for epoch 55 index 2:\n16/16 [==============================] - 0s 626us/step - loss: 1.8053\nEpoch 56 of 60\n\nTesting for epoch 56 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8205\n\nTesting for epoch 56 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8218\nEpoch 57 of 60\n\nTesting for epoch 57 index 1:\n16/16 [==============================] - 0s 666us/step - loss: 1.8259\n\nTesting for epoch 57 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8307\nEpoch 58 of 60\n\nTesting for epoch 58 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8576\n\nTesting for epoch 58 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8445\nEpoch 59 of 60\n\nTesting for epoch 59 index 1:\n16/16 [==============================] - 0s 742us/step - loss: 1.8687\n\nTesting for epoch 59 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8710\nEpoch 60 of 60\n\nTesting for epoch 60 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8824\n\nTesting for epoch 60 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8924\n\n\n\noutlier_SO_GAAL_one = list(clf.labels_)\n\n\noutlier_SO_GAAL_one = list(map(lambda x: 1 if x==0  else -1,outlier_SO_GAAL_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_SO_GAAL_one,tab_orbit)\n\n\n_conf.conf(\"SO-GAAL (Liu et al., 2019)\")\n\n\n\n\nAccuracy: 0.876\nPrecision: 0.959\nRecall: 0.908\nF1 Score: 0.933\n\n\n\ntwelve = eleven.append(_conf.tab)\n\n\n\nMO_GAAL\n\nclf = MO_GAAL()\nclf.fit(_df[['x', 'y','f']])\n_df['MO_GAAL_clf'] = clf.labels_\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n\n\nEpoch 1 of 60\n\nTesting for epoch 1 index 1:\n\nTesting for epoch 1 index 2:\nEpoch 2 of 60\n\nTesting for epoch 2 index 1:\n\nTesting for epoch 2 index 2:\nEpoch 3 of 60\n\nTesting for epoch 3 index 1:\n\nTesting for epoch 3 index 2:\nEpoch 4 of 60\n\nTesting for epoch 4 index 1:\n\nTesting for epoch 4 index 2:\nEpoch 5 of 60\n\nTesting for epoch 5 index 1:\n\nTesting for epoch 5 index 2:\nEpoch 6 of 60\n\nTesting for epoch 6 index 1:\n\nTesting for epoch 6 index 2:\nEpoch 7 of 60\n\nTesting for epoch 7 index 1:\n\nTesting for epoch 7 index 2:\nEpoch 8 of 60\n\nTesting for epoch 8 index 1:\n\nTesting for epoch 8 index 2:\nEpoch 9 of 60\n\nTesting for epoch 9 index 1:\n\nTesting for epoch 9 index 2:\nEpoch 10 of 60\n\nTesting for epoch 10 index 1:\n\nTesting for epoch 10 index 2:\nEpoch 11 of 60\n\nTesting for epoch 11 index 1:\n\nTesting for epoch 11 index 2:\nEpoch 12 of 60\n\nTesting for epoch 12 index 1:\n\nTesting for epoch 12 index 2:\nEpoch 13 of 60\n\nTesting for epoch 13 index 1:\n\nTesting for epoch 13 index 2:\nEpoch 14 of 60\n\nTesting for epoch 14 index 1:\n\nTesting for epoch 14 index 2:\nEpoch 15 of 60\n\nTesting for epoch 15 index 1:\n\nTesting for epoch 15 index 2:\nEpoch 16 of 60\n\nTesting for epoch 16 index 1:\n\nTesting for epoch 16 index 2:\nEpoch 17 of 60\n\nTesting for epoch 17 index 1:\n\nTesting for epoch 17 index 2:\nEpoch 18 of 60\n\nTesting for epoch 18 index 1:\n\nTesting for epoch 18 index 2:\nEpoch 19 of 60\n\nTesting for epoch 19 index 1:\n\nTesting for epoch 19 index 2:\nEpoch 20 of 60\n\nTesting for epoch 20 index 1:\n\nTesting for epoch 20 index 2:\nEpoch 21 of 60\n\nTesting for epoch 21 index 1:\n\nTesting for epoch 21 index 2:\n16/16 [==============================] - 0s 638us/step - loss: 0.5986\n16/16 [==============================] - 0s 1ms/step - loss: 1.2168\n16/16 [==============================] - 0s 643us/step - loss: 1.2657\n16/16 [==============================] - 0s 637us/step - loss: 1.2688\n16/16 [==============================] - 0s 1ms/step - loss: 1.2695\n16/16 [==============================] - 0s 1ms/step - loss: 1.2696\n16/16 [==============================] - 0s 653us/step - loss: 1.2696\n16/16 [==============================] - 0s 649us/step - loss: 1.2696\n16/16 [==============================] - 0s 711us/step - loss: 1.2696\n16/16 [==============================] - 0s 1ms/step - loss: 1.2696\nEpoch 22 of 60\n\nTesting for epoch 22 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.6392\n16/16 [==============================] - 0s 664us/step - loss: 1.2086\n16/16 [==============================] - 0s 711us/step - loss: 1.2461\n16/16 [==============================] - 0s 2ms/step - loss: 1.2488\n16/16 [==============================] - 0s 707us/step - loss: 1.2493\n16/16 [==============================] - 0s 628us/step - loss: 1.2494\n16/16 [==============================] - 0s 642us/step - loss: 1.2494\n16/16 [==============================] - 0s 698us/step - loss: 1.2494\n16/16 [==============================] - 0s 674us/step - loss: 1.2494\n16/16 [==============================] - 0s 788us/step - loss: 1.2494\n\nTesting for epoch 22 index 2:\n16/16 [==============================] - 0s 663us/step - loss: 0.6763\n16/16 [==============================] - 0s 1ms/step - loss: 1.2250\n16/16 [==============================] - 0s 1ms/step - loss: 1.2559\n16/16 [==============================] - 0s 642us/step - loss: 1.2583\n16/16 [==============================] - 0s 1ms/step - loss: 1.2588\n16/16 [==============================] - 0s 672us/step - loss: 1.2589\n16/16 [==============================] - 0s 629us/step - loss: 1.2589\n16/16 [==============================] - 0s 1ms/step - loss: 1.2589\n16/16 [==============================] - 0s 1ms/step - loss: 1.2589\n16/16 [==============================] - 0s 1ms/step - loss: 1.2589\nEpoch 23 of 60\n\nTesting for epoch 23 index 1:\n16/16 [==============================] - 0s 658us/step - loss: 0.7044\n16/16 [==============================] - 0s 682us/step - loss: 1.2426\n16/16 [==============================] - 0s 661us/step - loss: 1.2710\n16/16 [==============================] - 0s 1ms/step - loss: 1.2733\n16/16 [==============================] - 0s 757us/step - loss: 1.2738\n16/16 [==============================] - 0s 725us/step - loss: 1.2739\n16/16 [==============================] - 0s 1ms/step - loss: 1.2739\n16/16 [==============================] - 0s 1ms/step - loss: 1.2739\n16/16 [==============================] - 0s 638us/step - loss: 1.2739\n16/16 [==============================] - 0s 648us/step - loss: 1.2739\n\nTesting for epoch 23 index 2:\n16/16 [==============================] - 0s 660us/step - loss: 0.7203\n16/16 [==============================] - 0s 878us/step - loss: 1.2467\n16/16 [==============================] - 0s 647us/step - loss: 1.2715\n16/16 [==============================] - 0s 1ms/step - loss: 1.2737\n16/16 [==============================] - 0s 1ms/step - loss: 1.2741\n16/16 [==============================] - 0s 637us/step - loss: 1.2742\n16/16 [==============================] - 0s 656us/step - loss: 1.2742\n16/16 [==============================] - 0s 1ms/step - loss: 1.2742\n16/16 [==============================] - 0s 645us/step - loss: 1.2742\n16/16 [==============================] - 0s 689us/step - loss: 1.2742\nEpoch 24 of 60\n\nTesting for epoch 24 index 1:\n16/16 [==============================] - 0s 656us/step - loss: 0.7260\n16/16 [==============================] - 0s 1ms/step - loss: 1.2580\n16/16 [==============================] - 0s 655us/step - loss: 1.2817\n16/16 [==============================] - 0s 1ms/step - loss: 1.2839\n16/16 [==============================] - 0s 846us/step - loss: 1.2844\n16/16 [==============================] - 0s 696us/step - loss: 1.2844\n16/16 [==============================] - 0s 926us/step - loss: 1.2845\n16/16 [==============================] - 0s 661us/step - loss: 1.2845\n16/16 [==============================] - 0s 1ms/step - loss: 1.2845\n16/16 [==============================] - 0s 1ms/step - loss: 1.2845\n\nTesting for epoch 24 index 2:\n16/16 [==============================] - 0s 641us/step - loss: 0.7292\n16/16 [==============================] - 0s 632us/step - loss: 1.2735\n16/16 [==============================] - 0s 780us/step - loss: 1.2970\n16/16 [==============================] - 0s 1ms/step - loss: 1.2995\n16/16 [==============================] - 0s 637us/step - loss: 1.2999\n16/16 [==============================] - 0s 1ms/step - loss: 1.3000\n16/16 [==============================] - 0s 1ms/step - loss: 1.3000\n16/16 [==============================] - 0s 981us/step - loss: 1.3000\n16/16 [==============================] - 0s 1ms/step - loss: 1.3000\n16/16 [==============================] - 0s 1ms/step - loss: 1.3000\nEpoch 25 of 60\n\nTesting for epoch 25 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.7228\n16/16 [==============================] - 0s 1ms/step - loss: 1.2928\n16/16 [==============================] - 0s 1ms/step - loss: 1.3171\n16/16 [==============================] - 0s 821us/step - loss: 1.3198\n16/16 [==============================] - 0s 611us/step - loss: 1.3203\n16/16 [==============================] - 0s 690us/step - loss: 1.3204\n16/16 [==============================] - 0s 647us/step - loss: 1.3204\n16/16 [==============================] - 0s 1ms/step - loss: 1.3204\n16/16 [==============================] - 0s 974us/step - loss: 1.3204\n16/16 [==============================] - 0s 589us/step - loss: 1.3204\n\nTesting for epoch 25 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.7136\n16/16 [==============================] - 0s 1ms/step - loss: 1.3031\n16/16 [==============================] - 0s 643us/step - loss: 1.3286\n16/16 [==============================] - 0s 1ms/step - loss: 1.3313\n16/16 [==============================] - 0s 948us/step - loss: 1.3319\n16/16 [==============================] - 0s 1ms/step - loss: 1.3320\n16/16 [==============================] - 0s 802us/step - loss: 1.3320\n16/16 [==============================] - 0s 1ms/step - loss: 1.3320\n16/16 [==============================] - 0s 1ms/step - loss: 1.3320\n16/16 [==============================] - 0s 837us/step - loss: 1.3320\nEpoch 26 of 60\n\nTesting for epoch 26 index 1:\n16/16 [==============================] - 0s 631us/step - loss: 0.6966\n16/16 [==============================] - 0s 1ms/step - loss: 1.3288\n16/16 [==============================] - 0s 820us/step - loss: 1.3566\n16/16 [==============================] - 0s 934us/step - loss: 1.3598\n16/16 [==============================] - 0s 1ms/step - loss: 1.3604\n16/16 [==============================] - 0s 1ms/step - loss: 1.3605\n16/16 [==============================] - 0s 1ms/step - loss: 1.3605\n16/16 [==============================] - 0s 635us/step - loss: 1.3605\n16/16 [==============================] - 0s 865us/step - loss: 1.3605\n16/16 [==============================] - 0s 630us/step - loss: 1.3605\n\nTesting for epoch 26 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.6781\n16/16 [==============================] - 0s 1ms/step - loss: 1.3420\n16/16 [==============================] - 0s 862us/step - loss: 1.3719\n16/16 [==============================] - 0s 635us/step - loss: 1.3756\n16/16 [==============================] - 0s 611us/step - loss: 1.3763\n16/16 [==============================] - 0s 626us/step - loss: 1.3764\n16/16 [==============================] - 0s 796us/step - loss: 1.3764\n16/16 [==============================] - 0s 1ms/step - loss: 1.3764\n16/16 [==============================] - 0s 920us/step - loss: 1.3764\n16/16 [==============================] - 0s 596us/step - loss: 1.3764\nEpoch 27 of 60\n\nTesting for epoch 27 index 1:\n16/16 [==============================] - 0s 629us/step - loss: 0.6549\n16/16 [==============================] - 0s 1ms/step - loss: 1.3709\n16/16 [==============================] - 0s 712us/step - loss: 1.4048\n16/16 [==============================] - 0s 724us/step - loss: 1.4090\n16/16 [==============================] - 0s 749us/step - loss: 1.4098\n16/16 [==============================] - 0s 653us/step - loss: 1.4099\n16/16 [==============================] - 0s 629us/step - loss: 1.4099\n16/16 [==============================] - 0s 1ms/step - loss: 1.4099\n16/16 [==============================] - 0s 723us/step - loss: 1.4099\n16/16 [==============================] - 0s 634us/step - loss: 1.4099\n\nTesting for epoch 27 index 2:\n16/16 [==============================] - 0s 625us/step - loss: 0.6334\n16/16 [==============================] - 0s 618us/step - loss: 1.3962\n16/16 [==============================] - 0s 603us/step - loss: 1.4358\n16/16 [==============================] - 0s 1ms/step - loss: 1.4403\n16/16 [==============================] - 0s 1ms/step - loss: 1.4413\n16/16 [==============================] - 0s 1ms/step - loss: 1.4414\n16/16 [==============================] - 0s 893us/step - loss: 1.4415\n16/16 [==============================] - 0s 598us/step - loss: 1.4415\n16/16 [==============================] - 0s 1ms/step - loss: 1.4414\n16/16 [==============================] - 0s 1ms/step - loss: 1.4414\nEpoch 28 of 60\n\nTesting for epoch 28 index 1:\n16/16 [==============================] - 0s 662us/step - loss: 0.6050\n16/16 [==============================] - 0s 1ms/step - loss: 1.4078\n16/16 [==============================] - 0s 1ms/step - loss: 1.4521\n16/16 [==============================] - 0s 818us/step - loss: 1.4572\n16/16 [==============================] - 0s 993us/step - loss: 1.4584\n16/16 [==============================] - 0s 1ms/step - loss: 1.4585\n16/16 [==============================] - 0s 1ms/step - loss: 1.4586\n16/16 [==============================] - 0s 1ms/step - loss: 1.4586\n16/16 [==============================] - 0s 765us/step - loss: 1.4585\n16/16 [==============================] - 0s 1ms/step - loss: 1.4585\n\nTesting for epoch 28 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.5843\n16/16 [==============================] - 0s 739us/step - loss: 1.4360\n16/16 [==============================] - 0s 1ms/step - loss: 1.4867\n16/16 [==============================] - 0s 582us/step - loss: 1.4928\n16/16 [==============================] - 0s 1ms/step - loss: 1.4941\n16/16 [==============================] - 0s 684us/step - loss: 1.4943\n16/16 [==============================] - 0s 884us/step - loss: 1.4943\n16/16 [==============================] - 0s 746us/step - loss: 1.4943\n16/16 [==============================] - 0s 997us/step - loss: 1.4943\n16/16 [==============================] - 0s 1ms/step - loss: 1.4942\nEpoch 29 of 60\n\nTesting for epoch 29 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.5581\n16/16 [==============================] - 0s 1ms/step - loss: 1.4546\n16/16 [==============================] - 0s 1ms/step - loss: 1.5115\n16/16 [==============================] - 0s 1ms/step - loss: 1.5182\n16/16 [==============================] - 0s 1ms/step - loss: 1.5197\n16/16 [==============================] - 0s 1ms/step - loss: 1.5199\n16/16 [==============================] - 0s 1ms/step - loss: 1.5199\n16/16 [==============================] - 0s 612us/step - loss: 1.5199\n16/16 [==============================] - 0s 650us/step - loss: 1.5199\n16/16 [==============================] - 0s 642us/step - loss: 1.5199\n\nTesting for epoch 29 index 2:\n16/16 [==============================] - 0s 601us/step - loss: 0.5402\n16/16 [==============================] - 0s 900us/step - loss: 1.4761\n16/16 [==============================] - 0s 1ms/step - loss: 1.5388\n16/16 [==============================] - 0s 1ms/step - loss: 1.5458\n16/16 [==============================] - 0s 1ms/step - loss: 1.5476\n16/16 [==============================] - 0s 1ms/step - loss: 1.5478\n16/16 [==============================] - 0s 1ms/step - loss: 1.5478\n16/16 [==============================] - 0s 593us/step - loss: 1.5478\n16/16 [==============================] - 0s 1ms/step - loss: 1.5478\n16/16 [==============================] - 0s 1ms/step - loss: 1.5478\nEpoch 30 of 60\n\nTesting for epoch 30 index 1:\n16/16 [==============================] - 0s 769us/step - loss: 0.5184\n16/16 [==============================] - 0s 606us/step - loss: 1.5000\n16/16 [==============================] - 0s 908us/step - loss: 1.5668\n16/16 [==============================] - 0s 1ms/step - loss: 1.5748\n16/16 [==============================] - 0s 1ms/step - loss: 1.5767\n16/16 [==============================] - 0s 1ms/step - loss: 1.5769\n16/16 [==============================] - 0s 1ms/step - loss: 1.5769\n16/16 [==============================] - 0s 705us/step - loss: 1.5769\n16/16 [==============================] - 0s 613us/step - loss: 1.5769\n16/16 [==============================] - 0s 671us/step - loss: 1.5768\n\nTesting for epoch 30 index 2:\n16/16 [==============================] - 0s 603us/step - loss: 0.5062\n16/16 [==============================] - 0s 629us/step - loss: 1.5280\n16/16 [==============================] - 0s 751us/step - loss: 1.5999\n16/16 [==============================] - 0s 615us/step - loss: 1.6088\n16/16 [==============================] - 0s 929us/step - loss: 1.6109\n16/16 [==============================] - 0s 1ms/step - loss: 1.6112\n16/16 [==============================] - 0s 1ms/step - loss: 1.6112\n16/16 [==============================] - 0s 613us/step - loss: 1.6112\n16/16 [==============================] - 0s 1ms/step - loss: 1.6112\n16/16 [==============================] - 0s 876us/step - loss: 1.6112\nEpoch 31 of 60\n\nTesting for epoch 31 index 1:\n16/16 [==============================] - 0s 663us/step - loss: 0.4910\n16/16 [==============================] - 0s 1ms/step - loss: 1.5266\n16/16 [==============================] - 0s 1ms/step - loss: 1.6021\n16/16 [==============================] - 0s 1ms/step - loss: 1.6113\n16/16 [==============================] - 0s 763us/step - loss: 1.6135\n16/16 [==============================] - 0s 956us/step - loss: 1.6138\n16/16 [==============================] - 0s 611us/step - loss: 1.6139\n16/16 [==============================] - 0s 613us/step - loss: 1.6139\n16/16 [==============================] - 0s 1ms/step - loss: 1.6138\n16/16 [==============================] - 0s 718us/step - loss: 1.6138\n\nTesting for epoch 31 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4859\n16/16 [==============================] - 0s 622us/step - loss: 1.5514\n16/16 [==============================] - 0s 608us/step - loss: 1.6294\n16/16 [==============================] - 0s 1ms/step - loss: 1.6390\n16/16 [==============================] - 0s 634us/step - loss: 1.6414\n16/16 [==============================] - 0s 593us/step - loss: 1.6417\n16/16 [==============================] - 0s 656us/step - loss: 1.6417\n16/16 [==============================] - 0s 606us/step - loss: 1.6417\n16/16 [==============================] - 0s 627us/step - loss: 1.6417\n16/16 [==============================] - 0s 951us/step - loss: 1.6416\nEpoch 32 of 60\n\nTesting for epoch 32 index 1:\n16/16 [==============================] - 0s 886us/step - loss: 0.4775\n16/16 [==============================] - 0s 631us/step - loss: 1.5698\n16/16 [==============================] - 0s 1ms/step - loss: 1.6491\n16/16 [==============================] - 0s 1ms/step - loss: 1.6591\n16/16 [==============================] - 0s 642us/step - loss: 1.6617\n16/16 [==============================] - 0s 1ms/step - loss: 1.6620\n16/16 [==============================] - 0s 606us/step - loss: 1.6621\n16/16 [==============================] - 0s 649us/step - loss: 1.6620\n16/16 [==============================] - 0s 622us/step - loss: 1.6620\n16/16 [==============================] - 0s 621us/step - loss: 1.6619\n\nTesting for epoch 32 index 2:\n16/16 [==============================] - 0s 629us/step - loss: 0.4795\n16/16 [==============================] - 0s 638us/step - loss: 1.5898\n16/16 [==============================] - 0s 634us/step - loss: 1.6681\n16/16 [==============================] - 0s 1ms/step - loss: 1.6781\n16/16 [==============================] - 0s 677us/step - loss: 1.6809\n16/16 [==============================] - 0s 977us/step - loss: 1.6812\n16/16 [==============================] - 0s 1ms/step - loss: 1.6812\n16/16 [==============================] - 0s 1ms/step - loss: 1.6812\n16/16 [==============================] - 0s 1ms/step - loss: 1.6812\n16/16 [==============================] - 0s 1ms/step - loss: 1.6812\nEpoch 33 of 60\n\nTesting for epoch 33 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4773\n16/16 [==============================] - 0s 634us/step - loss: 1.5951\n16/16 [==============================] - 0s 1ms/step - loss: 1.6703\n16/16 [==============================] - 0s 599us/step - loss: 1.6803\n16/16 [==============================] - 0s 685us/step - loss: 1.6830\n16/16 [==============================] - 0s 617us/step - loss: 1.6833\n16/16 [==============================] - 0s 945us/step - loss: 1.6833\n16/16 [==============================] - 0s 1ms/step - loss: 1.6833\n16/16 [==============================] - 0s 1ms/step - loss: 1.6833\n16/16 [==============================] - 0s 602us/step - loss: 1.6832\n\nTesting for epoch 33 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4855\n16/16 [==============================] - 0s 1ms/step - loss: 1.6287\n16/16 [==============================] - 0s 1ms/step - loss: 1.7034\n16/16 [==============================] - 0s 1ms/step - loss: 1.7137\n16/16 [==============================] - 0s 639us/step - loss: 1.7163\n16/16 [==============================] - 0s 681us/step - loss: 1.7166\n16/16 [==============================] - 0s 620us/step - loss: 1.7167\n16/16 [==============================] - 0s 583us/step - loss: 1.7167\n16/16 [==============================] - 0s 1ms/step - loss: 1.7167\n16/16 [==============================] - 0s 1ms/step - loss: 1.7166\nEpoch 34 of 60\n\nTesting for epoch 34 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4900\n16/16 [==============================] - 0s 773us/step - loss: 1.6487\n16/16 [==============================] - 0s 624us/step - loss: 1.7219\n16/16 [==============================] - 0s 1ms/step - loss: 1.7322\n16/16 [==============================] - 0s 1ms/step - loss: 1.7348\n16/16 [==============================] - 0s 1ms/step - loss: 1.7352\n16/16 [==============================] - 0s 1ms/step - loss: 1.7352\n16/16 [==============================] - 0s 1ms/step - loss: 1.7352\n16/16 [==============================] - 0s 1ms/step - loss: 1.7352\n16/16 [==============================] - 0s 1ms/step - loss: 1.7351\n\nTesting for epoch 34 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.5048\n16/16 [==============================] - 0s 1ms/step - loss: 1.6447\n16/16 [==============================] - 0s 1ms/step - loss: 1.7129\n16/16 [==============================] - 0s 1ms/step - loss: 1.7226\n16/16 [==============================] - 0s 1ms/step - loss: 1.7250\n16/16 [==============================] - 0s 1ms/step - loss: 1.7253\n16/16 [==============================] - 0s 717us/step - loss: 1.7253\n16/16 [==============================] - 0s 830us/step - loss: 1.7253\n16/16 [==============================] - 0s 624us/step - loss: 1.7253\n16/16 [==============================] - 0s 682us/step - loss: 1.7252\nEpoch 35 of 60\n\nTesting for epoch 35 index 1:\n16/16 [==============================] - 0s 648us/step - loss: 0.5151\n16/16 [==============================] - 0s 1ms/step - loss: 1.6720\n16/16 [==============================] - 0s 838us/step - loss: 1.7389\n16/16 [==============================] - 0s 1ms/step - loss: 1.7487\n16/16 [==============================] - 0s 1ms/step - loss: 1.7510\n16/16 [==============================] - 0s 832us/step - loss: 1.7513\n16/16 [==============================] - 0s 1ms/step - loss: 1.7514\n16/16 [==============================] - 0s 595us/step - loss: 1.7514\n16/16 [==============================] - 0s 1ms/step - loss: 1.7514\n16/16 [==============================] - 0s 639us/step - loss: 1.7513\n\nTesting for epoch 35 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.5348\n16/16 [==============================] - 0s 870us/step - loss: 1.6661\n16/16 [==============================] - 0s 1ms/step - loss: 1.7277\n16/16 [==============================] - 0s 640us/step - loss: 1.7367\n16/16 [==============================] - 0s 897us/step - loss: 1.7389\n16/16 [==============================] - 0s 1ms/step - loss: 1.7391\n16/16 [==============================] - 0s 1ms/step - loss: 1.7392\n16/16 [==============================] - 0s 1ms/step - loss: 1.7392\n16/16 [==============================] - 0s 745us/step - loss: 1.7392\n16/16 [==============================] - 0s 1ms/step - loss: 1.7391\nEpoch 36 of 60\n\nTesting for epoch 36 index 1:\n16/16 [==============================] - 0s 688us/step - loss: 0.5491\n16/16 [==============================] - 0s 635us/step - loss: 1.6837\n16/16 [==============================] - 0s 601us/step - loss: 1.7423\n16/16 [==============================] - 0s 601us/step - loss: 1.7511\n16/16 [==============================] - 0s 1ms/step - loss: 1.7531\n16/16 [==============================] - 0s 656us/step - loss: 1.7534\n16/16 [==============================] - 0s 1ms/step - loss: 1.7534\n16/16 [==============================] - 0s 1ms/step - loss: 1.7534\n16/16 [==============================] - 0s 1ms/step - loss: 1.7534\n16/16 [==============================] - 0s 1ms/step - loss: 1.7534\n\nTesting for epoch 36 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.5738\n16/16 [==============================] - 0s 1ms/step - loss: 1.6942\n16/16 [==============================] - 0s 1ms/step - loss: 1.7482\n16/16 [==============================] - 0s 1ms/step - loss: 1.7566\n16/16 [==============================] - 0s 623us/step - loss: 1.7585\n16/16 [==============================] - 0s 741us/step - loss: 1.7588\n16/16 [==============================] - 0s 774us/step - loss: 1.7588\n16/16 [==============================] - 0s 1ms/step - loss: 1.7588\n16/16 [==============================] - 0s 1ms/step - loss: 1.7588\n16/16 [==============================] - 0s 1ms/step - loss: 1.7587\nEpoch 37 of 60\n\nTesting for epoch 37 index 1:\n16/16 [==============================] - 0s 604us/step - loss: 0.5910\n16/16 [==============================] - 0s 1ms/step - loss: 1.7056\n16/16 [==============================] - 0s 1ms/step - loss: 1.7570\n16/16 [==============================] - 0s 1ms/step - loss: 1.7652\n16/16 [==============================] - 0s 596us/step - loss: 1.7670\n16/16 [==============================] - 0s 944us/step - loss: 1.7673\n16/16 [==============================] - 0s 1ms/step - loss: 1.7673\n16/16 [==============================] - 0s 619us/step - loss: 1.7673\n16/16 [==============================] - 0s 617us/step - loss: 1.7673\n16/16 [==============================] - 0s 1ms/step - loss: 1.7673\n\nTesting for epoch 37 index 2:\n16/16 [==============================] - 0s 648us/step - loss: 0.6175\n16/16 [==============================] - 0s 628us/step - loss: 1.7136\n16/16 [==============================] - 0s 666us/step - loss: 1.7615\n16/16 [==============================] - 0s 612us/step - loss: 1.7696\n16/16 [==============================] - 0s 1ms/step - loss: 1.7713\n16/16 [==============================] - 0s 742us/step - loss: 1.7715\n16/16 [==============================] - 0s 626us/step - loss: 1.7716\n16/16 [==============================] - 0s 614us/step - loss: 1.7716\n16/16 [==============================] - 0s 623us/step - loss: 1.7715\n16/16 [==============================] - 0s 932us/step - loss: 1.7715\nEpoch 38 of 60\n\nTesting for epoch 38 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.6365\n16/16 [==============================] - 0s 1ms/step - loss: 1.7285\n16/16 [==============================] - 0s 1ms/step - loss: 1.7737\n16/16 [==============================] - 0s 1ms/step - loss: 1.7815\n16/16 [==============================] - 0s 1ms/step - loss: 1.7831\n16/16 [==============================] - 0s 617us/step - loss: 1.7834\n16/16 [==============================] - 0s 1ms/step - loss: 1.7834\n16/16 [==============================] - 0s 1ms/step - loss: 1.7834\n16/16 [==============================] - 0s 601us/step - loss: 1.7834\n16/16 [==============================] - 0s 929us/step - loss: 1.7834\n\nTesting for epoch 38 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.6645\n16/16 [==============================] - 0s 1ms/step - loss: 1.7425\n16/16 [==============================] - 0s 1ms/step - loss: 1.7848\n16/16 [==============================] - 0s 641us/step - loss: 1.7924\n16/16 [==============================] - 0s 1ms/step - loss: 1.7938\n16/16 [==============================] - 0s 1ms/step - loss: 1.7941\n16/16 [==============================] - 0s 821us/step - loss: 1.7941\n16/16 [==============================] - 0s 1ms/step - loss: 1.7941\n16/16 [==============================] - 0s 626us/step - loss: 1.7941\n16/16 [==============================] - 0s 636us/step - loss: 1.7941\nEpoch 39 of 60\n\nTesting for epoch 39 index 1:\n16/16 [==============================] - 0s 634us/step - loss: 0.6840\n16/16 [==============================] - 0s 759us/step - loss: 1.7590\n16/16 [==============================] - 0s 1ms/step - loss: 1.7997\n16/16 [==============================] - 0s 1ms/step - loss: 1.8070\n16/16 [==============================] - 0s 1ms/step - loss: 1.8084\n16/16 [==============================] - 0s 1ms/step - loss: 1.8086\n16/16 [==============================] - 0s 639us/step - loss: 1.8086\n16/16 [==============================] - 0s 621us/step - loss: 1.8086\n16/16 [==============================] - 0s 670us/step - loss: 1.8086\n16/16 [==============================] - 0s 1ms/step - loss: 1.8086\n\nTesting for epoch 39 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.7124\n16/16 [==============================] - 0s 999us/step - loss: 1.7783\n16/16 [==============================] - 0s 1ms/step - loss: 1.8175\n16/16 [==============================] - 0s 688us/step - loss: 1.8245\n16/16 [==============================] - 0s 1ms/step - loss: 1.8258\n16/16 [==============================] - 0s 1ms/step - loss: 1.8261\n16/16 [==============================] - 0s 716us/step - loss: 1.8261\n16/16 [==============================] - 0s 658us/step - loss: 1.8261\n16/16 [==============================] - 0s 632us/step - loss: 1.8261\n16/16 [==============================] - 0s 635us/step - loss: 1.8261\nEpoch 40 of 60\n\nTesting for epoch 40 index 1:\n16/16 [==============================] - 0s 921us/step - loss: 0.7273\n16/16 [==============================] - 0s 863us/step - loss: 1.7767\n16/16 [==============================] - 0s 995us/step - loss: 1.8143\n16/16 [==============================] - 0s 593us/step - loss: 1.8210\n16/16 [==============================] - 0s 1ms/step - loss: 1.8223\n16/16 [==============================] - 0s 877us/step - loss: 1.8225\n16/16 [==============================] - 0s 1ms/step - loss: 1.8226\n16/16 [==============================] - 0s 820us/step - loss: 1.8226\n16/16 [==============================] - 0s 1ms/step - loss: 1.8225\n16/16 [==============================] - 0s 1ms/step - loss: 1.8225\n\nTesting for epoch 40 index 2:\n16/16 [==============================] - 0s 688us/step - loss: 0.7514\n16/16 [==============================] - 0s 1ms/step - loss: 1.7804\n16/16 [==============================] - 0s 1ms/step - loss: 1.8152\n16/16 [==============================] - 0s 585us/step - loss: 1.8214\n16/16 [==============================] - 0s 1ms/step - loss: 1.8225\n16/16 [==============================] - 0s 619us/step - loss: 1.8227\n16/16 [==============================] - 0s 1ms/step - loss: 1.8228\n16/16 [==============================] - 0s 1ms/step - loss: 1.8228\n16/16 [==============================] - 0s 1ms/step - loss: 1.8227\n16/16 [==============================] - 0s 852us/step - loss: 1.8227\nEpoch 41 of 60\n\nTesting for epoch 41 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.7649\n16/16 [==============================] - 0s 1ms/step - loss: 1.7944\n16/16 [==============================] - 0s 1ms/step - loss: 1.8299\n16/16 [==============================] - 0s 804us/step - loss: 1.8361\n16/16 [==============================] - 0s 1ms/step - loss: 1.8372\n16/16 [==============================] - 0s 612us/step - loss: 1.8375\n16/16 [==============================] - 0s 1ms/step - loss: 1.8375\n16/16 [==============================] - 0s 1ms/step - loss: 1.8375\n16/16 [==============================] - 0s 1ms/step - loss: 1.8375\n16/16 [==============================] - 0s 641us/step - loss: 1.8375\n\nTesting for epoch 41 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.7882\n16/16 [==============================] - 0s 853us/step - loss: 1.8148\n16/16 [==============================] - 0s 659us/step - loss: 1.8491\n16/16 [==============================] - 0s 615us/step - loss: 1.8553\n16/16 [==============================] - 0s 931us/step - loss: 1.8563\n16/16 [==============================] - 0s 1ms/step - loss: 1.8566\n16/16 [==============================] - 0s 634us/step - loss: 1.8566\n16/16 [==============================] - 0s 861us/step - loss: 1.8566\n16/16 [==============================] - 0s 960us/step - loss: 1.8566\n16/16 [==============================] - 0s 1ms/step - loss: 1.8566\nEpoch 42 of 60\n\nTesting for epoch 42 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.7975\n16/16 [==============================] - 0s 646us/step - loss: 1.8225\n16/16 [==============================] - 0s 1ms/step - loss: 1.8555\n16/16 [==============================] - 0s 588us/step - loss: 1.8616\n16/16 [==============================] - 0s 898us/step - loss: 1.8626\n16/16 [==============================] - 0s 1ms/step - loss: 1.8628\n16/16 [==============================] - 0s 1ms/step - loss: 1.8628\n16/16 [==============================] - 0s 1ms/step - loss: 1.8628\n16/16 [==============================] - 0s 661us/step - loss: 1.8628\n16/16 [==============================] - 0s 636us/step - loss: 1.8628\n\nTesting for epoch 42 index 2:\n16/16 [==============================] - 0s 590us/step - loss: 0.8169\n16/16 [==============================] - 0s 1ms/step - loss: 1.8391\n16/16 [==============================] - 0s 633us/step - loss: 1.8715\n16/16 [==============================] - 0s 585us/step - loss: 1.8774\n16/16 [==============================] - 0s 615us/step - loss: 1.8784\n16/16 [==============================] - 0s 596us/step - loss: 1.8786\n16/16 [==============================] - 0s 1ms/step - loss: 1.8787\n16/16 [==============================] - 0s 1ms/step - loss: 1.8787\n16/16 [==============================] - 0s 631us/step - loss: 1.8787\n16/16 [==============================] - 0s 671us/step - loss: 1.8787\nEpoch 43 of 60\n\nTesting for epoch 43 index 1:\n16/16 [==============================] - 0s 865us/step - loss: 0.8231\n16/16 [==============================] - 0s 1ms/step - loss: 1.8496\n16/16 [==============================] - 0s 620us/step - loss: 1.8823\n16/16 [==============================] - 0s 664us/step - loss: 1.8883\n16/16 [==============================] - 0s 600us/step - loss: 1.8893\n16/16 [==============================] - 0s 1ms/step - loss: 1.8895\n16/16 [==============================] - 0s 1ms/step - loss: 1.8896\n16/16 [==============================] - 0s 1ms/step - loss: 1.8896\n16/16 [==============================] - 0s 628us/step - loss: 1.8896\n16/16 [==============================] - 0s 1ms/step - loss: 1.8895\n\nTesting for epoch 43 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.8391\n16/16 [==============================] - 0s 609us/step - loss: 1.8681\n16/16 [==============================] - 0s 871us/step - loss: 1.9009\n16/16 [==============================] - 0s 565us/step - loss: 1.9070\n16/16 [==============================] - 0s 1ms/step - loss: 1.9079\n16/16 [==============================] - 0s 610us/step - loss: 1.9081\n16/16 [==============================] - 0s 637us/step - loss: 1.9082\n16/16 [==============================] - 0s 1ms/step - loss: 1.9082\n16/16 [==============================] - 0s 1ms/step - loss: 1.9082\n16/16 [==============================] - 0s 880us/step - loss: 1.9082\nEpoch 44 of 60\n\nTesting for epoch 44 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.8437\n16/16 [==============================] - 0s 1ms/step - loss: 1.8798\n16/16 [==============================] - 0s 923us/step - loss: 1.9120\n16/16 [==============================] - 0s 1ms/step - loss: 1.9179\n16/16 [==============================] - 0s 612us/step - loss: 1.9189\n16/16 [==============================] - 0s 1ms/step - loss: 1.9191\n16/16 [==============================] - 0s 681us/step - loss: 1.9191\n16/16 [==============================] - 0s 1ms/step - loss: 1.9191\n16/16 [==============================] - 0s 1ms/step - loss: 1.9191\n16/16 [==============================] - 0s 1ms/step - loss: 1.9191\n\nTesting for epoch 44 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.8544\n16/16 [==============================] - 0s 645us/step - loss: 1.8871\n16/16 [==============================] - 0s 632us/step - loss: 1.9189\n16/16 [==============================] - 0s 1ms/step - loss: 1.9248\n16/16 [==============================] - 0s 1ms/step - loss: 1.9257\n16/16 [==============================] - 0s 1ms/step - loss: 1.9259\n16/16 [==============================] - 0s 836us/step - loss: 1.9259\n16/16 [==============================] - 0s 695us/step - loss: 1.9259\n16/16 [==============================] - 0s 1ms/step - loss: 1.9259\n16/16 [==============================] - 0s 808us/step - loss: 1.9259\nEpoch 45 of 60\n\nTesting for epoch 45 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.8583\n16/16 [==============================] - 0s 1ms/step - loss: 1.9099\n16/16 [==============================] - 0s 785us/step - loss: 1.9431\n16/16 [==============================] - 0s 782us/step - loss: 1.9491\n16/16 [==============================] - 0s 1ms/step - loss: 1.9501\n16/16 [==============================] - 0s 711us/step - loss: 1.9503\n16/16 [==============================] - 0s 617us/step - loss: 1.9503\n16/16 [==============================] - 0s 601us/step - loss: 1.9503\n16/16 [==============================] - 0s 671us/step - loss: 1.9503\n16/16 [==============================] - 0s 602us/step - loss: 1.9504\n\nTesting for epoch 45 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.8705\n16/16 [==============================] - 0s 1ms/step - loss: 1.9270\n16/16 [==============================] - 0s 928us/step - loss: 1.9599\n16/16 [==============================] - 0s 758us/step - loss: 1.9658\n16/16 [==============================] - 0s 2ms/step - loss: 1.9668\n16/16 [==============================] - 0s 1ms/step - loss: 1.9670\n16/16 [==============================] - 0s 594us/step - loss: 1.9670\n16/16 [==============================] - 0s 600us/step - loss: 1.9670\n16/16 [==============================] - 0s 599us/step - loss: 1.9670\n16/16 [==============================] - 0s 924us/step - loss: 1.9670\nEpoch 46 of 60\n\nTesting for epoch 46 index 1:\n16/16 [==============================] - 0s 643us/step - loss: 0.8683\n16/16 [==============================] - 0s 1ms/step - loss: 1.9287\n16/16 [==============================] - 0s 665us/step - loss: 1.9614\n16/16 [==============================] - 0s 1ms/step - loss: 1.9673\n16/16 [==============================] - 0s 1ms/step - loss: 1.9683\n16/16 [==============================] - 0s 1ms/step - loss: 1.9685\n16/16 [==============================] - 0s 654us/step - loss: 1.9685\n16/16 [==============================] - 0s 762us/step - loss: 1.9685\n16/16 [==============================] - 0s 628us/step - loss: 1.9685\n16/16 [==============================] - 0s 654us/step - loss: 1.9685\n\nTesting for epoch 46 index 2:\n16/16 [==============================] - 0s 659us/step - loss: 0.8815\n16/16 [==============================] - 0s 1ms/step - loss: 1.9514\n16/16 [==============================] - 0s 1ms/step - loss: 1.9846\n16/16 [==============================] - 0s 1ms/step - loss: 1.9905\n16/16 [==============================] - 0s 1ms/step - loss: 1.9915\n16/16 [==============================] - 0s 786us/step - loss: 1.9916\n16/16 [==============================] - 0s 655us/step - loss: 1.9917\n16/16 [==============================] - 0s 1ms/step - loss: 1.9917\n16/16 [==============================] - 0s 973us/step - loss: 1.9917\n16/16 [==============================] - 0s 1ms/step - loss: 1.9917\nEpoch 47 of 60\n\nTesting for epoch 47 index 1:\n16/16 [==============================] - 0s 741us/step - loss: 0.8773\n16/16 [==============================] - 0s 672us/step - loss: 1.9488\n16/16 [==============================] - 0s 660us/step - loss: 1.9816\n16/16 [==============================] - 0s 1ms/step - loss: 1.9875\n16/16 [==============================] - 0s 899us/step - loss: 1.9885\n16/16 [==============================] - 0s 1ms/step - loss: 1.9887\n16/16 [==============================] - 0s 885us/step - loss: 1.9887\n16/16 [==============================] - 0s 661us/step - loss: 1.9887\n16/16 [==============================] - 0s 1ms/step - loss: 1.9887\n16/16 [==============================] - 0s 972us/step - loss: 1.9887\n\nTesting for epoch 47 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 0.8923\n16/16 [==============================] - 0s 802us/step - loss: 1.9764\n16/16 [==============================] - 0s 733us/step - loss: 2.0094\n16/16 [==============================] - 0s 878us/step - loss: 2.0152\n16/16 [==============================] - 0s 667us/step - loss: 2.0162\n16/16 [==============================] - 0s 1ms/step - loss: 2.0164\n16/16 [==============================] - 0s 831us/step - loss: 2.0164\n16/16 [==============================] - 0s 1ms/step - loss: 2.0164\n16/16 [==============================] - 0s 1ms/step - loss: 2.0164\n16/16 [==============================] - 0s 1ms/step - loss: 2.0165\nEpoch 48 of 60\n\nTesting for epoch 48 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.8980\n16/16 [==============================] - 0s 829us/step - loss: 2.0059\n16/16 [==============================] - 0s 1ms/step - loss: 2.0403\n16/16 [==============================] - 0s 1ms/step - loss: 2.0464\n16/16 [==============================] - 0s 991us/step - loss: 2.0474\n16/16 [==============================] - 0s 657us/step - loss: 2.0476\n16/16 [==============================] - 0s 1ms/step - loss: 2.0476\n16/16 [==============================] - 0s 1ms/step - loss: 2.0476\n16/16 [==============================] - 0s 625us/step - loss: 2.0476\n16/16 [==============================] - 0s 1ms/step - loss: 2.0476\n\nTesting for epoch 48 index 2:\n16/16 [==============================] - 0s 640us/step - loss: 0.9077\n16/16 [==============================] - 0s 1ms/step - loss: 2.0130\n16/16 [==============================] - 0s 1ms/step - loss: 2.0470\n16/16 [==============================] - 0s 953us/step - loss: 2.0530\n16/16 [==============================] - 0s 1ms/step - loss: 2.0539\n16/16 [==============================] - 0s 1ms/step - loss: 2.0541\n16/16 [==============================] - 0s 1ms/step - loss: 2.0542\n16/16 [==============================] - 0s 1ms/step - loss: 2.0542\n16/16 [==============================] - 0s 1ms/step - loss: 2.0542\n16/16 [==============================] - 0s 1ms/step - loss: 2.0542\nEpoch 49 of 60\n\nTesting for epoch 49 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.9081\n16/16 [==============================] - 0s 1ms/step - loss: 2.0204\n16/16 [==============================] - 0s 1ms/step - loss: 2.0543\n16/16 [==============================] - 0s 694us/step - loss: 2.0602\n16/16 [==============================] - 0s 1ms/step - loss: 2.0611\n16/16 [==============================] - 0s 1ms/step - loss: 2.0613\n16/16 [==============================] - 0s 815us/step - loss: 2.0614\n16/16 [==============================] - 0s 1ms/step - loss: 2.0614\n16/16 [==============================] - 0s 804us/step - loss: 2.0614\n16/16 [==============================] - 0s 1ms/step - loss: 2.0614\n\nTesting for epoch 49 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.9192\n16/16 [==============================] - 0s 2ms/step - loss: 2.0292\n16/16 [==============================] - 0s 1ms/step - loss: 2.0625\n16/16 [==============================] - 0s 1ms/step - loss: 2.0683\n16/16 [==============================] - 0s 1ms/step - loss: 2.0692\n16/16 [==============================] - 0s 1ms/step - loss: 2.0693\n16/16 [==============================] - 0s 1ms/step - loss: 2.0694\n16/16 [==============================] - 0s 1ms/step - loss: 2.0694\n16/16 [==============================] - 0s 1ms/step - loss: 2.0694\n16/16 [==============================] - 0s 1ms/step - loss: 2.0694\nEpoch 50 of 60\n\nTesting for epoch 50 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.9220\n16/16 [==============================] - 0s 1ms/step - loss: 2.0413\n16/16 [==============================] - 0s 1ms/step - loss: 2.0749\n16/16 [==============================] - 0s 1ms/step - loss: 2.0807\n16/16 [==============================] - 0s 1ms/step - loss: 2.0816\n16/16 [==============================] - 0s 1ms/step - loss: 2.0818\n16/16 [==============================] - 0s 1ms/step - loss: 2.0819\n16/16 [==============================] - 0s 1ms/step - loss: 2.0819\n16/16 [==============================] - 0s 1ms/step - loss: 2.0819\n16/16 [==============================] - 0s 1ms/step - loss: 2.0819\n\nTesting for epoch 50 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 0.9344\n16/16 [==============================] - 0s 2ms/step - loss: 2.0501\n16/16 [==============================] - 0s 2ms/step - loss: 2.0831\n16/16 [==============================] - 0s 2ms/step - loss: 2.0889\n16/16 [==============================] - 0s 2ms/step - loss: 2.0898\n16/16 [==============================] - 0s 2ms/step - loss: 2.0900\n16/16 [==============================] - 0s 2ms/step - loss: 2.0900\n16/16 [==============================] - 0s 2ms/step - loss: 2.0900\n16/16 [==============================] - 0s 2ms/step - loss: 2.0900\n16/16 [==============================] - 0s 2ms/step - loss: 2.0900\nEpoch 51 of 60\n\nTesting for epoch 51 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 0.9430\n16/16 [==============================] - 0s 2ms/step - loss: 2.0784\n16/16 [==============================] - 0s 1ms/step - loss: 2.1121\n16/16 [==============================] - 0s 1ms/step - loss: 2.1180\n16/16 [==============================] - 0s 1ms/step - loss: 2.1189\n16/16 [==============================] - 0s 670us/step - loss: 2.1191\n16/16 [==============================] - 0s 707us/step - loss: 2.1191\n16/16 [==============================] - 0s 720us/step - loss: 2.1191\n16/16 [==============================] - 0s 685us/step - loss: 2.1191\n16/16 [==============================] - 0s 600us/step - loss: 2.1191\n\nTesting for epoch 51 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.9590\n16/16 [==============================] - 0s 1ms/step - loss: 2.0944\n16/16 [==============================] - 0s 2ms/step - loss: 2.1283\n16/16 [==============================] - 0s 1ms/step - loss: 2.1342\n16/16 [==============================] - 0s 2ms/step - loss: 2.1351\n16/16 [==============================] - 0s 2ms/step - loss: 2.1353\n16/16 [==============================] - 0s 2ms/step - loss: 2.1353\n16/16 [==============================] - 0s 1ms/step - loss: 2.1353\n16/16 [==============================] - 0s 1ms/step - loss: 2.1353\n16/16 [==============================] - 0s 1ms/step - loss: 2.1354\nEpoch 52 of 60\n\nTesting for epoch 52 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.9632\n16/16 [==============================] - 0s 1ms/step - loss: 2.1018\n16/16 [==============================] - 0s 1ms/step - loss: 2.1355\n16/16 [==============================] - 0s 1ms/step - loss: 2.1415\n16/16 [==============================] - 0s 1ms/step - loss: 2.1424\n16/16 [==============================] - 0s 1ms/step - loss: 2.1426\n16/16 [==============================] - 0s 1ms/step - loss: 2.1426\n16/16 [==============================] - 0s 2ms/step - loss: 2.1426\n16/16 [==============================] - 0s 1ms/step - loss: 2.1426\n16/16 [==============================] - 0s 1ms/step - loss: 2.1427\n\nTesting for epoch 52 index 2:\n16/16 [==============================] - 0s 593us/step - loss: 0.9811\n16/16 [==============================] - 0s 1ms/step - loss: 2.1158\n16/16 [==============================] - 0s 601us/step - loss: 2.1494\n16/16 [==============================] - 0s 604us/step - loss: 2.1553\n16/16 [==============================] - 0s 1ms/step - loss: 2.1562\n16/16 [==============================] - 0s 580us/step - loss: 2.1564\n16/16 [==============================] - 0s 606us/step - loss: 2.1564\n16/16 [==============================] - 0s 669us/step - loss: 2.1565\n16/16 [==============================] - 0s 933us/step - loss: 2.1565\n16/16 [==============================] - 0s 604us/step - loss: 2.1565\nEpoch 53 of 60\n\nTesting for epoch 53 index 1:\n16/16 [==============================] - 0s 617us/step - loss: 0.9849\n16/16 [==============================] - 0s 874us/step - loss: 2.1127\n16/16 [==============================] - 0s 601us/step - loss: 2.1443\n16/16 [==============================] - 0s 721us/step - loss: 2.1500\n16/16 [==============================] - 0s 709us/step - loss: 2.1508\n16/16 [==============================] - 0s 636us/step - loss: 2.1510\n16/16 [==============================] - 0s 641us/step - loss: 2.1510\n16/16 [==============================] - 0s 3ms/step - loss: 2.1510\n16/16 [==============================] - 0s 832us/step - loss: 2.1510\n16/16 [==============================] - 0s 943us/step - loss: 2.1511\n\nTesting for epoch 53 index 2:\n16/16 [==============================] - 0s 960us/step - loss: 1.0029\n16/16 [==============================] - 0s 847us/step - loss: 2.1223\n16/16 [==============================] - 0s 1ms/step - loss: 2.1535\n16/16 [==============================] - 0s 663us/step - loss: 2.1592\n16/16 [==============================] - 0s 1ms/step - loss: 2.1600\n16/16 [==============================] - 0s 1ms/step - loss: 2.1601\n16/16 [==============================] - 0s 1ms/step - loss: 2.1602\n16/16 [==============================] - 0s 1ms/step - loss: 2.1602\n16/16 [==============================] - 0s 1ms/step - loss: 2.1602\n16/16 [==============================] - 0s 1ms/step - loss: 2.1602\nEpoch 54 of 60\n\nTesting for epoch 54 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.0201\n16/16 [==============================] - 0s 2ms/step - loss: 2.1555\n16/16 [==============================] - 0s 698us/step - loss: 2.1869\n16/16 [==============================] - 0s 738us/step - loss: 2.1925\n16/16 [==============================] - 0s 974us/step - loss: 2.1933\n16/16 [==============================] - 0s 1ms/step - loss: 2.1935\n16/16 [==============================] - 0s 811us/step - loss: 2.1935\n16/16 [==============================] - 0s 1ms/step - loss: 2.1935\n16/16 [==============================] - 0s 1ms/step - loss: 2.1935\n16/16 [==============================] - 0s 685us/step - loss: 2.1935\n\nTesting for epoch 54 index 2:\n16/16 [==============================] - 0s 644us/step - loss: 1.0391\n16/16 [==============================] - 0s 1ms/step - loss: 2.1625\n16/16 [==============================] - 0s 1ms/step - loss: 2.1931\n16/16 [==============================] - 0s 894us/step - loss: 2.1987\n16/16 [==============================] - 0s 2ms/step - loss: 2.1995\n16/16 [==============================] - 0s 700us/step - loss: 2.1996\n16/16 [==============================] - 0s 1ms/step - loss: 2.1997\n16/16 [==============================] - 0s 851us/step - loss: 2.1997\n16/16 [==============================] - 0s 925us/step - loss: 2.1997\n16/16 [==============================] - 0s 868us/step - loss: 2.1997\nEpoch 55 of 60\n\nTesting for epoch 55 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 1.0520\n16/16 [==============================] - 0s 627us/step - loss: 2.1801\n16/16 [==============================] - 0s 1ms/step - loss: 2.2107\n16/16 [==============================] - 0s 633us/step - loss: 2.2163\n16/16 [==============================] - 0s 899us/step - loss: 2.2171\n16/16 [==============================] - 0s 711us/step - loss: 2.2172\n16/16 [==============================] - 0s 1ms/step - loss: 2.2173\n16/16 [==============================] - 0s 664us/step - loss: 2.2173\n16/16 [==============================] - 0s 1ms/step - loss: 2.2173\n16/16 [==============================] - 0s 1ms/step - loss: 2.2173\n\nTesting for epoch 55 index 2:\n16/16 [==============================] - 0s 589us/step - loss: 1.0727\n16/16 [==============================] - 0s 603us/step - loss: 2.1879\n16/16 [==============================] - 0s 581us/step - loss: 2.2176\n16/16 [==============================] - 0s 580us/step - loss: 2.2229\n16/16 [==============================] - 0s 582us/step - loss: 2.2236\n16/16 [==============================] - 0s 571us/step - loss: 2.2238\n16/16 [==============================] - 0s 574us/step - loss: 2.2238\n16/16 [==============================] - 0s 561us/step - loss: 2.2238\n16/16 [==============================] - 0s 506us/step - loss: 2.2238\n16/16 [==============================] - 0s 526us/step - loss: 2.2239\nEpoch 56 of 60\n\nTesting for epoch 56 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 1.0790\n16/16 [==============================] - 0s 1ms/step - loss: 2.1883\n16/16 [==============================] - 0s 2ms/step - loss: 2.2177\n16/16 [==============================] - 0s 2ms/step - loss: 2.2230\n16/16 [==============================] - 0s 2ms/step - loss: 2.2237\n16/16 [==============================] - 0s 2ms/step - loss: 2.2239\n16/16 [==============================] - 0s 2ms/step - loss: 2.2239\n16/16 [==============================] - 0s 2ms/step - loss: 2.2239\n16/16 [==============================] - 0s 2ms/step - loss: 2.2240\n16/16 [==============================] - 0s 2ms/step - loss: 2.2240\n\nTesting for epoch 56 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 1.1019\n16/16 [==============================] - 0s 2ms/step - loss: 2.2025\n16/16 [==============================] - 0s 2ms/step - loss: 2.2310\n16/16 [==============================] - 0s 2ms/step - loss: 2.2362\n16/16 [==============================] - 0s 2ms/step - loss: 2.2369\n16/16 [==============================] - 0s 2ms/step - loss: 2.2371\n16/16 [==============================] - 0s 2ms/step - loss: 2.2371\n16/16 [==============================] - 0s 2ms/step - loss: 2.2371\n16/16 [==============================] - 0s 2ms/step - loss: 2.2371\n16/16 [==============================] - 0s 2ms/step - loss: 2.2372\nEpoch 57 of 60\n\nTesting for epoch 57 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 1.1130\n16/16 [==============================] - 0s 2ms/step - loss: 2.2126\n16/16 [==============================] - 0s 2ms/step - loss: 2.2408\n16/16 [==============================] - 0s 2ms/step - loss: 2.2460\n16/16 [==============================] - 0s 2ms/step - loss: 2.2466\n16/16 [==============================] - 0s 2ms/step - loss: 2.2468\n16/16 [==============================] - 0s 2ms/step - loss: 2.2468\n16/16 [==============================] - 0s 2ms/step - loss: 2.2469\n16/16 [==============================] - 0s 2ms/step - loss: 2.2469\n16/16 [==============================] - 0s 2ms/step - loss: 2.2469\n\nTesting for epoch 57 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 1.1360\n16/16 [==============================] - 0s 2ms/step - loss: 2.2253\n16/16 [==============================] - 0s 2ms/step - loss: 2.2519\n16/16 [==============================] - 0s 2ms/step - loss: 2.2569\n16/16 [==============================] - 0s 2ms/step - loss: 2.2576\n16/16 [==============================] - 0s 2ms/step - loss: 2.2577\n16/16 [==============================] - 0s 2ms/step - loss: 2.2577\n16/16 [==============================] - 0s 2ms/step - loss: 2.2577\n16/16 [==============================] - 0s 2ms/step - loss: 2.2578\n16/16 [==============================] - 0s 1ms/step - loss: 2.2578\nEpoch 58 of 60\n\nTesting for epoch 58 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.1524\n16/16 [==============================] - 0s 2ms/step - loss: 2.2520\n16/16 [==============================] - 0s 1ms/step - loss: 2.2798\n16/16 [==============================] - 0s 2ms/step - loss: 2.2850\n16/16 [==============================] - 0s 2ms/step - loss: 2.2857\n16/16 [==============================] - 0s 1ms/step - loss: 2.2858\n16/16 [==============================] - 0s 2ms/step - loss: 2.2859\n16/16 [==============================] - 0s 966us/step - loss: 2.2859\n16/16 [==============================] - 0s 2ms/step - loss: 2.2859\n16/16 [==============================] - 0s 2ms/step - loss: 2.2859\n\nTesting for epoch 58 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 1.1690\n16/16 [==============================] - 0s 984us/step - loss: 2.2519\n16/16 [==============================] - 0s 2ms/step - loss: 2.2784\n16/16 [==============================] - 0s 2ms/step - loss: 2.2834\n16/16 [==============================] - 0s 925us/step - loss: 2.2840\n16/16 [==============================] - 0s 2ms/step - loss: 2.2842\n16/16 [==============================] - 0s 2ms/step - loss: 2.2842\n16/16 [==============================] - 0s 2ms/step - loss: 2.2842\n16/16 [==============================] - 0s 767us/step - loss: 2.2842\n16/16 [==============================] - 0s 1ms/step - loss: 2.2843\nEpoch 59 of 60\n\nTesting for epoch 59 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 1.1811\n16/16 [==============================] - 0s 2ms/step - loss: 2.2676\n16/16 [==============================] - 0s 2ms/step - loss: 2.2948\n16/16 [==============================] - 0s 2ms/step - loss: 2.2999\n16/16 [==============================] - 0s 2ms/step - loss: 2.3005\n16/16 [==============================] - 0s 2ms/step - loss: 2.3007\n16/16 [==============================] - 0s 2ms/step - loss: 2.3008\n16/16 [==============================] - 0s 1ms/step - loss: 2.3008\n16/16 [==============================] - 0s 657us/step - loss: 2.3008\n16/16 [==============================] - 0s 654us/step - loss: 2.3008\n\nTesting for epoch 59 index 2:\n16/16 [==============================] - 0s 3ms/step - loss: 1.1986\n16/16 [==============================] - 0s 600us/step - loss: 2.2691\n16/16 [==============================] - 0s 579us/step - loss: 2.2946\n16/16 [==============================] - 0s 2ms/step - loss: 2.2995\n16/16 [==============================] - 0s 2ms/step - loss: 2.3001\n16/16 [==============================] - 0s 2ms/step - loss: 2.3003\n16/16 [==============================] - 0s 2ms/step - loss: 2.3003\n16/16 [==============================] - 0s 2ms/step - loss: 2.3003\n16/16 [==============================] - 0s 2ms/step - loss: 2.3004\n16/16 [==============================] - 0s 2ms/step - loss: 2.3004\nEpoch 60 of 60\n\nTesting for epoch 60 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 1.2123\n16/16 [==============================] - 0s 2ms/step - loss: 2.2903\n16/16 [==============================] - 0s 2ms/step - loss: 2.3163\n16/16 [==============================] - 0s 2ms/step - loss: 2.3213\n16/16 [==============================] - 0s 2ms/step - loss: 2.3219\n16/16 [==============================] - 0s 2ms/step - loss: 2.3221\n16/16 [==============================] - 0s 2ms/step - loss: 2.3221\n16/16 [==============================] - 0s 2ms/step - loss: 2.3221\n16/16 [==============================] - 0s 2ms/step - loss: 2.3221\n16/16 [==============================] - 0s 2ms/step - loss: 2.3222\n\nTesting for epoch 60 index 2:\n16/16 [==============================] - 0s 795us/step - loss: 1.2301\n16/16 [==============================] - 0s 1ms/step - loss: 2.2941\n16/16 [==============================] - 0s 597us/step - loss: 2.3188\n16/16 [==============================] - 0s 524us/step - loss: 2.3236\n16/16 [==============================] - 0s 889us/step - loss: 2.3242\n16/16 [==============================] - 0s 675us/step - loss: 2.3243\n16/16 [==============================] - 0s 523us/step - loss: 2.3243\n16/16 [==============================] - 0s 577us/step - loss: 2.3243\n16/16 [==============================] - 0s 531us/step - loss: 2.3243\n16/16 [==============================] - 0s 646us/step - loss: 2.3244\n\n\n\noutlier_MO_GAAL_one = list(clf.labels_)\n\n\noutlier_MO_GAAL_one = list(map(lambda x: 1 if x==0  else -1,outlier_MO_GAAL_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_MO_GAAL_one,tab_orbit)\n\n\n_conf.conf(\"MO-GAAL (Liu et al., 2019)\")\n\n\n\n\nAccuracy: 0.950\nPrecision: 0.950\nRecall: 1.000\nF1 Score: 0.974\n\n\n\nthirteen = twelve.append(_conf.tab)\n\n\n\nLSCP\n\ndetectors = [KNN(), LOF(), OCSVM()]\nclf = LSCP(detectors)\nclf.fit(_df[['x', 'y','f']])\n_df['LSCP_clf'] = clf.labels_\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/pyod/models/lscp.py:382: UserWarning: The number of histogram bins is greater than the number of classifiers, reducing n_bins to n_clf.\n  warnings.warn(\n\n\n\noutlier_LSCP_one = list(clf.labels_)\n\n\noutlier_LSCP_one = list(map(lambda x: 1 if x==0  else -1,outlier_LSCP_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_LSCP_one,tab_orbit)\n\n\n_conf.conf(\"LSCP (Zhao et al., 2019)\")\n\n\n\n\nAccuracy: 0.948\nPrecision: 0.999\nRecall: 0.946\nF1 Score: 0.972\n\n\n\nfourteen_orbit = thirteen.append(_conf.tab)"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#orbit-result",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#orbit-result",
    "title": "Class code for Comparison Study",
    "section": "Orbit Result",
    "text": "Orbit Result\n\nround(fourteen_orbit,4)\n\n\n\n\n\n  \n    \n      \n      Accuracy\n      Precision\n      Recall\n      F1\n    \n  \n  \n    \n      GODE\n      0.997\n      0.9969\n      1.0000\n      0.9984\n    \n    \n      LOF (Breunig et al., 2000)\n      0.886\n      0.9872\n      0.8916\n      0.9369\n    \n    \n      kNN (Ramaswamy et al., 2000)\n      0.948\n      0.9989\n      0.9463\n      0.9719\n    \n    \n      CBLOF (He et al., 2003)\n      0.918\n      0.9568\n      0.9568\n      0.9568\n    \n    \n      OCSVM (Sch ̈olkopf et al., 2001)\n      0.923\n      0.9877\n      0.9305\n      0.9583\n    \n    \n      MCD (Hardin and Rocke, 2004)\n      0.866\n      0.9533\n      0.9032\n      0.9276\n    \n    \n      Feature Bagging (Lazarevic and Kumar, 2005)\n      0.912\n      0.9789\n      0.9274\n      0.9524\n    \n    \n      ABOD (Kriegel et al., 2008)\n      0.988\n      0.9937\n      0.9937\n      0.9937\n    \n    \n      Isolation Forest (Liu et al., 2008)\n      0.378\n      0.9970\n      0.3463\n      0.5141\n    \n    \n      HBOS (Goldstein and Dengel, 2012)\n      0.881\n      0.9612\n      0.9116\n      0.9357\n    \n    \n      SOS (Janssens et al., 2012)\n      0.881\n      0.9612\n      0.9116\n      0.9357\n    \n    \n      SO-GAAL (Liu et al., 2019)\n      0.876\n      0.9589\n      0.9084\n      0.9330\n    \n    \n      MO-GAAL (Liu et al., 2019)\n      0.950\n      0.9500\n      1.0000\n      0.9744\n    \n    \n      LSCP (Zhao et al., 2019)\n      0.948\n      0.9989\n      0.9463\n      0.9719"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#bunny",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#bunny",
    "title": "Class code for Comparison Study",
    "section": "Bunny",
    "text": "Bunny\n\nG = graphs.Bunny()\nn = G.N\n\n\ng = filters.Heat(G, tau=75) \n\n\nnormal = np.random.randn(n)\nunif = np.concatenate([np.random.uniform(low=3,high=7,size=60), np.random.uniform(low=-7,high=-3,size=60),np.zeros(n-120)]); np.random.shuffle(unif)\nnoise = normal + unif\nindex_of_trueoutlier2 = np.where(unif!=0)\n\n\nf = np.zeros(n)\nf[1000] = -3234\nf = g.filter(f, method='chebyshev') \n\n2022-11-26 07:54:05,353:[WARNING](pygsp.graphs.graph.lmax): The largest eigenvalue G.lmax is not available, we need to estimate it. Explicitly call G.estimate_lmax() or G.compute_fourier_basis() once beforehand to suppress the warning.\n\n\n\nG.coords.shape\n\n(2503, 3)\n\n\n\n_W = G.W.toarray()\n_x = G.coords[:,0]\n_y = G.coords[:,1]\n_z = -G.coords[:,2]\n\n\n_df = pd.DataFrame({'x' : _x, 'y' : _y, 'z' : _z, 'fnoise':f+noise,'f' : f, 'noise': noise})\n\n\noutlier_true_one_2 = unif.copy()\n\n\noutlier_true_one_2 = list(map(lambda x: -1 if x !=0  else 1,outlier_true_one_2))\n\n\nX = np.array(_df)[:,:4]\n\n\nGODE\n\n_BUNNY = BUNNY(_df)\n\n\n_BUNNY.fit(sd=20,ref=10)\n\n\noutlier_simul_one = (_BUNNY.df['Residual']**2).tolist()\n\n\noutlier_simul_one = list(map(lambda x: -1 if x > 10 else 1,outlier_simul_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_simul_one,tab_bunny)\n\n\n_conf.conf(\"GODE\")\n\n\n\n\nAccuracy: 0.995\nPrecision: 0.995\nRecall: 0.999\nF1 Score: 0.997\n\n\n\none = _conf.tab\n\n\n\nLOF\n\nclf = LocalOutlierFactor(n_neighbors=2)\n\n\n_conf = Conf_matrx(outlier_true_one_2,clf.fit_predict(X),tab_bunny)\n\n\n_conf.conf(\"LOF (Breunig et al., 2000)\")\n\n\n\n\nAccuracy: 0.928\nPrecision: 0.957\nRecall: 0.969\nF1 Score: 0.963\n\n\n\ntwo = one.append(_conf.tab)\n\n\n\nKNN\n\nclf = KNN()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['knn_Clf'] = clf.labels_\n\n\noutlier_KNN_one = list(clf.labels_)\n\n\noutlier_KNN_one = list(map(lambda x: 1 if x==0  else -1,outlier_KNN_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_KNN_one,tab_bunny)\n\n\n_conf.conf(\"kNN (Ramaswamy et al., 2000)\")\n\n\n\n\nAccuracy: 0.940\nPrecision: 0.996\nRecall: 0.941\nF1 Score: 0.968\n\n\n\nthree = two.append(_conf.tab)\n\n\n\nCBLOF\n\nclf = CBLOF(contamination=0.05,check_estimator=False, random_state=77)\nclf.fit(_df[['x', 'y','fnoise']])\n_df['CBLOF_Clf'] = clf.labels_\n\n\noutlier_CBLOF_one = list(clf.labels_)\n\n\noutlier_CBLOF_one = list(map(lambda x: 1 if x==0  else -1,outlier_CBLOF_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_CBLOF_one,tab_bunny)\n\n\n_conf.conf(\"CBLOF (He et al., 2003)\")\n\n\n\n\nAccuracy: 0.978\nPrecision: 0.989\nRecall: 0.987\nF1 Score: 0.988\n\n\n\nfour = three.append(_conf.tab)\n\n\n\nOCSVM\n\nclf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n\n\nclf.fit(X)\n\nOneClassSVM(gamma=0.1, nu=0.1)\n\n\n\noutlier_OSVM_one = list(clf.predict(X))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_OSVM_one,tab_bunny)\n\n\n_conf.conf(\"OCSVM (Sch ̈olkopf et al., 2001)\")\n\n\n\n\nAccuracy: 0.932\nPrecision: 0.991\nRecall: 0.937\nF1 Score: 0.963\n\n\n\nfive = four.append(_conf.tab)\n\n\n\nMCD\n\nclf = MCD()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['MCD_clf'] = clf.labels_\n\n\noutlier_MCD_one = list(clf.labels_)\n\n\noutlier_MCD_one = list(map(lambda x: 1 if x==0  else -1,outlier_MCD_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_MCD_one,tab_bunny)\n\n\n_conf.conf(\"MCD (Hardin and Rocke, 2004)\")\n\n\n\n\nAccuracy: 0.935\nPrecision: 0.993\nRecall: 0.938\nF1 Score: 0.965\n\n\n\nsix = five.append(_conf.tab)\n\n\n\nFeature Bagging\n\nclf = FeatureBagging()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['FeatureBagging_clf'] = clf.labels_\n\n\noutlier_FeatureBagging_one = list(clf.labels_)\n\n\noutlier_FeatureBagging_one = list(map(lambda x: 1 if x==0  else -1,outlier_FeatureBagging_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_FeatureBagging_one,tab_bunny)\n\n\n_conf.conf(\"Feature Bagging (Lazarevic and Kumar, 2005)\")\n\n\n\n\nAccuracy: 0.915\nPrecision: 0.982\nRecall: 0.928\nF1 Score: 0.954\n\n\n\nseven = six.append(_conf.tab)\n\n\n\nABOD\n\nclf = ABOD(contamination=0.05)\nclf.fit(_df[['x', 'y','fnoise']])\n_df['ABOD_Clf'] = clf.labels_\n\n\noutlier_ABOD_one = list(clf.labels_)\n\n\noutlier_ABOD_one = list(map(lambda x: 1 if x==0  else -1,outlier_ABOD_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_ABOD_one,tab_bunny)\n\n\n_conf.conf(\"ABOD (Kriegel et al., 2008)\")\n\n\n\n\nAccuracy: 0.977\nPrecision: 0.989\nRecall: 0.987\nF1 Score: 0.988\n\n\n\neight = seven.append(_conf.tab)\n\n\n\nIForest\n\nod = IForest(\n    threshold=0.,\n    n_estimators=100\n)\n\n\nod.fit(_df[['x', 'y','fnoise']])\n\n\npreds = od.predict(\n    _df[['x', 'y','fnoise']],\n    return_instance_score=True\n)\n\n\n_df['IF_alibi'] = preds['data']['is_outlier']\n\n\noutlier_alibi_one = _df['IF_alibi']\n\n\noutlier_alibi_one = list(map(lambda x: 1 if x==0  else -1,outlier_alibi_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_alibi_one,tab_bunny)\n\n\n_conf.conf(\"Isolation Forest (Liu et al., 2008)\")\n\n\n\n\nAccuracy: 0.794\nPrecision: 0.995\nRecall: 0.788\nF1 Score: 0.879\n\n\n\nnine = eight.append(_conf.tab)\n\n\n\nHBOS\n\nclf = HBOS()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['HBOS_clf'] = clf.labels_\n\n\noutlier_HBOS_one = list(clf.labels_)\n\n\noutlier_HBOS_one = list(map(lambda x: 1 if x==0  else -1,outlier_HBOS_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_HBOS_one,tab_bunny)\n\n\n_conf.conf(\"HBOS (Goldstein and Dengel, 2012)\")\n\n\n\n\nAccuracy: 0.895\nPrecision: 0.969\nRecall: 0.919\nF1 Score: 0.944\n\n\n\nten = nine.append(_conf.tab)\n\n\n\nSOS\n\noutlier_SOS_one = list(clf.labels_)\n\n\noutlier_SOS_one = list(map(lambda x: 1 if x==0  else -1,outlier_SOS_one))\n\n\nclf = SOS()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['SOS_clf'] = clf.labels_\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_SOS_one,tab_bunny)\n\n\n_conf.conf(\"SOS (Janssens et al., 2012)\")\n\n\n\n\nAccuracy: 0.895\nPrecision: 0.969\nRecall: 0.919\nF1 Score: 0.944\n\n\n\neleven = ten.append(_conf.tab)\n\n\n\nSO_GAAL\n\nclf = SO_GAAL()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['SO_GAAL_clf'] = clf.labels_\n\nEpoch 1 of 60\n\nTesting for epoch 1 index 1:\n\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n\n\n\nTesting for epoch 1 index 2:\n\nTesting for epoch 1 index 3:\n\nTesting for epoch 1 index 4:\n\nTesting for epoch 1 index 5:\nEpoch 2 of 60\n\nTesting for epoch 2 index 1:\n\nTesting for epoch 2 index 2:\n\nTesting for epoch 2 index 3:\n\nTesting for epoch 2 index 4:\n\nTesting for epoch 2 index 5:\nEpoch 3 of 60\n\nTesting for epoch 3 index 1:\n\nTesting for epoch 3 index 2:\n\nTesting for epoch 3 index 3:\n\nTesting for epoch 3 index 4:\n\nTesting for epoch 3 index 5:\nEpoch 4 of 60\n\nTesting for epoch 4 index 1:\n\nTesting for epoch 4 index 2:\n\nTesting for epoch 4 index 3:\n\nTesting for epoch 4 index 4:\n\nTesting for epoch 4 index 5:\nEpoch 5 of 60\n\nTesting for epoch 5 index 1:\n\nTesting for epoch 5 index 2:\n\nTesting for epoch 5 index 3:\n\nTesting for epoch 5 index 4:\n\nTesting for epoch 5 index 5:\nEpoch 6 of 60\n\nTesting for epoch 6 index 1:\n\nTesting for epoch 6 index 2:\n\nTesting for epoch 6 index 3:\n\nTesting for epoch 6 index 4:\n\nTesting for epoch 6 index 5:\nEpoch 7 of 60\n\nTesting for epoch 7 index 1:\n\nTesting for epoch 7 index 2:\n\nTesting for epoch 7 index 3:\n\nTesting for epoch 7 index 4:\n\nTesting for epoch 7 index 5:\nEpoch 8 of 60\n\nTesting for epoch 8 index 1:\n\nTesting for epoch 8 index 2:\n\nTesting for epoch 8 index 3:\n\nTesting for epoch 8 index 4:\n\nTesting for epoch 8 index 5:\nEpoch 9 of 60\n\nTesting for epoch 9 index 1:\n\nTesting for epoch 9 index 2:\n\nTesting for epoch 9 index 3:\n\nTesting for epoch 9 index 4:\n\nTesting for epoch 9 index 5:\nEpoch 10 of 60\n\nTesting for epoch 10 index 1:\n\nTesting for epoch 10 index 2:\n\nTesting for epoch 10 index 3:\n\nTesting for epoch 10 index 4:\n\nTesting for epoch 10 index 5:\nEpoch 11 of 60\n\nTesting for epoch 11 index 1:\n\nTesting for epoch 11 index 2:\n\nTesting for epoch 11 index 3:\n\nTesting for epoch 11 index 4:\n\nTesting for epoch 11 index 5:\nEpoch 12 of 60\n\nTesting for epoch 12 index 1:\n\nTesting for epoch 12 index 2:\n\nTesting for epoch 12 index 3:\n\nTesting for epoch 12 index 4:\n\nTesting for epoch 12 index 5:\nEpoch 13 of 60\n\nTesting for epoch 13 index 1:\n\nTesting for epoch 13 index 2:\n\nTesting for epoch 13 index 3:\n\nTesting for epoch 13 index 4:\n\nTesting for epoch 13 index 5:\nEpoch 14 of 60\n\nTesting for epoch 14 index 1:\n\nTesting for epoch 14 index 2:\n\nTesting for epoch 14 index 3:\n\nTesting for epoch 14 index 4:\n\nTesting for epoch 14 index 5:\nEpoch 15 of 60\n\nTesting for epoch 15 index 1:\n\nTesting for epoch 15 index 2:\n\nTesting for epoch 15 index 3:\n\nTesting for epoch 15 index 4:\n\nTesting for epoch 15 index 5:\nEpoch 16 of 60\n\nTesting for epoch 16 index 1:\n\nTesting for epoch 16 index 2:\n\nTesting for epoch 16 index 3:\n\nTesting for epoch 16 index 4:\n\nTesting for epoch 16 index 5:\nEpoch 17 of 60\n\nTesting for epoch 17 index 1:\n\nTesting for epoch 17 index 2:\n\nTesting for epoch 17 index 3:\n\nTesting for epoch 17 index 4:\n\nTesting for epoch 17 index 5:\nEpoch 18 of 60\n\nTesting for epoch 18 index 1:\n\nTesting for epoch 18 index 2:\n\nTesting for epoch 18 index 3:\n\nTesting for epoch 18 index 4:\n\nTesting for epoch 18 index 5:\nEpoch 19 of 60\n\nTesting for epoch 19 index 1:\n\nTesting for epoch 19 index 2:\n\nTesting for epoch 19 index 3:\n\nTesting for epoch 19 index 4:\n\nTesting for epoch 19 index 5:\nEpoch 20 of 60\n\nTesting for epoch 20 index 1:\n\nTesting for epoch 20 index 2:\n\nTesting for epoch 20 index 3:\n\nTesting for epoch 20 index 4:\n\nTesting for epoch 20 index 5:\nEpoch 21 of 60\n\nTesting for epoch 21 index 1:\n\nTesting for epoch 21 index 2:\n\nTesting for epoch 21 index 3:\n\nTesting for epoch 21 index 4:\n\nTesting for epoch 21 index 5:\nEpoch 22 of 60\n\nTesting for epoch 22 index 1:\n16/16 [==============================] - 0s 894us/step - loss: 1.8529\n\nTesting for epoch 22 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8921\n\nTesting for epoch 22 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9309\n\nTesting for epoch 22 index 4:\n16/16 [==============================] - 0s 690us/step - loss: 1.8584\n\nTesting for epoch 22 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8820\nEpoch 23 of 60\n\nTesting for epoch 23 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9128\n\nTesting for epoch 23 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9055\n\nTesting for epoch 23 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9463\n\nTesting for epoch 23 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9150\n\nTesting for epoch 23 index 5:\n16/16 [==============================] - 0s 755us/step - loss: 1.9138\nEpoch 24 of 60\n\nTesting for epoch 24 index 1:\n16/16 [==============================] - 0s 636us/step - loss: 2.0252\n\nTesting for epoch 24 index 2:\n16/16 [==============================] - 0s 640us/step - loss: 1.9456\n\nTesting for epoch 24 index 3:\n16/16 [==============================] - 0s 701us/step - loss: 1.9662\n\nTesting for epoch 24 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9841\n\nTesting for epoch 24 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0037\nEpoch 25 of 60\n\nTesting for epoch 25 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9889\n\nTesting for epoch 25 index 2:\n16/16 [==============================] - 0s 871us/step - loss: 1.9856\n\nTesting for epoch 25 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0014\n\nTesting for epoch 25 index 4:\n16/16 [==============================] - 0s 778us/step - loss: 2.0162\n\nTesting for epoch 25 index 5:\n16/16 [==============================] - 0s 664us/step - loss: 2.0739\nEpoch 26 of 60\n\nTesting for epoch 26 index 1:\n16/16 [==============================] - 0s 637us/step - loss: 2.0179\n\nTesting for epoch 26 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0133\n\nTesting for epoch 26 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0655\n\nTesting for epoch 26 index 4:\n16/16 [==============================] - 0s 637us/step - loss: 2.0657\n\nTesting for epoch 26 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0669\nEpoch 27 of 60\n\nTesting for epoch 27 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0880\n\nTesting for epoch 27 index 2:\n16/16 [==============================] - 0s 800us/step - loss: 2.0889\n\nTesting for epoch 27 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1112\n\nTesting for epoch 27 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0641\n\nTesting for epoch 27 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0520\nEpoch 28 of 60\n\nTesting for epoch 28 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0533\n\nTesting for epoch 28 index 2:\n16/16 [==============================] - 0s 601us/step - loss: 2.1067\n\nTesting for epoch 28 index 3:\n16/16 [==============================] - 0s 645us/step - loss: 2.1065\n\nTesting for epoch 28 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0956\n\nTesting for epoch 28 index 5:\n16/16 [==============================] - 0s 634us/step - loss: 2.0811\nEpoch 29 of 60\n\nTesting for epoch 29 index 1:\n16/16 [==============================] - 0s 633us/step - loss: 2.0727\n\nTesting for epoch 29 index 2:\n16/16 [==============================] - 0s 687us/step - loss: 2.1834\n\nTesting for epoch 29 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0984\n\nTesting for epoch 29 index 4:\n16/16 [==============================] - 0s 599us/step - loss: 2.1578\n\nTesting for epoch 29 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1489\nEpoch 30 of 60\n\nTesting for epoch 30 index 1:\n16/16 [==============================] - 0s 671us/step - loss: 2.1636\n\nTesting for epoch 30 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1516\n\nTesting for epoch 30 index 3:\n16/16 [==============================] - 0s 636us/step - loss: 2.1534\n\nTesting for epoch 30 index 4:\n16/16 [==============================] - 0s 776us/step - loss: 2.1465\n\nTesting for epoch 30 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1006\nEpoch 31 of 60\n\nTesting for epoch 31 index 1:\n16/16 [==============================] - 0s 768us/step - loss: 2.1580\n\nTesting for epoch 31 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1679\n\nTesting for epoch 31 index 3:\n16/16 [==============================] - 0s 932us/step - loss: 2.1854\n\nTesting for epoch 31 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1869\n\nTesting for epoch 31 index 5:\n16/16 [==============================] - 0s 600us/step - loss: 2.1570\nEpoch 32 of 60\n\nTesting for epoch 32 index 1:\n16/16 [==============================] - 0s 701us/step - loss: 2.2004\n\nTesting for epoch 32 index 2:\n16/16 [==============================] - 0s 664us/step - loss: 2.2094\n\nTesting for epoch 32 index 3:\n16/16 [==============================] - 0s 665us/step - loss: 2.2316\n\nTesting for epoch 32 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1808\n\nTesting for epoch 32 index 5:\n16/16 [==============================] - 0s 606us/step - loss: 2.2633\nEpoch 33 of 60\n\nTesting for epoch 33 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2481\n\nTesting for epoch 33 index 2:\n16/16 [==============================] - 0s 629us/step - loss: 2.2154\n\nTesting for epoch 33 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2065\n\nTesting for epoch 33 index 4:\n16/16 [==============================] - 0s 632us/step - loss: 2.2313\n\nTesting for epoch 33 index 5:\n16/16 [==============================] - 0s 728us/step - loss: 2.2298\nEpoch 34 of 60\n\nTesting for epoch 34 index 1:\n16/16 [==============================] - 0s 651us/step - loss: 2.2541\n\nTesting for epoch 34 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2413\n\nTesting for epoch 34 index 3:\n16/16 [==============================] - 0s 665us/step - loss: 2.1930\n\nTesting for epoch 34 index 4:\n16/16 [==============================] - 0s 607us/step - loss: 2.2856\n\nTesting for epoch 34 index 5:\n16/16 [==============================] - 0s 650us/step - loss: 2.2537\nEpoch 35 of 60\n\nTesting for epoch 35 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2461\n\nTesting for epoch 35 index 2:\n16/16 [==============================] - 0s 654us/step - loss: 2.3097\n\nTesting for epoch 35 index 3:\n16/16 [==============================] - 0s 831us/step - loss: 2.3159\n\nTesting for epoch 35 index 4:\n16/16 [==============================] - 0s 934us/step - loss: 2.2306\n\nTesting for epoch 35 index 5:\n16/16 [==============================] - 0s 654us/step - loss: 2.2956\nEpoch 36 of 60\n\nTesting for epoch 36 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2296\n\nTesting for epoch 36 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2378\n\nTesting for epoch 36 index 3:\n16/16 [==============================] - 0s 926us/step - loss: 2.2114\n\nTesting for epoch 36 index 4:\n16/16 [==============================] - 0s 716us/step - loss: 2.2166\n\nTesting for epoch 36 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2483\nEpoch 37 of 60\n\nTesting for epoch 37 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2669\n\nTesting for epoch 37 index 2:\n16/16 [==============================] - 0s 718us/step - loss: 2.2966\n\nTesting for epoch 37 index 3:\n16/16 [==============================] - 0s 776us/step - loss: 2.2346\n\nTesting for epoch 37 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3040\n\nTesting for epoch 37 index 5:\n16/16 [==============================] - 0s 780us/step - loss: 2.3003\nEpoch 38 of 60\n\nTesting for epoch 38 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2809\n\nTesting for epoch 38 index 2:\n16/16 [==============================] - 0s 789us/step - loss: 2.2804\n\nTesting for epoch 38 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2915\n\nTesting for epoch 38 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2829\n\nTesting for epoch 38 index 5:\n16/16 [==============================] - 0s 923us/step - loss: 2.3199\nEpoch 39 of 60\n\nTesting for epoch 39 index 1:\n16/16 [==============================] - 0s 980us/step - loss: 2.2642\n\nTesting for epoch 39 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3208\n\nTesting for epoch 39 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3127\n\nTesting for epoch 39 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3514\n\nTesting for epoch 39 index 5:\n16/16 [==============================] - 0s 829us/step - loss: 2.3363\nEpoch 40 of 60\n\nTesting for epoch 40 index 1:\n16/16 [==============================] - 0s 629us/step - loss: 2.3203\n\nTesting for epoch 40 index 2:\n16/16 [==============================] - 0s 658us/step - loss: 2.3100\n\nTesting for epoch 40 index 3:\n16/16 [==============================] - 0s 625us/step - loss: 2.2837\n\nTesting for epoch 40 index 4:\n16/16 [==============================] - 0s 640us/step - loss: 2.2877\n\nTesting for epoch 40 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3374\nEpoch 41 of 60\n\nTesting for epoch 41 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3149\n\nTesting for epoch 41 index 2:\n16/16 [==============================] - 0s 658us/step - loss: 2.3535\n\nTesting for epoch 41 index 3:\n16/16 [==============================] - 0s 652us/step - loss: 2.3861\n\nTesting for epoch 41 index 4:\n16/16 [==============================] - 0s 723us/step - loss: 2.3328\n\nTesting for epoch 41 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3450\nEpoch 42 of 60\n\nTesting for epoch 42 index 1:\n16/16 [==============================] - 0s 641us/step - loss: 2.3578\n\nTesting for epoch 42 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3235\n\nTesting for epoch 42 index 3:\n16/16 [==============================] - 0s 958us/step - loss: 2.3421\n\nTesting for epoch 42 index 4:\n16/16 [==============================] - 0s 593us/step - loss: 2.3656\n\nTesting for epoch 42 index 5:\n16/16 [==============================] - 0s 623us/step - loss: 2.3044\nEpoch 43 of 60\n\nTesting for epoch 43 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3273\n\nTesting for epoch 43 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3797\n\nTesting for epoch 43 index 3:\n16/16 [==============================] - 0s 654us/step - loss: 2.3372\n\nTesting for epoch 43 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3387\n\nTesting for epoch 43 index 5:\n16/16 [==============================] - 0s 608us/step - loss: 2.4377\nEpoch 44 of 60\n\nTesting for epoch 44 index 1:\n16/16 [==============================] - 0s 965us/step - loss: 2.4568\n\nTesting for epoch 44 index 2:\n16/16 [==============================] - 0s 640us/step - loss: 2.4050\n\nTesting for epoch 44 index 3:\n16/16 [==============================] - 0s 943us/step - loss: 2.3936\n\nTesting for epoch 44 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3910\n\nTesting for epoch 44 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4026\nEpoch 45 of 60\n\nTesting for epoch 45 index 1:\n16/16 [==============================] - 0s 647us/step - loss: 2.4177\n\nTesting for epoch 45 index 2:\n16/16 [==============================] - 0s 941us/step - loss: 2.4015\n\nTesting for epoch 45 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3971\n\nTesting for epoch 45 index 4:\n16/16 [==============================] - 0s 678us/step - loss: 2.3933\n\nTesting for epoch 45 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4488\nEpoch 46 of 60\n\nTesting for epoch 46 index 1:\n16/16 [==============================] - 0s 594us/step - loss: 2.3598\n\nTesting for epoch 46 index 2:\n16/16 [==============================] - 0s 655us/step - loss: 2.4883\n\nTesting for epoch 46 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4234\n\nTesting for epoch 46 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3641\n\nTesting for epoch 46 index 5:\n16/16 [==============================] - 0s 649us/step - loss: 2.4212\nEpoch 47 of 60\n\nTesting for epoch 47 index 1:\n16/16 [==============================] - 0s 828us/step - loss: 2.5119\n\nTesting for epoch 47 index 2:\n16/16 [==============================] - 0s 625us/step - loss: 2.4255\n\nTesting for epoch 47 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4828\n\nTesting for epoch 47 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4336\n\nTesting for epoch 47 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3916\nEpoch 48 of 60\n\nTesting for epoch 48 index 1:\n16/16 [==============================] - 0s 630us/step - loss: 2.4157\n\nTesting for epoch 48 index 2:\n16/16 [==============================] - 0s 621us/step - loss: 2.4543\n\nTesting for epoch 48 index 3:\n16/16 [==============================] - 0s 672us/step - loss: 2.3956\n\nTesting for epoch 48 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4783\n\nTesting for epoch 48 index 5:\n16/16 [==============================] - 0s 630us/step - loss: 2.4045\nEpoch 49 of 60\n\nTesting for epoch 49 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4787\n\nTesting for epoch 49 index 2:\n16/16 [==============================] - 0s 880us/step - loss: 2.4557\n\nTesting for epoch 49 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4497\n\nTesting for epoch 49 index 4:\n16/16 [==============================] - 0s 635us/step - loss: 2.4115\n\nTesting for epoch 49 index 5:\n16/16 [==============================] - 0s 613us/step - loss: 2.4469\nEpoch 50 of 60\n\nTesting for epoch 50 index 1:\n16/16 [==============================] - 0s 764us/step - loss: 2.4250\n\nTesting for epoch 50 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4706\n\nTesting for epoch 50 index 3:\n16/16 [==============================] - 0s 620us/step - loss: 2.3919\n\nTesting for epoch 50 index 4:\n16/16 [==============================] - 0s 698us/step - loss: 2.4463\n\nTesting for epoch 50 index 5:\n16/16 [==============================] - 0s 958us/step - loss: 2.4810\nEpoch 51 of 60\n\nTesting for epoch 51 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4359\n\nTesting for epoch 51 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4080\n\nTesting for epoch 51 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4634\n\nTesting for epoch 51 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5226\n\nTesting for epoch 51 index 5:\n16/16 [==============================] - 0s 894us/step - loss: 2.4385\nEpoch 52 of 60\n\nTesting for epoch 52 index 1:\n16/16 [==============================] - 0s 856us/step - loss: 2.5063\n\nTesting for epoch 52 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4672\n\nTesting for epoch 52 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5011\n\nTesting for epoch 52 index 4:\n16/16 [==============================] - 0s 618us/step - loss: 2.5610\n\nTesting for epoch 52 index 5:\n16/16 [==============================] - 0s 679us/step - loss: 2.5239\nEpoch 53 of 60\n\nTesting for epoch 53 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5248\n\nTesting for epoch 53 index 2:\n16/16 [==============================] - 0s 990us/step - loss: 2.5142\n\nTesting for epoch 53 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5164\n\nTesting for epoch 53 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3996\n\nTesting for epoch 53 index 5:\n16/16 [==============================] - 0s 894us/step - loss: 2.4939\nEpoch 54 of 60\n\nTesting for epoch 54 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4897\n\nTesting for epoch 54 index 2:\n16/16 [==============================] - 0s 617us/step - loss: 2.5320\n\nTesting for epoch 54 index 3:\n16/16 [==============================] - 0s 619us/step - loss: 2.5544\n\nTesting for epoch 54 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4986\n\nTesting for epoch 54 index 5:\n16/16 [==============================] - 0s 648us/step - loss: 2.5618\nEpoch 55 of 60\n\nTesting for epoch 55 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5605\n\nTesting for epoch 55 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4780\n\nTesting for epoch 55 index 3:\n16/16 [==============================] - 0s 665us/step - loss: 2.4659\n\nTesting for epoch 55 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4696\n\nTesting for epoch 55 index 5:\n16/16 [==============================] - 0s 643us/step - loss: 2.5610\nEpoch 56 of 60\n\nTesting for epoch 56 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4586\n\nTesting for epoch 56 index 2:\n16/16 [==============================] - 0s 665us/step - loss: 2.4735\n\nTesting for epoch 56 index 3:\n16/16 [==============================] - 0s 964us/step - loss: 2.5013\n\nTesting for epoch 56 index 4:\n16/16 [==============================] - 0s 840us/step - loss: 2.4765\n\nTesting for epoch 56 index 5:\n16/16 [==============================] - 0s 908us/step - loss: 2.5925\nEpoch 57 of 60\n\nTesting for epoch 57 index 1:\n16/16 [==============================] - 0s 644us/step - loss: 2.5213\n\nTesting for epoch 57 index 2:\n16/16 [==============================] - 0s 624us/step - loss: 2.5540\n\nTesting for epoch 57 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5273\n\nTesting for epoch 57 index 4:\n16/16 [==============================] - 0s 665us/step - loss: 2.5155\n\nTesting for epoch 57 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5001\nEpoch 58 of 60\n\nTesting for epoch 58 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5154\n\nTesting for epoch 58 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5593\n\nTesting for epoch 58 index 3:\n16/16 [==============================] - 0s 653us/step - loss: 2.4897\n\nTesting for epoch 58 index 4:\n16/16 [==============================] - 0s 621us/step - loss: 2.5391\n\nTesting for epoch 58 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5966\nEpoch 59 of 60\n\nTesting for epoch 59 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5325\n\nTesting for epoch 59 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5563\n\nTesting for epoch 59 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4993\n\nTesting for epoch 59 index 4:\n16/16 [==============================] - 0s 625us/step - loss: 2.5589\n\nTesting for epoch 59 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5403\nEpoch 60 of 60\n\nTesting for epoch 60 index 1:\n16/16 [==============================] - 0s 833us/step - loss: 2.5143\n\nTesting for epoch 60 index 2:\n16/16 [==============================] - 0s 808us/step - loss: 2.5618\n\nTesting for epoch 60 index 3:\n16/16 [==============================] - 0s 796us/step - loss: 2.5960\n\nTesting for epoch 60 index 4:\n16/16 [==============================] - 0s 599us/step - loss: 2.5405\n\nTesting for epoch 60 index 5:\n16/16 [==============================] - 0s 650us/step - loss: 2.5440\n\n\n\noutlier_SO_GAAL_one = list(clf.labels_)\n\n\noutlier_SO_GAAL_one = list(map(lambda x: 1 if x==0  else -1,outlier_SO_GAAL_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_SO_GAAL_one,tab_bunny)\n\n\n_conf.conf(\"SO-GAAL (Liu et al., 2019)\")\n\n\n\n\nAccuracy: 0.952\nPrecision: 0.952\nRecall: 1.000\nF1 Score: 0.975\n\n\n\ntwelve = eleven.append(_conf.tab)\n\n\n\nMO_GAAL\n\nclf = MO_GAAL()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['MO_GAAL_clf'] = clf.labels_\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n\n\nEpoch 1 of 60\n\nTesting for epoch 1 index 1:\n\nTesting for epoch 1 index 2:\n\nTesting for epoch 1 index 3:\n\nTesting for epoch 1 index 4:\n\nTesting for epoch 1 index 5:\nEpoch 2 of 60\n\nTesting for epoch 2 index 1:\n\nTesting for epoch 2 index 2:\n\nTesting for epoch 2 index 3:\n\nTesting for epoch 2 index 4:\n\nTesting for epoch 2 index 5:\nEpoch 3 of 60\n\nTesting for epoch 3 index 1:\n\nTesting for epoch 3 index 2:\n\nTesting for epoch 3 index 3:\n\nTesting for epoch 3 index 4:\n\nTesting for epoch 3 index 5:\nEpoch 4 of 60\n\nTesting for epoch 4 index 1:\n\nTesting for epoch 4 index 2:\n\nTesting for epoch 4 index 3:\n\nTesting for epoch 4 index 4:\n\nTesting for epoch 4 index 5:\nEpoch 5 of 60\n\nTesting for epoch 5 index 1:\n\nTesting for epoch 5 index 2:\n\nTesting for epoch 5 index 3:\n\nTesting for epoch 5 index 4:\n\nTesting for epoch 5 index 5:\nEpoch 6 of 60\n\nTesting for epoch 6 index 1:\n\nTesting for epoch 6 index 2:\n\nTesting for epoch 6 index 3:\n\nTesting for epoch 6 index 4:\n\nTesting for epoch 6 index 5:\nEpoch 7 of 60\n\nTesting for epoch 7 index 1:\n\nTesting for epoch 7 index 2:\n\nTesting for epoch 7 index 3:\n\nTesting for epoch 7 index 4:\n\nTesting for epoch 7 index 5:\nEpoch 8 of 60\n\nTesting for epoch 8 index 1:\n\nTesting for epoch 8 index 2:\n\nTesting for epoch 8 index 3:\n\nTesting for epoch 8 index 4:\n\nTesting for epoch 8 index 5:\nEpoch 9 of 60\n\nTesting for epoch 9 index 1:\n\nTesting for epoch 9 index 2:\n\nTesting for epoch 9 index 3:\n\nTesting for epoch 9 index 4:\n\nTesting for epoch 9 index 5:\nEpoch 10 of 60\n\nTesting for epoch 10 index 1:\n\nTesting for epoch 10 index 2:\n\nTesting for epoch 10 index 3:\n\nTesting for epoch 10 index 4:\n\nTesting for epoch 10 index 5:\nEpoch 11 of 60\n\nTesting for epoch 11 index 1:\n\nTesting for epoch 11 index 2:\n\nTesting for epoch 11 index 3:\n\nTesting for epoch 11 index 4:\n\nTesting for epoch 11 index 5:\nEpoch 12 of 60\n\nTesting for epoch 12 index 1:\n\nTesting for epoch 12 index 2:\n\nTesting for epoch 12 index 3:\n\nTesting for epoch 12 index 4:\n\nTesting for epoch 12 index 5:\nEpoch 13 of 60\n\nTesting for epoch 13 index 1:\n\nTesting for epoch 13 index 2:\n\nTesting for epoch 13 index 3:\n\nTesting for epoch 13 index 4:\n\nTesting for epoch 13 index 5:\nEpoch 14 of 60\n\nTesting for epoch 14 index 1:\n\nTesting for epoch 14 index 2:\n\nTesting for epoch 14 index 3:\n\nTesting for epoch 14 index 4:\n\nTesting for epoch 14 index 5:\nEpoch 15 of 60\n\nTesting for epoch 15 index 1:\n\nTesting for epoch 15 index 2:\n\nTesting for epoch 15 index 3:\n\nTesting for epoch 15 index 4:\n\nTesting for epoch 15 index 5:\nEpoch 16 of 60\n\nTesting for epoch 16 index 1:\n\nTesting for epoch 16 index 2:\n\nTesting for epoch 16 index 3:\n\nTesting for epoch 16 index 4:\n\nTesting for epoch 16 index 5:\nEpoch 17 of 60\n\nTesting for epoch 17 index 1:\n\nTesting for epoch 17 index 2:\n\nTesting for epoch 17 index 3:\n\nTesting for epoch 17 index 4:\n\nTesting for epoch 17 index 5:\nEpoch 18 of 60\n\nTesting for epoch 18 index 1:\n\nTesting for epoch 18 index 2:\n\nTesting for epoch 18 index 3:\n\nTesting for epoch 18 index 4:\n\nTesting for epoch 18 index 5:\nEpoch 19 of 60\n\nTesting for epoch 19 index 1:\n\nTesting for epoch 19 index 2:\n\nTesting for epoch 19 index 3:\n\nTesting for epoch 19 index 4:\n\nTesting for epoch 19 index 5:\nEpoch 20 of 60\n\nTesting for epoch 20 index 1:\n\nTesting for epoch 20 index 2:\n\nTesting for epoch 20 index 3:\n\nTesting for epoch 20 index 4:\n\nTesting for epoch 20 index 5:\nEpoch 21 of 60\n\nTesting for epoch 21 index 1:\n\nTesting for epoch 21 index 2:\n16/16 [==============================] - 0s 839us/step - loss: 0.2862\n16/16 [==============================] - 0s 1ms/step - loss: 1.3562\n16/16 [==============================] - 0s 879us/step - loss: 1.6391\n16/16 [==============================] - 0s 676us/step - loss: 1.7457\n16/16 [==============================] - 0s 668us/step - loss: 1.7800\n16/16 [==============================] - 0s 797us/step - loss: 1.7893\n16/16 [==============================] - 0s 1ms/step - loss: 1.7882\n16/16 [==============================] - 0s 750us/step - loss: 1.7810\n16/16 [==============================] - 0s 661us/step - loss: 1.7768\n16/16 [==============================] - 0s 1ms/step - loss: 1.7746\n\nTesting for epoch 21 index 3:\n16/16 [==============================] - 0s 709us/step - loss: 0.2829\n16/16 [==============================] - 0s 1ms/step - loss: 1.3627\n16/16 [==============================] - 0s 1ms/step - loss: 1.6520\n16/16 [==============================] - 0s 1ms/step - loss: 1.7617\n16/16 [==============================] - 0s 648us/step - loss: 1.7969\n16/16 [==============================] - 0s 655us/step - loss: 1.8064\n16/16 [==============================] - 0s 1ms/step - loss: 1.8050\n16/16 [==============================] - 0s 1ms/step - loss: 1.7975\n16/16 [==============================] - 0s 1ms/step - loss: 1.7932\n16/16 [==============================] - 0s 1ms/step - loss: 1.7909\n\nTesting for epoch 21 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2819\n16/16 [==============================] - 0s 662us/step - loss: 1.3750\n16/16 [==============================] - 0s 649us/step - loss: 1.6692\n16/16 [==============================] - 0s 664us/step - loss: 1.7821\n16/16 [==============================] - 0s 644us/step - loss: 1.8194\n16/16 [==============================] - 0s 671us/step - loss: 1.8316\n16/16 [==============================] - 0s 651us/step - loss: 1.8318\n16/16 [==============================] - 0s 661us/step - loss: 1.8249\n16/16 [==============================] - 0s 990us/step - loss: 1.8208\n16/16 [==============================] - 0s 1ms/step - loss: 1.8185\n\nTesting for epoch 21 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2784\n16/16 [==============================] - 0s 1ms/step - loss: 1.3590\n16/16 [==============================] - 0s 645us/step - loss: 1.6490\n16/16 [==============================] - 0s 1ms/step - loss: 1.7586\n16/16 [==============================] - 0s 655us/step - loss: 1.7914\n16/16 [==============================] - 0s 1ms/step - loss: 1.7998\n16/16 [==============================] - 0s 1ms/step - loss: 1.7975\n16/16 [==============================] - 0s 653us/step - loss: 1.7896\n16/16 [==============================] - 0s 675us/step - loss: 1.7852\n16/16 [==============================] - 0s 1ms/step - loss: 1.7829\nEpoch 22 of 60\n\nTesting for epoch 22 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2787\n16/16 [==============================] - 0s 1ms/step - loss: 1.3475\n16/16 [==============================] - 0s 646us/step - loss: 1.6341\n16/16 [==============================] - 0s 690us/step - loss: 1.7422\n16/16 [==============================] - 0s 1ms/step - loss: 1.7757\n16/16 [==============================] - 0s 1ms/step - loss: 1.7855\n16/16 [==============================] - 0s 1ms/step - loss: 1.7843\n16/16 [==============================] - 0s 1ms/step - loss: 1.7771\n16/16 [==============================] - 0s 1ms/step - loss: 1.7729\n16/16 [==============================] - 0s 1ms/step - loss: 1.7708\n\nTesting for epoch 22 index 2:\n16/16 [==============================] - 0s 680us/step - loss: 0.2756\n16/16 [==============================] - 0s 1ms/step - loss: 1.3616\n16/16 [==============================] - 0s 678us/step - loss: 1.6485\n16/16 [==============================] - 0s 946us/step - loss: 1.7539\n16/16 [==============================] - 0s 673us/step - loss: 1.7847\n16/16 [==============================] - 0s 656us/step - loss: 1.7921\n16/16 [==============================] - 0s 1ms/step - loss: 1.7895\n16/16 [==============================] - 0s 660us/step - loss: 1.7812\n16/16 [==============================] - 0s 1ms/step - loss: 1.7768\n16/16 [==============================] - 0s 730us/step - loss: 1.7745\n\nTesting for epoch 22 index 3:\n16/16 [==============================] - 0s 660us/step - loss: 0.2723\n16/16 [==============================] - 0s 1ms/step - loss: 1.3959\n16/16 [==============================] - 0s 642us/step - loss: 1.7002\n16/16 [==============================] - 0s 874us/step - loss: 1.8105\n16/16 [==============================] - 0s 1ms/step - loss: 1.8423\n16/16 [==============================] - 0s 1ms/step - loss: 1.8494\n16/16 [==============================] - 0s 657us/step - loss: 1.8460\n16/16 [==============================] - 0s 940us/step - loss: 1.8371\n16/16 [==============================] - 0s 634us/step - loss: 1.8324\n16/16 [==============================] - 0s 905us/step - loss: 1.8299\n\nTesting for epoch 22 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2702\n16/16 [==============================] - 0s 649us/step - loss: 1.3792\n16/16 [==============================] - 0s 867us/step - loss: 1.6818\n16/16 [==============================] - 0s 619us/step - loss: 1.7923\n16/16 [==============================] - 0s 859us/step - loss: 1.8248\n16/16 [==============================] - 0s 609us/step - loss: 1.8327\n16/16 [==============================] - 0s 1ms/step - loss: 1.8298\n16/16 [==============================] - 0s 584us/step - loss: 1.8209\n16/16 [==============================] - 0s 590us/step - loss: 1.8163\n16/16 [==============================] - 0s 602us/step - loss: 1.8139\n\nTesting for epoch 22 index 5:\n16/16 [==============================] - 0s 683us/step - loss: 0.2694\n16/16 [==============================] - 0s 794us/step - loss: 1.3853\n16/16 [==============================] - 0s 1ms/step - loss: 1.6907\n16/16 [==============================] - 0s 1ms/step - loss: 1.8014\n16/16 [==============================] - 0s 634us/step - loss: 1.8329\n16/16 [==============================] - 0s 1ms/step - loss: 1.8400\n16/16 [==============================] - 0s 694us/step - loss: 1.8367\n16/16 [==============================] - 0s 1ms/step - loss: 1.8275\n16/16 [==============================] - 0s 589us/step - loss: 1.8228\n16/16 [==============================] - 0s 1ms/step - loss: 1.8204\nEpoch 23 of 60\n\nTesting for epoch 23 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2679\n16/16 [==============================] - 0s 1ms/step - loss: 1.4280\n16/16 [==============================] - 0s 1ms/step - loss: 1.7530\n16/16 [==============================] - 0s 1ms/step - loss: 1.8709\n16/16 [==============================] - 0s 1ms/step - loss: 1.9036\n16/16 [==============================] - 0s 643us/step - loss: 1.9107\n16/16 [==============================] - 0s 1ms/step - loss: 1.9074\n16/16 [==============================] - 0s 1ms/step - loss: 1.8981\n16/16 [==============================] - 0s 1ms/step - loss: 1.8933\n16/16 [==============================] - 0s 1ms/step - loss: 1.8908\n\nTesting for epoch 23 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2673\n16/16 [==============================] - 0s 974us/step - loss: 1.4127\n16/16 [==============================] - 0s 820us/step - loss: 1.7343\n16/16 [==============================] - 0s 630us/step - loss: 1.8524\n16/16 [==============================] - 0s 606us/step - loss: 1.8844\n16/16 [==============================] - 0s 719us/step - loss: 1.8917\n16/16 [==============================] - 0s 1ms/step - loss: 1.8882\n16/16 [==============================] - 0s 611us/step - loss: 1.8784\n16/16 [==============================] - 0s 666us/step - loss: 1.8735\n16/16 [==============================] - 0s 1ms/step - loss: 1.8709\n\nTesting for epoch 23 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2645\n16/16 [==============================] - 0s 1ms/step - loss: 1.4021\n16/16 [==============================] - 0s 594us/step - loss: 1.7169\n16/16 [==============================] - 0s 587us/step - loss: 1.8300\n16/16 [==============================] - 0s 613us/step - loss: 1.8582\n16/16 [==============================] - 0s 1ms/step - loss: 1.8634\n16/16 [==============================] - 0s 1ms/step - loss: 1.8590\n16/16 [==============================] - 0s 644us/step - loss: 1.8494\n16/16 [==============================] - 0s 622us/step - loss: 1.8445\n16/16 [==============================] - 0s 617us/step - loss: 1.8420\n\nTesting for epoch 23 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2641\n16/16 [==============================] - 0s 1ms/step - loss: 1.4346\n16/16 [==============================] - 0s 1ms/step - loss: 1.7641\n16/16 [==============================] - 0s 860us/step - loss: 1.8848\n16/16 [==============================] - 0s 1ms/step - loss: 1.9154\n16/16 [==============================] - 0s 962us/step - loss: 1.9222\n16/16 [==============================] - 0s 634us/step - loss: 1.9176\n16/16 [==============================] - 0s 1ms/step - loss: 1.9075\n16/16 [==============================] - 0s 640us/step - loss: 1.9024\n16/16 [==============================] - 0s 1ms/step - loss: 1.8998\n\nTesting for epoch 23 index 5:\n16/16 [==============================] - 0s 935us/step - loss: 0.2571\n16/16 [==============================] - 0s 600us/step - loss: 1.4423\n16/16 [==============================] - 0s 1ms/step - loss: 1.7744\n16/16 [==============================] - 0s 1ms/step - loss: 1.8942\n16/16 [==============================] - 0s 1ms/step - loss: 1.9222\n16/16 [==============================] - 0s 1ms/step - loss: 1.9268\n16/16 [==============================] - 0s 702us/step - loss: 1.9205\n16/16 [==============================] - 0s 637us/step - loss: 1.9092\n16/16 [==============================] - 0s 1ms/step - loss: 1.9038\n16/16 [==============================] - 0s 638us/step - loss: 1.9011\nEpoch 24 of 60\n\nTesting for epoch 24 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2576\n16/16 [==============================] - 0s 1ms/step - loss: 1.4150\n16/16 [==============================] - 0s 586us/step - loss: 1.7381\n16/16 [==============================] - 0s 825us/step - loss: 1.8529\n16/16 [==============================] - 0s 848us/step - loss: 1.8794\n16/16 [==============================] - 0s 716us/step - loss: 1.8834\n16/16 [==============================] - 0s 1ms/step - loss: 1.8775\n16/16 [==============================] - 0s 598us/step - loss: 1.8670\n16/16 [==============================] - 0s 614us/step - loss: 1.8618\n16/16 [==============================] - 0s 1ms/step - loss: 1.8593\n\nTesting for epoch 24 index 2:\n16/16 [==============================] - 0s 604us/step - loss: 0.2602\n16/16 [==============================] - 0s 1ms/step - loss: 1.4321\n16/16 [==============================] - 0s 1ms/step - loss: 1.7581\n16/16 [==============================] - 0s 1ms/step - loss: 1.8731\n16/16 [==============================] - 0s 636us/step - loss: 1.8998\n16/16 [==============================] - 0s 1ms/step - loss: 1.9035\n16/16 [==============================] - 0s 607us/step - loss: 1.8975\n16/16 [==============================] - 0s 1ms/step - loss: 1.8867\n16/16 [==============================] - 0s 646us/step - loss: 1.8815\n16/16 [==============================] - 0s 1ms/step - loss: 1.8790\n\nTesting for epoch 24 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2559\n16/16 [==============================] - 0s 1ms/step - loss: 1.4384\n16/16 [==============================] - 0s 610us/step - loss: 1.7673\n16/16 [==============================] - 0s 1ms/step - loss: 1.8808\n16/16 [==============================] - 0s 663us/step - loss: 1.9062\n16/16 [==============================] - 0s 844us/step - loss: 1.9082\n16/16 [==============================] - 0s 780us/step - loss: 1.9007\n16/16 [==============================] - 0s 602us/step - loss: 1.8887\n16/16 [==============================] - 0s 735us/step - loss: 1.8831\n16/16 [==============================] - 0s 1ms/step - loss: 1.8805\n\nTesting for epoch 24 index 4:\n16/16 [==============================] - 0s 828us/step - loss: 0.2595\n16/16 [==============================] - 0s 1ms/step - loss: 1.4660\n16/16 [==============================] - 0s 1ms/step - loss: 1.8046\n16/16 [==============================] - 0s 1ms/step - loss: 1.9238\n16/16 [==============================] - 0s 613us/step - loss: 1.9510\n16/16 [==============================] - 0s 613us/step - loss: 1.9550\n16/16 [==============================] - 0s 607us/step - loss: 1.9486\n16/16 [==============================] - 0s 600us/step - loss: 1.9375\n16/16 [==============================] - 0s 1ms/step - loss: 1.9321\n16/16 [==============================] - 0s 1ms/step - loss: 1.9295\n\nTesting for epoch 24 index 5:\n16/16 [==============================] - 0s 602us/step - loss: 0.2490\n16/16 [==============================] - 0s 783us/step - loss: 1.4405\n16/16 [==============================] - 0s 962us/step - loss: 1.7687\n16/16 [==============================] - 0s 1ms/step - loss: 1.8795\n16/16 [==============================] - 0s 1ms/step - loss: 1.9005\n16/16 [==============================] - 0s 1ms/step - loss: 1.8995\n16/16 [==============================] - 0s 1ms/step - loss: 1.8901\n16/16 [==============================] - 0s 1ms/step - loss: 1.8774\n16/16 [==============================] - 0s 1ms/step - loss: 1.8717\n16/16 [==============================] - 0s 1ms/step - loss: 1.8690\nEpoch 25 of 60\n\nTesting for epoch 25 index 1:\n16/16 [==============================] - 0s 951us/step - loss: 0.2496\n16/16 [==============================] - 0s 610us/step - loss: 1.4539\n16/16 [==============================] - 0s 1ms/step - loss: 1.7891\n16/16 [==============================] - 0s 1ms/step - loss: 1.9046\n16/16 [==============================] - 0s 628us/step - loss: 1.9285\n16/16 [==============================] - 0s 645us/step - loss: 1.9295\n16/16 [==============================] - 0s 1ms/step - loss: 1.9219\n16/16 [==============================] - 0s 602us/step - loss: 1.9101\n16/16 [==============================] - 0s 1ms/step - loss: 1.9046\n16/16 [==============================] - 0s 888us/step - loss: 1.9020\n\nTesting for epoch 25 index 2:\n16/16 [==============================] - 0s 647us/step - loss: 0.2496\n16/16 [==============================] - 0s 600us/step - loss: 1.4771\n16/16 [==============================] - 0s 677us/step - loss: 1.8228\n16/16 [==============================] - 0s 1ms/step - loss: 1.9419\n16/16 [==============================] - 0s 663us/step - loss: 1.9656\n16/16 [==============================] - 0s 1ms/step - loss: 1.9663\n16/16 [==============================] - 0s 1ms/step - loss: 1.9579\n16/16 [==============================] - 0s 629us/step - loss: 1.9450\n16/16 [==============================] - 0s 638us/step - loss: 1.9393\n16/16 [==============================] - 0s 1ms/step - loss: 1.9365\n\nTesting for epoch 25 index 3:\n16/16 [==============================] - 0s 652us/step - loss: 0.2531\n16/16 [==============================] - 0s 591us/step - loss: 1.4810\n16/16 [==============================] - 0s 613us/step - loss: 1.8268\n16/16 [==============================] - 0s 1ms/step - loss: 1.9468\n16/16 [==============================] - 0s 992us/step - loss: 1.9711\n16/16 [==============================] - 0s 1ms/step - loss: 1.9715\n16/16 [==============================] - 0s 595us/step - loss: 1.9633\n16/16 [==============================] - 0s 1ms/step - loss: 1.9507\n16/16 [==============================] - 0s 638us/step - loss: 1.9451\n16/16 [==============================] - 0s 1ms/step - loss: 1.9424\n\nTesting for epoch 25 index 4:\n16/16 [==============================] - 0s 642us/step - loss: 0.2477\n16/16 [==============================] - 0s 608us/step - loss: 1.4965\n16/16 [==============================] - 0s 1ms/step - loss: 1.8427\n16/16 [==============================] - 0s 1ms/step - loss: 1.9619\n16/16 [==============================] - 0s 707us/step - loss: 1.9847\n16/16 [==============================] - 0s 1ms/step - loss: 1.9844\n16/16 [==============================] - 0s 1ms/step - loss: 1.9761\n16/16 [==============================] - 0s 627us/step - loss: 1.9633\n16/16 [==============================] - 0s 604us/step - loss: 1.9575\n16/16 [==============================] - 0s 1ms/step - loss: 1.9548\n\nTesting for epoch 25 index 5:\n16/16 [==============================] - 0s 666us/step - loss: 0.2442\n16/16 [==============================] - 0s 1ms/step - loss: 1.5123\n16/16 [==============================] - 0s 663us/step - loss: 1.8671\n16/16 [==============================] - 0s 868us/step - loss: 1.9886\n16/16 [==============================] - 0s 1ms/step - loss: 2.0106\n16/16 [==============================] - 0s 1ms/step - loss: 2.0090\n16/16 [==============================] - 0s 1ms/step - loss: 1.9998\n16/16 [==============================] - 0s 1ms/step - loss: 1.9861\n16/16 [==============================] - 0s 1ms/step - loss: 1.9801\n16/16 [==============================] - 0s 897us/step - loss: 1.9772\nEpoch 26 of 60\n\nTesting for epoch 26 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2455\n16/16 [==============================] - 0s 641us/step - loss: 1.4842\n16/16 [==============================] - 0s 1ms/step - loss: 1.8252\n16/16 [==============================] - 0s 604us/step - loss: 1.9429\n16/16 [==============================] - 0s 853us/step - loss: 1.9640\n16/16 [==============================] - 0s 1ms/step - loss: 1.9625\n16/16 [==============================] - 0s 616us/step - loss: 1.9538\n16/16 [==============================] - 0s 861us/step - loss: 1.9412\n16/16 [==============================] - 0s 1ms/step - loss: 1.9356\n16/16 [==============================] - 0s 1ms/step - loss: 1.9330\n\nTesting for epoch 26 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2433\n16/16 [==============================] - 0s 1ms/step - loss: 1.4858\n16/16 [==============================] - 0s 837us/step - loss: 1.8253\n16/16 [==============================] - 0s 1ms/step - loss: 1.9440\n16/16 [==============================] - 0s 1ms/step - loss: 1.9633\n16/16 [==============================] - 0s 1ms/step - loss: 1.9608\n16/16 [==============================] - 0s 1ms/step - loss: 1.9512\n16/16 [==============================] - 0s 595us/step - loss: 1.9375\n16/16 [==============================] - 0s 623us/step - loss: 1.9316\n16/16 [==============================] - 0s 632us/step - loss: 1.9289\n\nTesting for epoch 26 index 3:\n16/16 [==============================] - 0s 947us/step - loss: 0.2431\n16/16 [==============================] - 0s 727us/step - loss: 1.5060\n16/16 [==============================] - 0s 614us/step - loss: 1.8472\n16/16 [==============================] - 0s 619us/step - loss: 1.9665\n16/16 [==============================] - 0s 1ms/step - loss: 1.9845\n16/16 [==============================] - 0s 1ms/step - loss: 1.9803\n16/16 [==============================] - 0s 1ms/step - loss: 1.9697\n16/16 [==============================] - 0s 640us/step - loss: 1.9554\n16/16 [==============================] - 0s 1ms/step - loss: 1.9494\n16/16 [==============================] - 0s 1ms/step - loss: 1.9467\n\nTesting for epoch 26 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2399\n16/16 [==============================] - 0s 1ms/step - loss: 1.4785\n16/16 [==============================] - 0s 1ms/step - loss: 1.8107\n16/16 [==============================] - 0s 571us/step - loss: 1.9298\n16/16 [==============================] - 0s 1ms/step - loss: 1.9481\n16/16 [==============================] - 0s 1ms/step - loss: 1.9445\n16/16 [==============================] - 0s 625us/step - loss: 1.9342\n16/16 [==============================] - 0s 631us/step - loss: 1.9201\n16/16 [==============================] - 0s 646us/step - loss: 1.9141\n16/16 [==============================] - 0s 626us/step - loss: 1.9114\n\nTesting for epoch 26 index 5:\n16/16 [==============================] - 0s 880us/step - loss: 0.2394\n16/16 [==============================] - 0s 1ms/step - loss: 1.5075\n16/16 [==============================] - 0s 632us/step - loss: 1.8517\n16/16 [==============================] - 0s 687us/step - loss: 1.9741\n16/16 [==============================] - 0s 644us/step - loss: 1.9911\n16/16 [==============================] - 0s 671us/step - loss: 1.9874\n16/16 [==============================] - 0s 621us/step - loss: 1.9765\n16/16 [==============================] - 0s 604us/step - loss: 1.9621\n16/16 [==============================] - 0s 610us/step - loss: 1.9561\n16/16 [==============================] - 0s 1ms/step - loss: 1.9533\nEpoch 27 of 60\n\nTesting for epoch 27 index 1:\n16/16 [==============================] - 0s 626us/step - loss: 0.2399\n16/16 [==============================] - 0s 1ms/step - loss: 1.5196\n16/16 [==============================] - 0s 1ms/step - loss: 1.8658\n16/16 [==============================] - 0s 1ms/step - loss: 1.9878\n16/16 [==============================] - 0s 937us/step - loss: 2.0043\n16/16 [==============================] - 0s 1ms/step - loss: 2.0001\n16/16 [==============================] - 0s 755us/step - loss: 1.9881\n16/16 [==============================] - 0s 607us/step - loss: 1.9734\n16/16 [==============================] - 0s 1ms/step - loss: 1.9673\n16/16 [==============================] - 0s 675us/step - loss: 1.9644\n\nTesting for epoch 27 index 2:\n16/16 [==============================] - 0s 584us/step - loss: 0.2383\n16/16 [==============================] - 0s 1ms/step - loss: 1.5491\n16/16 [==============================] - 0s 1ms/step - loss: 1.9024\n16/16 [==============================] - 0s 587us/step - loss: 2.0252\n16/16 [==============================] - 0s 793us/step - loss: 2.0399\n16/16 [==============================] - 0s 1ms/step - loss: 2.0364\n16/16 [==============================] - 0s 589us/step - loss: 2.0246\n16/16 [==============================] - 0s 625us/step - loss: 2.0102\n16/16 [==============================] - 0s 604us/step - loss: 2.0042\n16/16 [==============================] - 0s 600us/step - loss: 2.0014\n\nTesting for epoch 27 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2373\n16/16 [==============================] - 0s 1ms/step - loss: 1.5548\n16/16 [==============================] - 0s 1ms/step - loss: 1.9083\n16/16 [==============================] - 0s 1ms/step - loss: 2.0331\n16/16 [==============================] - 0s 594us/step - loss: 2.0485\n16/16 [==============================] - 0s 609us/step - loss: 2.0460\n16/16 [==============================] - 0s 917us/step - loss: 2.0331\n16/16 [==============================] - 0s 1ms/step - loss: 2.0181\n16/16 [==============================] - 0s 1ms/step - loss: 2.0119\n16/16 [==============================] - 0s 1ms/step - loss: 2.0090\n\nTesting for epoch 27 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2391\n16/16 [==============================] - 0s 713us/step - loss: 1.5529\n16/16 [==============================] - 0s 1ms/step - loss: 1.9056\n16/16 [==============================] - 0s 666us/step - loss: 2.0294\n16/16 [==============================] - 0s 1ms/step - loss: 2.0429\n16/16 [==============================] - 0s 1ms/step - loss: 2.0388\n16/16 [==============================] - 0s 1ms/step - loss: 2.0248\n16/16 [==============================] - 0s 1ms/step - loss: 2.0092\n16/16 [==============================] - 0s 1ms/step - loss: 2.0028\n16/16 [==============================] - 0s 1ms/step - loss: 1.9998\n\nTesting for epoch 27 index 5:\n16/16 [==============================] - 0s 643us/step - loss: 0.2391\n16/16 [==============================] - 0s 1ms/step - loss: 1.5363\n16/16 [==============================] - 0s 896us/step - loss: 1.8850\n16/16 [==============================] - 0s 1ms/step - loss: 2.0052\n16/16 [==============================] - 0s 610us/step - loss: 2.0195\n16/16 [==============================] - 0s 1ms/step - loss: 2.0152\n16/16 [==============================] - 0s 1ms/step - loss: 2.0012\n16/16 [==============================] - 0s 1ms/step - loss: 1.9857\n16/16 [==============================] - 0s 619us/step - loss: 1.9794\n16/16 [==============================] - 0s 1ms/step - loss: 1.9765\nEpoch 28 of 60\n\nTesting for epoch 28 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2338\n16/16 [==============================] - 0s 1ms/step - loss: 1.5579\n16/16 [==============================] - 0s 624us/step - loss: 1.9117\n16/16 [==============================] - 0s 1ms/step - loss: 2.0320\n16/16 [==============================] - 0s 612us/step - loss: 2.0450\n16/16 [==============================] - 0s 645us/step - loss: 2.0389\n16/16 [==============================] - 0s 721us/step - loss: 2.0244\n16/16 [==============================] - 0s 696us/step - loss: 2.0088\n16/16 [==============================] - 0s 608us/step - loss: 2.0024\n16/16 [==============================] - 0s 826us/step - loss: 1.9995\n\nTesting for epoch 28 index 2:\n16/16 [==============================] - 0s 748us/step - loss: 0.2338\n16/16 [==============================] - 0s 1ms/step - loss: 1.5385\n16/16 [==============================] - 0s 1ms/step - loss: 1.8839\n16/16 [==============================] - 0s 670us/step - loss: 2.0016\n16/16 [==============================] - 0s 626us/step - loss: 2.0138\n16/16 [==============================] - 0s 625us/step - loss: 2.0076\n16/16 [==============================] - 0s 1ms/step - loss: 1.9939\n16/16 [==============================] - 0s 579us/step - loss: 1.9788\n16/16 [==============================] - 0s 898us/step - loss: 1.9727\n16/16 [==============================] - 0s 1ms/step - loss: 1.9699\n\nTesting for epoch 28 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2316\n16/16 [==============================] - 0s 590us/step - loss: 1.5536\n16/16 [==============================] - 0s 627us/step - loss: 1.9020\n16/16 [==============================] - 0s 1ms/step - loss: 2.0198\n16/16 [==============================] - 0s 1ms/step - loss: 2.0311\n16/16 [==============================] - 0s 1ms/step - loss: 2.0229\n16/16 [==============================] - 0s 646us/step - loss: 2.0074\n16/16 [==============================] - 0s 606us/step - loss: 1.9907\n16/16 [==============================] - 0s 591us/step - loss: 1.9841\n16/16 [==============================] - 0s 1ms/step - loss: 1.9812\n\nTesting for epoch 28 index 4:\n16/16 [==============================] - 0s 618us/step - loss: 0.2325\n16/16 [==============================] - 0s 1ms/step - loss: 1.5661\n16/16 [==============================] - 0s 654us/step - loss: 1.9179\n16/16 [==============================] - 0s 1ms/step - loss: 2.0361\n16/16 [==============================] - 0s 1ms/step - loss: 2.0486\n16/16 [==============================] - 0s 589us/step - loss: 2.0418\n16/16 [==============================] - 0s 676us/step - loss: 2.0272\n16/16 [==============================] - 0s 686us/step - loss: 2.0113\n16/16 [==============================] - 0s 664us/step - loss: 2.0049\n16/16 [==============================] - 0s 1ms/step - loss: 2.0020\n\nTesting for epoch 28 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2280\n16/16 [==============================] - 0s 1ms/step - loss: 1.5553\n16/16 [==============================] - 0s 646us/step - loss: 1.9013\n16/16 [==============================] - 0s 1ms/step - loss: 2.0131\n16/16 [==============================] - 0s 929us/step - loss: 2.0218\n16/16 [==============================] - 0s 838us/step - loss: 2.0122\n16/16 [==============================] - 0s 882us/step - loss: 1.9958\n16/16 [==============================] - 0s 602us/step - loss: 1.9791\n16/16 [==============================] - 0s 608us/step - loss: 1.9725\n16/16 [==============================] - 0s 657us/step - loss: 1.9696\nEpoch 29 of 60\n\nTesting for epoch 29 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2275\n16/16 [==============================] - 0s 767us/step - loss: 1.6042\n16/16 [==============================] - 0s 629us/step - loss: 1.9618\n16/16 [==============================] - 0s 1ms/step - loss: 2.0777\n16/16 [==============================] - 0s 1ms/step - loss: 2.0859\n16/16 [==============================] - 0s 645us/step - loss: 2.0750\n16/16 [==============================] - 0s 646us/step - loss: 2.0571\n16/16 [==============================] - 0s 900us/step - loss: 2.0390\n16/16 [==============================] - 0s 1ms/step - loss: 2.0319\n16/16 [==============================] - 0s 1ms/step - loss: 2.0288\n\nTesting for epoch 29 index 2:\n16/16 [==============================] - 0s 600us/step - loss: 0.2278\n16/16 [==============================] - 0s 625us/step - loss: 1.6003\n16/16 [==============================] - 0s 627us/step - loss: 1.9580\n16/16 [==============================] - 0s 599us/step - loss: 2.0748\n16/16 [==============================] - 0s 608us/step - loss: 2.0844\n16/16 [==============================] - 0s 1ms/step - loss: 2.0744\n16/16 [==============================] - 0s 911us/step - loss: 2.0568\n16/16 [==============================] - 0s 1ms/step - loss: 2.0391\n16/16 [==============================] - 0s 1ms/step - loss: 2.0323\n16/16 [==============================] - 0s 632us/step - loss: 2.0293\n\nTesting for epoch 29 index 3:\n16/16 [==============================] - 0s 606us/step - loss: 0.2275\n16/16 [==============================] - 0s 640us/step - loss: 1.5908\n16/16 [==============================] - 0s 787us/step - loss: 1.9416\n16/16 [==============================] - 0s 614us/step - loss: 2.0550\n16/16 [==============================] - 0s 655us/step - loss: 2.0633\n16/16 [==============================] - 0s 1ms/step - loss: 2.0544\n16/16 [==============================] - 0s 624us/step - loss: 2.0373\n16/16 [==============================] - 0s 590us/step - loss: 2.0202\n16/16 [==============================] - 0s 665us/step - loss: 2.0135\n16/16 [==============================] - 0s 1ms/step - loss: 2.0105\n\nTesting for epoch 29 index 4:\n16/16 [==============================] - 0s 637us/step - loss: 0.2262\n16/16 [==============================] - 0s 1ms/step - loss: 1.6456\n16/16 [==============================] - 0s 861us/step - loss: 2.0122\n16/16 [==============================] - 0s 1ms/step - loss: 2.1306\n16/16 [==============================] - 0s 1ms/step - loss: 2.1391\n16/16 [==============================] - 0s 1ms/step - loss: 2.1297\n16/16 [==============================] - 0s 621us/step - loss: 2.1118\n16/16 [==============================] - 0s 608us/step - loss: 2.0941\n16/16 [==============================] - 0s 1ms/step - loss: 2.0872\n16/16 [==============================] - 0s 652us/step - loss: 2.0842\n\nTesting for epoch 29 index 5:\n16/16 [==============================] - 0s 876us/step - loss: 0.2207\n16/16 [==============================] - 0s 656us/step - loss: 1.5952\n16/16 [==============================] - 0s 647us/step - loss: 1.9409\n16/16 [==============================] - 0s 611us/step - loss: 2.0496\n16/16 [==============================] - 0s 616us/step - loss: 2.0545\n16/16 [==============================] - 0s 1ms/step - loss: 2.0428\n16/16 [==============================] - 0s 599us/step - loss: 2.0236\n16/16 [==============================] - 0s 625us/step - loss: 2.0050\n16/16 [==============================] - 0s 897us/step - loss: 1.9980\n16/16 [==============================] - 0s 1ms/step - loss: 1.9949\nEpoch 30 of 60\n\nTesting for epoch 30 index 1:\n16/16 [==============================] - 0s 595us/step - loss: 0.2217\n16/16 [==============================] - 0s 1ms/step - loss: 1.6089\n16/16 [==============================] - 0s 1ms/step - loss: 1.9546\n16/16 [==============================] - 0s 1ms/step - loss: 2.0641\n16/16 [==============================] - 0s 638us/step - loss: 2.0694\n16/16 [==============================] - 0s 636us/step - loss: 2.0580\n16/16 [==============================] - 0s 1ms/step - loss: 2.0394\n16/16 [==============================] - 0s 1ms/step - loss: 2.0217\n16/16 [==============================] - 0s 673us/step - loss: 2.0150\n16/16 [==============================] - 0s 1ms/step - loss: 2.0120\n\nTesting for epoch 30 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2210\n16/16 [==============================] - 0s 828us/step - loss: 1.6202\n16/16 [==============================] - 0s 1ms/step - loss: 1.9660\n16/16 [==============================] - 0s 1ms/step - loss: 2.0750\n16/16 [==============================] - 0s 1ms/step - loss: 2.0805\n16/16 [==============================] - 0s 655us/step - loss: 2.0695\n16/16 [==============================] - 0s 1ms/step - loss: 2.0510\n16/16 [==============================] - 0s 1ms/step - loss: 2.0329\n16/16 [==============================] - 0s 681us/step - loss: 2.0260\n16/16 [==============================] - 0s 660us/step - loss: 2.0231\n\nTesting for epoch 30 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2210\n16/16 [==============================] - 0s 1ms/step - loss: 1.6647\n16/16 [==============================] - 0s 1ms/step - loss: 2.0279\n16/16 [==============================] - 0s 1ms/step - loss: 2.1422\n16/16 [==============================] - 0s 638us/step - loss: 2.1469\n16/16 [==============================] - 0s 1ms/step - loss: 2.1338\n16/16 [==============================] - 0s 787us/step - loss: 2.1132\n16/16 [==============================] - 0s 1ms/step - loss: 2.0938\n16/16 [==============================] - 0s 1ms/step - loss: 2.0865\n16/16 [==============================] - 0s 1ms/step - loss: 2.0833\n\nTesting for epoch 30 index 4:\n16/16 [==============================] - 0s 636us/step - loss: 0.2203\n16/16 [==============================] - 0s 640us/step - loss: 1.6660\n16/16 [==============================] - 0s 629us/step - loss: 2.0216\n16/16 [==============================] - 0s 1ms/step - loss: 2.1328\n16/16 [==============================] - 0s 607us/step - loss: 2.1365\n16/16 [==============================] - 0s 1ms/step - loss: 2.1239\n16/16 [==============================] - 0s 1ms/step - loss: 2.1042\n16/16 [==============================] - 0s 1ms/step - loss: 2.0851\n16/16 [==============================] - 0s 637us/step - loss: 2.0778\n16/16 [==============================] - 0s 1ms/step - loss: 2.0747\n\nTesting for epoch 30 index 5:\n16/16 [==============================] - 0s 975us/step - loss: 0.2161\n16/16 [==============================] - 0s 1ms/step - loss: 1.6263\n16/16 [==============================] - 0s 1ms/step - loss: 1.9708\n16/16 [==============================] - 0s 601us/step - loss: 2.0786\n16/16 [==============================] - 0s 806us/step - loss: 2.0799\n16/16 [==============================] - 0s 787us/step - loss: 2.0658\n16/16 [==============================] - 0s 870us/step - loss: 2.0449\n16/16 [==============================] - 0s 1ms/step - loss: 2.0250\n16/16 [==============================] - 0s 1ms/step - loss: 2.0177\n16/16 [==============================] - 0s 826us/step - loss: 2.0146\nEpoch 31 of 60\n\nTesting for epoch 31 index 1:\n16/16 [==============================] - 0s 611us/step - loss: 0.2170\n16/16 [==============================] - 0s 1ms/step - loss: 1.6675\n16/16 [==============================] - 0s 814us/step - loss: 2.0171\n16/16 [==============================] - 0s 610us/step - loss: 2.1244\n16/16 [==============================] - 0s 619us/step - loss: 2.1233\n16/16 [==============================] - 0s 1ms/step - loss: 2.1079\n16/16 [==============================] - 0s 856us/step - loss: 2.0871\n16/16 [==============================] - 0s 1ms/step - loss: 2.0677\n16/16 [==============================] - 0s 632us/step - loss: 2.0605\n16/16 [==============================] - 0s 602us/step - loss: 2.0574\n\nTesting for epoch 31 index 2:\n16/16 [==============================] - 0s 636us/step - loss: 0.2144\n16/16 [==============================] - 0s 709us/step - loss: 1.6430\n16/16 [==============================] - 0s 667us/step - loss: 1.9817\n16/16 [==============================] - 0s 682us/step - loss: 2.0881\n16/16 [==============================] - 0s 1ms/step - loss: 2.0878\n16/16 [==============================] - 0s 1ms/step - loss: 2.0724\n16/16 [==============================] - 0s 629us/step - loss: 2.0513\n16/16 [==============================] - 0s 1ms/step - loss: 2.0314\n16/16 [==============================] - 0s 884us/step - loss: 2.0241\n16/16 [==============================] - 0s 585us/step - loss: 2.0210\n\nTesting for epoch 31 index 3:\n16/16 [==============================] - 0s 726us/step - loss: 0.2157\n16/16 [==============================] - 0s 1ms/step - loss: 1.6566\n16/16 [==============================] - 0s 1ms/step - loss: 2.0048\n16/16 [==============================] - 0s 708us/step - loss: 2.1166\n16/16 [==============================] - 0s 1ms/step - loss: 2.1189\n16/16 [==============================] - 0s 1ms/step - loss: 2.1054\n16/16 [==============================] - 0s 684us/step - loss: 2.0858\n16/16 [==============================] - 0s 922us/step - loss: 2.0670\n16/16 [==============================] - 0s 1ms/step - loss: 2.0600\n16/16 [==============================] - 0s 602us/step - loss: 2.0569\n\nTesting for epoch 31 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2162\n16/16 [==============================] - 0s 1ms/step - loss: 1.6785\n16/16 [==============================] - 0s 1ms/step - loss: 2.0249\n16/16 [==============================] - 0s 1ms/step - loss: 2.1348\n16/16 [==============================] - 0s 630us/step - loss: 2.1339\n16/16 [==============================] - 0s 1ms/step - loss: 2.1192\n16/16 [==============================] - 0s 604us/step - loss: 2.0985\n16/16 [==============================] - 0s 660us/step - loss: 2.0787\n16/16 [==============================] - 0s 741us/step - loss: 2.0714\n16/16 [==============================] - 0s 1ms/step - loss: 2.0683\n\nTesting for epoch 31 index 5:\n16/16 [==============================] - 0s 636us/step - loss: 0.2121\n16/16 [==============================] - 0s 1ms/step - loss: 1.7297\n16/16 [==============================] - 0s 1ms/step - loss: 2.0914\n16/16 [==============================] - 0s 1ms/step - loss: 2.2077\n16/16 [==============================] - 0s 615us/step - loss: 2.2066\n16/16 [==============================] - 0s 615us/step - loss: 2.1895\n16/16 [==============================] - 0s 1ms/step - loss: 2.1665\n16/16 [==============================] - 0s 659us/step - loss: 2.1450\n16/16 [==============================] - 0s 1ms/step - loss: 2.1372\n16/16 [==============================] - 0s 911us/step - loss: 2.1339\nEpoch 32 of 60\n\nTesting for epoch 32 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2120\n16/16 [==============================] - 0s 787us/step - loss: 1.7164\n16/16 [==============================] - 0s 644us/step - loss: 2.0674\n16/16 [==============================] - 0s 629us/step - loss: 2.1800\n16/16 [==============================] - 0s 1ms/step - loss: 2.1770\n16/16 [==============================] - 0s 652us/step - loss: 2.1600\n16/16 [==============================] - 0s 620us/step - loss: 2.1373\n16/16 [==============================] - 0s 784us/step - loss: 2.1164\n16/16 [==============================] - 0s 644us/step - loss: 2.1088\n16/16 [==============================] - 0s 606us/step - loss: 2.1055\n\nTesting for epoch 32 index 2:\n16/16 [==============================] - 0s 685us/step - loss: 0.2108\n16/16 [==============================] - 0s 789us/step - loss: 1.7226\n16/16 [==============================] - 0s 653us/step - loss: 2.0726\n16/16 [==============================] - 0s 1ms/step - loss: 2.1850\n16/16 [==============================] - 0s 737us/step - loss: 2.1834\n16/16 [==============================] - 0s 659us/step - loss: 2.1657\n16/16 [==============================] - 0s 633us/step - loss: 2.1428\n16/16 [==============================] - 0s 851us/step - loss: 2.1217\n16/16 [==============================] - 0s 636us/step - loss: 2.1141\n16/16 [==============================] - 0s 599us/step - loss: 2.1108\n\nTesting for epoch 32 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2095\n16/16 [==============================] - 0s 894us/step - loss: 1.7267\n16/16 [==============================] - 0s 693us/step - loss: 2.0755\n16/16 [==============================] - 0s 1ms/step - loss: 2.1840\n16/16 [==============================] - 0s 1ms/step - loss: 2.1795\n16/16 [==============================] - 0s 932us/step - loss: 2.1586\n16/16 [==============================] - 0s 617us/step - loss: 2.1342\n16/16 [==============================] - 0s 1ms/step - loss: 2.1125\n16/16 [==============================] - 0s 672us/step - loss: 2.1046\n16/16 [==============================] - 0s 1ms/step - loss: 2.1013\n\nTesting for epoch 32 index 4:\n16/16 [==============================] - 0s 629us/step - loss: 0.2097\n16/16 [==============================] - 0s 656us/step - loss: 1.7201\n16/16 [==============================] - 0s 725us/step - loss: 2.0713\n16/16 [==============================] - 0s 610us/step - loss: 2.1837\n16/16 [==============================] - 0s 857us/step - loss: 2.1826\n16/16 [==============================] - 0s 1ms/step - loss: 2.1660\n16/16 [==============================] - 0s 619us/step - loss: 2.1441\n16/16 [==============================] - 0s 1ms/step - loss: 2.1235\n16/16 [==============================] - 0s 759us/step - loss: 2.1159\n16/16 [==============================] - 0s 1ms/step - loss: 2.1127\n\nTesting for epoch 32 index 5:\n16/16 [==============================] - 0s 617us/step - loss: 0.2078\n16/16 [==============================] - 0s 1ms/step - loss: 1.7352\n16/16 [==============================] - 0s 633us/step - loss: 2.0834\n16/16 [==============================] - 0s 617us/step - loss: 2.1909\n16/16 [==============================] - 0s 661us/step - loss: 2.1858\n16/16 [==============================] - 0s 635us/step - loss: 2.1659\n16/16 [==============================] - 0s 675us/step - loss: 2.1419\n16/16 [==============================] - 0s 1ms/step - loss: 2.1201\n16/16 [==============================] - 0s 638us/step - loss: 2.1122\n16/16 [==============================] - 0s 924us/step - loss: 2.1089\nEpoch 33 of 60\n\nTesting for epoch 33 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2075\n16/16 [==============================] - 0s 1ms/step - loss: 1.7201\n16/16 [==============================] - 0s 808us/step - loss: 2.0682\n16/16 [==============================] - 0s 1ms/step - loss: 2.1789\n16/16 [==============================] - 0s 1ms/step - loss: 2.1772\n16/16 [==============================] - 0s 832us/step - loss: 2.1596\n16/16 [==============================] - 0s 1ms/step - loss: 2.1366\n16/16 [==============================] - 0s 631us/step - loss: 2.1155\n16/16 [==============================] - 0s 1ms/step - loss: 2.1079\n16/16 [==============================] - 0s 717us/step - loss: 2.1048\n\nTesting for epoch 33 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2033\n16/16 [==============================] - 0s 880us/step - loss: 1.7117\n16/16 [==============================] - 0s 689us/step - loss: 2.0529\n16/16 [==============================] - 0s 646us/step - loss: 2.1588\n16/16 [==============================] - 0s 629us/step - loss: 2.1535\n16/16 [==============================] - 0s 977us/step - loss: 2.1332\n16/16 [==============================] - 0s 564us/step - loss: 2.1092\n16/16 [==============================] - 0s 647us/step - loss: 2.0876\n16/16 [==============================] - 0s 1ms/step - loss: 2.0798\n16/16 [==============================] - 0s 591us/step - loss: 2.0766\n\nTesting for epoch 33 index 3:\n16/16 [==============================] - 0s 990us/step - loss: 0.2022\n16/16 [==============================] - 0s 1ms/step - loss: 1.7204\n16/16 [==============================] - 0s 612us/step - loss: 2.0604\n16/16 [==============================] - 0s 631us/step - loss: 2.1673\n16/16 [==============================] - 0s 1ms/step - loss: 2.1620\n16/16 [==============================] - 0s 629us/step - loss: 2.1416\n16/16 [==============================] - 0s 929us/step - loss: 2.1173\n16/16 [==============================] - 0s 1ms/step - loss: 2.0956\n16/16 [==============================] - 0s 1ms/step - loss: 2.0879\n16/16 [==============================] - 0s 617us/step - loss: 2.0846\n\nTesting for epoch 33 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2030\n16/16 [==============================] - 0s 1ms/step - loss: 1.7209\n16/16 [==============================] - 0s 605us/step - loss: 2.0625\n16/16 [==============================] - 0s 1ms/step - loss: 2.1710\n16/16 [==============================] - 0s 700us/step - loss: 2.1667\n16/16 [==============================] - 0s 1ms/step - loss: 2.1474\n16/16 [==============================] - 0s 902us/step - loss: 2.1242\n16/16 [==============================] - 0s 667us/step - loss: 2.1033\n16/16 [==============================] - 0s 1ms/step - loss: 2.0957\n16/16 [==============================] - 0s 1ms/step - loss: 2.0925\n\nTesting for epoch 33 index 5:\n16/16 [==============================] - 0s 621us/step - loss: 0.2029\n16/16 [==============================] - 0s 610us/step - loss: 1.7547\n16/16 [==============================] - 0s 611us/step - loss: 2.1118\n16/16 [==============================] - 0s 597us/step - loss: 2.2254\n16/16 [==============================] - 0s 576us/step - loss: 2.2233\n16/16 [==============================] - 0s 580us/step - loss: 2.2050\n16/16 [==============================] - 0s 612us/step - loss: 2.1821\n16/16 [==============================] - 0s 605us/step - loss: 2.1610\n16/16 [==============================] - 0s 807us/step - loss: 2.1534\n16/16 [==============================] - 0s 1ms/step - loss: 2.1502\nEpoch 34 of 60\n\nTesting for epoch 34 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1969\n16/16 [==============================] - 0s 955us/step - loss: 1.7401\n16/16 [==============================] - 0s 642us/step - loss: 2.0891\n16/16 [==============================] - 0s 691us/step - loss: 2.1926\n16/16 [==============================] - 0s 1ms/step - loss: 2.1827\n16/16 [==============================] - 0s 706us/step - loss: 2.1592\n16/16 [==============================] - 0s 1ms/step - loss: 2.1331\n16/16 [==============================] - 0s 835us/step - loss: 2.1100\n16/16 [==============================] - 0s 1ms/step - loss: 2.1018\n16/16 [==============================] - 0s 1ms/step - loss: 2.0983\n\nTesting for epoch 34 index 2:\n16/16 [==============================] - 0s 659us/step - loss: 0.1983\n16/16 [==============================] - 0s 664us/step - loss: 1.7511\n16/16 [==============================] - 0s 1ms/step - loss: 2.1021\n16/16 [==============================] - 0s 632us/step - loss: 2.2065\n16/16 [==============================] - 0s 1ms/step - loss: 2.1983\n16/16 [==============================] - 0s 1ms/step - loss: 2.1764\n16/16 [==============================] - 0s 1ms/step - loss: 2.1510\n16/16 [==============================] - 0s 639us/step - loss: 2.1285\n16/16 [==============================] - 0s 641us/step - loss: 2.1205\n16/16 [==============================] - 0s 1ms/step - loss: 2.1171\n\nTesting for epoch 34 index 3:\n16/16 [==============================] - 0s 774us/step - loss: 0.1992\n16/16 [==============================] - 0s 643us/step - loss: 1.7271\n16/16 [==============================] - 0s 1ms/step - loss: 2.0661\n16/16 [==============================] - 0s 1ms/step - loss: 2.1656\n16/16 [==============================] - 0s 1ms/step - loss: 2.1575\n16/16 [==============================] - 0s 1ms/step - loss: 2.1343\n16/16 [==============================] - 0s 620us/step - loss: 2.1083\n16/16 [==============================] - 0s 675us/step - loss: 2.0856\n16/16 [==============================] - 0s 1ms/step - loss: 2.0776\n16/16 [==============================] - 0s 641us/step - loss: 2.0743\n\nTesting for epoch 34 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1973\n16/16 [==============================] - 0s 634us/step - loss: 1.7673\n16/16 [==============================] - 0s 641us/step - loss: 2.1133\n16/16 [==============================] - 0s 807us/step - loss: 2.2143\n16/16 [==============================] - 0s 1ms/step - loss: 2.2060\n16/16 [==============================] - 0s 680us/step - loss: 2.1823\n16/16 [==============================] - 0s 915us/step - loss: 2.1568\n16/16 [==============================] - 0s 1ms/step - loss: 2.1343\n16/16 [==============================] - 0s 637us/step - loss: 2.1262\n16/16 [==============================] - 0s 1ms/step - loss: 2.1229\n\nTesting for epoch 34 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1957\n16/16 [==============================] - 0s 1ms/step - loss: 1.7630\n16/16 [==============================] - 0s 1ms/step - loss: 2.1149\n16/16 [==============================] - 0s 1ms/step - loss: 2.2172\n16/16 [==============================] - 0s 1ms/step - loss: 2.2101\n16/16 [==============================] - 0s 785us/step - loss: 2.1867\n16/16 [==============================] - 0s 767us/step - loss: 2.1611\n16/16 [==============================] - 0s 1ms/step - loss: 2.1384\n16/16 [==============================] - 0s 1ms/step - loss: 2.1303\n16/16 [==============================] - 0s 835us/step - loss: 2.1270\nEpoch 35 of 60\n\nTesting for epoch 35 index 1:\n16/16 [==============================] - 0s 659us/step - loss: 0.1958\n16/16 [==============================] - 0s 636us/step - loss: 1.7773\n16/16 [==============================] - 0s 580us/step - loss: 2.1316\n16/16 [==============================] - 0s 632us/step - loss: 2.2359\n16/16 [==============================] - 0s 699us/step - loss: 2.2285\n16/16 [==============================] - 0s 642us/step - loss: 2.2029\n16/16 [==============================] - 0s 953us/step - loss: 2.1759\n16/16 [==============================] - 0s 1ms/step - loss: 2.1522\n16/16 [==============================] - 0s 1ms/step - loss: 2.1437\n16/16 [==============================] - 0s 843us/step - loss: 2.1402\n\nTesting for epoch 35 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1914\n16/16 [==============================] - 0s 908us/step - loss: 1.7573\n16/16 [==============================] - 0s 718us/step - loss: 2.1000\n16/16 [==============================] - 0s 1ms/step - loss: 2.1991\n16/16 [==============================] - 0s 626us/step - loss: 2.1890\n16/16 [==============================] - 0s 746us/step - loss: 2.1621\n16/16 [==============================] - 0s 649us/step - loss: 2.1343\n16/16 [==============================] - 0s 653us/step - loss: 2.1108\n16/16 [==============================] - 0s 1ms/step - loss: 2.1026\n16/16 [==============================] - 0s 1ms/step - loss: 2.0992\n\nTesting for epoch 35 index 3:\n16/16 [==============================] - 0s 615us/step - loss: 0.1948\n16/16 [==============================] - 0s 637us/step - loss: 1.8010\n16/16 [==============================] - 0s 1ms/step - loss: 2.1603\n16/16 [==============================] - 0s 628us/step - loss: 2.2660\n16/16 [==============================] - 0s 1ms/step - loss: 2.2587\n16/16 [==============================] - 0s 706us/step - loss: 2.2337\n16/16 [==============================] - 0s 637us/step - loss: 2.2068\n16/16 [==============================] - 0s 631us/step - loss: 2.1834\n16/16 [==============================] - 0s 1ms/step - loss: 2.1750\n16/16 [==============================] - 0s 1ms/step - loss: 2.1716\n\nTesting for epoch 35 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1911\n16/16 [==============================] - 0s 641us/step - loss: 1.7696\n16/16 [==============================] - 0s 841us/step - loss: 2.1178\n16/16 [==============================] - 0s 778us/step - loss: 2.2186\n16/16 [==============================] - 0s 1ms/step - loss: 2.2089\n16/16 [==============================] - 0s 1ms/step - loss: 2.1826\n16/16 [==============================] - 0s 1ms/step - loss: 2.1555\n16/16 [==============================] - 0s 1ms/step - loss: 2.1326\n16/16 [==============================] - 0s 1ms/step - loss: 2.1245\n16/16 [==============================] - 0s 1ms/step - loss: 2.1212\n\nTesting for epoch 35 index 5:\n16/16 [==============================] - 0s 728us/step - loss: 0.1921\n16/16 [==============================] - 0s 635us/step - loss: 1.8098\n16/16 [==============================] - 0s 1ms/step - loss: 2.1669\n16/16 [==============================] - 0s 661us/step - loss: 2.2703\n16/16 [==============================] - 0s 1ms/step - loss: 2.2623\n16/16 [==============================] - 0s 664us/step - loss: 2.2363\n16/16 [==============================] - 0s 650us/step - loss: 2.2087\n16/16 [==============================] - 0s 648us/step - loss: 2.1849\n16/16 [==============================] - 0s 642us/step - loss: 2.1766\n16/16 [==============================] - 0s 662us/step - loss: 2.1732\nEpoch 36 of 60\n\nTesting for epoch 36 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1943\n16/16 [==============================] - 0s 873us/step - loss: 1.7990\n16/16 [==============================] - 0s 1ms/step - loss: 2.1590\n16/16 [==============================] - 0s 1ms/step - loss: 2.2620\n16/16 [==============================] - 0s 1ms/step - loss: 2.2528\n16/16 [==============================] - 0s 600us/step - loss: 2.2259\n16/16 [==============================] - 0s 584us/step - loss: 2.1981\n16/16 [==============================] - 0s 1ms/step - loss: 2.1742\n16/16 [==============================] - 0s 1ms/step - loss: 2.1658\n16/16 [==============================] - 0s 1ms/step - loss: 2.1623\n\nTesting for epoch 36 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1929\n16/16 [==============================] - 0s 1ms/step - loss: 1.7656\n16/16 [==============================] - 0s 784us/step - loss: 2.1149\n16/16 [==============================] - 0s 619us/step - loss: 2.2138\n16/16 [==============================] - 0s 587us/step - loss: 2.2056\n16/16 [==============================] - 0s 590us/step - loss: 2.1795\n16/16 [==============================] - 0s 567us/step - loss: 2.1526\n16/16 [==============================] - 0s 616us/step - loss: 2.1299\n16/16 [==============================] - 0s 1ms/step - loss: 2.1219\n16/16 [==============================] - 0s 594us/step - loss: 2.1186\n\nTesting for epoch 36 index 3:\n16/16 [==============================] - 0s 761us/step - loss: 0.1915\n16/16 [==============================] - 0s 1ms/step - loss: 1.7777\n16/16 [==============================] - 0s 1ms/step - loss: 2.1276\n16/16 [==============================] - 0s 1ms/step - loss: 2.2233\n16/16 [==============================] - 0s 1ms/step - loss: 2.2119\n16/16 [==============================] - 0s 1ms/step - loss: 2.1831\n16/16 [==============================] - 0s 1ms/step - loss: 2.1549\n16/16 [==============================] - 0s 1ms/step - loss: 2.1315\n16/16 [==============================] - 0s 1ms/step - loss: 2.1235\n16/16 [==============================] - 0s 1ms/step - loss: 2.1202\n\nTesting for epoch 36 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1890\n16/16 [==============================] - 0s 947us/step - loss: 1.7926\n16/16 [==============================] - 0s 1ms/step - loss: 2.1436\n16/16 [==============================] - 0s 1ms/step - loss: 2.2395\n16/16 [==============================] - 0s 625us/step - loss: 2.2279\n16/16 [==============================] - 0s 1ms/step - loss: 2.1978\n16/16 [==============================] - 0s 1ms/step - loss: 2.1692\n16/16 [==============================] - 0s 1ms/step - loss: 2.1451\n16/16 [==============================] - 0s 1ms/step - loss: 2.1368\n16/16 [==============================] - 0s 1ms/step - loss: 2.1334\n\nTesting for epoch 36 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1867\n16/16 [==============================] - 0s 1ms/step - loss: 1.8739\n16/16 [==============================] - 0s 1ms/step - loss: 2.2491\n16/16 [==============================] - 0s 992us/step - loss: 2.3538\n16/16 [==============================] - 0s 1ms/step - loss: 2.3422\n16/16 [==============================] - 0s 870us/step - loss: 2.3119\n16/16 [==============================] - 0s 1ms/step - loss: 2.2828\n16/16 [==============================] - 0s 627us/step - loss: 2.2576\n16/16 [==============================] - 0s 615us/step - loss: 2.2488\n16/16 [==============================] - 0s 748us/step - loss: 2.2452\nEpoch 37 of 60\n\nTesting for epoch 37 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1852\n16/16 [==============================] - 0s 725us/step - loss: 1.8347\n16/16 [==============================] - 0s 646us/step - loss: 2.1942\n16/16 [==============================] - 0s 856us/step - loss: 2.2953\n16/16 [==============================] - 0s 661us/step - loss: 2.2842\n16/16 [==============================] - 0s 604us/step - loss: 2.2557\n16/16 [==============================] - 0s 602us/step - loss: 2.2280\n16/16 [==============================] - 0s 908us/step - loss: 2.2039\n16/16 [==============================] - 0s 1ms/step - loss: 2.1955\n16/16 [==============================] - 0s 1ms/step - loss: 2.1921\n\nTesting for epoch 37 index 2:\n16/16 [==============================] - 0s 896us/step - loss: 0.1867\n16/16 [==============================] - 0s 668us/step - loss: 1.7861\n16/16 [==============================] - 0s 1ms/step - loss: 2.1329\n16/16 [==============================] - 0s 777us/step - loss: 2.2272\n16/16 [==============================] - 0s 1ms/step - loss: 2.2136\n16/16 [==============================] - 0s 736us/step - loss: 2.1838\n16/16 [==============================] - 0s 606us/step - loss: 2.1556\n16/16 [==============================] - 0s 1ms/step - loss: 2.1315\n16/16 [==============================] - 0s 1ms/step - loss: 2.1232\n16/16 [==============================] - 0s 626us/step - loss: 2.1198\n\nTesting for epoch 37 index 3:\n16/16 [==============================] - 0s 579us/step - loss: 0.1835\n16/16 [==============================] - 0s 1ms/step - loss: 1.8104\n16/16 [==============================] - 0s 1ms/step - loss: 2.1585\n16/16 [==============================] - 0s 634us/step - loss: 2.2523\n16/16 [==============================] - 0s 702us/step - loss: 2.2377\n16/16 [==============================] - 0s 635us/step - loss: 2.2062\n16/16 [==============================] - 0s 604us/step - loss: 2.1763\n16/16 [==============================] - 0s 1ms/step - loss: 2.1511\n16/16 [==============================] - 0s 1ms/step - loss: 2.1427\n16/16 [==============================] - 0s 625us/step - loss: 2.1392\n\nTesting for epoch 37 index 4:\n16/16 [==============================] - 0s 613us/step - loss: 0.1853\n16/16 [==============================] - 0s 700us/step - loss: 1.8215\n16/16 [==============================] - 0s 1ms/step - loss: 2.1727\n16/16 [==============================] - 0s 723us/step - loss: 2.2672\n16/16 [==============================] - 0s 714us/step - loss: 2.2522\n16/16 [==============================] - 0s 621us/step - loss: 2.2214\n16/16 [==============================] - 0s 669us/step - loss: 2.1932\n16/16 [==============================] - 0s 607us/step - loss: 2.1690\n16/16 [==============================] - 0s 700us/step - loss: 2.1607\n16/16 [==============================] - 0s 651us/step - loss: 2.1573\n\nTesting for epoch 37 index 5:\n16/16 [==============================] - 0s 628us/step - loss: 0.1840\n16/16 [==============================] - 0s 600us/step - loss: 1.8633\n16/16 [==============================] - 0s 622us/step - loss: 2.2307\n16/16 [==============================] - 0s 620us/step - loss: 2.3265\n16/16 [==============================] - 0s 622us/step - loss: 2.3089\n16/16 [==============================] - 0s 652us/step - loss: 2.2739\n16/16 [==============================] - 0s 1ms/step - loss: 2.2421\n16/16 [==============================] - 0s 968us/step - loss: 2.2152\n16/16 [==============================] - 0s 1ms/step - loss: 2.2061\n16/16 [==============================] - 0s 851us/step - loss: 2.2024\nEpoch 38 of 60\n\nTesting for epoch 38 index 1:\n16/16 [==============================] - 0s 639us/step - loss: 0.1830\n16/16 [==============================] - 0s 1ms/step - loss: 1.8733\n16/16 [==============================] - 0s 621us/step - loss: 2.2434\n16/16 [==============================] - 0s 966us/step - loss: 2.3381\n16/16 [==============================] - 0s 1ms/step - loss: 2.3205\n16/16 [==============================] - 0s 1ms/step - loss: 2.2861\n16/16 [==============================] - 0s 1ms/step - loss: 2.2544\n16/16 [==============================] - 0s 1ms/step - loss: 2.2276\n16/16 [==============================] - 0s 936us/step - loss: 2.2186\n16/16 [==============================] - 0s 617us/step - loss: 2.2149\n\nTesting for epoch 38 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1816\n16/16 [==============================] - 0s 618us/step - loss: 1.8904\n16/16 [==============================] - 0s 1ms/step - loss: 2.2667\n16/16 [==============================] - 0s 707us/step - loss: 2.3649\n16/16 [==============================] - 0s 1ms/step - loss: 2.3499\n16/16 [==============================] - 0s 1ms/step - loss: 2.3184\n16/16 [==============================] - 0s 1ms/step - loss: 2.2894\n16/16 [==============================] - 0s 1ms/step - loss: 2.2641\n16/16 [==============================] - 0s 1ms/step - loss: 2.2555\n16/16 [==============================] - 0s 632us/step - loss: 2.2520\n\nTesting for epoch 38 index 3:\n16/16 [==============================] - 0s 607us/step - loss: 0.1833\n16/16 [==============================] - 0s 641us/step - loss: 1.8758\n16/16 [==============================] - 0s 1ms/step - loss: 2.2495\n16/16 [==============================] - 0s 617us/step - loss: 2.3460\n16/16 [==============================] - 0s 639us/step - loss: 2.3310\n16/16 [==============================] - 0s 930us/step - loss: 2.2983\n16/16 [==============================] - 0s 622us/step - loss: 2.2679\n16/16 [==============================] - 0s 1ms/step - loss: 2.2418\n16/16 [==============================] - 0s 624us/step - loss: 2.2328\n16/16 [==============================] - 0s 1ms/step - loss: 2.2291\n\nTesting for epoch 38 index 4:\n16/16 [==============================] - 0s 969us/step - loss: 0.1794\n16/16 [==============================] - 0s 1ms/step - loss: 1.9049\n16/16 [==============================] - 0s 1ms/step - loss: 2.2851\n16/16 [==============================] - 0s 615us/step - loss: 2.3832\n16/16 [==============================] - 0s 712us/step - loss: 2.3669\n16/16 [==============================] - 0s 616us/step - loss: 2.3326\n16/16 [==============================] - 0s 1ms/step - loss: 2.3017\n16/16 [==============================] - 0s 630us/step - loss: 2.2754\n16/16 [==============================] - 0s 837us/step - loss: 2.2665\n16/16 [==============================] - 0s 1ms/step - loss: 2.2628\n\nTesting for epoch 38 index 5:\n16/16 [==============================] - 0s 632us/step - loss: 0.1800\n16/16 [==============================] - 0s 626us/step - loss: 1.8637\n16/16 [==============================] - 0s 904us/step - loss: 2.2352\n16/16 [==============================] - 0s 1ms/step - loss: 2.3308\n16/16 [==============================] - 0s 1ms/step - loss: 2.3141\n16/16 [==============================] - 0s 1ms/step - loss: 2.2801\n16/16 [==============================] - 0s 1ms/step - loss: 2.2488\n16/16 [==============================] - 0s 1ms/step - loss: 2.2227\n16/16 [==============================] - 0s 919us/step - loss: 2.2139\n16/16 [==============================] - 0s 966us/step - loss: 2.2103\nEpoch 39 of 60\n\nTesting for epoch 39 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1775\n16/16 [==============================] - 0s 644us/step - loss: 1.9059\n16/16 [==============================] - 0s 669us/step - loss: 2.2860\n16/16 [==============================] - 0s 1ms/step - loss: 2.3835\n16/16 [==============================] - 0s 664us/step - loss: 2.3656\n16/16 [==============================] - 0s 1ms/step - loss: 2.3309\n16/16 [==============================] - 0s 689us/step - loss: 2.2992\n16/16 [==============================] - 0s 1ms/step - loss: 2.2725\n16/16 [==============================] - 0s 1ms/step - loss: 2.2635\n16/16 [==============================] - 0s 1ms/step - loss: 2.2598\n\nTesting for epoch 39 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1770\n16/16 [==============================] - 0s 882us/step - loss: 1.8485\n16/16 [==============================] - 0s 1ms/step - loss: 2.2087\n16/16 [==============================] - 0s 843us/step - loss: 2.3001\n16/16 [==============================] - 0s 1ms/step - loss: 2.2820\n16/16 [==============================] - 0s 1ms/step - loss: 2.2470\n16/16 [==============================] - 0s 808us/step - loss: 2.2158\n16/16 [==============================] - 0s 674us/step - loss: 2.1898\n16/16 [==============================] - 0s 665us/step - loss: 2.1810\n16/16 [==============================] - 0s 666us/step - loss: 2.1774\n\nTesting for epoch 39 index 3:\n16/16 [==============================] - 0s 702us/step - loss: 0.1778\n16/16 [==============================] - 0s 1ms/step - loss: 1.8604\n16/16 [==============================] - 0s 1ms/step - loss: 2.2199\n16/16 [==============================] - 0s 1ms/step - loss: 2.3081\n16/16 [==============================] - 0s 1ms/step - loss: 2.2886\n16/16 [==============================] - 0s 1ms/step - loss: 2.2537\n16/16 [==============================] - 0s 899us/step - loss: 2.2230\n16/16 [==============================] - 0s 847us/step - loss: 2.1975\n16/16 [==============================] - 0s 674us/step - loss: 2.1890\n16/16 [==============================] - 0s 648us/step - loss: 2.1856\n\nTesting for epoch 39 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1772\n16/16 [==============================] - 0s 1ms/step - loss: 1.9029\n16/16 [==============================] - 0s 1ms/step - loss: 2.2795\n16/16 [==============================] - 0s 929us/step - loss: 2.3721\n16/16 [==============================] - 0s 656us/step - loss: 2.3526\n16/16 [==============================] - 0s 685us/step - loss: 2.3171\n16/16 [==============================] - 0s 697us/step - loss: 2.2856\n16/16 [==============================] - 0s 686us/step - loss: 2.2594\n16/16 [==============================] - 0s 1ms/step - loss: 2.2505\n16/16 [==============================] - 0s 938us/step - loss: 2.2469\n\nTesting for epoch 39 index 5:\n16/16 [==============================] - 0s 736us/step - loss: 0.1773\n16/16 [==============================] - 0s 1ms/step - loss: 1.8690\n16/16 [==============================] - 0s 1ms/step - loss: 2.2337\n16/16 [==============================] - 0s 997us/step - loss: 2.3199\n16/16 [==============================] - 0s 1ms/step - loss: 2.2990\n16/16 [==============================] - 0s 965us/step - loss: 2.2626\n16/16 [==============================] - 0s 1ms/step - loss: 2.2305\n16/16 [==============================] - 0s 681us/step - loss: 2.2046\n16/16 [==============================] - 0s 701us/step - loss: 2.1959\n16/16 [==============================] - 0s 684us/step - loss: 2.1924\nEpoch 40 of 60\n\nTesting for epoch 40 index 1:\n16/16 [==============================] - 0s 857us/step - loss: 0.1749\n16/16 [==============================] - 0s 647us/step - loss: 1.8422\n16/16 [==============================] - 0s 695us/step - loss: 2.2055\n16/16 [==============================] - 0s 1ms/step - loss: 2.2933\n16/16 [==============================] - 0s 858us/step - loss: 2.2752\n16/16 [==============================] - 0s 1ms/step - loss: 2.2414\n16/16 [==============================] - 0s 1ms/step - loss: 2.2106\n16/16 [==============================] - 0s 702us/step - loss: 2.1853\n16/16 [==============================] - 0s 1ms/step - loss: 2.1768\n16/16 [==============================] - 0s 653us/step - loss: 2.1733\n\nTesting for epoch 40 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 0.1735\n16/16 [==============================] - 0s 2ms/step - loss: 1.8843\n16/16 [==============================] - 0s 2ms/step - loss: 2.2440\n16/16 [==============================] - 0s 2ms/step - loss: 2.3291\n16/16 [==============================] - 0s 2ms/step - loss: 2.3069\n16/16 [==============================] - 0s 2ms/step - loss: 2.2710\n16/16 [==============================] - 0s 2ms/step - loss: 2.2398\n16/16 [==============================] - 0s 2ms/step - loss: 2.2144\n16/16 [==============================] - 0s 2ms/step - loss: 2.2058\n16/16 [==============================] - 0s 1ms/step - loss: 2.2023\n\nTesting for epoch 40 index 3:\n16/16 [==============================] - 0s 776us/step - loss: 0.1737\n16/16 [==============================] - 0s 2ms/step - loss: 1.8765\n16/16 [==============================] - 0s 2ms/step - loss: 2.2332\n16/16 [==============================] - 0s 2ms/step - loss: 2.3172\n16/16 [==============================] - 0s 2ms/step - loss: 2.2945\n16/16 [==============================] - 0s 2ms/step - loss: 2.2569\n16/16 [==============================] - 0s 3ms/step - loss: 2.2234\n16/16 [==============================] - 0s 2ms/step - loss: 2.1967\n16/16 [==============================] - 0s 2ms/step - loss: 2.1880\n16/16 [==============================] - 0s 2ms/step - loss: 2.1844\n\nTesting for epoch 40 index 4:\n16/16 [==============================] - 0s 2ms/step - loss: 0.1761\n16/16 [==============================] - 0s 716us/step - loss: 1.8934\n16/16 [==============================] - 0s 703us/step - loss: 2.2560\n16/16 [==============================] - 0s 675us/step - loss: 2.3469\n16/16 [==============================] - 0s 683us/step - loss: 2.3263\n16/16 [==============================] - 0s 735us/step - loss: 2.2914\n16/16 [==============================] - 0s 2ms/step - loss: 2.2595\n16/16 [==============================] - 0s 2ms/step - loss: 2.2333\n16/16 [==============================] - 0s 2ms/step - loss: 2.2245\n16/16 [==============================] - 0s 1ms/step - loss: 2.2209\n\nTesting for epoch 40 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1722\n16/16 [==============================] - 0s 2ms/step - loss: 1.9280\n16/16 [==============================] - 0s 1ms/step - loss: 2.2976\n16/16 [==============================] - 0s 1ms/step - loss: 2.3871\n16/16 [==============================] - 0s 1ms/step - loss: 2.3629\n16/16 [==============================] - 0s 1ms/step - loss: 2.3248\n16/16 [==============================] - 0s 2ms/step - loss: 2.2908\n16/16 [==============================] - 0s 1ms/step - loss: 2.2634\n16/16 [==============================] - 0s 2ms/step - loss: 2.2542\n16/16 [==============================] - 0s 2ms/step - loss: 2.2505\nEpoch 41 of 60\n\nTesting for epoch 41 index 1:\n16/16 [==============================] - 0s 665us/step - loss: 0.1699\n16/16 [==============================] - 0s 649us/step - loss: 1.9410\n16/16 [==============================] - 0s 642us/step - loss: 2.3137\n16/16 [==============================] - 0s 647us/step - loss: 2.4031\n16/16 [==============================] - 0s 634us/step - loss: 2.3771\n16/16 [==============================] - 0s 655us/step - loss: 2.3376\n16/16 [==============================] - 0s 2ms/step - loss: 2.3020\n16/16 [==============================] - 0s 1ms/step - loss: 2.2737\n16/16 [==============================] - 0s 670us/step - loss: 2.2643\n16/16 [==============================] - 0s 643us/step - loss: 2.2604\n\nTesting for epoch 41 index 2:\n16/16 [==============================] - 0s 629us/step - loss: 0.1723\n16/16 [==============================] - 0s 2ms/step - loss: 1.8916\n16/16 [==============================] - 0s 807us/step - loss: 2.2597\n16/16 [==============================] - 0s 933us/step - loss: 2.3476\n16/16 [==============================] - 0s 933us/step - loss: 2.3238\n16/16 [==============================] - 0s 923us/step - loss: 2.2886\n16/16 [==============================] - 0s 952us/step - loss: 2.2566\n16/16 [==============================] - 0s 1ms/step - loss: 2.2308\n16/16 [==============================] - 0s 995us/step - loss: 2.2222\n16/16 [==============================] - 0s 911us/step - loss: 2.2188\n\nTesting for epoch 41 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1698\n16/16 [==============================] - 0s 2ms/step - loss: 1.9182\n16/16 [==============================] - 0s 1ms/step - loss: 2.2868\n16/16 [==============================] - 0s 755us/step - loss: 2.3684\n16/16 [==============================] - 0s 652us/step - loss: 2.3398\n16/16 [==============================] - 0s 650us/step - loss: 2.3013\n16/16 [==============================] - 0s 648us/step - loss: 2.2666\n16/16 [==============================] - 0s 660us/step - loss: 2.2391\n16/16 [==============================] - 0s 1ms/step - loss: 2.2300\n16/16 [==============================] - 0s 2ms/step - loss: 2.2264\n\nTesting for epoch 41 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1686\n16/16 [==============================] - 0s 648us/step - loss: 1.9577\n16/16 [==============================] - 0s 887us/step - loss: 2.3428\n16/16 [==============================] - 0s 1ms/step - loss: 2.4313\n16/16 [==============================] - 0s 645us/step - loss: 2.4052\n16/16 [==============================] - 0s 1ms/step - loss: 2.3674\n16/16 [==============================] - 0s 2ms/step - loss: 2.3327\n16/16 [==============================] - 0s 653us/step - loss: 2.3051\n16/16 [==============================] - 0s 2ms/step - loss: 2.2958\n16/16 [==============================] - 0s 2ms/step - loss: 2.2921\n\nTesting for epoch 41 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1707\n16/16 [==============================] - 0s 1ms/step - loss: 1.9381\n16/16 [==============================] - 0s 626us/step - loss: 2.3141\n16/16 [==============================] - 0s 2ms/step - loss: 2.3976\n16/16 [==============================] - 0s 1ms/step - loss: 2.3711\n16/16 [==============================] - 0s 2ms/step - loss: 2.3333\n16/16 [==============================] - 0s 2ms/step - loss: 2.2988\n16/16 [==============================] - 0s 2ms/step - loss: 2.2715\n16/16 [==============================] - 0s 1ms/step - loss: 2.2624\n16/16 [==============================] - 0s 2ms/step - loss: 2.2588\nEpoch 42 of 60\n\nTesting for epoch 42 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 0.1687\n16/16 [==============================] - 0s 2ms/step - loss: 1.9747\n16/16 [==============================] - 0s 2ms/step - loss: 2.3635\n16/16 [==============================] - 0s 2ms/step - loss: 2.4468\n16/16 [==============================] - 0s 2ms/step - loss: 2.4175\n16/16 [==============================] - 0s 2ms/step - loss: 2.3761\n16/16 [==============================] - 0s 1ms/step - loss: 2.3383\n16/16 [==============================] - 0s 2ms/step - loss: 2.3088\n16/16 [==============================] - 0s 1ms/step - loss: 2.2990\n16/16 [==============================] - 0s 2ms/step - loss: 2.2950\n\nTesting for epoch 42 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 0.1671\n16/16 [==============================] - 0s 1ms/step - loss: 1.9391\n16/16 [==============================] - 0s 2ms/step - loss: 2.3156\n16/16 [==============================] - 0s 1ms/step - loss: 2.3979\n16/16 [==============================] - 0s 2ms/step - loss: 2.3719\n16/16 [==============================] - 0s 2ms/step - loss: 2.3335\n16/16 [==============================] - 0s 2ms/step - loss: 2.2984\n16/16 [==============================] - 0s 2ms/step - loss: 2.2707\n16/16 [==============================] - 0s 2ms/step - loss: 2.2614\n16/16 [==============================] - 0s 2ms/step - loss: 2.2577\n\nTesting for epoch 42 index 3:\n16/16 [==============================] - 0s 2ms/step - loss: 0.1662\n16/16 [==============================] - 0s 1ms/step - loss: 1.9564\n16/16 [==============================] - 0s 1ms/step - loss: 2.3366\n16/16 [==============================] - 0s 2ms/step - loss: 2.4170\n16/16 [==============================] - 0s 2ms/step - loss: 2.3906\n16/16 [==============================] - 0s 2ms/step - loss: 2.3517\n16/16 [==============================] - 0s 1ms/step - loss: 2.3168\n16/16 [==============================] - 0s 1ms/step - loss: 2.2891\n16/16 [==============================] - 0s 2ms/step - loss: 2.2799\n16/16 [==============================] - 0s 3ms/step - loss: 2.2762\n\nTesting for epoch 42 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1664\n16/16 [==============================] - 0s 836us/step - loss: 1.9228\n16/16 [==============================] - 0s 857us/step - loss: 2.2967\n16/16 [==============================] - 0s 827us/step - loss: 2.3769\n16/16 [==============================] - 0s 2ms/step - loss: 2.3522\n16/16 [==============================] - 0s 2ms/step - loss: 2.3146\n16/16 [==============================] - 0s 2ms/step - loss: 2.2804\n16/16 [==============================] - 0s 2ms/step - loss: 2.2535\n16/16 [==============================] - 0s 2ms/step - loss: 2.2446\n16/16 [==============================] - 0s 2ms/step - loss: 2.2410\n\nTesting for epoch 42 index 5:\n16/16 [==============================] - 0s 968us/step - loss: 0.1647\n16/16 [==============================] - 0s 836us/step - loss: 1.9825\n16/16 [==============================] - 0s 1ms/step - loss: 2.3655\n16/16 [==============================] - 0s 991us/step - loss: 2.4400\n16/16 [==============================] - 0s 708us/step - loss: 2.4065\n16/16 [==============================] - 0s 988us/step - loss: 2.3618\n16/16 [==============================] - 0s 1ms/step - loss: 2.3236\n16/16 [==============================] - 0s 769us/step - loss: 2.2942\n16/16 [==============================] - 0s 630us/step - loss: 2.2846\n16/16 [==============================] - 0s 832us/step - loss: 2.2808\nEpoch 43 of 60\n\nTesting for epoch 43 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 0.1616\n16/16 [==============================] - 0s 2ms/step - loss: 1.9799\n16/16 [==============================] - 0s 2ms/step - loss: 2.3682\n16/16 [==============================] - 0s 2ms/step - loss: 2.4457\n16/16 [==============================] - 0s 2ms/step - loss: 2.4156\n16/16 [==============================] - 0s 2ms/step - loss: 2.3727\n16/16 [==============================] - 0s 2ms/step - loss: 2.3349\n16/16 [==============================] - 0s 2ms/step - loss: 2.3056\n16/16 [==============================] - 0s 2ms/step - loss: 2.2959\n16/16 [==============================] - 0s 2ms/step - loss: 2.2921\n\nTesting for epoch 43 index 2:\n16/16 [==============================] - 0s 594us/step - loss: 0.1670\n16/16 [==============================] - 0s 542us/step - loss: 1.9252\n16/16 [==============================] - 0s 603us/step - loss: 2.3035\n16/16 [==============================] - 0s 638us/step - loss: 2.3810\n16/16 [==============================] - 0s 646us/step - loss: 2.3532\n16/16 [==============================] - 0s 490us/step - loss: 2.3132\n16/16 [==============================] - 0s 517us/step - loss: 2.2779\n16/16 [==============================] - 0s 1ms/step - loss: 2.2503\n16/16 [==============================] - 0s 895us/step - loss: 2.2412\n16/16 [==============================] - 0s 570us/step - loss: 2.2376\n\nTesting for epoch 43 index 3:\n16/16 [==============================] - 0s 626us/step - loss: 0.1649\n16/16 [==============================] - 0s 921us/step - loss: 1.9655\n16/16 [==============================] - 0s 1ms/step - loss: 2.3495\n16/16 [==============================] - 0s 708us/step - loss: 2.4250\n16/16 [==============================] - 0s 1ms/step - loss: 2.3950\n16/16 [==============================] - 0s 1ms/step - loss: 2.3532\n16/16 [==============================] - 0s 681us/step - loss: 2.3167\n16/16 [==============================] - 0s 632us/step - loss: 2.2886\n16/16 [==============================] - 0s 1ms/step - loss: 2.2794\n16/16 [==============================] - 0s 644us/step - loss: 2.2756\n\nTesting for epoch 43 index 4:\n16/16 [==============================] - 0s 716us/step - loss: 0.1628\n16/16 [==============================] - 0s 631us/step - loss: 1.9172\n16/16 [==============================] - 0s 634us/step - loss: 2.2852\n16/16 [==============================] - 0s 601us/step - loss: 2.3543\n16/16 [==============================] - 0s 1ms/step - loss: 2.3241\n16/16 [==============================] - 0s 1ms/step - loss: 2.2817\n16/16 [==============================] - 0s 1ms/step - loss: 2.2446\n16/16 [==============================] - 0s 637us/step - loss: 2.2162\n16/16 [==============================] - 0s 1ms/step - loss: 2.2068\n16/16 [==============================] - 0s 622us/step - loss: 2.2031\n\nTesting for epoch 43 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1632\n16/16 [==============================] - 0s 614us/step - loss: 1.9494\n16/16 [==============================] - 0s 585us/step - loss: 2.3248\n16/16 [==============================] - 0s 1ms/step - loss: 2.3958\n16/16 [==============================] - 0s 605us/step - loss: 2.3639\n16/16 [==============================] - 0s 1ms/step - loss: 2.3212\n16/16 [==============================] - 0s 659us/step - loss: 2.2846\n16/16 [==============================] - 0s 920us/step - loss: 2.2564\n16/16 [==============================] - 0s 642us/step - loss: 2.2472\n16/16 [==============================] - 0s 1ms/step - loss: 2.2436\nEpoch 44 of 60\n\nTesting for epoch 44 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1623\n16/16 [==============================] - 0s 632us/step - loss: 1.9453\n16/16 [==============================] - 0s 701us/step - loss: 2.3227\n16/16 [==============================] - 0s 1ms/step - loss: 2.3937\n16/16 [==============================] - 0s 598us/step - loss: 2.3616\n16/16 [==============================] - 0s 623us/step - loss: 2.3172\n16/16 [==============================] - 0s 1ms/step - loss: 2.2785\n16/16 [==============================] - 0s 802us/step - loss: 2.2493\n16/16 [==============================] - 0s 1ms/step - loss: 2.2398\n16/16 [==============================] - 0s 900us/step - loss: 2.2360\n\nTesting for epoch 44 index 2:\n16/16 [==============================] - 0s 639us/step - loss: 0.1612\n16/16 [==============================] - 0s 709us/step - loss: 1.9442\n16/16 [==============================] - 0s 617us/step - loss: 2.3257\n16/16 [==============================] - 0s 598us/step - loss: 2.4020\n16/16 [==============================] - 0s 1ms/step - loss: 2.3753\n16/16 [==============================] - 0s 625us/step - loss: 2.3369\n16/16 [==============================] - 0s 611us/step - loss: 2.3024\n16/16 [==============================] - 0s 601us/step - loss: 2.2753\n16/16 [==============================] - 0s 764us/step - loss: 2.2663\n16/16 [==============================] - 0s 1ms/step - loss: 2.2626\n\nTesting for epoch 44 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1585\n16/16 [==============================] - 0s 1ms/step - loss: 2.0528\n16/16 [==============================] - 0s 609us/step - loss: 2.4585\n16/16 [==============================] - 0s 789us/step - loss: 2.5358\n16/16 [==============================] - 0s 587us/step - loss: 2.5023\n16/16 [==============================] - 0s 672us/step - loss: 2.4563\n16/16 [==============================] - 0s 805us/step - loss: 2.4162\n16/16 [==============================] - 0s 1ms/step - loss: 2.3857\n16/16 [==============================] - 0s 1ms/step - loss: 2.3757\n16/16 [==============================] - 0s 1ms/step - loss: 2.3718\n\nTesting for epoch 44 index 4:\n16/16 [==============================] - 0s 735us/step - loss: 0.1614\n16/16 [==============================] - 0s 1ms/step - loss: 2.0477\n16/16 [==============================] - 0s 646us/step - loss: 2.4531\n16/16 [==============================] - 0s 1ms/step - loss: 2.5273\n16/16 [==============================] - 0s 560us/step - loss: 2.4941\n16/16 [==============================] - 0s 604us/step - loss: 2.4496\n16/16 [==============================] - 0s 1ms/step - loss: 2.4104\n16/16 [==============================] - 0s 599us/step - loss: 2.3801\n16/16 [==============================] - 0s 1ms/step - loss: 2.3702\n16/16 [==============================] - 0s 1ms/step - loss: 2.3661\n\nTesting for epoch 44 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1584\n16/16 [==============================] - 0s 1ms/step - loss: 2.0297\n16/16 [==============================] - 0s 1ms/step - loss: 2.4309\n16/16 [==============================] - 0s 1ms/step - loss: 2.5028\n16/16 [==============================] - 0s 844us/step - loss: 2.4704\n16/16 [==============================] - 0s 721us/step - loss: 2.4258\n16/16 [==============================] - 0s 1ms/step - loss: 2.3870\n16/16 [==============================] - 0s 724us/step - loss: 2.3575\n16/16 [==============================] - 0s 1ms/step - loss: 2.3479\n16/16 [==============================] - 0s 1ms/step - loss: 2.3440\nEpoch 45 of 60\n\nTesting for epoch 45 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1607\n16/16 [==============================] - 0s 1ms/step - loss: 1.9936\n16/16 [==============================] - 0s 1ms/step - loss: 2.3817\n16/16 [==============================] - 0s 618us/step - loss: 2.4495\n16/16 [==============================] - 0s 1ms/step - loss: 2.4171\n16/16 [==============================] - 0s 1ms/step - loss: 2.3735\n16/16 [==============================] - 0s 929us/step - loss: 2.3355\n16/16 [==============================] - 0s 1ms/step - loss: 2.3067\n16/16 [==============================] - 0s 1ms/step - loss: 2.2974\n16/16 [==============================] - 0s 967us/step - loss: 2.2937\n\nTesting for epoch 45 index 2:\n16/16 [==============================] - 0s 625us/step - loss: 0.1592\n16/16 [==============================] - 0s 748us/step - loss: 2.0080\n16/16 [==============================] - 0s 948us/step - loss: 2.3934\n16/16 [==============================] - 0s 626us/step - loss: 2.4562\n16/16 [==============================] - 0s 1ms/step - loss: 2.4202\n16/16 [==============================] - 0s 1ms/step - loss: 2.3733\n16/16 [==============================] - 0s 600us/step - loss: 2.3338\n16/16 [==============================] - 0s 1ms/step - loss: 2.3041\n16/16 [==============================] - 0s 1ms/step - loss: 2.2945\n16/16 [==============================] - 0s 579us/step - loss: 2.2907\n\nTesting for epoch 45 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1563\n16/16 [==============================] - 0s 1ms/step - loss: 1.9989\n16/16 [==============================] - 0s 791us/step - loss: 2.3823\n16/16 [==============================] - 0s 592us/step - loss: 2.4454\n16/16 [==============================] - 0s 657us/step - loss: 2.4112\n16/16 [==============================] - 0s 731us/step - loss: 2.3670\n16/16 [==============================] - 0s 1ms/step - loss: 2.3288\n16/16 [==============================] - 0s 745us/step - loss: 2.2995\n16/16 [==============================] - 0s 595us/step - loss: 2.2900\n16/16 [==============================] - 0s 629us/step - loss: 2.2862\n\nTesting for epoch 45 index 4:\n16/16 [==============================] - 0s 948us/step - loss: 0.1570\n16/16 [==============================] - 0s 860us/step - loss: 1.9843\n16/16 [==============================] - 0s 836us/step - loss: 2.3656\n16/16 [==============================] - 0s 1ms/step - loss: 2.4254\n16/16 [==============================] - 0s 1ms/step - loss: 2.3894\n16/16 [==============================] - 0s 1ms/step - loss: 2.3424\n16/16 [==============================] - 0s 644us/step - loss: 2.3025\n16/16 [==============================] - 0s 606us/step - loss: 2.2727\n16/16 [==============================] - 0s 1ms/step - loss: 2.2631\n16/16 [==============================] - 0s 810us/step - loss: 2.2593\n\nTesting for epoch 45 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1540\n16/16 [==============================] - 0s 621us/step - loss: 2.0578\n16/16 [==============================] - 0s 754us/step - loss: 2.4644\n16/16 [==============================] - 0s 1ms/step - loss: 2.5294\n16/16 [==============================] - 0s 1ms/step - loss: 2.4946\n16/16 [==============================] - 0s 1ms/step - loss: 2.4481\n16/16 [==============================] - 0s 1ms/step - loss: 2.4079\n16/16 [==============================] - 0s 759us/step - loss: 2.3773\n16/16 [==============================] - 0s 1ms/step - loss: 2.3674\n16/16 [==============================] - 0s 634us/step - loss: 2.3635\nEpoch 46 of 60\n\nTesting for epoch 46 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1569\n16/16 [==============================] - 0s 624us/step - loss: 2.0009\n16/16 [==============================] - 0s 644us/step - loss: 2.3983\n16/16 [==============================] - 0s 620us/step - loss: 2.4650\n16/16 [==============================] - 0s 1ms/step - loss: 2.4337\n16/16 [==============================] - 0s 657us/step - loss: 2.3895\n16/16 [==============================] - 0s 662us/step - loss: 2.3506\n16/16 [==============================] - 0s 1ms/step - loss: 2.3207\n16/16 [==============================] - 0s 826us/step - loss: 2.3109\n16/16 [==============================] - 0s 1ms/step - loss: 2.3070\n\nTesting for epoch 46 index 2:\n16/16 [==============================] - 0s 648us/step - loss: 0.1564\n16/16 [==============================] - 0s 906us/step - loss: 1.9920\n16/16 [==============================] - 0s 1ms/step - loss: 2.3850\n16/16 [==============================] - 0s 632us/step - loss: 2.4473\n16/16 [==============================] - 0s 914us/step - loss: 2.4129\n16/16 [==============================] - 0s 650us/step - loss: 2.3665\n16/16 [==============================] - 0s 1ms/step - loss: 2.3262\n16/16 [==============================] - 0s 1ms/step - loss: 2.2963\n16/16 [==============================] - 0s 631us/step - loss: 2.2867\n16/16 [==============================] - 0s 1ms/step - loss: 2.2828\n\nTesting for epoch 46 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1523\n16/16 [==============================] - 0s 610us/step - loss: 1.9810\n16/16 [==============================] - 0s 1ms/step - loss: 2.3695\n16/16 [==============================] - 0s 599us/step - loss: 2.4291\n16/16 [==============================] - 0s 1ms/step - loss: 2.3939\n16/16 [==============================] - 0s 596us/step - loss: 2.3485\n16/16 [==============================] - 0s 651us/step - loss: 2.3100\n16/16 [==============================] - 0s 1ms/step - loss: 2.2810\n16/16 [==============================] - 0s 1ms/step - loss: 2.2716\n16/16 [==============================] - 0s 789us/step - loss: 2.2678\n\nTesting for epoch 46 index 4:\n16/16 [==============================] - 0s 605us/step - loss: 0.1539\n16/16 [==============================] - 0s 946us/step - loss: 2.0070\n16/16 [==============================] - 0s 1ms/step - loss: 2.3983\n16/16 [==============================] - 0s 562us/step - loss: 2.4547\n16/16 [==============================] - 0s 1ms/step - loss: 2.4185\n16/16 [==============================] - 0s 646us/step - loss: 2.3714\n16/16 [==============================] - 0s 1ms/step - loss: 2.3316\n16/16 [==============================] - 0s 1ms/step - loss: 2.3019\n16/16 [==============================] - 0s 1ms/step - loss: 2.2923\n16/16 [==============================] - 0s 629us/step - loss: 2.2885\n\nTesting for epoch 46 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1531\n16/16 [==============================] - 0s 1ms/step - loss: 2.0956\n16/16 [==============================] - 0s 1ms/step - loss: 2.5234\n16/16 [==============================] - 0s 649us/step - loss: 2.5872\n16/16 [==============================] - 0s 1ms/step - loss: 2.5491\n16/16 [==============================] - 0s 1ms/step - loss: 2.4983\n16/16 [==============================] - 0s 1ms/step - loss: 2.4550\n16/16 [==============================] - 0s 1ms/step - loss: 2.4226\n16/16 [==============================] - 0s 1ms/step - loss: 2.4122\n16/16 [==============================] - 0s 1ms/step - loss: 2.4080\nEpoch 47 of 60\n\nTesting for epoch 47 index 1:\n16/16 [==============================] - 0s 631us/step - loss: 0.1539\n16/16 [==============================] - 0s 877us/step - loss: 2.0259\n16/16 [==============================] - 0s 1ms/step - loss: 2.4331\n16/16 [==============================] - 0s 724us/step - loss: 2.4907\n16/16 [==============================] - 0s 631us/step - loss: 2.4546\n16/16 [==============================] - 0s 631us/step - loss: 2.4070\n16/16 [==============================] - 0s 1ms/step - loss: 2.3665\n16/16 [==============================] - 0s 1ms/step - loss: 2.3359\n16/16 [==============================] - 0s 995us/step - loss: 2.3261\n16/16 [==============================] - 0s 1ms/step - loss: 2.3222\n\nTesting for epoch 47 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1519\n16/16 [==============================] - 0s 639us/step - loss: 2.0542\n16/16 [==============================] - 0s 934us/step - loss: 2.4779\n16/16 [==============================] - 0s 1ms/step - loss: 2.5430\n16/16 [==============================] - 0s 666us/step - loss: 2.5111\n16/16 [==============================] - 0s 645us/step - loss: 2.4667\n16/16 [==============================] - 0s 606us/step - loss: 2.4283\n16/16 [==============================] - 0s 842us/step - loss: 2.3987\n16/16 [==============================] - 0s 617us/step - loss: 2.3890\n16/16 [==============================] - 0s 1ms/step - loss: 2.3851\n\nTesting for epoch 47 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1528\n16/16 [==============================] - 0s 957us/step - loss: 2.0201\n16/16 [==============================] - 0s 1ms/step - loss: 2.4226\n16/16 [==============================] - 0s 610us/step - loss: 2.4747\n16/16 [==============================] - 0s 626us/step - loss: 2.4384\n16/16 [==============================] - 0s 1ms/step - loss: 2.3899\n16/16 [==============================] - 0s 1ms/step - loss: 2.3487\n16/16 [==============================] - 0s 1ms/step - loss: 2.3181\n16/16 [==============================] - 0s 1ms/step - loss: 2.3083\n16/16 [==============================] - 0s 635us/step - loss: 2.3044\n\nTesting for epoch 47 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1513\n16/16 [==============================] - 0s 1ms/step - loss: 2.0221\n16/16 [==============================] - 0s 953us/step - loss: 2.4286\n16/16 [==============================] - 0s 584us/step - loss: 2.4816\n16/16 [==============================] - 0s 794us/step - loss: 2.4463\n16/16 [==============================] - 0s 1ms/step - loss: 2.3995\n16/16 [==============================] - 0s 1ms/step - loss: 2.3596\n16/16 [==============================] - 0s 1ms/step - loss: 2.3298\n16/16 [==============================] - 0s 635us/step - loss: 2.3202\n16/16 [==============================] - 0s 1ms/step - loss: 2.3163\n\nTesting for epoch 47 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1505\n16/16 [==============================] - 0s 1ms/step - loss: 2.0238\n16/16 [==============================] - 0s 1ms/step - loss: 2.4319\n16/16 [==============================] - 0s 1ms/step - loss: 2.4849\n16/16 [==============================] - 0s 628us/step - loss: 2.4502\n16/16 [==============================] - 0s 1ms/step - loss: 2.4034\n16/16 [==============================] - 0s 660us/step - loss: 2.3643\n16/16 [==============================] - 0s 614us/step - loss: 2.3350\n16/16 [==============================] - 0s 628us/step - loss: 2.3256\n16/16 [==============================] - 0s 1ms/step - loss: 2.3218\nEpoch 48 of 60\n\nTesting for epoch 48 index 1:\n16/16 [==============================] - 0s 637us/step - loss: 0.1506\n16/16 [==============================] - 0s 929us/step - loss: 2.0292\n16/16 [==============================] - 0s 635us/step - loss: 2.4452\n16/16 [==============================] - 0s 625us/step - loss: 2.5006\n16/16 [==============================] - 0s 639us/step - loss: 2.4658\n16/16 [==============================] - 0s 1ms/step - loss: 2.4185\n16/16 [==============================] - 0s 640us/step - loss: 2.3780\n16/16 [==============================] - 0s 1ms/step - loss: 2.3475\n16/16 [==============================] - 0s 593us/step - loss: 2.3377\n16/16 [==============================] - 0s 947us/step - loss: 2.3338\n\nTesting for epoch 48 index 2:\n16/16 [==============================] - 0s 642us/step - loss: 0.1520\n16/16 [==============================] - 0s 927us/step - loss: 2.0468\n16/16 [==============================] - 0s 907us/step - loss: 2.4645\n16/16 [==============================] - 0s 1ms/step - loss: 2.5202\n16/16 [==============================] - 0s 721us/step - loss: 2.4857\n16/16 [==============================] - 0s 1ms/step - loss: 2.4381\n16/16 [==============================] - 0s 1ms/step - loss: 2.3972\n16/16 [==============================] - 0s 1ms/step - loss: 2.3664\n16/16 [==============================] - 0s 755us/step - loss: 2.3565\n16/16 [==============================] - 0s 1ms/step - loss: 2.3525\n\nTesting for epoch 48 index 3:\n16/16 [==============================] - 0s 836us/step - loss: 0.1459\n16/16 [==============================] - 0s 1ms/step - loss: 2.0895\n16/16 [==============================] - 0s 1ms/step - loss: 2.5139\n16/16 [==============================] - 0s 1ms/step - loss: 2.5640\n16/16 [==============================] - 0s 773us/step - loss: 2.5243\n16/16 [==============================] - 0s 754us/step - loss: 2.4719\n16/16 [==============================] - 0s 617us/step - loss: 2.4279\n16/16 [==============================] - 0s 1ms/step - loss: 2.3952\n16/16 [==============================] - 0s 935us/step - loss: 2.3847\n16/16 [==============================] - 0s 1ms/step - loss: 2.3804\n\nTesting for epoch 48 index 4:\n16/16 [==============================] - 0s 635us/step - loss: 0.1465\n16/16 [==============================] - 0s 640us/step - loss: 2.0292\n16/16 [==============================] - 0s 610us/step - loss: 2.4415\n16/16 [==============================] - 0s 635us/step - loss: 2.4915\n16/16 [==============================] - 0s 634us/step - loss: 2.4547\n16/16 [==============================] - 0s 621us/step - loss: 2.4048\n16/16 [==============================] - 0s 644us/step - loss: 2.3627\n16/16 [==============================] - 0s 750us/step - loss: 2.3313\n16/16 [==============================] - 0s 1ms/step - loss: 2.3213\n16/16 [==============================] - 0s 705us/step - loss: 2.3173\n\nTesting for epoch 48 index 5:\n16/16 [==============================] - 0s 624us/step - loss: 0.1449\n16/16 [==============================] - 0s 1ms/step - loss: 2.0603\n16/16 [==============================] - 0s 1ms/step - loss: 2.4760\n16/16 [==============================] - 0s 1ms/step - loss: 2.5243\n16/16 [==============================] - 0s 1ms/step - loss: 2.4877\n16/16 [==============================] - 0s 1ms/step - loss: 2.4388\n16/16 [==============================] - 0s 1ms/step - loss: 2.3979\n16/16 [==============================] - 0s 615us/step - loss: 2.3674\n16/16 [==============================] - 0s 1ms/step - loss: 2.3576\n16/16 [==============================] - 0s 1ms/step - loss: 2.3537\nEpoch 49 of 60\n\nTesting for epoch 49 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1456\n16/16 [==============================] - 0s 1ms/step - loss: 2.1177\n16/16 [==============================] - 0s 1ms/step - loss: 2.5539\n16/16 [==============================] - 0s 1ms/step - loss: 2.6038\n16/16 [==============================] - 0s 641us/step - loss: 2.5627\n16/16 [==============================] - 0s 1ms/step - loss: 2.5085\n16/16 [==============================] - 0s 1ms/step - loss: 2.4635\n16/16 [==============================] - 0s 1ms/step - loss: 2.4301\n16/16 [==============================] - 0s 1ms/step - loss: 2.4196\n16/16 [==============================] - 0s 1ms/step - loss: 2.4155\n\nTesting for epoch 49 index 2:\n16/16 [==============================] - 0s 633us/step - loss: 0.1446\n16/16 [==============================] - 0s 650us/step - loss: 2.0868\n16/16 [==============================] - 0s 634us/step - loss: 2.5135\n16/16 [==============================] - 0s 605us/step - loss: 2.5636\n16/16 [==============================] - 0s 1ms/step - loss: 2.5272\n16/16 [==============================] - 0s 640us/step - loss: 2.4769\n16/16 [==============================] - 0s 1ms/step - loss: 2.4353\n16/16 [==============================] - 0s 608us/step - loss: 2.4045\n16/16 [==============================] - 0s 1ms/step - loss: 2.3946\n16/16 [==============================] - 0s 1ms/step - loss: 2.3907\n\nTesting for epoch 49 index 3:\n16/16 [==============================] - 0s 649us/step - loss: 0.1457\n16/16 [==============================] - 0s 624us/step - loss: 2.0862\n16/16 [==============================] - 0s 875us/step - loss: 2.5107\n16/16 [==============================] - 0s 590us/step - loss: 2.5578\n16/16 [==============================] - 0s 573us/step - loss: 2.5165\n16/16 [==============================] - 0s 1ms/step - loss: 2.4632\n16/16 [==============================] - 0s 1ms/step - loss: 2.4191\n16/16 [==============================] - 0s 1ms/step - loss: 2.3864\n16/16 [==============================] - 0s 1ms/step - loss: 2.3760\n16/16 [==============================] - 0s 611us/step - loss: 2.3718\n\nTesting for epoch 49 index 4:\n16/16 [==============================] - 0s 664us/step - loss: 0.1438\n16/16 [==============================] - 0s 610us/step - loss: 2.0347\n16/16 [==============================] - 0s 601us/step - loss: 2.4431\n16/16 [==============================] - 0s 1ms/step - loss: 2.4871\n16/16 [==============================] - 0s 636us/step - loss: 2.4479\n16/16 [==============================] - 0s 606us/step - loss: 2.3982\n16/16 [==============================] - 0s 618us/step - loss: 2.3571\n16/16 [==============================] - 0s 639us/step - loss: 2.3267\n16/16 [==============================] - 0s 657us/step - loss: 2.3170\n16/16 [==============================] - 0s 629us/step - loss: 2.3131\n\nTesting for epoch 49 index 5:\n16/16 [==============================] - 0s 641us/step - loss: 0.1474\n16/16 [==============================] - 0s 988us/step - loss: 2.0366\n16/16 [==============================] - 0s 1ms/step - loss: 2.4480\n16/16 [==============================] - 0s 903us/step - loss: 2.4936\n16/16 [==============================] - 0s 1ms/step - loss: 2.4559\n16/16 [==============================] - 0s 1ms/step - loss: 2.4066\n16/16 [==============================] - 0s 635us/step - loss: 2.3658\n16/16 [==============================] - 0s 1ms/step - loss: 2.3355\n16/16 [==============================] - 0s 797us/step - loss: 2.3259\n16/16 [==============================] - 0s 735us/step - loss: 2.3222\nEpoch 50 of 60\n\nTesting for epoch 50 index 1:\n16/16 [==============================] - 0s 696us/step - loss: 0.1436\n16/16 [==============================] - 0s 905us/step - loss: 2.1021\n16/16 [==============================] - 0s 841us/step - loss: 2.5288\n16/16 [==============================] - 0s 1ms/step - loss: 2.5741\n16/16 [==============================] - 0s 623us/step - loss: 2.5332\n16/16 [==============================] - 0s 637us/step - loss: 2.4820\n16/16 [==============================] - 0s 644us/step - loss: 2.4395\n16/16 [==============================] - 0s 949us/step - loss: 2.4076\n16/16 [==============================] - 0s 640us/step - loss: 2.3974\n16/16 [==============================] - 0s 1ms/step - loss: 2.3933\n\nTesting for epoch 50 index 2:\n16/16 [==============================] - 0s 745us/step - loss: 0.1413\n16/16 [==============================] - 0s 1ms/step - loss: 2.0536\n16/16 [==============================] - 0s 1ms/step - loss: 2.4615\n16/16 [==============================] - 0s 1ms/step - loss: 2.4976\n16/16 [==============================] - 0s 1ms/step - loss: 2.4550\n16/16 [==============================] - 0s 1ms/step - loss: 2.4020\n16/16 [==============================] - 0s 1ms/step - loss: 2.3590\n16/16 [==============================] - 0s 1ms/step - loss: 2.3276\n16/16 [==============================] - 0s 1ms/step - loss: 2.3176\n16/16 [==============================] - 0s 1ms/step - loss: 2.3137\n\nTesting for epoch 50 index 3:\n16/16 [==============================] - 0s 624us/step - loss: 0.1436\n16/16 [==============================] - 0s 1ms/step - loss: 2.0689\n16/16 [==============================] - 0s 1ms/step - loss: 2.4842\n16/16 [==============================] - 0s 1ms/step - loss: 2.5212\n16/16 [==============================] - 0s 1ms/step - loss: 2.4800\n16/16 [==============================] - 0s 613us/step - loss: 2.4299\n16/16 [==============================] - 0s 614us/step - loss: 2.3885\n16/16 [==============================] - 0s 622us/step - loss: 2.3575\n16/16 [==============================] - 0s 1ms/step - loss: 2.3477\n16/16 [==============================] - 0s 1ms/step - loss: 2.3438\n\nTesting for epoch 50 index 4:\n16/16 [==============================] - 0s 636us/step - loss: 0.1412\n16/16 [==============================] - 0s 622us/step - loss: 2.0655\n16/16 [==============================] - 0s 584us/step - loss: 2.4782\n16/16 [==============================] - 0s 999us/step - loss: 2.5137\n16/16 [==============================] - 0s 648us/step - loss: 2.4722\n16/16 [==============================] - 0s 843us/step - loss: 2.4209\n16/16 [==============================] - 0s 889us/step - loss: 2.3791\n16/16 [==============================] - 0s 1ms/step - loss: 2.3482\n16/16 [==============================] - 0s 1ms/step - loss: 2.3383\n16/16 [==============================] - 0s 1ms/step - loss: 2.3344\n\nTesting for epoch 50 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1411\n16/16 [==============================] - 0s 608us/step - loss: 2.0600\n16/16 [==============================] - 0s 591us/step - loss: 2.4682\n16/16 [==============================] - 0s 1ms/step - loss: 2.5009\n16/16 [==============================] - 0s 885us/step - loss: 2.4578\n16/16 [==============================] - 0s 1ms/step - loss: 2.4045\n16/16 [==============================] - 0s 918us/step - loss: 2.3612\n16/16 [==============================] - 0s 1ms/step - loss: 2.3293\n16/16 [==============================] - 0s 1ms/step - loss: 2.3193\n16/16 [==============================] - 0s 648us/step - loss: 2.3153\nEpoch 51 of 60\n\nTesting for epoch 51 index 1:\n16/16 [==============================] - 0s 658us/step - loss: 0.1459\n16/16 [==============================] - 0s 1ms/step - loss: 2.0524\n16/16 [==============================] - 0s 593us/step - loss: 2.4624\n16/16 [==============================] - 0s 1ms/step - loss: 2.4994\n16/16 [==============================] - 0s 1ms/step - loss: 2.4614\n16/16 [==============================] - 0s 647us/step - loss: 2.4144\n16/16 [==============================] - 0s 956us/step - loss: 2.3751\n16/16 [==============================] - 0s 619us/step - loss: 2.3456\n16/16 [==============================] - 0s 1ms/step - loss: 2.3363\n16/16 [==============================] - 0s 1ms/step - loss: 2.3326\n\nTesting for epoch 51 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1414\n16/16 [==============================] - 0s 691us/step - loss: 2.1374\n16/16 [==============================] - 0s 586us/step - loss: 2.5654\n16/16 [==============================] - 0s 1ms/step - loss: 2.5987\n16/16 [==============================] - 0s 601us/step - loss: 2.5514\n16/16 [==============================] - 0s 615us/step - loss: 2.4948\n16/16 [==============================] - 0s 952us/step - loss: 2.4484\n16/16 [==============================] - 0s 996us/step - loss: 2.4143\n16/16 [==============================] - 0s 1ms/step - loss: 2.4035\n16/16 [==============================] - 0s 1ms/step - loss: 2.3993\n\nTesting for epoch 51 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1408\n16/16 [==============================] - 0s 1ms/step - loss: 2.1433\n16/16 [==============================] - 0s 1ms/step - loss: 2.5741\n16/16 [==============================] - 0s 1ms/step - loss: 2.6115\n16/16 [==============================] - 0s 684us/step - loss: 2.5677\n16/16 [==============================] - 0s 566us/step - loss: 2.5132\n16/16 [==============================] - 0s 617us/step - loss: 2.4685\n16/16 [==============================] - 0s 1ms/step - loss: 2.4356\n16/16 [==============================] - 0s 1ms/step - loss: 2.4250\n16/16 [==============================] - 0s 977us/step - loss: 2.4208\n\nTesting for epoch 51 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1386\n16/16 [==============================] - 0s 626us/step - loss: 2.2159\n16/16 [==============================] - 0s 607us/step - loss: 2.6627\n16/16 [==============================] - 0s 1ms/step - loss: 2.7016\n16/16 [==============================] - 0s 616us/step - loss: 2.6579\n16/16 [==============================] - 0s 624us/step - loss: 2.6043\n16/16 [==============================] - 0s 593us/step - loss: 2.5597\n16/16 [==============================] - 0s 1ms/step - loss: 2.5264\n16/16 [==============================] - 0s 596us/step - loss: 2.5158\n16/16 [==============================] - 0s 1ms/step - loss: 2.5116\n\nTesting for epoch 51 index 5:\n16/16 [==============================] - 0s 988us/step - loss: 0.1406\n16/16 [==============================] - 0s 819us/step - loss: 2.1877\n16/16 [==============================] - 0s 1ms/step - loss: 2.6298\n16/16 [==============================] - 0s 1ms/step - loss: 2.6666\n16/16 [==============================] - 0s 1ms/step - loss: 2.6212\n16/16 [==============================] - 0s 611us/step - loss: 2.5652\n16/16 [==============================] - 0s 1ms/step - loss: 2.5194\n16/16 [==============================] - 0s 1ms/step - loss: 2.4854\n16/16 [==============================] - 0s 787us/step - loss: 2.4747\n16/16 [==============================] - 0s 1ms/step - loss: 2.4704\nEpoch 52 of 60\n\nTesting for epoch 52 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1391\n16/16 [==============================] - 0s 1ms/step - loss: 2.1726\n16/16 [==============================] - 0s 1ms/step - loss: 2.6126\n16/16 [==============================] - 0s 1ms/step - loss: 2.6499\n16/16 [==============================] - 0s 622us/step - loss: 2.6046\n16/16 [==============================] - 0s 1ms/step - loss: 2.5491\n16/16 [==============================] - 0s 602us/step - loss: 2.5038\n16/16 [==============================] - 0s 1ms/step - loss: 2.4703\n16/16 [==============================] - 0s 1ms/step - loss: 2.4596\n16/16 [==============================] - 0s 625us/step - loss: 2.4553\n\nTesting for epoch 52 index 2:\n16/16 [==============================] - 0s 722us/step - loss: 0.1402\n16/16 [==============================] - 0s 1ms/step - loss: 2.1077\n16/16 [==============================] - 0s 1ms/step - loss: 2.5214\n16/16 [==============================] - 0s 1ms/step - loss: 2.5507\n16/16 [==============================] - 0s 1ms/step - loss: 2.5038\n16/16 [==============================] - 0s 638us/step - loss: 2.4486\n16/16 [==============================] - 0s 1ms/step - loss: 2.4039\n16/16 [==============================] - 0s 1ms/step - loss: 2.3711\n16/16 [==============================] - 0s 1ms/step - loss: 2.3608\n16/16 [==============================] - 0s 806us/step - loss: 2.3567\n\nTesting for epoch 52 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1413\n16/16 [==============================] - 0s 1ms/step - loss: 2.0789\n16/16 [==============================] - 0s 635us/step - loss: 2.4879\n16/16 [==============================] - 0s 827us/step - loss: 2.5226\n16/16 [==============================] - 0s 1ms/step - loss: 2.4818\n16/16 [==============================] - 0s 1ms/step - loss: 2.4316\n16/16 [==============================] - 0s 1ms/step - loss: 2.3905\n16/16 [==============================] - 0s 1ms/step - loss: 2.3598\n16/16 [==============================] - 0s 702us/step - loss: 2.3501\n16/16 [==============================] - 0s 672us/step - loss: 2.3462\n\nTesting for epoch 52 index 4:\n16/16 [==============================] - 0s 645us/step - loss: 0.1397\n16/16 [==============================] - 0s 615us/step - loss: 2.2173\n16/16 [==============================] - 0s 828us/step - loss: 2.6506\n16/16 [==============================] - 0s 1ms/step - loss: 2.6828\n16/16 [==============================] - 0s 1ms/step - loss: 2.6351\n16/16 [==============================] - 0s 1ms/step - loss: 2.5777\n16/16 [==============================] - 0s 603us/step - loss: 2.5312\n16/16 [==============================] - 0s 586us/step - loss: 2.4972\n16/16 [==============================] - 0s 874us/step - loss: 2.4864\n16/16 [==============================] - 0s 806us/step - loss: 2.4822\n\nTesting for epoch 52 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1400\n16/16 [==============================] - 0s 1ms/step - loss: 2.1024\n16/16 [==============================] - 0s 1ms/step - loss: 2.5108\n16/16 [==============================] - 0s 1ms/step - loss: 2.5405\n16/16 [==============================] - 0s 656us/step - loss: 2.4959\n16/16 [==============================] - 0s 1ms/step - loss: 2.4414\n16/16 [==============================] - 0s 1ms/step - loss: 2.3972\n16/16 [==============================] - 0s 928us/step - loss: 2.3648\n16/16 [==============================] - 0s 1ms/step - loss: 2.3547\n16/16 [==============================] - 0s 1ms/step - loss: 2.3508\nEpoch 53 of 60\n\nTesting for epoch 53 index 1:\n16/16 [==============================] - 0s 638us/step - loss: 0.1365\n16/16 [==============================] - 0s 618us/step - loss: 2.0904\n16/16 [==============================] - 0s 1ms/step - loss: 2.4896\n16/16 [==============================] - 0s 894us/step - loss: 2.5170\n16/16 [==============================] - 0s 1ms/step - loss: 2.4731\n16/16 [==============================] - 0s 995us/step - loss: 2.4215\n16/16 [==============================] - 0s 1ms/step - loss: 2.3790\n16/16 [==============================] - 0s 625us/step - loss: 2.3476\n16/16 [==============================] - 0s 634us/step - loss: 2.3377\n16/16 [==============================] - 0s 645us/step - loss: 2.3337\n\nTesting for epoch 53 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1351\n16/16 [==============================] - 0s 1ms/step - loss: 2.1565\n16/16 [==============================] - 0s 618us/step - loss: 2.5687\n16/16 [==============================] - 0s 762us/step - loss: 2.5966\n16/16 [==============================] - 0s 1ms/step - loss: 2.5494\n16/16 [==============================] - 0s 1ms/step - loss: 2.4930\n16/16 [==============================] - 0s 648us/step - loss: 2.4471\n16/16 [==============================] - 0s 608us/step - loss: 2.4134\n16/16 [==============================] - 0s 959us/step - loss: 2.4028\n16/16 [==============================] - 0s 1ms/step - loss: 2.3986\n\nTesting for epoch 53 index 3:\n16/16 [==============================] - 0s 809us/step - loss: 0.1389\n16/16 [==============================] - 0s 1ms/step - loss: 2.1419\n16/16 [==============================] - 0s 1ms/step - loss: 2.5512\n16/16 [==============================] - 0s 677us/step - loss: 2.5797\n16/16 [==============================] - 0s 668us/step - loss: 2.5344\n16/16 [==============================] - 0s 671us/step - loss: 2.4806\n16/16 [==============================] - 0s 657us/step - loss: 2.4371\n16/16 [==============================] - 0s 1ms/step - loss: 2.4050\n16/16 [==============================] - 0s 652us/step - loss: 2.3948\n16/16 [==============================] - 0s 1ms/step - loss: 2.3908\n\nTesting for epoch 53 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1349\n16/16 [==============================] - 0s 1ms/step - loss: 2.1901\n16/16 [==============================] - 0s 716us/step - loss: 2.6044\n16/16 [==============================] - 0s 1ms/step - loss: 2.6295\n16/16 [==============================] - 0s 1ms/step - loss: 2.5800\n16/16 [==============================] - 0s 632us/step - loss: 2.5219\n16/16 [==============================] - 0s 1ms/step - loss: 2.4752\n16/16 [==============================] - 0s 1ms/step - loss: 2.4411\n16/16 [==============================] - 0s 1ms/step - loss: 2.4303\n16/16 [==============================] - 0s 608us/step - loss: 2.4260\n\nTesting for epoch 53 index 5:\n16/16 [==============================] - 0s 626us/step - loss: 0.1361\n16/16 [==============================] - 0s 1ms/step - loss: 2.1862\n16/16 [==============================] - 0s 634us/step - loss: 2.6026\n16/16 [==============================] - 0s 1ms/step - loss: 2.6289\n16/16 [==============================] - 0s 602us/step - loss: 2.5794\n16/16 [==============================] - 0s 608us/step - loss: 2.5210\n16/16 [==============================] - 0s 945us/step - loss: 2.4747\n16/16 [==============================] - 0s 1ms/step - loss: 2.4412\n16/16 [==============================] - 0s 1ms/step - loss: 2.4307\n16/16 [==============================] - 0s 1ms/step - loss: 2.4265\nEpoch 54 of 60\n\nTesting for epoch 54 index 1:\n16/16 [==============================] - 0s 591us/step - loss: 0.1354\n16/16 [==============================] - 0s 716us/step - loss: 2.1140\n16/16 [==============================] - 0s 645us/step - loss: 2.5078\n16/16 [==============================] - 0s 1ms/step - loss: 2.5307\n16/16 [==============================] - 0s 576us/step - loss: 2.4835\n16/16 [==============================] - 0s 1ms/step - loss: 2.4278\n16/16 [==============================] - 0s 1ms/step - loss: 2.3835\n16/16 [==============================] - 0s 721us/step - loss: 2.3512\n16/16 [==============================] - 0s 683us/step - loss: 2.3411\n16/16 [==============================] - 0s 973us/step - loss: 2.3371\n\nTesting for epoch 54 index 2:\n16/16 [==============================] - 0s 746us/step - loss: 0.1375\n16/16 [==============================] - 0s 1ms/step - loss: 2.1227\n16/16 [==============================] - 0s 1ms/step - loss: 2.5193\n16/16 [==============================] - 0s 1ms/step - loss: 2.5434\n16/16 [==============================] - 0s 1ms/step - loss: 2.4969\n16/16 [==============================] - 0s 928us/step - loss: 2.4432\n16/16 [==============================] - 0s 851us/step - loss: 2.4006\n16/16 [==============================] - 0s 661us/step - loss: 2.3696\n16/16 [==============================] - 0s 980us/step - loss: 2.3598\n16/16 [==============================] - 0s 1ms/step - loss: 2.3560\n\nTesting for epoch 54 index 3:\n16/16 [==============================] - 0s 939us/step - loss: 0.1385\n16/16 [==============================] - 0s 1ms/step - loss: 2.1281\n16/16 [==============================] - 0s 629us/step - loss: 2.5285\n16/16 [==============================] - 0s 1ms/step - loss: 2.5562\n16/16 [==============================] - 0s 598us/step - loss: 2.5119\n16/16 [==============================] - 0s 633us/step - loss: 2.4583\n16/16 [==============================] - 0s 638us/step - loss: 2.4151\n16/16 [==============================] - 0s 639us/step - loss: 2.3836\n16/16 [==============================] - 0s 900us/step - loss: 2.3737\n16/16 [==============================] - 0s 1ms/step - loss: 2.3698\n\nTesting for epoch 54 index 4:\n16/16 [==============================] - 0s 650us/step - loss: 0.1350\n16/16 [==============================] - 0s 636us/step - loss: 2.1239\n16/16 [==============================] - 0s 1ms/step - loss: 2.5171\n16/16 [==============================] - 0s 853us/step - loss: 2.5400\n16/16 [==============================] - 0s 596us/step - loss: 2.4914\n16/16 [==============================] - 0s 619us/step - loss: 2.4353\n16/16 [==============================] - 0s 1ms/step - loss: 2.3913\n16/16 [==============================] - 0s 1ms/step - loss: 2.3594\n16/16 [==============================] - 0s 1ms/step - loss: 2.3494\n16/16 [==============================] - 0s 665us/step - loss: 2.3455\n\nTesting for epoch 54 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1328\n16/16 [==============================] - 0s 1ms/step - loss: 2.1880\n16/16 [==============================] - 0s 1ms/step - loss: 2.5967\n16/16 [==============================] - 0s 619us/step - loss: 2.6189\n16/16 [==============================] - 0s 603us/step - loss: 2.5676\n16/16 [==============================] - 0s 1ms/step - loss: 2.5074\n16/16 [==============================] - 0s 1ms/step - loss: 2.4597\n16/16 [==============================] - 0s 1ms/step - loss: 2.4251\n16/16 [==============================] - 0s 1ms/step - loss: 2.4143\n16/16 [==============================] - 0s 1ms/step - loss: 2.4100\nEpoch 55 of 60\n\nTesting for epoch 55 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1320\n16/16 [==============================] - 0s 624us/step - loss: 2.2799\n16/16 [==============================] - 0s 824us/step - loss: 2.7184\n16/16 [==============================] - 0s 644us/step - loss: 2.7484\n16/16 [==============================] - 0s 655us/step - loss: 2.6979\n16/16 [==============================] - 0s 690us/step - loss: 2.6378\n16/16 [==============================] - 0s 1ms/step - loss: 2.5895\n16/16 [==============================] - 0s 637us/step - loss: 2.5547\n16/16 [==============================] - 0s 806us/step - loss: 2.5438\n16/16 [==============================] - 0s 757us/step - loss: 2.5395\n\nTesting for epoch 55 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1319\n16/16 [==============================] - 0s 1ms/step - loss: 2.2685\n16/16 [==============================] - 0s 1ms/step - loss: 2.6951\n16/16 [==============================] - 0s 1ms/step - loss: 2.7210\n16/16 [==============================] - 0s 1ms/step - loss: 2.6693\n16/16 [==============================] - 0s 905us/step - loss: 2.6086\n16/16 [==============================] - 0s 985us/step - loss: 2.5598\n16/16 [==============================] - 0s 897us/step - loss: 2.5247\n16/16 [==============================] - 0s 1ms/step - loss: 2.5138\n16/16 [==============================] - 0s 726us/step - loss: 2.5094\n\nTesting for epoch 55 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1306\n16/16 [==============================] - 0s 628us/step - loss: 2.1294\n16/16 [==============================] - 0s 571us/step - loss: 2.5162\n16/16 [==============================] - 0s 651us/step - loss: 2.5364\n16/16 [==============================] - 0s 657us/step - loss: 2.4872\n16/16 [==============================] - 0s 620us/step - loss: 2.4317\n16/16 [==============================] - 0s 627us/step - loss: 2.3874\n16/16 [==============================] - 0s 631us/step - loss: 2.3554\n16/16 [==============================] - 0s 911us/step - loss: 2.3455\n16/16 [==============================] - 0s 1ms/step - loss: 2.3416\n\nTesting for epoch 55 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1298\n16/16 [==============================] - 0s 633us/step - loss: 2.2952\n16/16 [==============================] - 0s 1ms/step - loss: 2.7216\n16/16 [==============================] - 0s 1ms/step - loss: 2.7446\n16/16 [==============================] - 0s 1ms/step - loss: 2.6897\n16/16 [==============================] - 0s 611us/step - loss: 2.6254\n16/16 [==============================] - 0s 1ms/step - loss: 2.5745\n16/16 [==============================] - 0s 1ms/step - loss: 2.5375\n16/16 [==============================] - 0s 1ms/step - loss: 2.5260\n16/16 [==============================] - 0s 935us/step - loss: 2.5215\n\nTesting for epoch 55 index 5:\n16/16 [==============================] - 0s 604us/step - loss: 0.1304\n16/16 [==============================] - 0s 1ms/step - loss: 2.1990\n16/16 [==============================] - 0s 598us/step - loss: 2.6016\n16/16 [==============================] - 0s 1ms/step - loss: 2.6241\n16/16 [==============================] - 0s 1ms/step - loss: 2.5736\n16/16 [==============================] - 0s 753us/step - loss: 2.5147\n16/16 [==============================] - 0s 1ms/step - loss: 2.4677\n16/16 [==============================] - 0s 626us/step - loss: 2.4338\n16/16 [==============================] - 0s 718us/step - loss: 2.4233\n16/16 [==============================] - 0s 852us/step - loss: 2.4191\nEpoch 56 of 60\n\nTesting for epoch 56 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1279\n16/16 [==============================] - 0s 629us/step - loss: 2.1722\n16/16 [==============================] - 0s 1ms/step - loss: 2.5696\n16/16 [==============================] - 0s 1ms/step - loss: 2.5888\n16/16 [==============================] - 0s 687us/step - loss: 2.5353\n16/16 [==============================] - 0s 644us/step - loss: 2.4735\n16/16 [==============================] - 0s 1ms/step - loss: 2.4255\n16/16 [==============================] - 0s 1ms/step - loss: 2.3910\n16/16 [==============================] - 0s 1ms/step - loss: 2.3803\n16/16 [==============================] - 0s 1ms/step - loss: 2.3761\n\nTesting for epoch 56 index 2:\n16/16 [==============================] - 0s 725us/step - loss: 0.1313\n16/16 [==============================] - 0s 1ms/step - loss: 2.1862\n16/16 [==============================] - 0s 644us/step - loss: 2.5858\n16/16 [==============================] - 0s 1ms/step - loss: 2.6081\n16/16 [==============================] - 0s 1ms/step - loss: 2.5577\n16/16 [==============================] - 0s 1ms/step - loss: 2.4986\n16/16 [==============================] - 0s 1ms/step - loss: 2.4523\n16/16 [==============================] - 0s 1ms/step - loss: 2.4191\n16/16 [==============================] - 0s 708us/step - loss: 2.4087\n16/16 [==============================] - 0s 632us/step - loss: 2.4046\n\nTesting for epoch 56 index 3:\n16/16 [==============================] - 0s 620us/step - loss: 0.1281\n16/16 [==============================] - 0s 632us/step - loss: 2.2017\n16/16 [==============================] - 0s 607us/step - loss: 2.5988\n16/16 [==============================] - 0s 573us/step - loss: 2.6178\n16/16 [==============================] - 0s 653us/step - loss: 2.5652\n16/16 [==============================] - 0s 1ms/step - loss: 2.5044\n16/16 [==============================] - 0s 808us/step - loss: 2.4563\n16/16 [==============================] - 0s 648us/step - loss: 2.4216\n16/16 [==============================] - 0s 724us/step - loss: 2.4108\n16/16 [==============================] - 0s 727us/step - loss: 2.4066\n\nTesting for epoch 56 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1283\n16/16 [==============================] - 0s 643us/step - loss: 2.2432\n16/16 [==============================] - 0s 1ms/step - loss: 2.6497\n16/16 [==============================] - 0s 612us/step - loss: 2.6688\n16/16 [==============================] - 0s 1ms/step - loss: 2.6142\n16/16 [==============================] - 0s 837us/step - loss: 2.5514\n16/16 [==============================] - 0s 836us/step - loss: 2.5023\n16/16 [==============================] - 0s 1ms/step - loss: 2.4675\n16/16 [==============================] - 0s 641us/step - loss: 2.4567\n16/16 [==============================] - 0s 1ms/step - loss: 2.4524\n\nTesting for epoch 56 index 5:\n16/16 [==============================] - 0s 876us/step - loss: 0.1307\n16/16 [==============================] - 0s 1ms/step - loss: 2.2389\n16/16 [==============================] - 0s 1ms/step - loss: 2.6476\n16/16 [==============================] - 0s 1ms/step - loss: 2.6694\n16/16 [==============================] - 0s 646us/step - loss: 2.6174\n16/16 [==============================] - 0s 651us/step - loss: 2.5567\n16/16 [==============================] - 0s 630us/step - loss: 2.5086\n16/16 [==============================] - 0s 645us/step - loss: 2.4744\n16/16 [==============================] - 0s 1ms/step - loss: 2.4637\n16/16 [==============================] - 0s 634us/step - loss: 2.4595\nEpoch 57 of 60\n\nTesting for epoch 57 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1294\n16/16 [==============================] - 0s 1ms/step - loss: 2.2039\n16/16 [==============================] - 0s 1ms/step - loss: 2.6016\n16/16 [==============================] - 0s 1ms/step - loss: 2.6199\n16/16 [==============================] - 0s 618us/step - loss: 2.5666\n16/16 [==============================] - 0s 1ms/step - loss: 2.5066\n16/16 [==============================] - 0s 1ms/step - loss: 2.4594\n16/16 [==============================] - 0s 1ms/step - loss: 2.4255\n16/16 [==============================] - 0s 967us/step - loss: 2.4150\n16/16 [==============================] - 0s 594us/step - loss: 2.4109\n\nTesting for epoch 57 index 2:\n16/16 [==============================] - 0s 638us/step - loss: 0.1284\n16/16 [==============================] - 0s 633us/step - loss: 2.2483\n16/16 [==============================] - 0s 996us/step - loss: 2.6576\n16/16 [==============================] - 0s 1ms/step - loss: 2.6801\n16/16 [==============================] - 0s 610us/step - loss: 2.6274\n16/16 [==============================] - 0s 1ms/step - loss: 2.5661\n16/16 [==============================] - 0s 1ms/step - loss: 2.5171\n16/16 [==============================] - 0s 905us/step - loss: 2.4817\n16/16 [==============================] - 0s 1ms/step - loss: 2.4707\n16/16 [==============================] - 0s 1ms/step - loss: 2.4664\n\nTesting for epoch 57 index 3:\n16/16 [==============================] - 0s 629us/step - loss: 0.1308\n16/16 [==============================] - 0s 1ms/step - loss: 2.2241\n16/16 [==============================] - 0s 776us/step - loss: 2.6243\n16/16 [==============================] - 0s 1ms/step - loss: 2.6430\n16/16 [==============================] - 0s 1ms/step - loss: 2.5906\n16/16 [==============================] - 0s 647us/step - loss: 2.5309\n16/16 [==============================] - 0s 632us/step - loss: 2.4834\n16/16 [==============================] - 0s 617us/step - loss: 2.4493\n16/16 [==============================] - 0s 622us/step - loss: 2.4387\n16/16 [==============================] - 0s 619us/step - loss: 2.4346\n\nTesting for epoch 57 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1249\n16/16 [==============================] - 0s 1ms/step - loss: 2.2824\n16/16 [==============================] - 0s 886us/step - loss: 2.6944\n16/16 [==============================] - 0s 1ms/step - loss: 2.7119\n16/16 [==============================] - 0s 840us/step - loss: 2.6561\n16/16 [==============================] - 0s 1ms/step - loss: 2.5941\n16/16 [==============================] - 0s 749us/step - loss: 2.5456\n16/16 [==============================] - 0s 633us/step - loss: 2.5112\n16/16 [==============================] - 0s 631us/step - loss: 2.5004\n16/16 [==============================] - 0s 598us/step - loss: 2.4962\n\nTesting for epoch 57 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1268\n16/16 [==============================] - 0s 641us/step - loss: 2.2303\n16/16 [==============================] - 0s 987us/step - loss: 2.6315\n16/16 [==============================] - 0s 755us/step - loss: 2.6520\n16/16 [==============================] - 0s 1ms/step - loss: 2.6023\n16/16 [==============================] - 0s 1ms/step - loss: 2.5447\n16/16 [==============================] - 0s 1ms/step - loss: 2.4980\n16/16 [==============================] - 0s 1ms/step - loss: 2.4644\n16/16 [==============================] - 0s 1ms/step - loss: 2.4540\n16/16 [==============================] - 0s 1ms/step - loss: 2.4499\nEpoch 58 of 60\n\nTesting for epoch 58 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1253\n16/16 [==============================] - 0s 1ms/step - loss: 2.2804\n16/16 [==============================] - 0s 1ms/step - loss: 2.6909\n16/16 [==============================] - 0s 1ms/step - loss: 2.7047\n16/16 [==============================] - 0s 1ms/step - loss: 2.6461\n16/16 [==============================] - 0s 890us/step - loss: 2.5810\n16/16 [==============================] - 0s 1ms/step - loss: 2.5304\n16/16 [==============================] - 0s 1ms/step - loss: 2.4944\n16/16 [==============================] - 0s 1ms/step - loss: 2.4833\n16/16 [==============================] - 0s 1ms/step - loss: 2.4788\n\nTesting for epoch 58 index 2:\n16/16 [==============================] - 0s 749us/step - loss: 0.1268\n16/16 [==============================] - 0s 1ms/step - loss: 2.2590\n16/16 [==============================] - 0s 613us/step - loss: 2.6587\n16/16 [==============================] - 0s 617us/step - loss: 2.6734\n16/16 [==============================] - 0s 1ms/step - loss: 2.6186\n16/16 [==============================] - 0s 1ms/step - loss: 2.5570\n16/16 [==============================] - 0s 950us/step - loss: 2.5081\n16/16 [==============================] - 0s 1ms/step - loss: 2.4730\n16/16 [==============================] - 0s 661us/step - loss: 2.4621\n16/16 [==============================] - 0s 623us/step - loss: 2.4579\n\nTesting for epoch 58 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1254\n16/16 [==============================] - 0s 854us/step - loss: 2.2600\n16/16 [==============================] - 0s 1ms/step - loss: 2.6602\n16/16 [==============================] - 0s 655us/step - loss: 2.6753\n16/16 [==============================] - 0s 1ms/step - loss: 2.6206\n16/16 [==============================] - 0s 1ms/step - loss: 2.5580\n16/16 [==============================] - 0s 706us/step - loss: 2.5089\n16/16 [==============================] - 0s 1ms/step - loss: 2.4739\n16/16 [==============================] - 0s 1ms/step - loss: 2.4630\n16/16 [==============================] - 0s 1ms/step - loss: 2.4588\n\nTesting for epoch 58 index 4:\n16/16 [==============================] - 0s 681us/step - loss: 0.1267\n16/16 [==============================] - 0s 775us/step - loss: 2.2358\n16/16 [==============================] - 0s 608us/step - loss: 2.6324\n16/16 [==============================] - 0s 628us/step - loss: 2.6479\n16/16 [==============================] - 0s 610us/step - loss: 2.5940\n16/16 [==============================] - 0s 608us/step - loss: 2.5341\n16/16 [==============================] - 0s 637us/step - loss: 2.4871\n16/16 [==============================] - 0s 611us/step - loss: 2.4534\n16/16 [==============================] - 0s 641us/step - loss: 2.4430\n16/16 [==============================] - 0s 763us/step - loss: 2.4389\n\nTesting for epoch 58 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1232\n16/16 [==============================] - 0s 1ms/step - loss: 2.3007\n16/16 [==============================] - 0s 1ms/step - loss: 2.7090\n16/16 [==============================] - 0s 631us/step - loss: 2.7214\n16/16 [==============================] - 0s 973us/step - loss: 2.6639\n16/16 [==============================] - 0s 667us/step - loss: 2.5993\n16/16 [==============================] - 0s 1ms/step - loss: 2.5482\n16/16 [==============================] - 0s 1ms/step - loss: 2.5115\n16/16 [==============================] - 0s 1ms/step - loss: 2.5002\n16/16 [==============================] - 0s 1ms/step - loss: 2.4957\nEpoch 59 of 60\n\nTesting for epoch 59 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1249\n16/16 [==============================] - 0s 703us/step - loss: 2.2634\n16/16 [==============================] - 0s 1ms/step - loss: 2.6562\n16/16 [==============================] - 0s 1ms/step - loss: 2.6629\n16/16 [==============================] - 0s 1ms/step - loss: 2.6041\n16/16 [==============================] - 0s 872us/step - loss: 2.5390\n16/16 [==============================] - 0s 617us/step - loss: 2.4883\n16/16 [==============================] - 0s 687us/step - loss: 2.4527\n16/16 [==============================] - 0s 615us/step - loss: 2.4419\n16/16 [==============================] - 0s 783us/step - loss: 2.4376\n\nTesting for epoch 59 index 2:\n16/16 [==============================] - 0s 700us/step - loss: 0.1249\n16/16 [==============================] - 0s 601us/step - loss: 2.2865\n16/16 [==============================] - 0s 625us/step - loss: 2.6880\n16/16 [==============================] - 0s 581us/step - loss: 2.7004\n16/16 [==============================] - 0s 1ms/step - loss: 2.6446\n16/16 [==============================] - 0s 714us/step - loss: 2.5815\n16/16 [==============================] - 0s 1ms/step - loss: 2.5315\n16/16 [==============================] - 0s 1ms/step - loss: 2.4957\n16/16 [==============================] - 0s 740us/step - loss: 2.4847\n16/16 [==============================] - 0s 1ms/step - loss: 2.4803\n\nTesting for epoch 59 index 3:\n16/16 [==============================] - 0s 584us/step - loss: 0.1245\n16/16 [==============================] - 0s 787us/step - loss: 2.2729\n16/16 [==============================] - 0s 1ms/step - loss: 2.6637\n16/16 [==============================] - 0s 1ms/step - loss: 2.6712\n16/16 [==============================] - 0s 608us/step - loss: 2.6147\n16/16 [==============================] - 0s 654us/step - loss: 2.5526\n16/16 [==============================] - 0s 1ms/step - loss: 2.5033\n16/16 [==============================] - 0s 891us/step - loss: 2.4684\n16/16 [==============================] - 0s 697us/step - loss: 2.4576\n16/16 [==============================] - 0s 639us/step - loss: 2.4534\n\nTesting for epoch 59 index 4:\n16/16 [==============================] - 0s 637us/step - loss: 0.1250\n16/16 [==============================] - 0s 941us/step - loss: 2.2686\n16/16 [==============================] - 0s 599us/step - loss: 2.6612\n16/16 [==============================] - 0s 1ms/step - loss: 2.6698\n16/16 [==============================] - 0s 778us/step - loss: 2.6129\n16/16 [==============================] - 0s 691us/step - loss: 2.5509\n16/16 [==============================] - 0s 1ms/step - loss: 2.5026\n16/16 [==============================] - 0s 623us/step - loss: 2.4683\n16/16 [==============================] - 0s 885us/step - loss: 2.4578\n16/16 [==============================] - 0s 701us/step - loss: 2.4537\n\nTesting for epoch 59 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1234\n16/16 [==============================] - 0s 1ms/step - loss: 2.3238\n16/16 [==============================] - 0s 803us/step - loss: 2.7242\n16/16 [==============================] - 0s 601us/step - loss: 2.7327\n16/16 [==============================] - 0s 647us/step - loss: 2.6743\n16/16 [==============================] - 0s 970us/step - loss: 2.6098\n16/16 [==============================] - 0s 926us/step - loss: 2.5591\n16/16 [==============================] - 0s 1ms/step - loss: 2.5229\n16/16 [==============================] - 0s 1ms/step - loss: 2.5118\n16/16 [==============================] - 0s 678us/step - loss: 2.5074\nEpoch 60 of 60\n\nTesting for epoch 60 index 1:\n16/16 [==============================] - 0s 642us/step - loss: 0.1248\n16/16 [==============================] - 0s 597us/step - loss: 2.2835\n16/16 [==============================] - 0s 629us/step - loss: 2.6774\n16/16 [==============================] - 0s 1ms/step - loss: 2.6863\n16/16 [==============================] - 0s 644us/step - loss: 2.6273\n16/16 [==============================] - 0s 1ms/step - loss: 2.5622\n16/16 [==============================] - 0s 956us/step - loss: 2.5114\n16/16 [==============================] - 0s 664us/step - loss: 2.4755\n16/16 [==============================] - 0s 875us/step - loss: 2.4643\n16/16 [==============================] - 0s 593us/step - loss: 2.4600\n\nTesting for epoch 60 index 2:\n16/16 [==============================] - 0s 716us/step - loss: 0.1237\n16/16 [==============================] - 0s 626us/step - loss: 2.2727\n16/16 [==============================] - 0s 1ms/step - loss: 2.6608\n16/16 [==============================] - 0s 1ms/step - loss: 2.6688\n16/16 [==============================] - 0s 1ms/step - loss: 2.6110\n16/16 [==============================] - 0s 919us/step - loss: 2.5473\n16/16 [==============================] - 0s 1ms/step - loss: 2.4975\n16/16 [==============================] - 0s 780us/step - loss: 2.4622\n16/16 [==============================] - 0s 716us/step - loss: 2.4513\n16/16 [==============================] - 0s 638us/step - loss: 2.4470\n\nTesting for epoch 60 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1213\n16/16 [==============================] - 0s 616us/step - loss: 2.3241\n16/16 [==============================] - 0s 618us/step - loss: 2.7152\n16/16 [==============================] - 0s 1ms/step - loss: 2.7211\n16/16 [==============================] - 0s 1ms/step - loss: 2.6613\n16/16 [==============================] - 0s 631us/step - loss: 2.5967\n16/16 [==============================] - 0s 741us/step - loss: 2.5461\n16/16 [==============================] - 0s 633us/step - loss: 2.5105\n16/16 [==============================] - 0s 2ms/step - loss: 2.4996\n16/16 [==============================] - 0s 599us/step - loss: 2.4952\n\nTesting for epoch 60 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1217\n16/16 [==============================] - 0s 1ms/step - loss: 2.3310\n16/16 [==============================] - 0s 1ms/step - loss: 2.7333\n16/16 [==============================] - 0s 1ms/step - loss: 2.7466\n16/16 [==============================] - 0s 1ms/step - loss: 2.6903\n16/16 [==============================] - 0s 833us/step - loss: 2.6273\n16/16 [==============================] - 0s 608us/step - loss: 2.5776\n16/16 [==============================] - 0s 1ms/step - loss: 2.5424\n16/16 [==============================] - 0s 626us/step - loss: 2.5314\n16/16 [==============================] - 0s 709us/step - loss: 2.5270\n\nTesting for epoch 60 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1211\n16/16 [==============================] - 0s 1ms/step - loss: 2.2989\n16/16 [==============================] - 0s 1ms/step - loss: 2.6855\n16/16 [==============================] - 0s 1ms/step - loss: 2.6923\n16/16 [==============================] - 0s 1ms/step - loss: 2.6322\n16/16 [==============================] - 0s 650us/step - loss: 2.5665\n16/16 [==============================] - 0s 1ms/step - loss: 2.5155\n16/16 [==============================] - 0s 1ms/step - loss: 2.4800\n16/16 [==============================] - 0s 844us/step - loss: 2.4690\n16/16 [==============================] - 0s 593us/step - loss: 2.4647\n\n\n\noutlier_MO_GAAL_one = list(clf.labels_)\n\n\noutlier_MO_GAAL_one = list(map(lambda x: 1 if x==0  else -1,outlier_MO_GAAL_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_MO_GAAL_one,tab_bunny)\n\n\n_conf.conf(\"MO-GAAL (Liu et al., 2019)\")\n\n\n\n\nAccuracy: 0.952\nPrecision: 0.952\nRecall: 1.000\nF1 Score: 0.975\n\n\n\nthirteen = twelve.append(_conf.tab)\n\n\n\nLSCP\n\ndetectors = [KNN(), LOF(), OCSVM()]\nclf = LSCP(detectors)\nclf.fit(_df[['x', 'y','fnoise']])\n_df['LSCP_clf'] = clf.labels_\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/pyod/models/lscp.py:382: UserWarning: The number of histogram bins is greater than the number of classifiers, reducing n_bins to n_clf.\n  warnings.warn(\n\n\n\noutlier_LSCP_one = list(clf.labels_)\n\n\noutlier_LSCP_one = list(map(lambda x: 1 if x==0  else -1,outlier_LSCP_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_LSCP_one,tab_bunny)\n\n\n_conf.conf(\"LSCP (Zhao et al., 2019)\")\n\n\n\n\nAccuracy: 0.940\nPrecision: 0.996\nRecall: 0.941\nF1 Score: 0.967\n\n\n\nfourteen_bunny = thirteen.append(_conf.tab)"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#bunny-result",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#bunny-result",
    "title": "Class code for Comparison Study",
    "section": "Bunny Result",
    "text": "Bunny Result\n\nround(fourteen_bunny,4)\n\n\n\n\n\n  \n    \n      \n      Accuracy\n      Precision\n      Recall\n      F1\n    \n  \n  \n    \n      GODE\n      0.9948\n      0.9954\n      0.9992\n      0.9973\n    \n    \n      LOF (Breunig et al., 2000)\n      0.9285\n      0.9569\n      0.9685\n      0.9627\n    \n    \n      kNN (Ramaswamy et al., 2000)\n      0.9405\n      0.9960\n      0.9413\n      0.9679\n    \n    \n      CBLOF (He et al., 2003)\n      0.9776\n      0.9895\n      0.9870\n      0.9882\n    \n    \n      OCSVM (Sch ̈olkopf et al., 2001)\n      0.9321\n      0.9911\n      0.9371\n      0.9633\n    \n    \n      MCD (Hardin and Rocke, 2004)\n      0.9349\n      0.9929\n      0.9383\n      0.9648\n    \n    \n      Feature Bagging (Lazarevic and Kumar, 2005)\n      0.9149\n      0.9818\n      0.9278\n      0.9540\n    \n    \n      ABOD (Kriegel et al., 2008)\n      0.9768\n      0.9891\n      0.9866\n      0.9878\n    \n    \n      Isolation Forest (Liu et al., 2008)\n      0.7942\n      0.9947\n      0.7881\n      0.8794\n    \n    \n      HBOS (Goldstein and Dengel, 2012)\n      0.8953\n      0.9695\n      0.9190\n      0.9436\n    \n    \n      SOS (Janssens et al., 2012)\n      0.8953\n      0.9695\n      0.9190\n      0.9436\n    \n    \n      SO-GAAL (Liu et al., 2019)\n      0.9521\n      0.9521\n      1.0000\n      0.9754\n    \n    \n      MO-GAAL (Liu et al., 2019)\n      0.9521\n      0.9521\n      1.0000\n      0.9754\n    \n    \n      LSCP (Zhao et al., 2019)\n      0.9397\n      0.9956\n      0.9408\n      0.9674"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html",
    "href": "posts/GODE/2022-09-02-paper_simulation.html",
    "title": "Simulation",
    "section": "",
    "text": "Simulation"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#imports",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#imports",
    "title": "Simulation",
    "section": "imports",
    "text": "imports\n\nimport rpy2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport plotly.graph_objects as go\n\n\nimport tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport plotly.express as px\nimport warnings\nwarnings.simplefilter(\"ignore\", np.ComplexWarning)\nfrom haversine import haversine\nfrom IPython.display import HTML\n\n\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr\n\n\nfrom plotly.subplots import make_subplots"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#ebayesthresh",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#ebayesthresh",
    "title": "Simulation",
    "section": "EbayesThresh",
    "text": "EbayesThresh\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(EbayesThresh)\nset.seed(1)\nx <- rnorm(1000) + sample(c( runif(25,-7,7), rep(0,975)))\n#plot(x,type='l')\n#mu <- EbayesThresh::ebayesthresh(x,sdev=2)\n#lines(mu,col=2,lty=2,lwd=2)\n\n\nR + python\n- R환경에 있던 x를 가지고 오기\n\n%R -o x \n\n- R환경에 있는 ebayesthresh 함수를 가지고 오기\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\n\n\nxhat = np.array(ebayesthresh(FloatVector(x)))\n\n\n#plt.plot(x)\n#plt.plot(xhat)"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#시도-1",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#시도-1",
    "title": "Simulation",
    "section": "시도 1",
    "text": "시도 1\n\n_x = np.linspace(0,2,1000)\n_y1 = 5*_x\n_y = _y1 + x # x is epsilon\n\n\ndf1=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\nw=np.zeros((1000,1000))\n\n\nfor i in range(1000):\n    for j in range(1000):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nclass SIMUL:\n    def __init__(self,df):\n        self.df = df\n        self.y = df.y.to_numpy()\n        self.y1 = df.y1.to_numpy()\n        self.x = df.x.to_numpy()\n        self.n = len(self.y)\n        self.W = w\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)      \n    def fit(self,sd=5): # fit with ebayesthresh\n        self._eigen()\n        self.ybar = self.Psi.T @ self.y # fbar := graph fourier transform of f\n        self.power = self.ybar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.ybar**2),sd=sd))\n        self.ybar_threshed = np.where(self.power_threshed>0,self.ybar,0)\n        self.yhat = self.Psi@self.ybar_threshed\n        self.df = self.df.assign(yHat = self.yhat)\n        self.df = self.df.assign(Residual = self.df.y- self.df.yHat)\n        self.differ=(np.abs(self.y-self.yhat)-np.min(np.abs(self.y-self.yhat)))/(np.max(np.abs(self.y-self.yhat))-np.min(np.abs(self.y-self.yhat))) #color 표현은 위핸 표준화\n        self.df = self.df.assign(differ = self.differ)\n        #with plt.style.context('seaborn-dark'):\n            #plt.figure(figsize=(16,10))\n            #plt.scatter(self.x,self.y,c=self.differ3,cmap='Purples',s=50)\n            #plt.plot(self.x,self.yhat, 'k--')\n    def vis(self,ref=60):\n        fig = go.Figure()\n        fig.add_scatter(x=self.x,y=self.y,mode=\"markers\",marker=dict(size=2, color=\"#9fc5e8\"),name='y',opacity=0.7)\n        fig.add_scatter(x=self.x,y=self.yhat,mode=\"markers\",marker=dict(size=2, color=\"#000000\"),name='yhat',opacity=0.7)\n        fig.add_scatter(x=self.df.query('Residual**2>@ref')['x'],y=self.df.query('Residual**2>@ref')['y'],mode=\"markers\",marker=dict(size=3, color=\"#f20505\"),name='R square',opacity=1)\n        fig.add_trace(go.Scatter(x=self.x,y=self.y1,mode='lines',line_color='#0000FF',name='underline'))\n        fig.update_layout(width=1000,height=1000,autosize=False,margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n    def sub(self):\n        fig, axs = plt.subplots(2,2,figsize=(16,10))\n\n        axs[0,0].plot(self.power)\n        axs[0,0].plot(self.power_threshed)\n        axs[0,0].set_title('power_threshed')\n\n        axs[0,1].plot(self.power[1:])\n        axs[0,1].plot(self.power_threshed[1:])\n        axs[0,1].set_title('power_threshed 1:')\n\n        axs[1,0].plot(self.power[2:])\n        axs[1,0].plot(self.power_threshed[2:])\n        axs[1,0].set_title('power_threshed 2:')\n\n        axs[1,1].plot((self.df.Residual)**2)\n        axs[1,1].set_title('Residual square')\n\n        plt.tight_layout()\n        plt.show()\n    def subvis(self,ref=60):\n        fig = make_subplots(rows=2, cols=2, subplot_titles=(\"y\", \"yhat\", \"Residual Square\", \"Graph\"))\n                            \n        fig.add_scatter(x=self.x,y=self.y, mode=\"markers\",marker=dict(size=3, color=\"#9fc5e8\"),name='y',opacity=0.7,row=1,col=1)\n        fig.add_trace(go.Scatter(x=self.x,y=self.y1,mode='lines',line_color='#000000',name='underline'),row=1,col=1)\n        \n        fig.add_scatter(x=self.x,y=self.yhat, mode=\"markers\",marker=dict(size=3, color=\"#999999\"),name='yhat',opacity=0.7,row=1,col=2)\n        fig.add_trace(go.Scatter(x=self.x,y=self.y1,mode='lines',line_color='#000000',name='underline'),row=1,col=2)\n        \n        fig.add_scatter(x=self.df.query('Residual**2>@ref')['x'],y=self.df.query('Residual**2>@ref')['y'], mode=\"markers\",marker=dict(size=3, color=\"#f20505\"),name='R square',opacity=1,row=2,col=1)\n        fig.add_trace(go.Scatter(x=self.x,y=self.y1,mode='lines',line_color='#000000',name='underline'),row=2,col=1)\n        \n        fig.add_scatter(x=self.x,y=self.y, mode=\"markers\",marker=dict(size=3, color=\"#9fc5e8\"),name='y',opacity=0.7,row=2,col=2)        \n        fig.add_scatter(x=self.x,y=self.yhat, mode=\"markers\",marker=dict(size=3, color=\"#999999\"),name='yhat',opacity=0.7,row=2,col=2)        \n        fig.add_scatter(x=self.df.query('Residual**2>@ref')['x'],y=self.df.query('Residual**2>@ref')['y'], mode=\"markers\",marker=dict(size=3, color=\"#f20505\"),name='R square',opacity=1,row=2,col=2)\n        fig.add_trace(go.Scatter(x=self.x,y=self.y1,mode='lines',line_color='#000000',name='underline'),row=2,col=2)\n        \n        fig.update_xaxes(range=[0, 2], row=1, col=1)\n        fig.update_yaxes(range=[-5, 15], row=1, col=1)\n        \n        fig.update_xaxes(range=[0, 2], row=1, col=2)\n        fig.update_yaxes(range=[-5, 15], row=1, col=2)\n        \n        fig.update_xaxes(range=[0, 2], row=2, col=1)\n        fig.update_yaxes(range=[-5, 15], row=2, col=1)\n        \n        fig.update_xaxes(range=[0, 2], row=2, col=2)\n        fig.update_yaxes(range=[-5, 15], row=2, col=2)\n        \n        fig.update_layout(width=1000,height=1000,autosize=False,showlegend=False,title_text=\"The result\")\n        \n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n\n\nclass SIMUL2(SIMUL):\n    def fit2(self,sd=5,ref=60,cuts=0,cutf=995):\n        self.fit()\n        with plt.style.context('seaborn-dark'):\n            plt.figure(figsize=(16,10))\n            plt.scatter(self.x,self.y,c=self.differ3,cmap='Purples',s=50)\n            plt.scatter(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],color='red',s=50)\n            plt.plot(self.x,self.y1,'b--')\n            plt.plot(self.x[cuts:cutf],self.yhat[cuts:cutf], 'k--')\n    def fit3(self,sd=5,ref=30,ymin=-5,ymax=20,cuts=0,cutf=995):\n        self.fit()\n        with plt.style.context('seaborn-dark'):\n            fig, axs = plt.subplots(2,2,figsize=(16,10))\n\n            axs[0,0].scatter(self.x,self.y,c=self.differ,cmap='Purples',s=50)\n            axs[0,0].set_title('y')\n            axs[0,0].set_ylim([ymin,ymax])\n            \n\n            axs[0,1].plot(self.x[cuts:cutf],self.yhat[cuts:cutf], 'k')\n            axs[0,1].plot(self.x[cuts:cutf],self.y1[cuts:cutf], 'b',alpha=0.5)\n            axs[0,1].set_title('yhat')\n            axs[0,1].set_ylim([ymin,ymax])\n\n            axs[1,0].scatter(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],color='red',s=50,marker='*')\n            axs[1,0].plot(self.x[cuts:cutf],self.y1[cuts:cutf], 'b',alpha=0.5)\n            axs[1,0].set_title('Residual square')\n            axs[1,0].set_ylim([ymin,ymax])\n\n            axs[1,1].scatter(self.x,self.y,c=self.differ,cmap='Purples',s=50)\n            axs[1,1].plot(self.x[cuts:cutf],self.yhat[cuts:cutf], 'k')\n            axs[1,1].scatter(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],color='red',s=50,marker='*')\n            axs[1,1].set_title('Graph')\n            axs[1,1].set_ylim([ymin,ymax])\n\n            plt.tight_layout()\n            plt.show()\n\n\n_simul = SIMUL2(df1)\n\n\n_simul.fit3(sd=5,ref=20,ymin=-10,ymax=15)\n\n\n\n\n\n#_simul.vis()\n\n\n_simul.sub()\n\n\n\n\n\n#_simul.subvis()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#시도-2",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#시도-2",
    "title": "Simulation",
    "section": "시도 2",
    "text": "시도 2\n\n_x = np.linspace(0,2,1000)\n_y1 = 5*_x**2\n_y = _y1 + x # x is epsilon\n\n\ndf2=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul = SIMUL2(df2)\n\n\n_simul.fit3(sd=6,ref=20,ymin=-10,ymax=25)\n\n\n\n\n\n#_simul.vis()\n\n\n_simul.sub()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#시도-3",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#시도-3",
    "title": "Simulation",
    "section": "시도 3",
    "text": "시도 3\n\n_x = np.linspace(0,2,1000)\n_y1 = 5*_x**3 \n_y = _y1 + x # x is epsilon\n\n\ndf3=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul = SIMUL2(df3)\n\n\n_simul.fit3(ymin=-10,ymax=45)\n\n\n\n\n\n#_simul.vis()\n\n\n_simul.sub()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#시도-4",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#시도-4",
    "title": "Simulation",
    "section": "시도 4",
    "text": "시도 4\n\n_x = np.linspace(0,2,1000)\n_y1 = -2+ 3*np.cos(_x) + 1*np.cos(2*_x) + 5*np.cos(5*_x)\n_y = _y1 + x\n\n\n# _x = np.linspace(0,2,1000)\n# _y1 = 5*np.sin(_x) \n# _y = _y1 + x # x is epsilon\n\n\ndf4=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul = SIMUL2(df4)\n\n\n_simul.fit3(ref=10,ymin=-15,ymax=10)\n\n\n\n\n\n#_simul.vis()\n\n\n_simul.sub()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#시도-5",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#시도-5",
    "title": "Simulation",
    "section": "시도 5",
    "text": "시도 5\n\n# _x = np.linspace(0,2,1000)\n# _y1 =  3*np.cos(_x) + 1*np.cos(_x**2) + 0.5*np.cos(5*_x) \n# _y = _y1 + x # x is epsilon\n\n\n_x = np.linspace(0,2,1000)\n_y1 =  3*np.sin(_x) + 1*np.sin(_x**2) + 5*np.sin(5*_x) \n_y = _y1 + x # x is epsilon\n\n\ndf5=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul = SIMUL2(df5)\n\n\n_simul.fit3(ref=15,ymin=-10,ymax=15,cuts=5)\n\n\n\n\n\n#_simul.vis()\n\n\n_simul.sub()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#d-시도-1",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#d-시도-1",
    "title": "Simulation",
    "section": "3D 시도 1",
    "text": "3D 시도 1\n\n### Example 2\nnp.random.seed(777)\npi=np.pi\nn=1000\nang=np.linspace(-pi,pi-2*pi/n,n)\nr=2+np.sin(np.linspace(0,6*pi,n))\nvx=r*np.cos(ang)\nvy=r*np.sin(ang)\nf1=10*np.sin(np.linspace(0,3*pi,n))\nf = f1 + x\n\n# 1. \np=plt.figure(figsize=(12,4), dpi=200)  # Make figure object \n\n# 2. \nax=p.add_subplot(1,1,1, projection='3d')\nax.grid(False)\nax.ticklabel_format(style='sci', axis='x',scilimits=(0,0))\nax.ticklabel_format(style='sci', axis='y',scilimits=(0,0))\nax.ticklabel_format(style='sci', axis='z',scilimits=(0,0))\ntop = f\nbottom = np.zeros_like(top)\nwidth=depth=0.05\n#ax.bar3d(vx, vy, bottom, width, depth, top, shade=False)\nax.scatter3D(vx,vy,f,zdir='z',s=10,marker='.')\nax.scatter3D(vx,vy,f1,zdir='z',s=10,marker='.')\nax.bar3d(vx, vy, bottom, width, depth, 0, color='Black',shade=False)\nax.set_xlim(-3,3)\nax.set_ylim(-3,3)\nax.set_zlim(-10,10)\n\ndf = pd.DataFrame({'x' : vx, 'y' : vy, 'f' : f, 'f1' : f1})\n\n\nclass SIMUL:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.f.to_numpy()\n        self.f1 = df.f1.to_numpy()\n        self.x = df.x.to_numpy()\n        self.y = df.y.to_numpy()\n        self.n = len(self.f)\n        self.theta= None\n    def get_distance(self):\n        self.D = np.zeros([self.n,self.n])\n        locations = np.stack([self.x, self.y],axis=1)\n        for i in tqdm.tqdm(range(self.n)):\n            for j in range(i,self.n):\n                self.D[i,j]=np.linalg.norm(locations[i]-locations[j])\n        self.D = self.D + self.D.T\n    def get_weightmatrix(self,theta=1,beta=0.5,kappa=4000):\n        self.theta = theta\n        dist = np.where(self.D < kappa,self.D,0)\n        self.W = np.exp(-(dist/self.theta)**2)\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)       \n    def fit(self,sd=5,ref=60): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.f # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2),sd=sd))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(fHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.f- self.df.fHat)\n        self.dif=(np.abs(self.f-self.fhat)-np.min(np.abs(self.f-self.fhat)))/(np.max(np.abs(self.f-self.fhat))-np.min(np.abs(self.f-self.fhat)))\n        self.df = self.df.assign(dif = self.dif)\n        self.bottom = np.zeros_like(self.f)\n        self.width=0.05\n        self.depth=0.05\n        \n        fig, axs = plt.subplots(2,2,figsize=(16,16),subplot_kw={\"projection\":\"3d\"})\n        axs[0,0].grid(False)\n        axs[0,0].scatter3D(self.x,self.y,self.f,c=self.dif,cmap='winter',zdir='z',s=50,marker='.',alpha=0.2)\n        axs[0,0].plot3D(self.x,self.y,[0]*1000,'black')\n        axs[0,0].set_xlim(-3,3)\n        axs[0,0].set_ylim(-3,3)\n        axs[0,0].set_zlim(-10,10)\n        axs[0,0].view_init(elev=20., azim=40)\n        \n        axs[0,1].grid(False)\n        axs[0,1].scatter3D(self.x,self.y,self.fhat,color='black',zdir='z',s=50,marker='.',alpha=0.2)\n        axs[0,1].plot3D(self.x,self.y,self.f1,'blue')\n        axs[0,1].plot3D(self.x,self.y,[0]*1000,'black')\n        axs[0,1].set_xlim(-3,3)\n        axs[0,1].set_ylim(-3,3)\n        axs[0,1].set_zlim(-10,10)\n        axs[0,1].view_init(elev=20., azim=40)\n        \n        axs[1,0].grid(False)\n        axs[1,0].scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['f'],color='red',zdir='z',s=100,marker='.',alpha=1)\n        axs[1,0].plot3D(self.x,self.y,self.f1,'blue')\n        axs[1,0].plot3D(self.x,self.y,[0]*1000,'black')\n        axs[1,0].set_xlim(-3,3)\n        axs[1,0].set_ylim(-3,3)\n        axs[1,0].set_zlim(-10,10)\n        axs[1,0].view_init(elev=20., azim=40)\n        \n        axs[1,1].grid(False)\n        axs[1,1].scatter3D(self.x,self.y,self.f,c=self.dif,cmap='winter',zdir='z',s=50,marker='.',alpha=0.2)\n        axs[1,1].scatter3D(self.x,self.y,self.fhat,color='black',zdir='z',s=50,marker='.',alpha=0.2)\n        axs[1,1].scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['f'],color='red',zdir='z',s=100,marker='.',alpha=1)\n        axs[1,1].plot3D(self.x,self.y,self.f1,'black')\n        axs[1,1].plot3D(self.x,self.y,[0]*1000,'black')\n        axs[1,1].set_xlim(-3,3)\n        axs[1,1].set_ylim(-3,3)\n        axs[1,1].set_zlim(-10,10)\n        axs[1,1].view_init(elev=20., azim=40)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        # p = plt.figure(figsize=(16,16))\n        # ax = p.add_subplot(1,1,1, projection='3d')\n        # ax.grid(False)\n        # ax.ticklabel_format(style='sci', axis='x',scilimits=(0,0))\n        # ax.ticklabel_format(style='sci', axis='y',scilimits=(0,0))\n        # ax.ticklabel_format(style='sci', axis='z',scilimits=(0,0))\n        # ax.scatter3D(self.x,self.y,self.f,c=self.dif,cmap='winter',zdir='z',s=50,marker='.',alpha=0.2)\n        # ax.scatter3D(self.x,self.y,self.fhat,color='black',zdir='z',s=50,marker='.',alpha=0.2)\n        # #ax.plot3D(self.x,self.y,self.fhat,'black')\n        # ax.scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['f'],color='red',zdir='z',s=100,marker='.',alpha=1)\n        # ax.plot3D(self.x,self.y,self.f1,'black')\n        # ax.plot3D(self.x,self.y,[0]*1000,'black')\n        # ax.set_xlim(-3,3)\n        # ax.set_ylim(-3,3)\n        # ax.set_zlim(-10,10)\n    def vis(self,ref=60):\n        fig = go.Figure()\n        fig.add_scatter3d(x=self.x,y=self.y,z=self.f, mode=\"markers\",marker=dict(size=3, color=\"#9fc5e8\"),name='f',opacity=0.2)\n        fig.add_scatter3d(x=self.x,y=self.y,z=self.fhat, mode=\"markers\",marker=dict(size=3, color=\"#999999\"),name='fhat',opacity=0.2)\n        #fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=self.fhat,mode='lines',line_color='#000000'))\n        fig.add_scatter3d(x=self.df.query('Residual**2>@ref')['x'],y=self.df.query('Residual**2>@ref')['y'],z=self.df.query('Residual**2>@ref')['f'], mode=\"markers\",marker=dict(size=3, color=\"#f20505\"),name='R square',opacity=1)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=self.f1,mode='lines',line_color='#000000',name='underline'))\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=[0]*1000,mode='lines',line_color='#000000',name='z=0'))\n        fig.update_layout(width=1000,height=1000,autosize=False,margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n    def sub(self):\n        fig, axs = plt.subplots(2,2,figsize=(16,10))\n\n        axs[0,0].plot(_simul.power)\n        axs[0,0].plot(_simul.power_threshed)\n        axs[0,0].set_title('power_threshed')\n\n        axs[0,1].plot(_simul.power[1:])\n        axs[0,1].plot(_simul.power_threshed[1:])\n        axs[0,1].set_title('power_threshed 1:')\n\n        axs[1,0].plot(_simul.power[2:])\n        axs[1,0].plot(_simul.power_threshed[2:])\n        axs[1,0].set_title('power_threshed 2:')\n\n        axs[1,1].plot((_simul.df.Residual)**2)\n        axs[1,1].set_title('Residual square')\n\n        plt.tight_layout()\n        plt.show()\n    def subvis(self,ref=60):\n        fig = make_subplots(2,2,specs=[[{'type': 'surface'}, {'type': 'surface'}],[{'type': 'surface'}, {'type': 'surface'}]],subplot_titles=(\"f\", \"fhat\", \"Residual Square\", \"Graph\"))\n        \n        fig.add_scatter3d(x=self.x,y=self.y,z=self.f, mode=\"markers\",marker=dict(size=3, color=\"#9fc5e8\"),name='f',opacity=0.2,row=1,col=1)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=self.f1,mode='lines',line_color='#000000',name='underline'),row=1,col=1)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=[0]*1000,mode='lines',line_color='#000000',name='z=0'),row=1,col=1)\n        \n        fig.add_scatter3d(x=self.x,y=self.y,z=self.fhat, mode=\"markers\",marker=dict(size=3, color=\"#999999\"),name='fhat',opacity=0.2,row=1,col=2)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=self.f1,mode='lines',line_color='#000000',name='underline'),row=1,col=2)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=[0]*1000,mode='lines',line_color='#000000',name='z=0'),row=1,col=2)\n        \n        fig.add_scatter3d(x=self.df.query('Residual**2>@ref')['x'],y=self.df.query('Residual**2>@ref')['y'],z=self.df.query('Residual**2>@ref')['f'], mode=\"markers\",marker=dict(size=3, color=\"#f20505\"),name='R square',opacity=1,row=2,col=1)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=self.f1,mode='lines',line_color='#000000',name='underline'),row=2,col=1)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=[0]*1000,mode='lines',line_color='#000000',name='z=0'),row=2,col=1)\n        \n        fig.add_scatter3d(x=self.x,y=self.y,z=self.f, mode=\"markers\",marker=dict(size=3, color=\"#9fc5e8\"),name='f',opacity=0.2,row=2,col=2)        \n        fig.add_scatter3d(x=self.x,y=self.y,z=self.fhat, mode=\"markers\",marker=dict(size=3, color=\"#999999\"),name='fhat',opacity=0.2,row=2,col=2)        \n        fig.add_scatter3d(x=self.df.query('Residual**2>@ref')['x'],y=self.df.query('Residual**2>@ref')['y'],z=self.df.query('Residual**2>@ref')['f'], mode=\"markers\",marker=dict(size=3, color=\"#f20505\"),name='R square',opacity=1,row=2,col=2)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=self.f1,mode='lines',line_color='#000000',name='underline'),row=2,col=2)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=[0]*1000,mode='lines',line_color='#000000',name='z=0'),row=2,col=2)\n        \n        fig.update_layout(scene = dict(xaxis = dict(range=[-3,3],),\n                                         yaxis = dict(range=[-3,3],),\n                                         zaxis = dict(range=[-10,10],),),\n                                      width=1000,height=1000,autosize=False)\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n\n\n_simul = SIMUL(df)\n\n\n_simul.get_distance()\n\n100%|██████████| 1000/1000 [00:01<00:00, 532.20it/s]\n\n\n\n_simul.D[_simul.D>0].mean()\n\n2.6888234729389295\n\n\n\nplt.hist(_simul.D[_simul.D>0])\n\n(array([ 66308.,  64352.,  68358., 177302., 166964., 114648.,  94344.,\n        111136.,  75508.,  60080.]),\n array([0.00628415, 0.54637775, 1.08647135, 1.62656495, 2.16665855,\n        2.70675214, 3.24684574, 3.78693934, 4.32703294, 4.86712654,\n        5.40722013]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n\n_simul.get_weightmatrix(theta=(2.6888234729389295),kappa=2500) \n\n\n_simul.fit(sd=5,ref=20)\n\n\n\n\n\n#_simul.vis(ref=20)\n\n\n#_simul.subvis(ref=20)\n\n\n_simul.sub()\n\n\n\n\n\n#_simul.subvis()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#d-시도-2",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#d-시도-2",
    "title": "Simulation",
    "section": "3D 시도 2",
    "text": "3D 시도 2\n\n### Example 2\nnp.random.seed(777)\npi=np.pi\nn=1000\nang=np.linspace(-pi,pi-2*pi/n,n)\nr=2+np.sin(np.linspace(0,8*pi,n))\nvx=r*np.cos(ang)\nvy=r*np.sin(ang)\nf1=10*np.sin(np.linspace(0,3*pi,n))\nf = f1 + x\n\n\ndf1 = pd.DataFrame({'x' : vx, 'y' : vy, 'f' : f,'f1':f1})\n\n\n_simul = SIMUL(df1)\n\n\n_simul.get_distance()\n\n100%|██████████| 1000/1000 [00:01<00:00, 521.69it/s]\n\n\n\n_simul.D[_simul.D>0].mean()\n\n2.6984753461932702\n\n\n\nplt.hist(_simul.D[_simul.D>0])\n\n(array([ 63450.,  64118., 146970., 169756., 138202., 126198., 162650.,\n         75642.,  28416.,  23598.]),\n array([0.0062838 , 0.60565122, 1.20501864, 1.80438605, 2.40375347,\n        3.00312089, 3.6024883 , 4.20185572, 4.80122314, 5.40059055,\n        5.99995797]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n\n_simul.get_weightmatrix(theta=(2.6984753461932702),kappa=2500) \n\n\n_simul.fit(sd=5,ref=30)\n\n\n\n\n\n#_simul.vis(ref=50)\n\n\n_simul.sub()\n\n\n\n\n\n#_simul.subvis()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#d-시도-3",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#d-시도-3",
    "title": "Simulation",
    "section": "3D 시도 3",
    "text": "3D 시도 3\n\n### Example 2\nnp.random.seed(777)\npi=np.pi\nn=1000\nang=np.linspace(-pi,pi-2*pi/n,n)\nr=2+np.sin(np.linspace(0,6*pi,n))\nvx=r*np.cos(ang)\nvy=r*np.sin(ang)\nf1=10*np.sin(np.linspace(0,6*pi,n))\nf = f1 + x\n\n\ndf2 = pd.DataFrame({'x' : vx, 'y' : vy, 'f' : f,'f1':f1})\n\n\n_simul = SIMUL(df2)\n\n\n_simul.get_distance()\n\n100%|██████████| 1000/1000 [00:02<00:00, 463.80it/s]\n\n\n\n_simul.D[_simul.D>0].mean()\n\n2.6888234729389295\n\n\n\nplt.hist(_simul.D[_simul.D>0])\n\n(array([ 66308.,  64352.,  68358., 177302., 166964., 114648.,  94344.,\n        111136.,  75508.,  60080.]),\n array([0.00628415, 0.54637775, 1.08647135, 1.62656495, 2.16665855,\n        2.70675214, 3.24684574, 3.78693934, 4.32703294, 4.86712654,\n        5.40722013]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n\n_simul.get_weightmatrix(theta=(2.6984753461932702),kappa=2500) \n\n\n_simul.fit(sd=5,ref=30)\n\n\n\n\n\n#_simul.vis(ref=50)\n\n\n_simul.sub()\n\n\n\n\n\n#_simul.subvis()"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html",
    "title": "Earthquake",
    "section": "",
    "text": "Real analysis"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html#imports",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html#imports",
    "title": "Earthquake",
    "section": "imports",
    "text": "imports\n\nimport tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport plotly.express as px\nimport warnings\nwarnings.simplefilter(\"ignore\", np.ComplexWarning)\nfrom haversine import haversine\nfrom IPython.display import HTML\nimport plotly.graph_objects as go\n\n\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html#load-data-and-clean-it",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html#load-data-and-clean-it",
    "title": "Earthquake",
    "section": "load data and clean it",
    "text": "load data and clean it\n- load\n\ndf= pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      Date\n      Latitude\n      Longitude\n      Magnitude\n    \n  \n  \n    \n      0\n      01/02/1965\n      19.2460\n      145.6160\n      6.0\n    \n    \n      1\n      01/04/1965\n      1.8630\n      127.3520\n      5.8\n    \n    \n      2\n      01/05/1965\n      -20.5790\n      -173.9720\n      6.2\n    \n    \n      3\n      01/08/1965\n      -59.0760\n      -23.5570\n      5.8\n    \n    \n      4\n      01/09/1965\n      11.9380\n      126.4270\n      5.8\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      23407\n      12/28/2016\n      38.3917\n      -118.8941\n      5.6\n    \n    \n      23408\n      12/28/2016\n      38.3777\n      -118.8957\n      5.5\n    \n    \n      23409\n      12/28/2016\n      36.9179\n      140.4262\n      5.9\n    \n    \n      23410\n      12/29/2016\n      -9.0283\n      118.6639\n      6.3\n    \n    \n      23411\n      12/30/2016\n      37.3973\n      141.4103\n      5.5\n    \n  \n\n23412 rows × 4 columns\n\n\n\n\ndf_korea= pd.read_csv('earthquake_korea2.csv').iloc[:,[1,2,5,6]].rename(columns={'규모':'Magnitude'})\n\nhttps://www.weather.go.kr/w/eqk-vol/search/korea.do?schOption=&xls=0&startTm=2012-01-02&endTm=2022-06-17&startSize=2&endSize=&startLat=&endLat=&startLon=&endLon=&lat=&lon=&dist=&keyword=&dpType=m\n\ndf_global= pd.concat([pd.read_csv('00_05.csv'),pd.read_csv('05_10.csv'),pd.read_csv('10_15.csv'),pd.read_csv('15_20.csv')]).iloc[:,[0,1,2,4]].rename(columns={'latitude':'Latitude','longitude':'Longitude','mag':'Magnitude'}).reset_index().iloc[:,1:]\n\nhttps://www.usgs.gov/programs/earthquake-hazards/lists-maps-and-statistics\n- cleaning\n\ndf.Date[df.Date == '1975-02-23T02:58:41.000Z']\n\n3378    1975-02-23T02:58:41.000Z\nName: Date, dtype: object\n\n\n\ndf.iloc[3378,0] = '02/03/1975'\n\n\ndf.Date[df.Date == '1985-04-28T02:53:41.530Z']\n\n7512    1985-04-28T02:53:41.530Z\nName: Date, dtype: object\n\n\n\ndf.iloc[7512,0] = '04/28/1985'\n\n\ndf.Date[df.Date == '2011-03-13T02:23:34.520Z']\n\n20650    2011-03-13T02:23:34.520Z\nName: Date, dtype: object\n\n\n\ndf.iloc[20650,0] = '03/13/2011'\n\n\ndf= df.assign(Year=list(map(lambda x: x.split('/')[-1], df.Date))).iloc[:,1:]\ndf\n\n\n\n\n\n  \n    \n      \n      Latitude\n      Longitude\n      Magnitude\n      Year\n    \n  \n  \n    \n      0\n      19.2460\n      145.6160\n      6.0\n      1965\n    \n    \n      1\n      1.8630\n      127.3520\n      5.8\n      1965\n    \n    \n      2\n      -20.5790\n      -173.9720\n      6.2\n      1965\n    \n    \n      3\n      -59.0760\n      -23.5570\n      5.8\n      1965\n    \n    \n      4\n      11.9380\n      126.4270\n      5.8\n      1965\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      23407\n      38.3917\n      -118.8941\n      5.6\n      2016\n    \n    \n      23408\n      38.3777\n      -118.8957\n      5.5\n      2016\n    \n    \n      23409\n      36.9179\n      140.4262\n      5.9\n      2016\n    \n    \n      23410\n      -9.0283\n      118.6639\n      6.3\n      2016\n    \n    \n      23411\n      37.3973\n      141.4103\n      5.5\n      2016\n    \n  \n\n23412 rows × 4 columns\n\n\n\n\ndf.Year = df.Year.astype(np.float64)\n\n\ndf_korea = df_korea.assign(Year=list(map(lambda x: x.split('/')[0], df_korea.발생시각))).iloc[:,1:]\ndf_korea = df_korea.assign(Latitude=list(map(lambda x: x.split(' ')[0], df_korea.위도))).iloc[:,[0,2,3,4]]\ndf_korea = df_korea.assign(Longitude=list(map(lambda x: x.split(' ')[0], df_korea.경도))).iloc[:,[0,2,3,4]]\n\n\ndf_global = df_global.assign(Year=list(map(lambda x: x.split('-')[0], df_global.time))).iloc[:,1:]\n\n\ndf_korea.Year = df_korea.Year.astype(np.float64)\ndf_korea.Latitude = df_korea.Latitude.astype(np.float64)\ndf_korea.Longitude = df_korea.Longitude.astype(np.float64)\ndf_global.Year = df_global.Year.astype(np.float64)"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html#define-class",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html#define-class",
    "title": "Earthquake",
    "section": "define class",
    "text": "define class\n\nclass MooYaHo:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.Magnitude.to_numpy()\n        self.year = df.Year.to_numpy()\n        self.lat = df.Latitude.to_numpy()\n        self.long = df.Longitude.to_numpy()\n        self.n = len(self.f)\n        \n        self.theta= None\n    def get_distance(self):\n        self.D = np.zeros([self.n,self.n])\n        locations = np.stack([self.lat, self.long],axis=1)\n        for i in tqdm.tqdm(range(self.n)):\n            for j in range(i,self.n): \n                self.D[i,j]=haversine(locations[i],locations[j])\n        self.D = self.D+self.D.T\n    def get_weightmatrix(self,theta=1,beta=0.5,kappa=4000):\n        self.theta = theta\n        dist = np.where(self.D<kappa,self.D,0)\n        self.W = np.exp(-(dist/self.theta)**2)\n\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)        \n    def fit(self,m):\n        self._eigen()\n        self.fhat = self.Psi[:,0:m]@self.Psi[:,0:m].T@self.f\n        self.df = self.df.assign(MagnitudeHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.Magnitude- self.df.MagnitudeHat)\n        plt.plot(self.f,'.')\n        plt.plot(self.fhat,'x')\n        \n    def vis(self,MagThresh=7,ResThresh=1):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=5,\n                        center=dict(lat=37, lon=160), \n                        zoom=1.5,\n                        height=900,\n                        opacity = 0.4,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 8,\n                      marker_color= 'red',\n                      opacity = 0.6\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= 8,\n                      marker_color= 'blue',\n                      opacity = 0.5\n                      )\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n    def visf(self):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=5,\n                        center=dict(lat=37, lon=160), \n                        zoom=1.5,\n                        height=900,\n                        opacity = 0.7,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n    def visfhat(self):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='MagnitudeHat', \n                        radius=5,\n                        center=dict(lat=37, lon=160), \n                        zoom=1.5,\n                        height=900,\n                        opacity = 0.7,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n    def visres(self,MagThresh=7,ResThresh=1):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z=[0] * len(self.df), \n                        radius=5,\n                        center=dict(lat=37, lon=160), \n                        zoom=1.5,\n                        height=900,\n                        opacity = 0.7,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= 8,\n                      marker_color= 'blue',\n                      opacity = 0.7\n                      )\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n\n\nclass MooYaHo2(MooYaHo): # ebayesthresh 기능추가\n    def fit2(self,ref=0.5): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.f # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2)))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(MagnitudeHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.Magnitude- self.df.MagnitudeHat)\n        self.con = np.where(self.df.Residual>0.7,1,0)\n        #plt.plot(self.f,'.')\n        #plt.plot(self.fhat,'x')\n\n#         fig, axs = plt.subplots(2,2,figsize=(16,10))\n\n#         axs[0,0].plot(self.f,'b')\n#         axs[0,0].set_title('Magnitude')\n#         axs[0,0].set_ylim([4.5,9])\n\n#         axs[0,1].plot(self.fhat,'k')\n#         axs[0,1].set_title('MagnitudeHat')\n#         axs[0,1].set_ylim([4.5,9])\n\n#         axs[1,0].plot(self.con,'r*')\n#         axs[1,0].set_title('Residual square')\n\n#         axs[1,1].plot(self.f,'b')\n#         axs[1,1].plot(self.fhat,'k')\n#         axs[1,1].plot(self.con,'r*')\n#         axs[1,1].set_title('Graph')\n#         axs[1,1].set_ylim([4.5,9])\n\n#         plt.tight_layout()\n#         plt.show()\n\n\nclass MooYaHo3(MooYaHo2):\n    def vis(self,MagThresh=7,ResThresh=1):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=5,\n                        center=dict(lat=37, lon=126), \n                        zoom=5.7,\n                        height=900,\n                        opacity = 0.3,\n                        mapbox_style=\"stamen-terrain\")\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 8,\n                      marker_color= 'red',\n                      opacity = 0.5\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= 8,\n                      marker_color= 'blue',\n                      opacity = 0.5\n                      )\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n\n\n       ebayesthresh = importr('EbayesThresh').ebayesthresh"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html#analysis_df_global20102015",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html#analysis_df_global20102015",
    "title": "Earthquake",
    "section": "analysis_df_global(2010~2015)",
    "text": "analysis_df_global(2010~2015)\n- make instance for analysis\n\nmoo_global=MooYaHo2(df_global.query(\"2010 <= Year < 2015\"))\n\n- get distance\n\nmoo_global.get_distance()\n\n100%|██████████| 12498/12498 [03:14<00:00, 64.19it/s] \n\n\n\nmoo_global.D[moo_global.D>0].mean()\n\n8810.865423093777\n\n\n\nplt.hist(moo_global.D[moo_global.D>0])\n\n(array([14176290., 16005894., 21186674., 22331128., 19394182., 17548252.,\n        16668048., 13316436., 12973260.,  2582550.]),\n array([8.97930163e-02, 2.00141141e+03, 4.00273303e+03, 6.00405465e+03,\n        8.00537626e+03, 1.00066979e+04, 1.20080195e+04, 1.40093411e+04,\n        1.60106627e+04, 1.80119844e+04, 2.00133060e+04]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n- weight matrix\n\nmoo_global.get_weightmatrix(theta=(8810.865423093777),kappa=2500) \n\n- fit\n\nmoo_global.fit2()\n\n\nmoo_global.df.sort_values(\"Residual\",ascending=False).iloc[:40,:]\n\n\n\n\n\n  \n    \n      \n      Latitude\n      Longitude\n      Magnitude\n      Year\n      MagnitudeHat\n      Residual\n    \n  \n  \n    \n      11094\n      -36.1220\n      -72.8980\n      8.8\n      2010.0\n      7.754649\n      1.045351\n    \n    \n      10727\n      -36.2170\n      -73.2570\n      6.7\n      2010.0\n      5.750555\n      0.949445\n    \n    \n      27513\n      0.8020\n      92.4630\n      8.2\n      2012.0\n      7.253721\n      0.946279\n    \n    \n      26109\n      -10.9280\n      166.0180\n      7.1\n      2013.0\n      6.194467\n      0.905533\n    \n    \n      30291\n      38.0580\n      144.5900\n      7.7\n      2011.0\n      6.804163\n      0.895837\n    \n    \n      31673\n      -17.5410\n      168.0690\n      7.3\n      2010.0\n      6.431399\n      0.868601\n    \n    \n      27527\n      2.3270\n      93.0630\n      8.6\n      2012.0\n      7.736731\n      0.863269\n    \n    \n      30311\n      38.2970\n      142.3730\n      9.1\n      2011.0\n      8.255988\n      0.844012\n    \n    \n      24388\n      -19.6097\n      -70.7691\n      8.2\n      2014.0\n      7.394885\n      0.805115\n    \n    \n      28408\n      -28.9930\n      -176.2380\n      7.4\n      2011.0\n      6.596810\n      0.803190\n    \n    \n      23863\n      -55.4703\n      -28.3669\n      6.9\n      2014.0\n      6.097310\n      0.802690\n    \n    \n      32803\n      -36.1220\n      -72.8980\n      8.8\n      2010.0\n      7.999521\n      0.800479\n    \n    \n      27672\n      -22.1410\n      170.3400\n      6.6\n      2012.0\n      5.830724\n      0.769276\n    \n    \n      24958\n      -60.2738\n      -46.4011\n      7.7\n      2013.0\n      6.938609\n      0.761391\n    \n    \n      31196\n      -56.5860\n      -142.2920\n      6.4\n      2010.0\n      5.641422\n      0.758578\n    \n    \n      26943\n      2.1900\n      126.8370\n      6.6\n      2012.0\n      5.849246\n      0.750754\n    \n    \n      9118\n      -19.7020\n      167.9470\n      7.3\n      2010.0\n      6.550810\n      0.749190\n    \n    \n      32001\n      7.8810\n      91.9360\n      7.5\n      2010.0\n      6.785405\n      0.714595\n    \n    \n      31229\n      -3.4870\n      100.0820\n      7.8\n      2010.0\n      7.086996\n      0.713004\n    \n    \n      29640\n      38.2760\n      141.5880\n      7.1\n      2011.0\n      6.387718\n      0.712282\n    \n    \n      11356\n      18.4430\n      -72.5710\n      7.0\n      2010.0\n      6.288026\n      0.711974\n    \n    \n      30296\n      36.2810\n      141.1110\n      7.9\n      2011.0\n      7.197845\n      0.702155\n    \n    \n      28574\n      -21.6110\n      -179.5280\n      7.3\n      2011.0\n      6.601124\n      0.698876\n    \n    \n      23633\n      -32.6953\n      -71.4416\n      6.4\n      2014.0\n      5.705553\n      0.694447\n    \n    \n      25517\n      10.7010\n      -42.5940\n      6.6\n      2013.0\n      5.908930\n      0.691070\n    \n    \n      28001\n      -10.6170\n      165.1600\n      6.4\n      2012.0\n      5.722428\n      0.677572\n    \n    \n      25773\n      30.3080\n      102.8880\n      6.6\n      2013.0\n      5.922831\n      0.677169\n    \n    \n      29004\n      38.0340\n      143.2640\n      7.0\n      2011.0\n      6.332486\n      0.667514\n    \n    \n      23815\n      14.7240\n      -92.4614\n      6.9\n      2014.0\n      6.237697\n      0.662303\n    \n    \n      24360\n      -20.3113\n      -70.5756\n      6.5\n      2014.0\n      5.839327\n      0.660673\n    \n    \n      25633\n      -23.0090\n      -177.2320\n      7.4\n      2013.0\n      6.743190\n      0.656810\n    \n    \n      30256\n      36.8230\n      141.8240\n      6.1\n      2011.0\n      5.446211\n      0.653789\n    \n    \n      9520\n      -3.4870\n      100.0820\n      7.8\n      2010.0\n      7.146346\n      0.653654\n    \n    \n      29133\n      52.0500\n      -171.8360\n      7.3\n      2011.0\n      6.646652\n      0.653348\n    \n    \n      32492\n      -34.2900\n      -71.8910\n      6.9\n      2010.0\n      6.247025\n      0.652975\n    \n    \n      24066\n      4.2485\n      92.7574\n      6.0\n      2014.0\n      5.353450\n      0.646550\n    \n    \n      24359\n      -20.5709\n      -70.4931\n      7.7\n      2014.0\n      7.056532\n      0.643468\n    \n    \n      26083\n      -10.9940\n      165.7410\n      6.6\n      2013.0\n      5.958927\n      0.641073\n    \n    \n      32768\n      -37.7730\n      -75.0480\n      7.4\n      2010.0\n      6.762054\n      0.637946\n    \n    \n      10845\n      -36.6650\n      -73.3740\n      6.6\n      2010.0\n      5.963286\n      0.636714\n    \n  \n\n\n\n\n(2010~2014 시도) - 21번째 Ouest Department, Haiti 아이티 지진 2010년 진도 7.0 - 24번쨰 Puchuncavi, Valparaíso, Chile 칠레 지진 2014년 진도 6.4 - 28번째 Baoxing County, Yaan, Sichuan, China 중국 쓰촨성 지진 2013년 진도 6.6\n(2010~2015 시도_결과 좋지 않음?!) - 23번째 2010년 West New Britain Province, Papua New Guinea 진도 7.3 - 24번째 2011년 Kuzawa Terayama, Tanagura, Higashishirakawa District, Fukushima 963-5671, Japan 진도 6.6 - 29번째 2015년 Kishim, Afghanistan 진도 7.5\n- vis\n\n#moo_global.visf()\n\n\n#moo_global.visfhat()\n\n\n#moo_global.visres()\n\n\n#moo_global.vis(MagThresh=6.9,ResThresh=0.5)"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html#analysis_df_global20152020",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html#analysis_df_global20152020",
    "title": "Earthquake",
    "section": "analysis_df_global(2015~2020)",
    "text": "analysis_df_global(2015~2020)\n- make instance for analysis\n\nmoo_global=MooYaHo2(df_global.query(\"2015 <= Year <= 2020\"))\n\n- get distance\n\nmoo_global.get_distance()\n\n100%|██████████| 11239/11239 [02:38<00:00, 71.12it/s] \n\n\n\nmoo_global.D[moo_global.D>0].mean()\n\n8814.318793468068\n\n\n\nplt.hist(moo_global.D[moo_global.D>0])\n\n(array([10894274., 13618924., 16426520., 17583818., 16025000., 15684642.,\n        13794372., 10946494.,  9072574.,  2254138.]),\n array([2.54728455e-02, 2.00123511e+03, 4.00244475e+03, 6.00365439e+03,\n        8.00486402e+03, 1.00060737e+04, 1.20072833e+04, 1.40084929e+04,\n        1.60097026e+04, 1.80109122e+04, 2.00121218e+04]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n- weight matrix\n\nmoo_global.get_weightmatrix(theta=(8814.318793468068),kappa=2500) \n\n- fit\n\nmoo_global.fit2()\n\n\nmoo_global.df.sort_values(\"Residual\",ascending=False).iloc[:30,:]\n\n\n\n\n\n  \n    \n      \n      Latitude\n      Longitude\n      Magnitude\n      Year\n      MagnitudeHat\n      Residual\n    \n  \n  \n    \n      41735\n      -31.5729\n      -71.6744\n      8.3\n      2015.0\n      7.247889\n      1.052111\n    \n    \n      36993\n      -18.1125\n      -178.1530\n      8.2\n      2018.0\n      7.151339\n      1.048661\n    \n    \n      21952\n      -31.5729\n      -71.6744\n      8.3\n      2015.0\n      7.377015\n      0.922985\n    \n    \n      36363\n      -21.9496\n      169.4266\n      7.5\n      2018.0\n      6.641801\n      0.858199\n    \n    \n      39771\n      -10.6812\n      161.3273\n      7.8\n      2016.0\n      6.952781\n      0.847219\n    \n    \n      41015\n      -4.9521\n      94.3299\n      7.8\n      2016.0\n      6.982741\n      0.817259\n    \n    \n      33896\n      -33.2927\n      -177.8571\n      7.4\n      2020.0\n      6.584337\n      0.815663\n    \n    \n      39932\n      -42.7373\n      173.0540\n      7.8\n      2016.0\n      6.994644\n      0.805356\n    \n    \n      21719\n      -8.3381\n      124.8754\n      6.5\n      2015.0\n      5.698561\n      0.801439\n    \n    \n      33404\n      54.6020\n      -159.6258\n      7.6\n      2020.0\n      6.825914\n      0.774086\n    \n    \n      33593\n      -27.9686\n      -71.3062\n      6.8\n      2020.0\n      6.068130\n      0.731870\n    \n    \n      21559\n      38.2107\n      72.7797\n      7.2\n      2015.0\n      6.476211\n      0.723789\n    \n    \n      36263\n      55.0999\n      164.6993\n      7.3\n      2018.0\n      6.591954\n      0.708046\n    \n    \n      42106\n      -36.3601\n      -73.8120\n      6.4\n      2015.0\n      5.712923\n      0.687077\n    \n    \n      37769\n      -6.0699\n      142.7536\n      7.5\n      2018.0\n      6.826692\n      0.673308\n    \n    \n      36848\n      -18.4743\n      179.3502\n      7.9\n      2018.0\n      7.233195\n      0.666805\n    \n    \n      39584\n      -43.4064\n      -73.9413\n      7.6\n      2016.0\n      6.936485\n      0.663515\n    \n    \n      22246\n      -9.3070\n      158.4030\n      6.7\n      2015.0\n      6.036540\n      0.663460\n    \n    \n      36301\n      -58.5446\n      -26.3856\n      7.1\n      2018.0\n      6.448234\n      0.651766\n    \n    \n      42271\n      27.8087\n      86.0655\n      7.3\n      2015.0\n      6.651371\n      0.648629\n    \n    \n      40675\n      -56.2409\n      -26.9353\n      7.2\n      2016.0\n      6.551931\n      0.648069\n    \n    \n      42622\n      -10.7598\n      164.1216\n      6.1\n      2015.0\n      5.469452\n      0.630548\n    \n    \n      35487\n      -30.6441\n      -178.0995\n      7.3\n      2019.0\n      6.673190\n      0.626810\n    \n    \n      22521\n      -7.2175\n      154.5567\n      7.1\n      2015.0\n      6.493164\n      0.606836\n    \n    \n      42475\n      -4.7294\n      152.5623\n      7.5\n      2015.0\n      6.897835\n      0.602165\n    \n    \n      36101\n      -30.0404\n      -71.3815\n      6.7\n      2019.0\n      6.098721\n      0.601279\n    \n    \n      37927\n      56.0039\n      -149.1658\n      7.9\n      2018.0\n      7.307120\n      0.592880\n    \n    \n      38520\n      15.0222\n      -93.8993\n      8.2\n      2017.0\n      7.608529\n      0.591471\n    \n    \n      21912\n      -31.5173\n      -71.8040\n      6.7\n      2015.0\n      6.110265\n      0.589735\n    \n    \n      41887\n      -9.3438\n      158.0525\n      6.6\n      2015.0\n      6.012902\n      0.587098\n    \n  \n\n\n\n\n바다 아닌 거 - 8번째 2016년 Rotherham, New Zealand 뉴질랜드 카이코우라 지진 진도 7.8 - 9번째 2015년 Langkuru Utara, Pureman, Alor Regency, East Nusa Tenggara, Indonesia 수마트라 진도 6.5 - 15번째 2018년 Hela Province, Papua New Guinea 파푸아뉴기니 진도 7.5 - 20번째 2015년 Kalinchok, Nepal 네팔 진도 7.3 - 26번째 2019년 Coquimbo, Chile 칠레 코킴보주 진도 6.7\n- vis\n\n#moo_global.vis(MagThresh=7,ResThresh=0.3)\n\n\n\npd.read_html('https://en.wikipedia.org/wiki/Lists_of_21st-century_earthquakes',encoding='utf-8')[0].query('Magnitude<=7')# List of deadliest earthquakes\n\n\n\n\n\n  \n    \n      \n      Rank\n      Fatalities\n      Magnitude\n      Location\n      Event\n      Date\n    \n  \n  \n    \n      1\n      2\n      220000\n      7.0\n      Haiti\n      2010 Haiti earthquake\n      January 12, 2010\n    \n    \n      4\n      5\n      26271\n      6.6\n      Iran\n      2003 Bam earthquake\n      December 26, 2003\n    \n    \n      8\n      9\n      5782\n      6.4\n      Indonesia\n      2006 Yogyakarta earthquake\n      May 26, 2006\n    \n    \n      10\n      11\n      2968\n      6.9\n      China\n      2010 Yushu earthquake\n      April 13, 2010\n    \n    \n      11\n      12\n      2266\n      6.8\n      Algeria\n      2003 Boumerdès earthquake\n      May 21, 2003\n    \n    \n      14\n      15\n      1163\n      6.0\n      Afghanistan\n      June 2022 Afghanistan earthquake\n      June 21, 2022\n    \n  \n\n\n\n\n\npd.read_html('https://en.wikipedia.org/wiki/Lists_of_21st-century_earthquakes',encoding='utf-8')[3] # Deadliest earthquakes by year\n\n\n\n\n\n  \n    \n      \n      Year\n      Fatalities\n      Magnitude\n      Event\n      Location\n      Date\n      Death toll\n    \n  \n  \n    \n      0\n      2001\n      20085\n      7.7\n      2001 Gujarat earthquake\n      India\n      January 26\n      21357\n    \n    \n      1\n      2002\n      1000\n      6.1\n      2002 Hindu Kush earthquakes\n      Afghanistan\n      March 25\n      1685\n    \n    \n      2\n      2003\n      26271\n      6.6\n      2003 Bam earthquake\n      Iran\n      December 26\n      33819\n    \n    \n      3\n      2004\n      227898\n      9.1\n      2004 Indian Ocean earthquake and tsunami\n      Indonesia, Indian Ocean\n      December 26\n      227898\n    \n    \n      4\n      2005\n      87351\n      7.6\n      2005 Kashmir earthquake\n      Pakistan\n      October 8\n      87992\n    \n    \n      5\n      2006\n      5782\n      6.4\n      2006 Yogyakarta earthquake\n      Indonesia\n      May 26\n      6605\n    \n    \n      6\n      2007\n      519\n      8.0\n      2007 Peru earthquake\n      Peru\n      August 15\n      708\n    \n    \n      7\n      2008\n      87587\n      7.9\n      2008 Sichuan earthquake\n      China\n      May 12\n      88708\n    \n    \n      8\n      2009\n      1115\n      7.6\n      2009 Sumatra earthquakes\n      Indonesia\n      September 30\n      1790\n    \n    \n      9\n      2010\n      160000\n      7.0\n      2010 Haiti earthquake\n      Haiti\n      January 12\n      164627\n    \n    \n      10\n      2011\n      20896\n      9.0\n      2011 Tōhoku earthquake and tsunami\n      Japan\n      March 11\n      21492\n    \n    \n      11\n      2012\n      306\n      6.4\n      2012 East Azerbaijan earthquakes\n      Iran\n      August 11\n      689\n    \n    \n      12\n      2013\n      825\n      7.7\n      2013 Balochistan earthquakes\n      Pakistan\n      September 24\n      1572\n    \n    \n      13\n      2014\n      729\n      6.1\n      2014 Ludian earthquake\n      China\n      August 3\n      756\n    \n    \n      14\n      2015\n      8964\n      7.8\n      2015 Nepal earthquake\n      Nepal\n      April 25\n      9624\n    \n    \n      15\n      2016\n      673\n      7.8\n      2016 Ecuador earthquake\n      Ecuador\n      April 16\n      1339\n    \n    \n      16\n      2017\n      630\n      7.3\n      2017 Iran–Iraq earthquake\n      Iran\n      November 12\n      1232\n    \n    \n      17\n      2018\n      4340\n      7.5\n      2018 Sulawesi earthquake and tsunami\n      Indonesia\n      September 28\n      5239\n    \n    \n      18\n      2019\n      51\n      6.4\n      2019 Albania earthquake\n      Albania\n      November 26\n      288\n    \n    \n      19\n      2020\n      119\n      7.0\n      2020 Aegean Sea earthquake\n      Turkey/ Greece\n      October 30\n      207\n    \n    \n      20\n      2021\n      2248\n      7.2\n      2021 Haiti earthquake\n      Haiti\n      August 14\n      2476\n    \n    \n      21\n      2022\n      1163\n      6.0\n      June 2022 Afghanistan earthquake\n      Afghanistan\n      June 21\n      1405\n    \n  \n\n\n\n\n\n\nclass eachlocation(MooYaHo2):\n    def haiti(self,MagThresh=7,ResThresh=1,adjzoom=5,adjmarkersize = 40):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=15,\n                        center=dict(lat=18.4430, lon=-72.5710), \n                        zoom= adjzoom,\n                        height=900,\n                        opacity = 0.8,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-3,3])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 5,\n                      marker_color= 'blue',\n                      opacity = 0.1\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= adjmarkersize,\n                      marker_color= 'red',\n                      opacity = 0.8\n                      )\n        fig.add_trace(go.Scattermapbox(\n                    lat=self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                    lon=self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                    mode='markers',\n                    marker=go.scattermapbox.Marker(\n                        size=20,\n                        color='rgb(255, 255, 255)',\n                        opacity=0.4\n                    )\n                ))\n        return fig \n    def lquique(self,MagThresh=7,ResThresh=1,adjzoom=5, adjmarkersize= 40):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=15,\n                        center=dict(lat=-32.6953, lon=-71.4416), \n                        zoom=adjzoom,\n                        height=900,\n                        opacity = 0.8,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 5,\n                      marker_color= 'blue',\n                      opacity = 0.1\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= adjmarkersize,\n                      marker_color= 'red',\n                      opacity = 0.8\n                      )\n        fig.add_trace(go.Scattermapbox(\n                    lat=self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                    lon=self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                    mode='markers',\n                    marker=go.scattermapbox.Marker(\n                        size=20,\n                        color='rgb(255, 255, 255)',\n                        opacity=0.8\n                    )\n                ))\n        return fig \n    def sichuan(self,MagThresh=7,ResThresh=1,adjzoom=5,adjmarkersize=40):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=15,\n                        center=dict(lat=30.3080, lon=102.8880), \n                        zoom=adjzoom,\n                        height=900,\n                        opacity = 0.6,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 5,\n                      marker_color= 'blue',\n                      opacity = 0.1\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= adjmarkersize,\n                      marker_color= 'red',\n                      opacity = 0.8\n                      )\n        fig.add_trace(go.Scattermapbox(\n                    lat=self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                    lon=self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                    mode='markers',\n                    marker=go.scattermapbox.Marker(\n                        size=20,\n                        color='rgb(255, 255, 255)',\n                        opacity=0.8\n                    )\n                ))\n        return fig \n\n\neach_location=eachlocation(df_global.query(\"2010 <= Year < 2015\"))\n\n- get distance\n\neach_location.get_distance()\n\n100%|██████████| 12498/12498 [03:20<00:00, 62.38it/s] \n\n\n\neach_location.D[each_location.D>0].mean()\n\n8810.865423093777\n\n\n\nplt.hist(each_location.D[each_location.D>0])\n\n(array([14176290., 16005894., 21186674., 22331128., 19394182., 17548252.,\n        16668048., 13316436., 12973260.,  2582550.]),\n array([8.97930163e-02, 2.00141141e+03, 4.00273303e+03, 6.00405465e+03,\n        8.00537626e+03, 1.00066979e+04, 1.20080195e+04, 1.40093411e+04,\n        1.60106627e+04, 1.80119844e+04, 2.00133060e+04]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n- weight matrix\n\neach_location.get_weightmatrix(theta=(8810.865423093777),kappa=2500) \n\n- fit\n\neach_location.fit2()\n\n\neach_location.haiti(MagThresh=6.9,ResThresh=0.5)\n\n\n                                                \n\n\n\neach_location.lquique(MagThresh=8,ResThresh=0.4,adjzoom=4.3)\n\n\n                                                \n\n\n\neach_location.sichuan(MagThresh=6.5,ResThresh=0.4)"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html#칠레",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html#칠레",
    "title": "Earthquake",
    "section": "칠레",
    "text": "칠레\n\ndf_chile_ex= pd.concat([pd.read_csv('05_10.csv'),pd.read_csv('10_15.csv')]).iloc[:,[0,1,2,4]].rename(columns={'latitude':'Latitude','longitude':'Longitude','mag':'Magnitude'}).reset_index().iloc[:,1:]\n\n\ndf_chile = df_chile_ex.assign(Year=list(map(lambda x: x.split('-')[0], df_chile_ex.time))).iloc[:,1:]\n\n\ndf_chile = df_chile.assign(Month=list(map(lambda x: x.split('-')[1], df_chile_ex.time)))\n\n\ndf_chile.Year = df_chile.Year.astype(np.float64)\ndf_chile.Month = df_chile.Month.astype(np.float64)\n\n\nchile_location=eachlocation(df_chile.query(\"2010 <= Year < 2015\"))\n\n\nchile_location.get_distance()\n\n100%|██████████| 12498/12498 [03:18<00:00, 62.95it/s]  \n\n\n\nchile_location.get_weightmatrix(theta=(chile_location.D[chile_location.D>0].mean()),kappa=2500) \n\n\nchile_location.fit2()\n\n아이티\n\nchile_location.df.assign(Residual2 = chile_location.df.Residual**2).reset_index().iloc[2324:2330,:]\n\n\n\n\n\n  \n    \n      \n      index\n      Latitude\n      Longitude\n      Magnitude\n      Year\n      Month\n      MagnitudeHat\n      Residual\n      Residual2\n    \n  \n  \n    \n      2324\n      2324\n      18.463\n      -72.626\n      5.0\n      2010.0\n      1.0\n      5.334262\n      -0.334262\n      0.111731\n    \n    \n      2325\n      2325\n      18.387\n      -72.784\n      6.0\n      2010.0\n      1.0\n      5.719565\n      0.280435\n      0.078644\n    \n    \n      2326\n      2326\n      18.443\n      -72.571\n      7.0\n      2010.0\n      1.0\n      6.288026\n      0.711974\n      0.506907\n    \n    \n      2327\n      2327\n      -5.417\n      133.731\n      5.5\n      2010.0\n      1.0\n      5.530625\n      -0.030625\n      0.000938\n    \n    \n      2328\n      2328\n      15.437\n      -88.761\n      5.1\n      2010.0\n      1.0\n      5.125565\n      -0.025565\n      0.000654\n    \n    \n      2329\n      2329\n      -16.861\n      -174.228\n      5.3\n      2010.0\n      1.0\n      5.471571\n      -0.171571\n      0.029437\n    \n  \n\n\n\n\n칠레\n\nchile_location.df.assign(Residual2 = chile_location.df.Residual**2).reset_index().query(\"-56.5 < Latitude & Latitude <-17.4 & -81.5 < Longitude & Longitude < -61.5 & Year == 2014 & Month == 8\")\n\n\n\n\n\n  \n    \n      \n      index\n      Latitude\n      Longitude\n      Magnitude\n      Year\n      Month\n      MagnitudeHat\n      Residual\n      Residual2\n    \n  \n  \n    \n      2997\n      14603\n      -32.6953\n      -71.4416\n      6.4\n      2014.0\n      8.0\n      5.705553\n      0.694447\n      0.482257\n    \n    \n      2999\n      14605\n      -20.1745\n      -69.0385\n      5.6\n      2014.0\n      8.0\n      5.497368\n      0.102632\n      0.010533\n    \n    \n      3032\n      14638\n      -20.1580\n      -70.0230\n      5.3\n      2014.0\n      8.0\n      5.291126\n      0.008874\n      0.000079\n    \n    \n      3046\n      14652\n      -23.9047\n      -66.7371\n      5.0\n      2014.0\n      8.0\n      4.909951\n      0.090049\n      0.008109\n    \n    \n      3057\n      14663\n      -33.7770\n      -72.2030\n      5.2\n      2014.0\n      8.0\n      5.382720\n      -0.182720\n      0.033387\n    \n  \n\n\n\n\n중국\n\nchile_location.df.assign(Residual2 = chile_location.df.Residual**2).reset_index().iloc[5136:5142,:]\n\n\n\n\n\n  \n    \n      \n      index\n      Latitude\n      Longitude\n      Magnitude\n      Year\n      Month\n      MagnitudeHat\n      Residual\n      Residual2\n    \n  \n  \n    \n      5136\n      16742\n      30.209\n      102.862\n      5.0\n      2013.0\n      4.0\n      5.027420\n      -0.027420\n      0.000752\n    \n    \n      5137\n      16743\n      30.308\n      102.888\n      6.6\n      2013.0\n      4.0\n      5.922831\n      0.677169\n      0.458558\n    \n    \n      5138\n      16744\n      39.693\n      143.258\n      5.0\n      2013.0\n      4.0\n      4.758333\n      0.241667\n      0.058403\n    \n    \n      5139\n      16745\n      49.965\n      157.652\n      6.1\n      2013.0\n      4.0\n      5.797293\n      0.302707\n      0.091632\n    \n    \n      5140\n      16746\n      -11.976\n      121.632\n      5.8\n      2013.0\n      4.0\n      5.854233\n      -0.054233\n      0.002941\n    \n    \n      5141\n      16747\n      -14.966\n      166.857\n      5.2\n      2013.0\n      4.0\n      5.228670\n      -0.028670\n      0.000822"
  },
  {
    "objectID": "posts/GODE/2022-12-27-DFT_study.html",
    "href": "posts/GODE/2022-12-27-DFT_study.html",
    "title": "Discrete Fourier Transform",
    "section": "",
    "text": "DFT\nhttps://miruetoto.quarto.pub/yechan/posts/CGSP/2022-12-24-CGSP-Chap-8-3-DFT.html#fnref1\nhttps://miruetoto.github.io/yechan/%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88/2019/11/24/(%EB%85%B8%ED%8A%B8)-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88-%EC%B6%94%EB%A1%A0-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88.html"
  },
  {
    "objectID": "posts/GODE/2022-12-27-DFT_study.html#import",
    "href": "posts/GODE/2022-12-27-DFT_study.html#import",
    "title": "Discrete Fourier Transform",
    "section": "import",
    "text": "import\n\nimport numpy as np\n\n\nForward operator A\n\nA = np.array([[0, 0, 0, 0, 1],\n    [1, 0, 0, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, 0, 1, 0, 0],\n    [0, 0, 0, 1, 0]])\nA\n\narray([[0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0]])\n\n\n\nnp.transpose(A)@A\n\narray([[1, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 1]])\n\n\n\nnote: A is orthogonal matrix\n\n\ns = np.array([[1],[22],[333],[4444],[55555]])\ns\n\narray([[    1],\n       [   22],\n       [  333],\n       [ 4444],\n       [55555]])\n\n\n\nA@s\n\narray([[55555],\n       [    1],\n       [   22],\n       [  333],\n       [ 4444]])\n\n\n\nA@A@s\n\narray([[ 4444],\n       [55555],\n       [    1],\n       [   22],\n       [  333]])\n\n\n\nA@A@A@s\n\narray([[  333],\n       [ 4444],\n       [55555],\n       [    1],\n       [   22]])\n\n\n\nnote : thus A is a forward operator,A* is a backward operator.\n\n\n\nDFT\n\\(A = DFT^* \\Lambda DFT\\)\n\nλ, ψ = np.linalg.eig(A)\nλ, ψ\n\n(array([-0.80901699+0.58778525j, -0.80901699-0.58778525j,\n         0.30901699+0.95105652j,  0.30901699-0.95105652j,\n         1.        +0.j        ]),\n array([[-0.3618034+0.26286556j, -0.3618034-0.26286556j,\n         -0.3618034-0.26286556j, -0.3618034+0.26286556j,\n          0.4472136+0.j        ],\n        [ 0.4472136+0.j        ,  0.4472136-0.j        ,\n         -0.3618034+0.26286556j, -0.3618034-0.26286556j,\n          0.4472136+0.j        ],\n        [-0.3618034-0.26286556j, -0.3618034+0.26286556j,\n          0.1381966+0.4253254j ,  0.1381966-0.4253254j ,\n          0.4472136+0.j        ],\n        [ 0.1381966+0.4253254j ,  0.1381966-0.4253254j ,\n          0.4472136+0.j        ,  0.4472136-0.j        ,\n          0.4472136+0.j        ],\n        [ 0.1381966-0.4253254j ,  0.1381966+0.4253254j ,\n          0.1381966-0.4253254j ,  0.1381966+0.4253254j ,\n          0.4472136+0.j        ]]))\n\n\n\nλ.shape, ψ.shape\n\n((5,), (5, 5))\n\n\n\nA \n\narray([[0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0]])\n\n\n\n(ψ @ np.diag(λ) @ ψ.transpose()).round(2)\n\narray([[-0.  +0.j,  0.45+0.j,  0.28+0.j,  0.72+0.j, -0.45+0.j],\n       [ 0.45+0.j,  0.28+0.j,  0.72+0.j, -0.45+0.j, -0.  +0.j],\n       [ 0.28+0.j,  0.72+0.j, -0.45+0.j,  0.  +0.j,  0.45+0.j],\n       [ 0.72+0.j, -0.45+0.j,  0.  +0.j,  0.45+0.j,  0.28+0.j],\n       [-0.45+0.j, -0.  +0.j,  0.45+0.j,  0.28+0.j,  0.72+0.j]])\n\n\n?\ndefine \\(\\psi^* = DFT\\)\n\nDFT = np.transpose(ψ)\nDFT\n\narray([[-0.3618034+0.26286556j,  0.4472136+0.j        ,\n        -0.3618034-0.26286556j,  0.1381966+0.4253254j ,\n         0.1381966-0.4253254j ],\n       [-0.3618034-0.26286556j,  0.4472136-0.j        ,\n        -0.3618034+0.26286556j,  0.1381966-0.4253254j ,\n         0.1381966+0.4253254j ],\n       [-0.3618034-0.26286556j, -0.3618034+0.26286556j,\n         0.1381966+0.4253254j ,  0.4472136+0.j        ,\n         0.1381966-0.4253254j ],\n       [-0.3618034+0.26286556j, -0.3618034-0.26286556j,\n         0.1381966-0.4253254j ,  0.4472136-0.j        ,\n         0.1381966+0.4253254j ],\n       [ 0.4472136+0.j        ,  0.4472136+0.j        ,\n         0.4472136+0.j        ,  0.4472136+0.j        ,\n         0.4472136+0.j        ]])\n\n\n\nλ[3,]\n\n(0.30901699437494734-0.9510565162951535j)\n\n\n\na=np.array([1,2,3,4])\nnp.diag(np.diag(a))\n\narray([1, 2, 3, 4])\n\n\n\nλ\n\narray([-0.80901699+0.58778525j, -0.80901699-0.58778525j,\n        0.30901699+0.95105652j,  0.30901699-0.95105652j,\n        1.        +0.j        ])\n\n\n\n(np.matrix(ψ)@np.matrix(np.diag(λ))@np.matrix(ψ).H).round(3)\n\nmatrix([[-0.+0.j,  0.+0.j, -0.+0.j,  0.+0.j,  1.+0.j],\n        [ 1.+0.j, -0.+0.j,  0.+0.j, -0.+0.j, -0.+0.j],\n        [-0.+0.j,  1.+0.j, -0.+0.j,  0.+0.j,  0.+0.j],\n        [ 0.+0.j, -0.+0.j,  1.+0.j,  0.+0.j, -0.+0.j],\n        [-0.+0.j, -0.+0.j,  0.+0.j,  1.+0.j, -0.+0.j]])\n\n\n\nnp.matrix(ψ).H\n\nmatrix([[-0.3618034-0.26286556j,  0.4472136-0.j        ,\n         -0.3618034+0.26286556j,  0.1381966-0.4253254j ,\n          0.1381966+0.4253254j ],\n        [-0.3618034+0.26286556j,  0.4472136+0.j        ,\n         -0.3618034-0.26286556j,  0.1381966+0.4253254j ,\n          0.1381966-0.4253254j ],\n        [-0.3618034+0.26286556j, -0.3618034-0.26286556j,\n          0.1381966-0.4253254j ,  0.4472136-0.j        ,\n          0.1381966+0.4253254j ],\n        [-0.3618034-0.26286556j, -0.3618034+0.26286556j,\n          0.1381966+0.4253254j ,  0.4472136+0.j        ,\n          0.1381966-0.4253254j ],\n        [ 0.4472136-0.j        ,  0.4472136-0.j        ,\n          0.4472136-0.j        ,  0.4472136-0.j        ,\n          0.4472136-0.j        ]])\n\n\n\n\nSpectral components and Frequencies\n\\[\\{ 1,\\psi_1, \\psi_2, \\psi_3,\\dots, \\psi_{N-1} \\}\\]\nThese vectors are called spectral components.\nIn Physics and in operator theory, these eigenvalues are the frequencies of the signal.\nEigenvalues of \\(A\\)"
  },
  {
    "objectID": "posts/GODE/Untitled.html",
    "href": "posts/GODE/Untitled.html",
    "title": "Seoyeon's Blog for study",
    "section": "",
    "text": "import numpy as np\n\n\na=10\nb=20\nn=200\n\n\narr1 = np.array([a+(b-a)/(n-1) * (i-1) for i in range(1,n+1)])\n\n\\[a+\\frac{(b-a)i}{n-1}\\] for \\(i=1,2,3,\\dots, n\\)\n\narr2 = np.linspace(a,b,n)\n\n\narr1[:5]\n\narray([10.        , 10.05025126, 10.10050251, 10.15075377, 10.20100503])\n\n\n\narr2[:5]\n\narray([10.        , 10.05025126, 10.10050251, 10.15075377, 10.20100503])"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, there ;)\n참고\n\nsudo reeboot 로 재부팅 후\njupyter notebook & 입력 후\nenter 로 나가서\nexit 하기\n재부팅시 drop box open ~ (~/.dropbox-dist/dropboxd)\nnvidia-smi 로 GPU 상태 확인 ! 안 켜져 있다면? (sudo ./NVIDIA-Linux-x86_64-495.46.run(tab누르면 나옴))\n\n드랍박스 상태 - dropbox status\n드롭박스 시작 - dropbox start\nterminal check\n\ntop\nnvidia-smi\nps aux | grep jupyter-lab\n\n필요없는 거 ’kill 0000’로 프로세스 끄기"
  }
]