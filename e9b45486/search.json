[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": ":)",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n\n\nOld Faithful\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\nSTGCN papers review\n\n\nSEOYEON CHOI\n\n\n\n\nMar 3, 2023\n\n\nSY 1st ITSTGCN\n\n\nSEOYEON CHOI\n\n\n\n\n\nFeb 19, 2023\n\n\nMontevideoBus lag 4 Randomly Missing comparison Table\n\n\nSEOYEON CHOI\n\n\n\n\n\nFeb 16, 2023\n\n\nEnglandCovidDatasetLoader lag 4 Randomly Missing comparison Table\n\n\nSEOYEON CHOI\n\n\n\n\n\nFeb 16, 2023\n\n\nPedalMeDatasetLoader lag 4 Randomly Missing comparison Table\n\n\nSEOYEON CHOI\n\n\n\n\n\nFeb 15, 2023\n\n\nGNAR lag 1 Block Missing comparison Table\n\n\nSEOYEON CHOI\n\n\n\n\n\nFeb 15, 2023\n\n\nESTGCN Comparison Table\n\n\nSEOYEON CHOI\n\n\n\n\n\nFeb 15, 2023\n\n\nGNAR lag 1 Randomly Missing comparison Table by Number of Filter and gnar forecast updating\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 15, 2023\n\n\n2nd ITSTGCN\n\n\nGUEBIN CHOI\n\n\n\n\nFeb 15, 2023\n\n\n1st ITSTGCN\n\n\nGUEBIN CHOI\n\n\n\n\n\nFeb 15, 2023\n\n\nGNAR lag 1 Randomly Missing comparison Table by Missing Rate\n\n\nSEOYEON CHOI\n\n\n\n\n\nFeb 15, 2023\n\n\nWikiMath lag 4 Randomly Missing comparison Table\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 13, 2023\n\n\nasdf\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 11, 2023\n\n\nClass of Method(WikiMath) lag 4 80% repeat\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 8, 2023\n\n\nClass of Method(GNAR) lag 1 Block Missing repeat\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 7, 2023\n\n\nClass of Method(WikiMath) lag 1\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 6, 2023\n\n\nClass of Method(GNAR) lag 1 80% Missing repeat\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 6, 2023\n\n\nClass of Method(GNAR) lag 2\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 5, 2023\n\n\nChap 12.2 ~ 3: Power Spectral Density and its Estimators\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 3, 2023\n\n\nChap 12.2: Weakly Stationary Graph Processes\n\n\nSEOYEON CHOI\n\n\n\n\nFeb 1, 2023\n\n\nChap 8.3: Discrete Fourier Transform\n\n\nSEOYEON CHOI\n\n\n\n\nJan 28, 2023\n\n\nClass of Method(GNAR) lag 1\n\n\nSEOYEON CHOI\n\n\n\n\nJan 28, 2023\n\n\nClass of Method(WikiMath) lag 4\n\n\nSEOYEON CHOI\n\n\n\n\nJan 26, 2023\n\n\nClass of Method\n\n\nGuebin Choi\n\n\n\n\nJan 21, 2023\n\n\nClass of Method\n\n\nSEOYEON CHOI\n\n\n\n\nJan 20, 2023\n\n\n1st ST-GCN Example dividing train and test\n\n\nSEOYEON CHOI\n\n\n\n\nJan 17, 2023\n\n\n2nd ST-GCN Example dividing train and test\n\n\nSEOYEON CHOI\n\n\n\n\nJan 11, 2023\n\n\nGCN Algorithm Example 1\n\n\nSEOYEON CHOI\n\n\n\n\nJan 5, 2023\n\n\nGNAR data\n\n\nSEOYEON CHOI\n\n\n\n\nJan 2, 2023\n\n\nquarto blog tips\n\n\nSEOYEON CHOI\n\n\n\n\nDec 31, 2022\n\n\nStudy for Spaces\n\n\nSEOYEON CHOI\n\n\n\n\nDec 28, 2022\n\n\nSimulation of geometric-temporal\n\n\nSEOYEON CHOI\n\n\n\n\nDec 27, 2022\n\n\nDiscrete Fourier Transform\n\n\nSEOYEON CHOI\n\n\n\n\nDec 21, 2022\n\n\nPyTorch ST-GCN Dataset\n\n\nSEOYEON CHOI\n\n\n\n\nDec 5, 2022\n\n\nGCN\n\n\nSEOYEON CHOI\n\n\n\n\nDec 5, 2022\n\n\nTORCH_GEOMETRIC.NN\n\n\nSEOYEON CHOI\n\n\n\n\nDec 1, 2022\n\n\nStudy\n\n\nSEOYEON CHOI\n\n\n\n\nDec 1, 2022\n\n\nGraph code\n\n\nSEOYEON CHOI\n\n\n\n\nNov 24, 2022\n\n\nClass code for Comparison Study\n\n\nSEOYEON CHOI\n\n\n\n\nNov 24, 2022\n\n\nEarthquake\n\n\nSEOYEON CHOI\n\n\n\n\nSep 2, 2022\n\n\nSimulation\n\n\nSEOYEON CHOI\n\n\n\n\nSep 1, 2022\n\n\nQuarto tip\n\n\nSEOYEON CHOI\n\n\n\n\nSep 1, 2022\n\n\nGODE\n\n\nSEOYEON CHOI\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Study/index.html",
    "href": "posts/Study/index.html",
    "title": "Study",
    "section": "",
    "text": "About Study"
  },
  {
    "objectID": "posts/Study/Untitled1.html",
    "href": "posts/Study/Untitled1.html",
    "title": "Old Faithful",
    "section": "",
    "text": "import\nhttps://quarto.org/docs/interactive/shiny/"
  },
  {
    "objectID": "posts/Study/2022-12-31-Space-study.html",
    "href": "posts/Study/2022-12-31-Space-study.html",
    "title": "Study for Spaces",
    "section": "",
    "text": "Spaces"
  },
  {
    "objectID": "posts/Study/2022-12-31-Space-study.html#ë‚´ì ê³µê°„",
    "href": "posts/Study/2022-12-31-Space-study.html#ë‚´ì ê³µê°„",
    "title": "Study for Spaces",
    "section": "ë‚´ì ê³µê°„",
    "text": "ë‚´ì ê³µê°„\n\\[|a||b| \\cos \\theta\\]\n\nê¸¸ì´ + ê°ì˜ ê°œë…\n\nProjection"
  },
  {
    "objectID": "posts/Study/2022-12-31-Space-study.html#ë°”ë‚˜íê³µê°„",
    "href": "posts/Study/2022-12-31-Space-study.html#ë°”ë‚˜íê³µê°„",
    "title": "Study for Spaces",
    "section": "ë°”ë‚˜íê³µê°„",
    "text": "ë°”ë‚˜íê³µê°„\n\\[|---|\\]\n\nê¸¸ì´ + ê·¹í•œì˜ ê°œë…"
  },
  {
    "objectID": "posts/Study/2022-12-31-Space-study.html#ë…¸ë¦„ê³µê°„",
    "href": "posts/Study/2022-12-31-Space-study.html#ë…¸ë¦„ê³µê°„",
    "title": "Study for Spaces",
    "section": "ë…¸ë¦„ê³µê°„",
    "text": "ë…¸ë¦„ê³µê°„\n\\[|   |\\]\n\nê¸¸ì´ì˜ ê°œë…"
  },
  {
    "objectID": "posts/Study/2022-12-31-Space-study.html#íë² ë¥´íŠ¸ê³µê°„ìœ í´ë¦¬ë“œ-ë¹„ìœ í´ë¦¬ë“œ-ëª¨ë‘-ì¡´ì¬",
    "href": "posts/Study/2022-12-31-Space-study.html#íë² ë¥´íŠ¸ê³µê°„ìœ í´ë¦¬ë“œ-ë¹„ìœ í´ë¦¬ë“œ-ëª¨ë‘-ì¡´ì¬",
    "title": "Study for Spaces",
    "section": "íë² ë¥´íŠ¸ê³µê°„(ìœ í´ë¦¬ë“œ + ë¹„ìœ í´ë¦¬ë“œ ëª¨ë‘ ì¡´ì¬)",
    "text": "íë² ë¥´íŠ¸ê³µê°„(ìœ í´ë¦¬ë“œ + ë¹„ìœ í´ë¦¬ë“œ ëª¨ë‘ ì¡´ì¬)\ní“¨ë¦¬ì— í•´ì„ - ê¸¸ì´ + ê° + ê·¹í•œì˜ ê°œë…"
  },
  {
    "objectID": "posts/Study/2023-03-16-stgcn_paper_review_1.html",
    "href": "posts/Study/2023-03-16-stgcn_paper_review_1.html",
    "title": "STGCN papers review",
    "section": "",
    "text": "STGCN papers review\n\nPyTorch Geometric Temporal\nì´ ë°ì´í„°ëŠ” ì„¸ ì¢…ë¥˜ì˜ datasetì„ ê°€ì§€ê³  ìˆê³ , ê° ë°ì´í„°ëŠ” snapshotìœ¼ë¡œ êµ¬ë¶„ë˜ì–´ ìˆìŒ\n\nDynamic graph with temporal signal(A dynamic graph with a temporal signal)\n\nì—£ì§€ ë³€í•¨, ì‹œê·¸ë„ ë³€í•¨\n\nDynamic graph with static signal(A dynamic graph with a static signal)\n\nì—£ì§€ ë³€í•¨, ì‹œê·¸ë„ ê³ ì •\n\nStatic graph with temporal signal(A static graph with a temporal signal)\n\nì—£ì§€ ê³ ì •, ì‹œê·¸ë„ ë³€í•¨\n\n\nSpatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting\n\n!git add .\n!git commit -m .\n!git push"
  },
  {
    "objectID": "posts/Study/2023-02-01-ch8_DFT.html",
    "href": "posts/Study/2023-02-01-ch8_DFT.html",
    "title": "Chap 8.3: Discrete Fourier Transform",
    "section": "",
    "text": "Chap 8.3: Discrete Fourier Transform\n\n\nusing LinearAlgebra, FFTW\n\nShift Operator Bê°€ ì¡´ì¬í•  ë–„,\n\nì§êµ í–‰ë ¬orthogonal\ncyclic shift operator(\\(BS_n = S_{n-1}\\)) \\(\\to\\) \\(S_{n-1}\\)ì€ vector space components\n\n\\(\\star\\) ì‹œê³„ì—´ì˜ back shift operator ë¡œ ìƒê°í•  ìˆ˜ ìˆê³ , foward shift operatorë„ ê°€ëŠ¥í•˜ë‹¤.\n\\(\\star\\) cyclic operatorì´ì–´ì•¼ í•˜ëŠ” ì´ìœ ? ì±…ì˜ ì •ì˜ ì´ìš© ë° back/forward shift operatorëŠ” ê³ ìœ ë¶„í•´ ì•ˆ ë  ìˆ˜ë„.\nì´ í–‰ë ¬ì„ ê³ ìœ ë¶„í•´(full rank)í•˜ì—¬ ë‚˜ì˜¨ ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°ê°€ ì¡´ì¬í•œë‹¤.\n\nB= [0 0 0 0 1\n    1 0 0 0 0 \n    0 1 0 0 0\n    0 0 1 0 0\n    0 0 0 1 0] # cyclic shift operator B\nB'B # matrix B is orthogonal\n\n5Ã—5 Matrix{Int64}:\n 1  0  0  0  0\n 0  1  0  0  0\n 0  0  1  0  0\n 0  0  0  1  0\n 0  0  0  0  1\n\n\n\ns = [1,2,3,4,5]\nB*s # matrix B is a cyclic shift operator\n\n5-element Vector{Int64}:\n 5\n 1\n 2\n 3\n 4\n\n\n\nB^2*s\n\n5-element Vector{Int64}:\n 4\n 5\n 1\n 2\n 3\n\n\nì´ ê³ ìœ ê°’\\(\\lambda\\), ê³ ìœ ë²¡í„°\\(\\psi\\)ê°€ ì¡´ì¬í•œë‹¤ë©´ BëŠ” \\(DFT^* \\Lambda DFT\\)ë¡œ í‘œí˜„ ê°€ëŠ¥í•Ÿí•˜ë‹¤.\n\n\\(DFT^*\\)\n\nconjugate matrix\n\\(\\psi\\)ì¸ë° DFTë¡œ í‘œí˜„ \\(\\to\\) ê·¸ë˜í”„ ë„ë©”ì¸ìœ¼ë¡œ í™•ì¥ì´ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸\n\n\nì—¬ê¸°ì„œ \\(DFT^*\\)ëŠ” \\(\\psi^*_k = DFT_k = \\frac{1}{\\sqrt{N}} \\begin{bmatrix} 1 \\\\ \\dots \\\\ e^{-j\\frac{2\\pi}{N}(N-1)k} \\end{bmatrix}\\)ë¡œì„œ í‘œí˜„(\\(\\in C^N\\) ê¸¸ì´ê°€ \\(N\\)ì¸ vector(ë³µì†Œìˆ˜))\n\nunitary and symmetric\n\nunitary \\(\\to\\) complex spaceì—ì„œ ì •ê·œì§êµê¸°ì €ë¥¼ ì´ë£¨ê³ , \\(A(A^*)^\\top = I, \\psi^{-1} = \\psi^*, \\psi^* \\psi = \\psi \\psi^* = I\\)\n\nìœ„ \\(k\\)ê°œì˜ ë²¡í„°ë“¤ì€ spectral components ì´ë‹¤.\nthe complex exponential sinusodal functions\n\nì—¬ê¸°ì„œ \\(\\lambda\\)ëŠ” the frequencies of the signal ë¡œì„œ ì •ì˜ë  ìˆ˜ ìˆë‹¤.\n\níŠ¹ì§•\n\ndistinct\npositive\nequally spaced\nincreasing from \\(0\\) ro \\(\\frac{N-1}{N}\\)\n\n\n\nÎ», Ïˆ = eigen(B)\n\nEigen{ComplexF64, ComplexF64, Matrix{ComplexF64}, Vector{ComplexF64}}\nvalues:\n5-element Vector{ComplexF64}:\n -0.8090169943749472 - 0.5877852522924725im\n -0.8090169943749472 + 0.5877852522924725im\n 0.30901699437494734 - 0.9510565162951536im\n 0.30901699437494734 + 0.9510565162951536im\n  0.9999999999999998 + 0.0im\nvectors:\n5Ã—5 Matrix{ComplexF64}:\n  0.138197+0.425325im   0.138197-0.425325im  â€¦  0.447214+0.0im\n -0.361803-0.262866im  -0.361803+0.262866im     0.447214+0.0im\n  0.447214-0.0im        0.447214+0.0im          0.447214+0.0im\n -0.361803+0.262866im  -0.361803-0.262866im     0.447214+0.0im\n  0.138197-0.425325im   0.138197+0.425325im     0.447214+0.0im\n\n\n\nB = Ïˆ * Diagonal(Î») * Ïˆ'\n\n5Ã—5 Matrix{ComplexF64}:\n    2.498e-16-9.67158e-18im  â€¦           1.0+1.81709e-18im\n          1.0+2.1793e-18im       5.55112e-16-1.09573e-17im\n -3.88578e-16-7.89355e-18im     -3.21902e-16+8.57473e-18im\n -4.16334e-16-8.06149e-18im       -4.996e-16-8.9293e-18im\n  2.99888e-16+1.53977e-17im      3.99189e-16+1.42498e-17im\n\n\n\nDFT = Ïˆ'\n\n5Ã—5 adjoint(::Matrix{ComplexF64}) with eltype ComplexF64:\n  0.138197-0.425325im  -0.361803+0.262866im  â€¦  0.138197+0.425325im\n  0.138197+0.425325im  -0.361803-0.262866im     0.138197-0.425325im\n -0.361803-0.262866im  -0.361803+0.262866im     0.138197-0.425325im\n -0.361803+0.262866im  -0.361803-0.262866im     0.138197+0.425325im\n  0.447214-0.0im        0.447214-0.0im          0.447214-0.0im\n\n\n\\(B = \\psi \\Lambda \\psi^{-1}\\)\n\n\\(\\psi^H := DFT\\) ì´ë ‡ê²Œ ì •ì˜í•œë‹¤ë©´ Fì˜ ê³ ìœ ë²¡í„°ì˜ conjugate\n\\(F\\) \\(\\to\\) \\(BF = I\\)\n\\(\\psi \\Lambda \\psi^H F = I\\)\në§Œì•½, \\(F\\)ê°€ \\(\\psi^H\\Lambda^{-1}\\psi\\)ë¼ë©´, \\(\\psi \\Lambda \\psi^H\\psi^H\\Lambda \\psi = I\\)\në”°ë¼ì„œ \\(F\\)ëŠ” \\(\\psi^{-1} \\Lambda^{-1} \\psi\\)ë¡œ ê³ ìœ ë¶„í•´\n\n\nx = [1,2-im,-im,-1+2im]\n\n4-element Vector{Complex{Int64}}:\n  1 + 0im\n  2 - 1im\n  0 - 1im\n -1 + 2im\n\n\n\n_DFT = reshape([i*j for i in 0:3 for j in 0:3], (4,4))\n_DFT\n\n4Ã—4 Matrix{Int64}:\n 0  0  0  0\n 0  1  2  3\n 0  2  4  6\n 0  3  6  9\n\n\n\nf = x -> exp(-im * (2Ï€/4) * x)\nDFT = _DFT .|> f\n\n4Ã—4 Matrix{ComplexF64}:\n 1.0-0.0im           1.0-0.0im          â€¦           1.0-0.0im\n 1.0-0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0-0.0im          -1.0-1.22465e-16im             -1.0-3.67394e-16im\n 1.0-0.0im  -1.83697e-16+1.0im              5.51091e-16-1.0im\n\n\n\nDFT * x\n\n4-element Vector{ComplexF64}:\n                   2.0 + 0.0im\n   -1.9999999999999998 - 2.0000000000000004im\n 8.881784197001252e-16 - 1.9999999999999998im\n    3.9999999999999987 + 4.000000000000001im\n\n\n\nfft(x)\n\n4-element Vector{ComplexF64}:\n  2.0 + 0.0im\n -2.0 - 2.0im\n  0.0 - 2.0im\n  4.0 + 4.0im\n\n\nDFTì˜ ë‘ ë²ˆì¨° ì •ì˜\n\në³µì†Œìˆ˜ sequence \\(\\{x_n\\}\\)ì„ ê·œì¹™ì— ë”°ë¼ \\(\\{X_k\\}\\)ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒ\nê·œì¹™: \\(x_k = \\sum^{N-1}_{n=0} x_n e^{-i\\frac{2\\pi}{N}kn}\\)\n\níŠ¹íˆ, \\(k=0\\)ì´ë©´ \\(X_0 = \\sum^{N-1}{n=0}x_n\\), constant term ì´ ë˜ì–´ \\(\\beta_0\\)ì˜ ì—­í• ì„ í•œë‹¤.\n\n\ní–‰ë ¬ë¡œ í‘œí˜„í•œë‹¤ë©´, \\(\\begin{bmatrix}X_k \\\\ \\dots \\end{bmatrix} = DFT = \\begin{bmatrix}x_n \\\\ \\dots \\end{bmatrix}\\)\n\n\\(x_k = DFT^{-1}X_k\\)\n\n\\(x_k\\) = bias, ê´€ì¸¡ê°’\n\\(DFT^{-1}\\): ì„¤ëª…ë³€ìˆ˜, unitaryë¼ \\(DFT^{-1} = DFT = DFT^*\\), symmetric, orthogonal(ì„¤ëª…ë³€ìˆ˜ê°€ ë…ë¦½ì ì´ë¼ ë‹¤ì¤‘ê³µì„ ì„±ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.)\n\në‹¤ì¤‘ê³µì„ ì„±ì´ ìˆìœ¼ë©´ ê° ì„¤ëª…ë³€ìˆ˜ì˜ ì„¤ëª…ì´ ì•ˆ ë  ìˆ˜ë„ ìˆê³  ê·¸ ì„¤ëª…ë³€ìˆ˜ë¥¼ í•´ì„í•˜ê¸°ë„ ì–´ë ¤ì›Œì§.\n\n\\(X_k = \\beta\\), codfficient(í‘¸ë¦¬ì— ë³€í™˜ì˜ ê²°ê³¼ì´ë‹¤)\n\n\nDFT í–‰ë ¬ì˜ íŠ¹ì§•\n\nìœ ë‹ˆí„°ë¦¬unitary í–‰ë ¬, ì¦‰, \\(DFT^* = DFT, DFT^*DFT = I\\)\nëŒ€ì¹­symmetric í–‰ë ¬ \\(\\to\\) ê·¸ë ‡ê¸° ë–„ë¬¸ì— ì´ í–‰ë ¬ì˜ ì¼¤ë ˆì „ì¹˜ëŠ” \\(i = \\sqrt{-1}\\) ëŒ€ì‹  \\(i\\)ë¥¼ ë„£ì€ ê²ƒê³¼ ê°™ìŒ.\n\n\ninverse DFTëŠ” \\(i = -i\\)ë¥¼ ë„£ì€ í–‰ë ¬, ì¦‰ DFTì˜ ì¼¤ë ˆì „ì¹˜ = inverse DFT\n\n\\(DFT = \\frac{1}{\\sqrt{N}}\\begin{bmatrix} 1 & 1 & 1 & \\dots & 1 \\\\ 1 & e^{-i\\frac{2\\pi}{N}1} & e^{-i\\frac{2\\pi}{N}1} & \\dots & e^{-i\\frac{2\\pi}{N}(N-1)} \\\\ 1 & e^{-i\\frac{2\\pi}{N}2} & e^{-i\\frac{2\\pi}{N}4} & \\dots & e^{-i\\frac{2\\pi}{N}(2(N-1)} \\\\ \\dots & \\dots & \\dots & \\dots \\\\ 1 & e^{-i\\frac{2\\pi}{N}(N-1)} & e^{-i\\frac{2\\pi}{N}2(N-1)} & \\dots & e^{-i\\frac{2\\pi}{N}(N-1)^2}\\end{bmatrix}\\)\n\nDFT = (1/âˆš4)*DFT # ìœ„ì˜ ì •ì˜ ì¶©ì¡±ìœ„í•´ 1/sqrt(4)ê³±í•¨\nDFT'DFT .|> round # ìœ ë‹ˆí„°ë¦¬í–‰ë ¬ì„ì„ í™•ì¸!\n\n4Ã—4 Matrix{ComplexF64}:\n  0.0+0.0im  -0.0-0.0im   0.0-0.0im   0.0-0.0im\n -0.0+0.0im   0.0+0.0im  -0.0-0.0im   0.0-0.0im\n  0.0+0.0im  -0.0+0.0im   0.0+0.0im  -0.0-0.0im\n  0.0+0.0im   0.0+0.0im  -0.0+0.0im   0.0+0.0im"
  },
  {
    "objectID": "posts/Study/Untitled.html",
    "href": "posts/Study/Untitled.html",
    "title": "asdf",
    "section": "",
    "text": "1+1\n\n2\n\n\n\n!git add .\n!git commit -m .\n!git push \n!quarto publish --no-browser --no-prompt \n\n[main 0eda2b1] .\n 2 files changed, 146 insertions(+)\n create mode 100644 posts/Study/.ipynb_checkpoints/Untitled-checkpoint.ipynb\n create mode 100644 posts/Study/Untitled.ipynb\nEnumerating objects: 10, done.\nCounting objects: 100% (10/10), done.\nDelta compression using up to 28 threads\nCompressing objects: 100% (6/6), done.\nWriting objects: 100% (6/6), 1.07 KiB | 1.07 MiB/s, done.\nTotal 6 (delta 4), reused 0 (delta 0)\nremote: Resolving deltas: 100% (4/4), completed with 4 local objects.\nTo https://github.com/seoyeonc/blog.git\n   4e1306e..0eda2b1  main -> main\nFrom https://github.com/seoyeonc/blog\n * branch            gh-pages   -> FETCH_HEAD\nRendering for publish:\n\n[ 1/45] index.qmd\nWARNING: File /home/csy/Dropbox/blog/posts/Study/2022-12-31-Space-study.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/Study/2023-02-01-ch8_DFT.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/Study/Untitled.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/Study/2023-02-02-ch12.2_Weakly Stationary Graph Processes.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/Study/2023-02-05-ch12_2_3_Power Spectral Density and its Estimators.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/Quarto_tip/2023-01-02-quarto_tips.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2022-12-21-ST-GCN_Dataset.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-01-26-guebin.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-01-05-GNAR.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-02-15-ESTGCN_DATASET.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-01-20-Algorithm_traintest.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-01-18-Algorithm_traintest_2.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_7.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-02-08-ESTGCN_GNAR_DATA_4.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2022-12-07-torchgcn.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-01-21-Class.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_6.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2022-12-28-gcn_simulation.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GCN/2023-01-11-Algorithm_EX_1.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GODE/2022-12-01-graph_code_guebin.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GODE/2022-11-19-class_code_for_paper.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GODE/2022-09-02-paper_simulation.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GODE/2022-10-02-Earthquake_real.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GODE/2022-12-27-DFT_study.ipynb in the listing 'listing' contains no metadata.\nWARNING: File /home/csy/Dropbox/blog/posts/GODE/Untitled.ipynb in the listing 'listing' contains no metadata.\n[ 2/45] posts/Study/index.qmd\n[ 3/45] posts/Study/2022-12-31-Space-study.ipynb\n[ 4/45] posts/Study/2023-02-01-ch8_DFT.ipynb\n[ 5/45] posts/Study/Untitled.ipynb\n[ 6/45] posts/Study/2023-02-02-ch12.2_Weakly Stationary Graph Processes.ipynb\n[ 7/45] posts/Study/2023-02-05-ch12_2_3_Power Spectral Density and its Estimators.ipynb\n[ 8/45] posts/Quarto_tip/index.qmd\n[ 9/45] posts/Quarto_tip/2023-01-02-quarto_tips.ipynb\n[10/45] posts/GCN/2022-12-21-ST-GCN_Dataset.ipynb\n[11/45] posts/GCN/index.qmd\n[12/45] posts/GCN/2023-01-26-guebin.ipynb\n[13/45] posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.ipynb\n[14/45] posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.ipynb\n[15/45] posts/GCN/2023-01-05-GNAR.ipynb\n[16/45] posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.ipynb\n[17/45] posts/GCN/2023-02-15-ESTGCN_DATASET.ipynb\n[18/45] posts/GCN/2023-01-20-Algorithm_traintest.ipynb\n[19/45] posts/GCN/2023-01-18-Algorithm_traintest_2.ipynb\n[20/45] posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.ipynb\n[21/45] posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_7.ipynb\n[22/45] posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.ipynb\n[23/45] posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.ipynb\n[24/45] posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.ipynb\n[25/45] posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.ipynb\n[26/45] posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.ipynb\n[27/45] posts/GCN/2023-02-08-ESTGCN_GNAR_DATA_4.ipynb\n[28/45] posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.ipynb\n[29/45] posts/GCN/2022-12-07-torchgcn.ipynb\n[30/45] posts/GCN/2023-01-21-Class.ipynb\n[31/45] posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.ipynb\n[32/45] posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_6.ipynb\n[33/45] posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.ipynb\n[34/45] posts/GCN/2022-12-28-gcn_simulation.ipynb\n[35/45] posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.ipynb\n[36/45] posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.ipynb\n[37/45] posts/GCN/2023-01-11-Algorithm_EX_1.ipynb\n[38/45] posts/GODE/2022-12-01-graph_code_guebin.ipynb\n[39/45] posts/GODE/index.qmd\n[40/45] posts/GODE/2022-11-19-class_code_for_paper.ipynb\n[41/45] posts/GODE/2022-09-02-paper_simulation.ipynb\n[42/45] posts/GODE/2022-10-02-Earthquake_real.ipynb\n[43/45] posts/GODE/2022-12-27-DFT_study.ipynb\n[44/45] posts/GODE/Untitled.ipynb\n[45/45] about.qmd\n\nPreparing worktree (resetting branch 'gh-pages'; was at 4d28be3)\nBranch 'gh-pages' set up to track remote branch 'gh-pages' from 'origin'.\nHEAD is now at 4d28be3 Built site for gh-pages\n[gh-pages 74dec2f] Built site for gh-pages\n 50 files changed, 2350 insertions(+), 1522 deletions(-)\n create mode 100644 posts/Study/Untitled.html\norigin  https://github.com/seoyeonc/blog.git (fetch)\norigin  https://github.com/seoyeonc/blog.git (push)\nTo https://github.com/seoyeonc/blog.git\n   4d28be3..74dec2f  HEAD -> gh-pages\n\nNOTE: GitHub Pages sites use caching so you might need to click the refresh\nbutton within your web browser to see changes after deployment.\n\n[âœ“] Published to https://seoyeonc.github.io/blog/\n\nNOTE: GitHub Pages deployments normally take a few minutes (your site updates\nwill be visible once the deploy completes)"
  },
  {
    "objectID": "posts/Study/2023-02-02-ch12.2_Weakly Stationary Graph Processes.html",
    "href": "posts/Study/2023-02-02-ch12.2_Weakly Stationary Graph Processes.html",
    "title": "Chap 12.2: Weakly Stationary Graph Processes",
    "section": "",
    "text": "Chap 12.2: Weakly Stationary Graph Processes\n\n\nusing LinearAlgebra, FFTW\n\nSimultaneiusly Diagnalizable\n\nmatrix A, Bê°€ ê³ ìœ ë¶„í•´ ê°€ëŠ¥í• ë•Œ, \\(B = V_B \\Lambda_B V_B^{-1}, A = V_A \\Lambda_A V_A^{-1}\\)ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆê³ , íŠ¹íˆ \\(V_A = V_B\\)ë¼ë©´,\n\n\\(AB = V \\Lambda_A V^{-1}V \\Lambda_B V^{-1}, BA = V \\Lambda_B V^{-1}V \\Lambda_A V^{-1}\\)\n\\(V \\Lambda_A \\Lambda_B V^{-1} = AB = BA = V \\Lambda_B \\Lambda_A V^{-1}\\)\n\n\\(\\Lambda\\)ëŠ” diagonal matrixë¼ ê°€ëŠ¥í•˜ë‹¤.\n\n\n\nCommute\n\n\\(AB = BA\\) ê°€ ê°€ëŠ¥í• ë•Œ,\n\nShift invariant filter\n\n\\(z h(z) = h(z)z, h(z) = h_0 Z^0 + h_1 z^{-1} + \\dots + h_{N-1}z^{-(N-1)} \\to\\) z-transform\n\\(Bh(B) = h(B)B, H = h(B) = h_0 B^0 + h_1 B^1 + \\dots h_{N-1}B^{N-1} \\to\\) cycluc shift\n\n\\(h(B) = \\frac{1}{3} + \\frac{1}{3}B + \\frac{1}{3} B^2\\)\n\n\\(\\to\\) matrix : ì„±ì§ˆ 1. ìë£Œ 2. ë³€í™˜\n\n\n\n\\(x\\)ê°€ ì •ìƒì‹œê³„ì—´ì´ë¼ë©´, ëª¨ë“  \\(l = 0,1,2,\\dots\\)ì— ëŒ€í•˜ì—¬ \\(E(XX^H) = E((B^l X)(B^lX)^H)\\)ê°€ ì„±ë¦½í•œë‹¤.\n\\(x\\)ê°€ ì •ìƒì‹œê³„ì—´ì´ë¼ë©´, \\(X = Hn\\)(ë‹¨, \\(n\\)ì€ white noiseë¥¼ ë§Œì¡±í•˜ëŠ” shift invariant operator)\n\n\\(H = \\sum^{N-1}_{l=0} h_l B^l\\) ì´ ì¡´ì¬í•œë‹¤.\n\n\\(\\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\dots \\\\ x_n \\end{bmatrix} = \\begin{bmatrix}\\frac{1}{3} &\\frac{1}{3} & 0 & 0 & \\dots \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & \\dots & \\dots \\\\ \\dots & \\dots & \\dots & \\dots \\\\ \\dots &\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\end{bmatrix} \\times \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\dots \\\\ \\epsilon_n \\end{bmatrix}\\)\n\\(x_1 = (\\epsilon_1 + \\epsilon_2) \\times \\frac{1}{3}\\)\n\\(x_2 = (\\epsilon_1 + \\epsilon_2 + \\epsilon_3) \\times \\frac{1}{3}\\)\n\\(\\dots\\)\n\\(x_t = \\frac{1}{3}\\epsilon_t + \\frac{1}{3}\\epsilon_{t-1} + \\dots + \\frac{1}{3}\\epsilon_{t-N}\\)\n\\(E(e^xt)\\)\n\n\\(E(n) = 0, E(nn^H) = I\\)\n\n\\(E(XX^T) = E(Hnn^TH^T) = HE(nn^T)H = HH^T\\)\n\\(n = \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\dots \\\\ \\epsilon_n \\end{pmatrix}\\)\n\\(nn^T = \\begin{bmatrix} \\epsilon_1\\epsilon_1 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 \\\\ \\epsilon_2\\epsilon_1 & \\epsilon_2\\epsilon_2 & \\epsilon_2\\epsilon_3 \\\\ \\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3\\epsilon_3 \\end{bmatrix}\\)\n\\(E(\\begin{bmatrix} \\epsilon_1\\epsilon_1 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 \\\\ \\epsilon_2\\epsilon_1 & \\epsilon_2\\epsilon_2 & \\epsilon_2\\epsilon_3 \\\\ \\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3\\epsilon_3 \\end{bmatrix}) = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} = I\\)\nex1)\n\\(x(1,2,3) \\xrightarrow[]{h} x^2(1,4,9) = \\tilde{x}\\)\n\\(h(x) = x^2\\)\nex2)\n\\(x(1,2,3) \\xrightarrow[]{h} 2x(2,4,6) = x^2\\)\n\\(h(x) = 2x\\)\n\nx = [1,2,3]\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\n\nH = [2 0 0 \n    0 2 0 \n    0 0 2 ] \n\n3Ã—3 Matrix{Int64}:\n 2  0  0\n 0  2  0\n 0  0  2\n\n\n\nH*x\n\n3-element Vector{Int64}:\n 2\n 4\n 6\n\n\n\nB = [0 1 0 \n    0 0 1 \n    1 0 0 ] \n\n3Ã—3 Matrix{Int64}:\n 0  1  0\n 0  0  1\n 1  0  0\n\n\n\nH*B\n\n3Ã—3 Matrix{Int64}:\n 0  2  0\n 0  0  2\n 2  0  0\n\n\n\nB*H\n\n3Ã—3 Matrix{Int64}:\n 0  2  0\n 0  0  2\n 2  0  0\n\n\n\\(HB = BH\\)\n\nB*x\n\n3-element Vector{Int64}:\n 2\n 3\n 1\n\n\n\nH*B*x\n\n3-element Vector{Int64}:\n 4\n 6\n 2\n\n\nex3)\n\\(x(1,2,3) \\xrightarrow[]{h} \\tilde{x}(1,0,0)\\)\n\nH = [1 0 0 \n    0 0 0 \n    0 0 0 ] \n\n3Ã—3 Matrix{Int64}:\n 1  0  0\n 0  0  0\n 0  0  0\n\n\n\nx = [1,2,3]\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\n\nB = [0 1 0 \n    0 0 1 \n    1 0 0 ] \n\n3Ã—3 Matrix{Int64}:\n 0  1  0\n 0  0  1\n 1  0  0\n\n\n\nH*B\n\n3Ã—3 Matrix{Int64}:\n 0  1  0\n 0  0  0\n 0  0  0\n\n\n\nB*H\n\n3Ã—3 Matrix{Int64}:\n 0  0  0\n 0  0  0\n 1  0  0\n\n\n\\(HB \\neq BH\\)\n\nH*x\n\n3-element Vector{Int64}:\n 1\n 0\n 0\n\n\n\nB*x\n\n3-element Vector{Int64}:\n 2\n 3\n 1\n\n\n\nH*B*x\n\n3-element Vector{Int64}:\n 2\n 0\n 0\n\n\n\nB*H*x\n\n3-element Vector{Int64}:\n 0\n 0\n 1\n\n\n\nZ-transform\nhttps://en.wikipedia.org/wiki/Z-transform\nì–´ë– í•œ ì—°ì†í•¨ìˆ˜ \\(f(x)\\)ê°€ ìˆì„ë•Œ ì•„ë˜ì™€ ê°™ì€ ë³€í™˜ì„ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\n\n\\(\\int^{\\infty}_{-\\infty} f(x) \\times e^{-\\text{ ë³µì†Œìˆ˜ }x} dx \\sim E(e^{-tx}) \\to\\) ë¼í”Œë¼ìŠ¤\n\\(\\int^{\\infty}_{-\\infty}f(x) e^{- \\text{ ìˆœí—ˆìˆ˜ }x} dx \\to\\) í“¨ë¦¬ì— ë³€í™˜\n\n\nì—¬ê¸°ì„œ \\(f(x)\\)ê°€ í™•ë¥ ë°€ë„í•¨ìˆ˜ë¥¼ ì˜ë¯¸í•  ìˆ˜ë„ ìˆìœ¼ë‚˜ signalì„ ì˜ë¯¸í•  ìˆ˜ë„ ìˆë‹¤.\nsignalì„ ì˜ë¯¸í•˜ëŠ” ê²½ìš° ì¤‘ íŠ¹íˆ ì´ì‚° signalì„ ì˜ë¯¸í•  ìˆ˜ë„ ìˆë‹¤.\nì˜ˆë¥¼ ë“¤ì–´ì„œ \\(f(0) = x_0, f(1) = x_1 , \\dots\\)\n\nì´ ê²½ìš° ë¼í”Œë¼ìŠ¤ transformì€ \\(\\int^{\\infty}_{-\\infty}f(t) e^{-st}dt\\),\nì´ì‚°í˜•ì´ë©´ \\(\\sum^{\\infty}_{t=0} x(t) e^{-st} = \\sum^{\\infty}_{t=0} x(t) z^{-t}\\)\n\në‹¨, \\(e^{s} = z\\)\n\n\n\n\\(x\\)ê°€ ì •ìƒì‹œê³„ì—´ì´ë¼ë©´, ì„ì˜ì˜ \\(n\\)ì— ëŒ€í•˜ì—¬ \\(x = Hn\\)ì„ ë§Œì¡±í•˜ëŠ” shift invariant operator Hê°€ í•­ìƒ ì¡´ì¬í•œë‹¤.\n\\(C_x = E(XX^H) = HH^H\\)ë¼ í‘œí˜„ ê°€ëŠ¥, \\(H,B\\)ëŠ” ê°™ì€ ê³ ìœ í–‰ë ¬ì„ ê°€ì§„ë‹¤. \\(\\to\\) \\(C_x\\)ëŠ” \\(B\\)ì™€ ê°™ì€ ê³ ìœ í–‰ë ¬ì„ ê°€ì§„ë‹¤. \\(\\to\\) simultaneously diagonalizable ë„ ë§Œì¡±\n\\(C_x = \\psi \\times \\text{ diagonal matrix } \\times \\psi^H = DFT^H \\times \\text{ diagonal matrix } \\times DFT\\)\n\n\\(DFT^H = DFT\\)ë‹ˆê¹Œ ìˆœì„œëŠ” ìƒê´€ì´ ì—†ìŒ\n\n\\(x\\)ê°€ ì •ìƒì‹œê³„ì—´ì´ë¼ë©´ \\(C_x\\)ëŠ” DFTí–‰ë ¬ë¡œ ëŒ€ê°í™”ê°€ ê°€ëŠ¥í•˜ë‹¤.\n\\(C_x = E(XX^T)\\)\n\\(X_1,X_2,X_3 = X\\)ì¼ ë–„, \\(cov = \\begin{bmatrix} cov(x_1,x_1) & cov(x_1,x_2) & \\dots \\\\ cov(x_2,x_1) & cov(x_2,x_2) & \\dots \\\\ cov(x_3,x_1) & cov(x_3,x_2) & \\dots\\end{bmatrix}\\)\n\\(cov(x_1,x_2) = E(x_1,x_2) - E(x_1)E(x_2) = E(x_1x_2) - 0\\)\n\\(cov(x) = \\begin{bmatrix} E(x_1,x_1) & E(x_1,x_2) & \\dots \\\\ E(x_2,x_1) & E(x_2,x_2) & \\dots \\\\ E(x_3,x_1) & E(x_3,x_2) & \\dots\\end{bmatrix}\\)\nê·¸ëŸ°ë° \\(XX^T = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\\\dots\\end{bmatrix}\\begin{bmatrix} x_1 & x_2 & x_3 & \\dots\\end{bmatrix} = \\begin{bmatrix} x_1x_1 & x_1x_2 & x_1x_3\\\\x_2x_1 & x_2x_2 & x_2x_3 \\\\x_3x_1 & x_3x_2 & x_3x_3 \\end{bmatrix}\\)\n\\(E(XX^T) = E(\\begin{bmatrix} x_1x_1 & x_1x_2 & x_1x_3\\\\x_2x_1 & x_2x_2 & x_2x_3 \\\\x_3x_1 & x_3x_2 & x_3x_3 \\end{bmatrix}) = cov(X) = C_x\\)"
  },
  {
    "objectID": "posts/Study/2023-02-05-ch12_2_3_Power Spectral Density and its Estimators.html",
    "href": "posts/Study/2023-02-05-ch12_2_3_Power Spectral Density and its Estimators.html",
    "title": "Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "",
    "text": "Chap 12.2 ~ 3: Power Spectral Density and its Estimators\n\\(\\star\\) ì •ìƒì‹œê³„ì—´ì€ ìœ í•œì°¨ìˆ˜ì˜ ARMAë¡œ ê·¼ì‚¬í•  ìˆ˜ ìˆë‹¤.\n\\(C_x = E(XX^\\top)\\)ì— ACFëª¨ë“  ì •ë³´ê°€ í•¨ìœ ë˜ì–´ ìˆê³ , ì´ ë–„ \\(X\\)ëŠ” í™•ë¥ ë²¡í„°ì´ë‹¤.\n\\(XX^\\top = \\begin{bmatrix} X_t X_t & X_t X_{t-1} & \\dots \\\\ X_{t-1} X_t & X_{t-1} X_{t-1} & \\dots \\\\ \\dots & \\dots & \\dots \\end{bmatrix}\\)\në˜í•œ ì´ \\(C_x\\)ëŠ” \\(C_x = Vdiag(p)V^{H}\\)ë¡œ ë¶„í•´ ê°€ëŠ¥ \\(\\to\\) ì–‘ì •ì¹˜ í–‰ë ¬, ê³ ìœ ë¶„í•´ ê°€ëŠ¥\nSpectral analysis \\(C_x\\)ì—ì„œ \\(p\\)ë¥¼ íŠ¹ì •í•œ ë’¤ \\(p\\)ì—ì„œ \\(C_x\\) í•´ì„í•˜ëŠ” ë°©ë²•ë¡ \n\\(p = E((V^{H}X))^2\\)\n\\(\\to V^{H}_X = DFT\\times X\\)ì´ë¯€ë¡œ\n\\(DFT: \\sin, \\cos\\) ì£¼ê¸°ì˜ ì¡°í•©ìœ¼ë¡œ ëª¨ë“  ì‹œê³„ì—´ ì„¤ëª… ê°€ëŠ¥\n\\(\\star\\) graph shift operator, GSO, \\(S = V\\Lambda V^{H}\\)\n\\(\\tilde{x} = GFT{x} = V^Hx\\)"
  },
  {
    "objectID": "posts/Study/2023-02-05-ch12_2_3_Power Spectral Density and its Estimators.html#p-í‘œí˜„ì‹",
    "href": "posts/Study/2023-02-05-ch12_2_3_Power Spectral Density and its Estimators.html#p-í‘œí˜„ì‹",
    "title": "Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "\\(p\\) í‘œí˜„ì‹",
    "text": "\\(p\\) í‘œí˜„ì‹\n\n\\(C_x = Vdiag(p)V^H \\sim diag(V^HC_xV), C_x = E(XX^H) \\sim \\frac{1}{R}\\sum^R_{r=1} x_i x_r^H\\)\n\n\n\\(\\hat{p}_{cg} = diag(V^H \\hat{C}_x V) := diag[V^H[\\frac{1}{R}\\sum^R_{r=1} x_r x_r^H] V]\\)\n\\(\\hat{p}_{cg} = diag(V^H \\hat{C}_x V) = diag[V^H x_r x_r^H V]\\)\n\nì–´ì°¨í”¼ ëŒ€ê°ì„  ì›ì†Œì— ì •ë³´ê°€ ìˆì–´ì„œ ëŒ€ê° í–‰ë ¬ë¡œ ì‚°ì¶œë˜ì§€ ì•Šì•˜ì„ë•Œ, ëŒ€ê° í–‰ë ¬ì´ ì•„ë‹Œ ì›ì†Œ ë²„ë ¤ë„ ì˜í–¥ì—†ìŒ\n\n\n\n\\(p=E[(V^Hx)^2] \\sim p=E[(V^Hx)^2] \\sim \\frac{1}{R}\\sum^R_{i=1}|V^Hx_r|^2\\)\n\n\n\\(\\hat{p}_{pg} = \\frac{1}{R}\\sum^R_{r=1} |V^H x_r |^2\\)\n\\(\\hat{p}_{cg} = |V^H x_r |^2\\)\n\ní“¨ë¦¬ì— ë³€í™˜ì—ì„œ \\(\\tilde{x} = V^H x\\) ê²°ê³¼ì— ì ˆëŒ€ê°’ ì·¨í•˜ê³  ì œê³±í•œ ê²ƒê³¼ ê°™ë‹¤.\n\n\n\n\\(C_x = \\sum^N_{i=1} p_ivec(v_i,V^H_i) = G_{np}P \\sim C_x = V diag(p)V^H, \\hat{C}_x G_{np}p\\)\n\n\n3ì˜ \\(p\\)ê°€ íšŒê·€ì—ì„œ \\(\\beta\\) ì—­í• , \\(G_{np}\\) íšŒê·€ì—ì„œ ì„¤ëª…ë³€ìˆ˜ ì—­í• .\n\\(G_{np} V^* \\otimes V\\), í¬ë¡œë„¤ì»¤ ê³± \\(\\begin{pmatrix}A & C \\\\ B & D \\end{pmatrix}\\begin{pmatrix}A & B & C \\\\ D & E & F \\\\G & H & I \\end{pmatrix}\\begin{pmatrix} AA & AB & AC & BA & BB & BC &\\dots \\\\ \\dots\\end{pmatrix}\\)\n\n\nA = [1 3\n    4 5]\nvec(A)\n\n4-element Vector{Int64}:\n 1\n 4\n 3\n 5\n\n\n\nB = [1 2 3\n    4 5 6\n    7 8 9]\n\n3Ã—3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\n\n\nkron(A,B) # í¬ë¡œë„¤ì»¤ ê³±  kronecker product\n\n4Ã—4 Matrix{Int64}:\n  0   5   0  10\n  6   7  12  14\n  0  15   0  20\n 18  21  24  28\n\n\n\nC = [1 2 3]\nD = [2 4 6\n    2 5 7]\n\n2Ã—3 Matrix{Int64}:\n 2  4  6\n 2  5  7\n\n\n\nhcat([kron(C[:,i],D[:,i]) for i in 1:size(C)[2]]...)\n\n2Ã—3 Matrix{Int64}:\n 2   8  18\n 2  10  21\n\n\n\ncolumnwise_kron = \n(C,D) -> hcat([kron(C[:,i],D[:,i]) for i in 1:size(C)[2]]...) # column-wise kronecker product\n\n#11 (generic function with 1 method)\n\n\n\ncolumnwise_kron(C,D)\n\n2Ã—3 Matrix{Int64}:\n 2   8  18\n 2  10  21"
  },
  {
    "objectID": "posts/Study/2023-02-05-ch12_2_3_Power Spectral Density and its Estimators.html#property-12.1",
    "href": "posts/Study/2023-02-05-ch12_2_3_Power Spectral Density and its Estimators.html#property-12.1",
    "title": "Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "Property 12.1",
    "text": "Property 12.1\n\\(f_T = f_x * f_y \\to M_T = M_x \\bullet M_y\\)\në§ˆì¹˜ \\(f_x\\)ë¥¼ \\(M_x\\)ë¡œ ë°”ê¾¸ê³  \\(f_Y\\)ë¥¼ \\(M_Y\\)ë¡œ ë°”ê¾¸ê³  \\(f_T\\)ë¥¼ \\(M_T\\)ë¡œ ë°”ê¾¸ê³  \\(*\\)ì—°ì‚°ì€ \\(\\bullet\\) ì—°ì‚°ìœ¼ë¡œ ë°”ê¾¸ë©´ ë˜ëŠ”ë“¯ì´ ë³´ì´ê³ , ì‹¤ì œë¡œ ê·¸ë ‡ë‹¤.\n\\(Hx = h * x \\to Hx = \\tilde{h} \\bullet \\tilde{x}\\)\ntime domain ì—ì„œ convolution ì—°ì‚°ì€ freq-domain ì—ì„œ * ì—°ì‚°ê³¼ ê°™ìŒ\nì˜ˆì‹œ: Xì˜ pdf ì™€ Yì˜ pdfë¥¼ ê°ê° f,g ë¼ê³  í•˜ì. ê·¸ë¦¬ê³  mgf ë¥¼ ê°ê° F,G ë¼ê³  í•˜ì.\ntime domain: X+Yì˜ pdfëŠ” conv(f,g)\nfreq domain: X+Yì˜ mgfëŠ” F*G\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nhttps://dlsun.github.io/probability/sums-continuous.html\n\nSums of Continuous Random Variables\n\n\n\nimage.png\n\n\nhttps://en.wikipedia.org/wiki/Convolution_theorem\n\n\nConvolution Theorem\nIn mathematics, the convolution theorem states that under suitable conditions the Fourier transform of a convolution of two functions (or signals) is the pointwise product of their Fourier transforms. More generally, convolution in one domain (e.g., time domain) equals point-wise multiplication in the other domain (e.g., frequency domain). Other versions of the convolution theorem are applicable to various Fourier-related transforms.\n\n\n\nimage.png\n\n\nhttps://faculty.math.illinois.edu/~hildebr/370/370mgfproblems.pdf\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/Study/2023-02-05-ch12_2_3_Power Spectral Density and its Estimators.html#rì˜-ê°œë…",
    "href": "posts/Study/2023-02-05-ch12_2_3_Power Spectral Density and its Estimators.html#rì˜-ê°œë…",
    "title": "Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "\\(R\\)ì˜ ê°œë…",
    "text": "\\(R\\)ì˜ ê°œë…\ní™•ë¥ ë³€ìˆ˜\nì˜ˆ) ë™ì „ ë˜ì§€ê¸°ë¥¼ í•œë‹¤ê³  í–ˆì„ë•Œ ì‚¬ê±´, event \\(\\to\\) H ì•, T ë’¤\ní™•ë¥ ë²¡í„°\nì˜ˆ) ë™ì „ ë˜ì§€ê¸°ë¥¼ ë‘ ë²ˆ í–ˆì„ ë–„, ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ì§‘í•©,\n\n\\(HH\\)\n\\(HT\\)\n\\(TH\\)\n\\(TT\\)\n\në™ì „ì„ 100íšŒ ë˜ì§ˆë•Œ, ì´ë–„ ê° ì‚¬ê±´ì€ ë…ë¦½.\n\\(X = \\{ X_1, X_2, \\dots, X_{100}\\}\\)\n\\(E(X_{77}) = 0.5\\)ì¼ ê²ƒì´ë‹¤.\nì´ê²Œ ì •ë§ \\(0.5\\)ì¸ì§€ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ í™•ì¸í•´ë³¸ë‹¤ë©´,\n\n100íšŒì˜ í‰ê· ì„ ì œì‹œí•˜ëŠ” ë°©ë²•(R=1)\nsimulationì„ Në²ˆ í•˜ì—¬ ê·¸ ì¤‘ 77ë²ˆì§¸ì˜ ê°’ì„ í‰ê· ë‚´ëŠ” ë°©ë²•(R=N)\n\nì´ ìˆëŠ”ë°, 2ë²ˆì˜ ë°©ë²•ì€ í˜„ì‹¤ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥. ë”°ë¼ì„œ 1ë²ˆì„ ì œì‹œí•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.\n\nlet \n    N=100\n    n=1\n    p=0.5\n    X=rand(Binomial(n,p),N)\n    \"\"\"\n    mean: $(n*p),simulation: $(mean(X))\"\"\"\nend\n\n\"mean: 0.5,simulation: 0.49\"\n\n\n- 1ë²ˆì˜ ë°©ë²•\n\nhistogram(rand(Bernoulli(0.5),10000),bins=0:0.01:1.01)\n\n\n\n\n\n\n\nimage.png\n\n\n\\(R\\) = 1\n\nì‹¤ì œë¡œ, í•œ ê´€ì¸¡ì¹˜ì—ì„œ ì‹œê³„ì—´ í•˜ë‚˜ë§Œ ê°–ì„ ìˆ˜ ìˆê³ , ì—¬ëŸ¬ê°œì˜ ì‹œê³„ì—´ì„ ê°€ì§„ë‹¤ëŠ” ê²ƒì€ ë¹„í˜„ì‹¤ì ì´ë‹¤.\nì±…ì—ì„œ ëŒë¦° \\(R=100\\)ì˜ ê°œë…ì€ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œì„œ ê°€ëŠ¥,\n\nì™¼ìª½ ìœ„ ì‚¬ì§„: \\(R=1\\)ë•Œ ë¶„ì‚°ì´ í¬ê³ , \\(R=100\\)ì¼ë•Œ TRUEê°’ì´ ì˜ ë§ìŒ\nì™¼ìª½ ì•„ë˜ ì‚¬ì§„: \\(R=1\\)ì¼ë•Œ ë¶„ì‚°ì´ ì‘ì•„ì§€ê³ , R=100ì¼ë•Œ TRUEê°’ê³¼ í¸ì°¨biasê°€ ìƒê²¼ë‹¤.\ní˜„ì‹¤ì ìœ¼ë¡œ \\(R=1\\)ë§Œ ì–»ì–´ì§€ê¸° ë•Œë¬¸ì— ìœ„(periodogram)ë³´ë‹¤ ì•„ë˜(Win. Period) ë°©ë²•ì„ ì“°ëŠ” ê²ƒì´ ë” ë‚«ë‹¤ê³  ì €ìëŠ” ì œì•ˆí•¨\n\n\n\\(\\star R=1000\\) ì¼ë•Œë„ ë™ì¼í•˜ë‹¤ê³  ë§í•˜ê¸° ìœ„í•´ì„œ ì •ìƒì˜ ê°œë…ì´ í•„ìš”í•˜ë‹¤, ì •ìƒì„±ì„ ë„ê³  ìˆì–´ì•¼ ì‹œê³„ì—´ì„ ë§ì´ ê°€ì§€ê³  ìˆì–´ë„ ì¼ì •í•˜ë‹¤ê³  ë§í•  ìˆ˜ ìˆê¸° ë•Œë¬¸"
  },
  {
    "objectID": "posts/Quarto_tip/index.html",
    "href": "posts/Quarto_tip/index.html",
    "title": "Quarto tip",
    "section": "",
    "text": "About tips of quarto blog"
  },
  {
    "objectID": "posts/Quarto_tip/2023-01-02-quarto_tips.html",
    "href": "posts/Quarto_tip/2023-01-02-quarto_tips.html",
    "title": "quarto blog tips",
    "section": "",
    "text": "note, tip, warning, caution, and important.\n\n\ncallout\nThere are three types of callouts.\n:::{.callout-note}\nNote that there are five types of callouts, including:\n`note`, `tip`, `warning`, `caution`, and `important`.\n:::\n\n:::{.callout-tip}\n## Tip With Caption\n\nThis is an example of a callout with a caption.\n:::\n\n\ncollapse\n\n\n\n\n\n\n\ntype\ninfo\n\n\n\n\ndefault\nThe default appearance with colored header and an icon.\n\n\nsimple\nA lighter weight appearance that doesnâ€™t include a colored header background.\n\n\nminimal\nA minimal treatment that applies borders to the callout, but doesnâ€™t include a header background color or icon.\n\n\n\n::: {.callout-note appearance=\"simple\"}\n\n## Pay Attention\n\nUsing callouts is an effective way to highlight content that your reader give special consideration or attention.\n\n:::\n\n\nLists\nMarkdown Syntax Output\n* unordered list\n    + sub-item 1\n    + sub-item 2\n        - sub-sub-item 1\n1. ordered list\n2. item 2\n    i) sub-item 1\n         A.  sub-sub-item 1\n\n\nVideo\n\n\n\nSub figures\n::: {#fig-elephants layout-ncol=2}\n\n![Surus](surus.png){#fig-surus}\n\n![Hanno](hanno.png){#fig-hanno}\n\nFamous Elephants\n:::\n\n\nFigure Panels\n::: {layout-ncol=2}\n![Surus](surus.png)\n\n![Hanno](hanno.png)\n:::"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html",
    "title": "PyTorch ST-GCN Dataset",
    "section": "",
    "text": "PyTorch Geometric Temporal Dataset\nhttps://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/dataset.html#module-torch_geometric_temporal.dataset.chickenpox\në…¼ë¬¸\n|Dataset|Signal|Graph|Frequency|ğ‘‡|ã…£ğ‘‰ã…£ | |:â€“:|:â€“:||:â€“:||:â€“:| |Chickenpox Hungary|Temporal|Static|Weekly|522|20| |Windmill Large|Temporal|Static|Hourly|17,472|319| |Windmill Medium|Temporal|Static|Hourly|17,472|26| |Windmill Small|Temporal|Static|Hourly|17,472|11| |Pedal Me Deliveries|Temporal|Static|Weekly|36|15| |Wikipedia Math|Temporal|Static|Daily|731|1,068| |Twitter Tennis RG|Static|Dynamic|Hourly|120|1000| |Twitter Tennis UO|Static|Dynamic|Hourly|112|1000| |Covid19 England|Temporal|Dynamic|Daily|61|129| |Montevideo Buses|Temporal|Static|Hourly|744|675| |MTM-1 Hand Motions|Temporal|Static|1/24 Seconds|14,469|21|"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#chickenpoxdatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#chickenpoxdatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "ChickenpoxDatasetLoader",
    "text": "ChickenpoxDatasetLoader\nChickenpox Hungary\n\nA spatiotemporal dataset about the officially reported cases of chickenpox in Hungary. The nodes are counties and edges describe direct neighbourhood relationships. The dataset covers the weeks between 2005 and 2015 without missingness.\n\në°ì´í„°ì •ë¦¬\n\nT = 519\nN = 20 # number of nodes\nE = 102 # edges\n\\(f(v,t)\\)ì˜ ì°¨ì›? (1,)\nì‹œê°„ì— ë”°ë¼ì„œ Number of nodesê°€ ë³€í•˜ëŠ”ì§€? False\nì‹œê°„ì— ë”°ë¼ì„œ Number of nodesê°€ ë³€í•˜ëŠ”ì§€? False\nX: (20,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (20,) (N,), \\(f(v,t_4)\\)\nì˜ˆì œì½”ë“œì ìš©ê°€ëŠ¥ì—¬ë¶€: Yes\n\n- Nodes : 20\n\nvertices are counties\n\n-Edges : 102\n\nedges are neighbourhoods\n\n- Time : 519\n\nbetween 2004 and 2014\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import  ChickenpoxDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset(lags=1)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n519\n\n\n\n(data[0][1]).x.type,(data[0][1]).edge_index.type,(data[0][1]).edge_attr.type,(data[0][1]).y.type\n\n(<function Tensor.type>,\n <function Tensor.type>,\n <function Tensor.type>,\n <function Tensor.type>)\n\n\n\nmax((data[4][1]).x[0])\n\ntensor(2.1339)\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(20)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[519, Data(x=[20, 1], edge_index=[2, 102], edge_attr=[102], y=[20])]\n\n\n\nlen(data[0][1].edge_index[0])\n\n102\n\n\n\nedge_list=[]\nfor i in range(519):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(20, 61)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntimeë³„ ê°™ì€ edge ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆë‚˜ í™•ì¸\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import  ChickenpoxDatasetLoader\nloader = ChickenpoxDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.0011,  0.0286,  0.3547,  0.2954]), tensor(0.7106))\n\n\n\\(t=0\\)ì—ì„œ \\(X\\)ì™€ \\(y\\)ë¥¼ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0]\n\ntensor([0.0286, 0.3547, 0.2954, 0.7106])\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0]\n\ntensor([ 0.3547,  0.2954,  0.7106, -0.6831])\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\ní•˜ë‚˜ì˜ ë…¸ë“œì— ê¸¸ì´ê°€ \\(T\\)ì¸ ì‹œê³„ì—´ì´ ë§µí•‘ë˜ì–´ ìˆìŒ. (ë…¸ë“œëŠ” ì´ 20ê°œ)\nê° ë…¸ë“œë§ˆë‹¤ ì•„ë˜ì™€ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ì˜ˆì¸¡ì´ ë¨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{20}\\}, t=1,2,\\dots,519\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:52<00:00,  1.05s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([20, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 102])\n\n\n\n_edge_attr.shape\n\ntorch.Size([102])\n\n\n\n_y.shape\n\ntorch.Size([20])\n\n\n\n_x.shape\n\ntorch.Size([20, 4])\n\n\nx\n\nVertex features are lagged weekly counts of the chickenpox cases (we included 4 lags). y\nThe target is the weekly number of cases for the upcoming week\n\n\n_x\n\ntensor([[-1.0814e-03,  2.8571e-02,  3.5474e-01,  2.9544e-01],\n        [-7.1114e-01, -5.9843e-01,  1.9051e-01,  1.0922e+00],\n        [-3.2281e+00, -2.2910e-01,  1.6103e+00, -1.5487e+00],\n        [ 6.4750e-01, -2.2117e+00, -9.6858e-01,  1.1862e+00],\n        [-1.7302e-01, -9.4717e-01,  1.0347e+00, -6.3751e-01],\n        [ 3.6345e-01, -7.5468e-01,  2.9768e-01, -1.6273e-01],\n        [-3.4174e+00,  1.7031e+00, -1.6434e+00,  1.7434e+00],\n        [-1.9641e+00,  5.5208e-01,  1.1811e+00,  6.7002e-01],\n        [-2.2133e+00,  3.0492e+00, -2.3839e+00,  1.8545e+00],\n        [-3.3141e-01,  9.5218e-01, -3.7281e-01, -8.2971e-02],\n        [-1.8380e+00, -5.8728e-01, -3.5514e-02, -7.2298e-02],\n        [-3.4669e-01, -1.9827e-01,  3.9540e-01, -2.4774e-01],\n        [ 1.4219e+00, -1.3266e+00,  5.2338e-01, -1.6374e-01],\n        [-7.7044e-01,  3.2872e-01, -1.0400e+00,  3.4945e-01],\n        [-7.8061e-01, -6.5022e-01,  1.4361e+00, -1.2864e-01],\n        [-1.0993e+00,  1.2732e-01,  5.3621e-01,  1.9023e-01],\n        [ 2.4583e+00, -1.7811e+00,  5.0732e-02, -9.4371e-01],\n        [ 1.0945e+00, -1.5922e+00,  1.3818e-01,  1.1855e+00],\n        [-7.0875e-01, -2.2460e-01, -7.0875e-01,  1.5630e+00],\n        [-1.8228e+00,  7.8633e-01, -5.6172e-01,  1.2647e+00]])\n\n\n\n_y\n\ntensor([ 0.7106, -0.0725,  2.6099,  1.7870,  0.8024, -0.2614, -0.8370,  1.9674,\n        -0.4212,  0.1655,  1.2519,  2.3743,  0.7877,  0.4531, -0.1721, -0.0614,\n         1.0452,  0.3203, -1.3791,  0.0036])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#pedalmedatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#pedalmedatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "PedalMeDatasetLoader",
    "text": "PedalMeDatasetLoader\nPedal Me Deliveries\n\nA dataset about the number of weekly bicycle package deliveries by Pedal Me in London during 2020 and 2021. Nodes in the graph represent geographical units and edges are proximity based mutual adjacency relationships.\n\në°ì´í„°ì •ë¦¬\n\nT = 33\nV = ì§€ì—­ì˜ ì§‘í•©\nN = 15 # number of nodes\nE = 225 # edges\n\\(f(v,t)\\)ì˜ ì°¨ì›? (1,) # number of deliveries\nì‹œê°„ì— ë”°ë¼ì„œ Nì´ ë³€í•˜ëŠ”ì§€? False\nì‹œê°„ì— ë”°ë¼ì„œ Eê°€ ë³€í•˜ëŠ”ì§€? False\nX: (15,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (15,) (N,), \\(f(v,t_4)\\)\nì˜ˆì œì½”ë“œì ìš©ê°€ëŠ¥ì—¬ë¶€: Yes\n\n- Nodes : 15\n\nvertices are localities\n\n-Edges : 225\n\nedges are spatial_connections\n\n- Time : 33\n\nbetween 2020 and 2021\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import  PedalMeDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = PedalMeDatasetLoader()\n\ndataset = loader.get_dataset(lags=1)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n33\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([15, 1]), torch.Size([2, 225]), torch.Size([225]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(15)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[33, Data(x=[15, 1], edge_index=[2, 225], edge_attr=[225], y=[15])]\n\n\n\nedge_list=[]\nfor i in range(33):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(15, 120)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntimeë³„ ê°™ì€ edge ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆë‚˜ í™•ì¸\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import  PedalMeDatasetLoader\nloader = PedalMeDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([ 3.0574, -0.0477, -0.3076,  0.2437]), tensor(-0.2710))\n\n\n\\(t=0\\)ì—ì„œ \\(X\\)ì™€ \\(y\\)ë¥¼ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0], (data[1][1]).y[0]\n\n(tensor([-0.0477, -0.3076,  0.2437, -0.2710]), tensor(0.2490))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0], (data[2][1]).y[0]\n\n(tensor([-0.3076,  0.2437, -0.2710,  0.2490]), tensor(-0.0357))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\ní•˜ë‚˜ì˜ ë…¸ë“œì— ê¸¸ì´ê°€ \\(T\\)ì¸ ì‹œê³„ì—´ì´ ë§µí•‘ë˜ì–´ ìˆìŒ. (ë…¸ë“œëŠ” ì´ 15ê°œ)\nê° ë…¸ë“œë§ˆë‹¤ ì•„ë˜ì™€ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ì˜ˆì¸¡ì´ ë¨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{15}\\}, t=1,2,\\dots,33\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:03<00:00, 16.04it/s]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([15, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 225])\n\n\n\n_edge_attr.shape\n\ntorch.Size([225])\n\n\n\n_y.shape\n\ntorch.Size([15])\n\n\n\n_x.shape\n\ntorch.Size([15, 4])\n\n\nx\n\nVertex features are lagged weekly counts of the delivery demands (we included 4 lags).\nì£¼ë§ˆë‹¤ ë°°ë‹¬ ìˆ˜ìš”ì˜ ìˆ˜ê°€ ì–¼ë§ˆë‚˜ ë ì§€ percentageë¡œ, t-4ì‹œì ê¹Œì§€?\n\ny\n\nThe target is the weekly number of deliveries the upcoming week. Our dataset consist of more than 30 snapshots (weeks).\nê·¸ ë‹¤ìŒì£¼ì— ë°°ë‹¬ì˜ ìˆ˜ê°€ ëª‡ í¼ì„¼íŠ¸ë¡œ ë°œìƒí• ì§€?\n\n\n_x[0:3]\n\ntensor([[ 3.0574, -0.0477, -0.3076,  0.2437],\n        [ 3.2126,  0.1240,  0.0764,  0.5582],\n        [ 1.9071, -0.8883,  1.5280, -0.7184]])\n\n\n\n_y\n\ntensor([-0.2710,  0.0888,  0.4733,  0.0907, -0.3129,  0.1184,  0.5886, -0.6571,\n         0.2647,  0.2338,  0.1720,  0.5720, -0.9568, -0.4138, -0.5271])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#wikimathsdatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#wikimathsdatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "WikiMathsDatasetLoader",
    "text": "WikiMathsDatasetLoader\nWikipedia Math\n\nContains Wikipedia pages about popular mathematics topics and edges describe the links from one page to another. Features describe the number of daily visits between 2019 and 2021 March.\n\në°ì´í„°ì •ë¦¬\n\nT = 722\nV = ìœ„í‚¤í”¼ë””ì•„ í˜ì´ì§€\nN = 1068 # number of nodes\nE = 27079 # edges\n\\(f(v,t)\\)ì˜ ì°¨ì›? (1,) # í•´ë‹¹í˜ì´ì§€ë¥¼ ìœ ì €ê°€ ë°©ë¬¸í•œ íšŸìˆ˜\nì‹œê°„ì— ë”°ë¼ì„œ Nì´ ë³€í•˜ëŠ”ì§€? False\nì‹œê°„ì— ë”°ë¼ì„œ Eê°€ ë³€í•˜ëŠ”ì§€? False\nX: (1068,8) (N,8), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3),f(v,t_4),f(v,t_5),f(v,t_6),f(v,t_7)\\)\ny: (1068,) (N,), \\(f(v,t_8)\\)\nì˜ˆì œì½”ë“œì ìš©ê°€ëŠ¥ì—¬ë¶€: Yes\n\n- Nodes : 1068\n\nvertices are Wikipedia pages\n\n-Edges : 27079\n\nedges are links between them\n\n- Time : 722\n\nWikipedia pages between March 16th 2019 and March 15th 2021\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import  WikiMathsDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n722\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([1068, 8]), torch.Size([2, 27079]), torch.Size([27079]))\n\n\n\n(data[10][1]).x\n\ntensor([[ 0.4972,  0.6838,  0.7211,  ..., -0.8513,  0.1881,  1.3820],\n        [ 0.5457,  0.6016,  0.7071,  ..., -0.4599, -0.6089, -0.0626],\n        [ 0.6305,  1.1404,  0.8779,  ..., -0.5370,  0.7422,  0.3862],\n        ...,\n        [ 0.8699,  0.5451,  1.9254,  ..., -0.8351,  0.3828,  0.3828],\n        [ 0.2451,  0.9629,  1.0526,  ..., -0.9213,  0.8731, -0.1138],\n        [ 0.0200, -0.0871,  0.2342,  ..., -0.4712,  0.0717,  0.2859]])\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(1068)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(722):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(1068, 27079)\n\n\n\nnx.draw(G,node_color='green',node_size=100,width=1)\n\n\n\n\n\ntimeë³„ ê°™ì€ edge ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆë‚˜ í™•ì¸\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\nnp.where(data[11][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\nnp.where(data[11][1].edge_index != data[20][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\nhttps://www.kaggle.com/code/mapologo/loading-wikipedia-math-essentials\n\nfrom torch_geometric_temporal.dataset import  WikiMathsDatasetLoader\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset(lags=8)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.4323, -0.4739,  0.2659,  0.4844,  0.5367,  0.6412,  0.2179, -0.7617]),\n tensor(-0.4067))\n\n\n\\(t=0\\)ì—ì„œ \\(X\\)ì™€ \\(y\\)ë¥¼ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ.\n\nX:= \\(x_0,x_1,x_2,x_3,,x_4,x_5,x_6,x_7\\)\ny:= \\(x_9\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-0.4739,  0.2659,  0.4844,  0.5367,  0.6412,  0.2179, -0.7617, -0.4067]),\n tensor(0.3064))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8\\)\ny:= \\(x_9\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([ 0.2659,  0.4844,  0.5367,  0.6412,  0.2179, -0.7617, -0.4067,  0.3064]),\n tensor(0.4972))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9\\)\ny:=\\(x_{10}\\)\n\ní•˜ë‚˜ì˜ ë…¸ë“œì— ê¸¸ì´ê°€ \\(T\\)ì¸ ì‹œê³„ì—´ì´ ë§µí•‘ë˜ì–´ ìˆìŒ. (ë…¸ë“œëŠ” ì´ 1068ê°œ)\nê° ë…¸ë“œë§ˆë‹¤ ì•„ë˜ì™€ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ì˜ˆì¸¡ì´ ë¨\n\n\\((x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7) \\to (x_8)\\)\n\\((x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8) \\to (x_9)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{1068}\\}, t=1,2,\\dots,722\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=8, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [09:28<00:00, 11.37s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([1068, 8])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 27079])\n\n\nì–´ë–¤ í˜ì´ì§€ì— referê°€ ë˜ì—ˆëŠ”ì§€\n\n_edge_index[0][:5],_edge_index[1][:5]\n\n(tensor([0, 0, 0, 0, 0]), tensor([1, 2, 3, 4, 5]))\n\n\n\n_edge_attr.shape\n\ntorch.Size([27079])\n\n\n\n_edge_attr[:5]\n\ntensor([1., 4., 2., 2., 5.])\n\n\n\nWeights represent the number of links found at the source Wikipedia page linking to the target Wikipedia page.\n\nê°€ì¤‘ì¹˜ëŠ” ì—£ì§€ë³„ í•œ í˜ì´ì§€ì— referë˜ì—ˆëŠ”ì§€, ëª‡ ë²ˆ ë˜ì—ˆë‚˜ ìˆ˜ ë‚˜ì˜´\n\n_y.shape\n\ntorch.Size([1068])\n\n\n\n_x.shape\n\ntorch.Size([1068, 8])\n\n\nx\n\nlag ë¥¼ ëª‡ìœ¼ë¡œ ì§€ì •í•˜ëŠëƒì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì¶”ì¶œ\n\ny\n\nThe target is the daily user visits to the Wikipedia pages between March 16th 2019 and March 15th 2021 which results in 731 periods.\në§¤ì¼ ìœ„í‚¤í”¼ë””ì•„ í•´ë‹¹ í˜ì´ì§€ì— ëª‡ ëª…ì˜ ìœ ì €ê°€ ë°©ë¬¸í•˜ëŠ”ì§€!\nìŒìˆ˜ê°€ ì™œ ë‚˜ì˜¤ì§€..\n\n\n_x[0:3]\n\ntensor([[-0.4323, -0.4739,  0.2659,  0.4844,  0.5367,  0.6412,  0.2179, -0.7617],\n        [-0.4041, -0.4165, -0.0751,  0.1484,  0.4153,  0.4464, -0.3916, -0.8137],\n        [-0.3892,  0.0634,  0.5913,  0.5370,  0.4646,  0.2776, -0.0724, -0.8116]])\n\n\n\n_y[:3]\n\ntensor([-0.4067, -0.1620, -0.4043])\n\n\n\ny_hat[:3].data\n\ntensor([[-0.0648],\n        [ 0.0314],\n        [-1.0724]])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#windmilloutputlargedatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#windmilloutputlargedatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "WindmillOutputLargeDatasetLoader",
    "text": "WindmillOutputLargeDatasetLoader\nHourly energy output of windmills from a European country for more than 2 years. Vertices represent 319 windmills and weighted edges describe the strength of relationships. The target variable allows for regression tasks.\në°ì´í„°ì •ë¦¬\n\nT = 17470\nV = í’ë ¥ë°œì „ì†Œ\nN = 319 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)ì˜ ì°¨ì›? (1,) # Hourly energy output\nì‹œê°„ì— ë”°ë¼ì„œ Nì´ ë³€í•˜ëŠ”ì§€? False\nì‹œê°„ì— ë”°ë¼ì„œ Eê°€ ë³€í•˜ëŠ”ì§€? False\nX: (319,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (319,) (N,), \\(f(v,t_4)\\)\nì˜ˆì œì½”ë“œì ìš©ê°€ëŠ¥ì—¬ë¶€: Yes\n\n- Nodes : 319\n\nvertices represent 319 windmills\n\n-Edges : 101761\n\nweighted edges describe the strength of relationships.\n\n- Time : 17470\n\nmore than 2 years\n\n\nfrom torch_geometric_temporal.dataset import  WindmillOutputLargeDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WindmillOutputLargeDatasetLoader()\n\ndataset = loader.get_dataset(lags=1)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n17470\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([319, 1]), torch.Size([2, 101761]), torch.Size([101761]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(319)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[17470, Data(x=[319, 1], edge_index=[2, 101761], edge_attr=[101761], y=[319])]\n\n\ntimeì´ ë„ˆë¬´ ë§ì•„ì„œ ì¼ë¶€ë§Œ ì‹œê°í™”í•¨!!\n\nedge_list=[]\nfor i in range(1000):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(319, 51040)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntimeë³„ ê°™ì€ edge ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆë‚˜ í™•ì¸\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import  WindmillOutputLargeDatasetLoader\nloader = WindmillOutputLargeDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.5711, -0.7560,  2.6278, -0.8674]), tensor(-0.9877))\n\n\n\\(t=0\\)ì—ì„œ \\(X\\)ì™€ \\(y\\)ë¥¼ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0], (data[1][1]).y[0]\n\n(tensor([-0.7560,  2.6278, -0.8674, -0.9877]), tensor(-0.8583))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([ 2.6278, -0.8674, -0.9877, -0.8583]), tensor(0.4282))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\ní•˜ë‚˜ì˜ ë…¸ë“œì— ê¸¸ì´ê°€ \\(T\\)ì¸ ì‹œê³„ì—´ì´ ë§µí•‘ë˜ì–´ ìˆìŒ. (ë…¸ë“œëŠ” ì´ 319ê°œ)\nê° ë…¸ë“œë§ˆë‹¤ ì•„ë˜ì™€ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ì˜ˆì¸¡ì´ ë¨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{319}\\}, t=1,2,\\dots,17470\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(5)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [1:06:03<00:00, 792.70s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([319, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 101761])\n\n\n\n_edge_attr.shape\n\ntorch.Size([101761])\n\n\n\n_y.shape\n\ntorch.Size([319])\n\n\n\n_x.shape\n\ntorch.Size([319, 4])\n\n\nx\n\n\n\ny\n\nThe target variable allows for regression tasks.\n\n\n_x[0:3]\n\ntensor([[-0.5711, -0.7560,  2.6278, -0.8674],\n        [-0.6936, -0.7264,  2.4113, -0.6052],\n        [-0.8666, -0.7785,  2.2759, -0.6759]])\n\n\n\n_y[0]\n\ntensor(-0.9877)"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#windmilloutputmediumdatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#windmilloutputmediumdatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "WindmillOutputMediumDatasetLoader",
    "text": "WindmillOutputMediumDatasetLoader\nHourly energy output of windmills from a European country for more than 2 years. Vertices represent 26 windmills and weighted edges describe the strength of relationships. The target variable allows for regression tasks.\në°ì´í„°ì •ë¦¬\n\nT = 17470\nV = í’ë ¥ë°œì „ì†Œ\nN = 319 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)ì˜ ì°¨ì›? (1,) # Hourly energy output\nì‹œê°„ì— ë”°ë¼ì„œ Nì´ ë³€í•˜ëŠ”ì§€? False\nì‹œê°„ì— ë”°ë¼ì„œ Eê°€ ë³€í•˜ëŠ”ì§€? False\nX: (319,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (319,) (N,), \\(f(v,t_4)\\)\nì˜ˆì œì½”ë“œì ìš©ê°€ëŠ¥ì—¬ë¶€: Yes\n\n- Nodes : 26\n\nvertices represent 26 windmills\n\n-Edges : 225\n\nweighted edges describe the strength of relationships\n\n- Time : 676\n\nmore than 2 years\n\n\nfrom torch_geometric_temporal.dataset import  WindmillOutputMediumDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WindmillOutputMediumDatasetLoader()\n\ndataset = loader.get_dataset(lags=1)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n17470\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([26, 1]), torch.Size([2, 676]), torch.Size([676]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(26)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[17470, Data(x=[26, 1], edge_index=[2, 676], edge_attr=[676], y=[26])]\n\n\n\nedge_list=[]\nfor i in range(17463):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(26, 351)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntimeë³„ ê°™ì€ edge ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆë‚˜ í™•ì¸\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import  WindmillOutputMediumDatasetLoader\nloader = WindmillOutputMediumDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.2170, -0.2055, -0.1587, -0.1930]), tensor(-0.2149))\n\n\n\\(t=0\\)ì—ì„œ \\(X\\)ì™€ \\(y\\)ë¥¼ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-0.2055, -0.1587, -0.1930, -0.2149]), tensor(-0.2336))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([-0.1587, -0.1930, -0.2149, -0.2336]), tensor(-0.1785))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\ní•˜ë‚˜ì˜ ë…¸ë“œì— ê¸¸ì´ê°€ \\(T\\)ì¸ ì‹œê³„ì—´ì´ ë§µí•‘ë˜ì–´ ìˆìŒ. (ë…¸ë“œëŠ” ì´ 26ê°œ)\nê° ë…¸ë“œë§ˆë‹¤ ì•„ë˜ì™€ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ì˜ˆì¸¡ì´ ë¨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{26}\\}, t=1,2,\\dots,177470\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(5)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:23<00:00, 40.73s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([26, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 676])\n\n\n\n_edge_attr.shape\n\ntorch.Size([676])\n\n\n\n_y.shape\n\ntorch.Size([26])\n\n\n\n_x.shape\n\ntorch.Size([26, 4])\n\n\nx\n\n\n\ny\n\nThe target variable allows for regression tasks.\n\n\n_x[0:3]\n\ntensor([[-0.2170, -0.2055, -0.1587, -0.1930],\n        [-0.1682, -0.2708, -0.1051,  1.1786],\n        [ 1.1540, -0.6707, -0.8291, -0.6823]])\n\n\n\n_y[0]\n\ntensor(-0.2149)"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#windmilloutputsmalldatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#windmilloutputsmalldatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "WindmillOutputSmallDatasetLoader",
    "text": "WindmillOutputSmallDatasetLoader\nHourly energy output of windmills from a European country for more than 2 years. Vertices represent 11 windmills and weighted edges describe the strength of relationships. The target variable allows for regression tasks.\në°ì´í„°ì •ë¦¬\n\nT = 17470\nV = í’ë ¥ë°œì „ì†Œ\nN = 319 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)ì˜ ì°¨ì›? (1,) # Hourly energy output\nì‹œê°„ì— ë”°ë¼ì„œ Nì´ ë³€í•˜ëŠ”ì§€? False\nì‹œê°„ì— ë”°ë¼ì„œ Eê°€ ë³€í•˜ëŠ”ì§€? False\nX: (319,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (319,) (N,), \\(f(v,t_4)\\)\nì˜ˆì œì½”ë“œì ìš©ê°€ëŠ¥ì—¬ë¶€: Yes\n\n- Nodes : 11\n\nvertices represent 11 windmills\n\n-Edges : 121\n\nweighted edges describe the strength of relationships\n\n- Time : 17470\n\nmore than 2 years\n\n\nfrom torch_geometric_temporal.dataset import WindmillOutputSmallDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WindmillOutputSmallDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n17463\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([11, 8]), torch.Size([2, 121]), torch.Size([121]))\n\n\n\ndata[-1]\n\n[17463, Data(x=[11, 8], edge_index=[2, 121], edge_attr=[121], y=[11])]\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(11)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(17463):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(11, 66)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntimeë³„ ê°™ì€ edge ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆë‚˜ í™•ì¸\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import WindmillOutputSmallDatasetLoader\nloader = WindmillOutputSmallDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([ 0.8199, -0.4972,  0.4923, -0.8299]), tensor(-0.6885))\n\n\n\\(t=0\\)ì—ì„œ \\(X\\)ì™€ \\(y\\)ë¥¼ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-0.4972,  0.4923, -0.8299, -0.6885]), tensor(0.7092))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([ 0.4923, -0.8299, -0.6885,  0.7092]), tensor(-0.9356))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\ní•˜ë‚˜ì˜ ë…¸ë“œì— ê¸¸ì´ê°€ \\(T\\)ì¸ ì‹œê³„ì—´ì´ ë§µí•‘ë˜ì–´ ìˆìŒ. (ë…¸ë“œëŠ” ì´ 11ê°œ)\nê° ë…¸ë“œë§ˆë‹¤ ì•„ë˜ì™€ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ì˜ˆì¸¡ì´ ë¨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{11}\\}, t=1,2,\\dots,17463\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(5)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:55<00:00, 35.01s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([11, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 121])\n\n\n\n_edge_attr.shape\n\ntorch.Size([121])\n\n\n\n_y.shape\n\ntorch.Size([11])\n\n\n\n_x.shape\n\ntorch.Size([11, 4])\n\n\nx\n\n\n\ny\n\nThe target variable allows for regression tasks.\n\n\n_x[0:3]\n\ntensor([[ 0.8199, -0.4972,  0.4923, -0.8299],\n        [ 1.1377, -0.3742,  0.3668, -0.8333],\n        [ 0.9979, -0.5643,  0.4070, -0.8918]])\n\n\n\n_y\n\ntensor([-0.6885, -0.6594, -0.6303, -0.6983, -0.5416, -0.6186, -0.6031, -0.7580,\n        -0.6659, -0.5948, -0.5088])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#metrladatasetloader_real-world-traffic-dataset",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#metrladatasetloader_real-world-traffic-dataset",
    "title": "PyTorch ST-GCN Dataset",
    "section": "METRLADatasetLoader_real world traffic dataset",
    "text": "METRLADatasetLoader_real world traffic dataset\nA traffic forecasting dataset based on Los Angeles Metropolitan traffic conditions. The dataset contains traffic readings collected from 207 loop detectors on highways in Los Angeles County in aggregated 5 minute intervals for 4 months between March 2012 to June 2012.\në°ì´í„°ì •ë¦¬\n\nT = 33\nV = êµ¬ì—­\nN = 207 # number of nodes\nE = 225\n\\(f(v,t)\\)ì˜ ì°¨ì›? (3,) # Hourly energy output\nì‹œê°„ì— ë”°ë¼ì„œ Nì´ ë³€í•˜ëŠ”ì§€? False\nì‹œê°„ì— ë”°ë¼ì„œ Eê°€ ë³€í•˜ëŠ”ì§€? False\nX: (207,4) (N,2,12), \\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11}\\)\ny: (207,) (N,), \\((x_{12})\\)\nì˜ˆì œì½”ë“œì ìš©ê°€ëŠ¥ì—¬ë¶€: No\n\nhttps://arxiv.org/pdf/1707.01926.pdf\n- Nodes : 207\n\nvertices are localities\n\n-Edges : 225\n\nedges are spatial_connections\n\n- Time : 33\n\nbetween 2020 and 2021\nper weeks\n\n\nfrom torch_geometric_temporal.dataset import METRLADatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = METRLADatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n34248\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([207, 2, 12]), torch.Size([2, 1722]), torch.Size([1722]))\n\n\n\ndata[-1]\n\n[34248,\n Data(x=[207, 2, 12], edge_index=[2, 1722], edge_attr=[1722], y=[207, 12])]\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(20)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(1000):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(207, 1520)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntimeë³„ ê°™ì€ edge ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆë‚˜ í™•ì¸\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\në…¼ë¬¸ ë‚´ìš© ì¤‘\n\n\n\nimage.png\n\n\n\nfrom torch_geometric_temporal.dataset import METRLADatasetLoader\nloader = METRLADatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\n\n\n\n\n\nNote\n\n\n\nlags option ì—†ì–´ì„œ error ëœ¸ : get_dataset() got an unexpected keyword argument â€˜lagsâ€™\n\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([[ 0.5332,  0.4486,  0.5146, -2.6522, -2.6522,  0.1847,  0.6383,  0.4961,\n           0.7497,  0.4899,  0.5751,  0.4280],\n         [-1.7292, -1.7171, -1.7051, -1.6930, -1.6810, -1.6689, -1.6569, -1.6448,\n          -1.6328, -1.6207, -1.6087, -1.5966]]),\n tensor([0.3724, 0.2452, 0.4961, 0.6521, 0.1126, 0.5311, 0.5091, 0.4713, 0.4218,\n         0.3909, 0.4761, 0.5641]))\n\n\n\\(t=0\\)ì—ì„œ \\(X,Z\\)ì™€ \\(y\\)ë¥¼ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ.\n\nX:= \\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11}\\)\nZ:= \\(z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11}\\)\ny:= \\(x_{12}\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([[ 0.4486,  0.5146, -2.6522, -2.6522,  0.1847,  0.6383,  0.4961,  0.7497,\n           0.4899,  0.5751,  0.4280,  0.3724],\n         [-1.7171, -1.7051, -1.6930, -1.6810, -1.6689, -1.6569, -1.6448, -1.6328,\n          -1.6207, -1.6087, -1.5966, -1.5846]]),\n tensor([ 0.2452,  0.4961,  0.6521,  0.1126,  0.5311,  0.5091,  0.4713,  0.4218,\n          0.3909,  0.4761,  0.5641, -0.0022]))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12}\\)\nZ:= \\(z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12}\\)\ny:= \\(x_{13}\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([[ 0.5146, -2.6522, -2.6522,  0.1847,  0.6383,  0.4961,  0.7497,  0.4899,\n           0.5751,  0.4280,  0.3724,  0.2452],\n         [-1.7051, -1.6930, -1.6810, -1.6689, -1.6569, -1.6448, -1.6328, -1.6207,\n          -1.6087, -1.5966, -1.5846, -1.5725]]),\n tensor([ 0.4961,  0.6521,  0.1126,  0.5311,  0.5091,  0.4713,  0.4218,  0.3909,\n          0.4761,  0.5641, -0.0022,  0.4218]))\n\n\n\nX:= \\(x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12},x_{13}\\)\nZ:= \\(z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12},z_{13}\\)\ny:= \\(x_{14}\\)\n\ní•˜ë‚˜ì˜ ë…¸ë“œì— ê¸¸ì´ê°€ \\(T\\)ì¸ ì‹œê³„ì—´ì´ ë§µí•‘ë˜ì–´ ìˆìŒ. (ë…¸ë“œëŠ” ì´ 207ê°œ)\nê° ë…¸ë“œë§ˆë‹¤ ì•„ë˜ì™€ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ì˜ˆì¸¡ì´ ë¨\n\n\\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11} \\to (x_{12})\\)\n\\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12},z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12} \\to (x_{13})\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{207}\\}, t=1,2,\\dots,34248\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=1, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([207, 2, 12])\n\n\n\nnode 207ê°œ, traffic sensor 2ê°œ\n\n\n_edge_index.shape\n\ntorch.Size([2, 1722])\n\n\n\n_edge_attr.shape\n\ntorch.Size([1722])\n\n\n\n_y.shape\n\ntorch.Size([207, 12])\n\n\n\n_x.shape\n\ntorch.Size([207, 2, 12])\n\n\ny\n\ntraffic speed\n\n\n_x[0]\n\ntensor([[ 0.5332,  0.4486,  0.5146, -2.6522, -2.6522,  0.1847,  0.6383,  0.4961,\n          0.7497,  0.4899,  0.5751,  0.4280],\n        [-1.7292, -1.7171, -1.7051, -1.6930, -1.6810, -1.6689, -1.6569, -1.6448,\n         -1.6328, -1.6207, -1.6087, -1.5966]])\n\n\n\n_y[0]\n\ntensor([0.3724, 0.2452, 0.4961, 0.6521, 0.1126, 0.5311, 0.5091, 0.4713, 0.4218,\n        0.3909, 0.4761, 0.5641])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#pemsbaydatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#pemsbaydatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "PemsBayDatasetLoader",
    "text": "PemsBayDatasetLoader\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1111/tgis.12644\nA traffic forecasting dataset as described in Diffusion Convolution Layer Paper.\nThis traffic dataset is collected by California Transportation Agencies (CalTrans) Performance Measurement System (PeMS). It is represented by a network of 325 traffic sensors in the Bay Area with 6 months of traffic readings ranging from Jan 1st 2017 to May 31th 2017 in 5 minute intervals.\në°ì´í„°ì •ë¦¬\n\nT = 17470\nV = í’ë ¥ë°œì „ì†Œ\nN = 325 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)ì˜ ì°¨ì›? (1,) # Hourly energy output\nì‹œê°„ì— ë”°ë¼ì„œ Nì´ ë³€í•˜ëŠ”ì§€? False\nì‹œê°„ì— ë”°ë¼ì„œ Eê°€ ë³€í•˜ëŠ”ì§€? False\nX: (325,2,12) (N,2,12),\n\n\\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11}\\)\n\\(z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11}\\)\n\ny: (325,) (N,2,12),\n\n\\(x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23},x_{24}\\)\n\\(z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23},z_{24}\\)\n\nì˜ˆì œì½”ë“œì ìš©ê°€ëŠ¥ì—¬ë¶€: No\n\n- Nodes : 325\n\nvertices are sensors\n\n-Edges : 2694\n\nweighted edges are between seonsor paris measured by the road nretwork distance\n\n- Time : 52081\n\n6 months of traffic readings ranging from Jan 1st 2017 to May 31th 2017 in 5 minute intervals\n\n\nfrom torch_geometric_temporal.dataset import PemsBayDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = PemsBayDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n52081\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([325, 2, 12]), torch.Size([2, 2694]), torch.Size([2694]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(325)).tolist()\n\n\ndata[-1]\n\n[52081,\n Data(x=[325, 2, 12], edge_index=[2, 2694], edge_attr=[2694], y=[325, 2, 12])]\n\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(1000):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(325, 2404)\n\n\n\nnx.draw(G,node_color='green',node_size=50,font_color='white',width=1)\n\n\n\n\n\ntimeë³„ ê°™ì€ edge ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆë‚˜ í™•ì¸\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import PemsBayDatasetLoader\nloader = PemsBayDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([[ 0.9821,  0.9928,  1.0251,  1.0574,  1.0466,  1.0681,  0.9821,  1.0251,\n           1.0143,  0.9928,  0.9498,  0.9821],\n         [-1.6127, -1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397, -1.5275,\n          -1.5153, -1.5032, -1.4910, -1.4788]]),\n tensor([[ 1.0143,  0.9821,  0.9821,  1.0036,  1.0143,  0.9605,  0.9498,  1.0251,\n           0.9928,  0.9928,  0.9498,  0.9928],\n         [-1.4667, -1.4545, -1.4423, -1.4302, -1.4180, -1.4058, -1.3937, -1.3815,\n          -1.3694, -1.3572, -1.3450, -1.3329]]))\n\n\n\\(t=0\\)ì—ì„œ \\(X,Z\\)ì™€ \\(y,s\\)ë¥¼ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ.\n\nX:= \\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11}\\)\nZ:= \\(z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11}\\)\ny:= \\(x_{12},x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23}\\)\ns:= \\(z_{12},z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23}\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([[ 0.9928,  1.0251,  1.0574,  1.0466,  1.0681,  0.9821,  1.0251,  1.0143,\n           0.9928,  0.9498,  0.9821,  1.0143],\n         [-1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397, -1.5275, -1.5153,\n          -1.5032, -1.4910, -1.4788, -1.4667]]),\n tensor([[ 0.9821,  0.9821,  1.0036,  1.0143,  0.9605,  0.9498,  1.0251,  0.9928,\n           0.9928,  0.9498,  0.9928,  0.9821],\n         [-1.4545, -1.4423, -1.4302, -1.4180, -1.4058, -1.3937, -1.3815, -1.3694,\n          -1.3572, -1.3450, -1.3329, -1.3207]]))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12}\\)\nZ:= \\(z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12}\\)\ny:= \\(x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23},x_{24}\\)\ns:= \\(z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23},z_{24}\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([[ 1.0251,  1.0574,  1.0466,  1.0681,  0.9821,  1.0251,  1.0143,  0.9928,\n           0.9498,  0.9821,  1.0143,  0.9821],\n         [-1.5883, -1.5762, -1.5640, -1.5518, -1.5397, -1.5275, -1.5153, -1.5032,\n          -1.4910, -1.4788, -1.4667, -1.4545]]),\n tensor([[ 0.9821,  1.0036,  1.0143,  0.9605,  0.9498,  1.0251,  0.9928,  0.9928,\n           0.9498,  0.9928,  0.9821,  1.0143],\n         [-1.4423, -1.4302, -1.4180, -1.4058, -1.3937, -1.3815, -1.3694, -1.3572,\n          -1.3450, -1.3329, -1.3207, -1.3085]]))\n\n\n\nX:= \\(x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12},x_{13}\\)\nZ:= \\(z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12},z_{13}\\)\ny:= \\(x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23},x_{24},x_{25}\\)\ns:= \\(z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23},z_{24},z_{25}\\)\n\ní•˜ë‚˜ì˜ ë…¸ë“œì— ê¸¸ì´ê°€ \\(T\\)ì¸ ì‹œê³„ì—´ì´ ë§µí•‘ë˜ì–´ ìˆìŒ. (ë…¸ë“œëŠ” ì´ 325ê°œ)\nê° ë…¸ë“œë§ˆë‹¤ ì•„ë˜ì™€ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ì˜ˆì¸¡ì´ ë¨\n\n\\(x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11} \\to x_{12},x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23}\\)\n\\(z_0,z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11} \\to z_{12},z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23}\\)\n\\(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10},x_{11},x_{12} \\to x_{13},x_{14},x_{15},x_{16},x_{17},x_{18},x_{19},x_{20},x_{21},x_{22},x_{23},x_{24}\\)\n\\(z_1,z_2,z_3,z_4,z_5,z_6,z_7,z_8,z_9,z_{10},z_{11},z_{12} \\to z_{13},z_{14},z_{15},z_{16},z_{17},z_{18},z_{19},z_{20},z_{21},z_{22},z_{23},z_{24}\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{325}\\}, t=1,2,\\dots,52081\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([325, 2, 12])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 2694])\n\n\n\n_edge_attr.shape\n\ntorch.Size([2694])\n\n\n\n_y.shape\n\ntorch.Size([325, 2, 12])\n\n\n\n_x.shape\n\ntorch.Size([325, 2, 12])\n\n\nx\n\n.!\n\ny\n\ncapturing temporal dependencies..?\n\nedges connect sensors\nFor instance, the traffic conditions on one road on Wednesday at 3:00 p.m. are similar to the traffic conditions on Thursday at the same time.\n\n_x[0:3]\n\ntensor([[[ 0.9821,  0.9928,  1.0251,  1.0574,  1.0466,  1.0681,  0.9821,\n           1.0251,  1.0143,  0.9928,  0.9498,  0.9821],\n         [-1.6127, -1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397,\n          -1.5275, -1.5153, -1.5032, -1.4910, -1.4788]],\n\n        [[ 0.6054,  0.5839,  0.6592,  0.6269,  0.6808,  0.6377,  0.6700,\n           0.6054,  0.6162,  0.6162,  0.5839,  0.5947],\n         [-1.6127, -1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397,\n          -1.5275, -1.5153, -1.5032, -1.4910, -1.4788]],\n\n        [[ 0.9390,  0.9175,  0.8960,  0.9175,  0.9067,  0.9175,  0.9175,\n           0.8852,  0.9283,  0.8960,  0.9067,  0.8960],\n         [-1.6127, -1.6005, -1.5883, -1.5762, -1.5640, -1.5518, -1.5397,\n          -1.5275, -1.5153, -1.5032, -1.4910, -1.4788]]])\n\n\n\n_y[0]\n\ntensor([[ 1.0143,  0.9821,  0.9821,  1.0036,  1.0143,  0.9605,  0.9498,  1.0251,\n          0.9928,  0.9928,  0.9498,  0.9928],\n        [-1.4667, -1.4545, -1.4423, -1.4302, -1.4180, -1.4058, -1.3937, -1.3815,\n         -1.3694, -1.3572, -1.3450, -1.3329]])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#englandcoviddatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#englandcoviddatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "EnglandCovidDatasetLoader",
    "text": "EnglandCovidDatasetLoader\nCovid19 England\n\nA dataset about mass mobility between regions in England and the number of confirmed COVID-19 cases from March to May 2020 [38]. Each day contains a different mobility graph and node features corresponding to the number of cases in the previous days. Mobility stems from Facebook Data For Good 1 and cases from gov.uk 2\n\nhttps://arxiv.org/pdf/2009.08388.pdf\në°ì´í„°ì •ë¦¬\n\nT = 52\nV = ì§€ì—­\nN = 129 # number of nodes\nE = 2158\n\\(f(v,t)\\)ì˜ ì°¨ì›? (1,) # ì½”ë¡œë‚˜í™•ì§„ììˆ˜\nì‹œê°„ì— ë”°ë¼ì„œ Number of nodesê°€ ë³€í•˜ëŠ”ì§€? False\nì‹œê°„ì— ë”°ë¼ì„œ Number of nodesê°€ ë³€í•˜ëŠ”ì§€? False\nX: (20,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (20,) (N,), \\(f(v,t_4)\\)\nì˜ˆì œì½”ë“œì ìš©ê°€ëŠ¥ì—¬ë¶€: Yes\n\n- Nodes : 129\n\nvertices are correspond to the number of COVID-19 cases in the region in the past window days.\n\n-Edges : 2158\n\nthe spatial edges capture county-to-county movement at a specific date, and a county is connected to a number of past instances of itself with temporal edges.\n\n- Time : 52\n\nfrom 3 March to 12 of May\n\n\nfrom torch_geometric_temporal.dataset import EnglandCovidDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = EnglandCovidDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n52\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([129, 8]), torch.Size([2, 2158]), torch.Size([2158]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(129)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\ndata[-1]\n\n[52, Data(x=[129, 8], edge_index=[2, 1424], edge_attr=[1424], y=[129])]\n\n\n\nlen(data[0][1].edge_index[0])\n\n2158\n\n\n\nedge_list=[]\nfor i in range(52):\n    for j in range(100):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(129, 1230)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntimeë³„ ê°™ì€ edge ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆë‚˜ í™•ì¸\n\nnp.where(data[2][1].edge_index !=data[2][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import EnglandCovidDatasetLoader\nloader = EnglandCovidDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-1.4697, -1.9283, -1.6990, -1.8137]), tensor(-1.8137))\n\n\n\\(t=0\\)ì—ì„œ \\(X\\)ì™€ \\(y\\)ë¥¼ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-1.9283, -1.6990, -1.8137, -1.8137]), tensor(-0.8965))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([-1.6990, -1.8137, -1.8137, -0.8965]), tensor(-1.1258))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\ní•˜ë‚˜ì˜ ë…¸ë“œì— ê¸¸ì´ê°€ \\(T\\)ì¸ ì‹œê³„ì—´ì´ ë§µí•‘ë˜ì–´ ìˆìŒ. (ë…¸ë“œëŠ” ì´ 129ê°œ)\nê° ë…¸ë“œë§ˆë‹¤ ì•„ë˜ì™€ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ì˜ˆì¸¡ì´ ë¨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{129}\\}, t=1,2,\\dots,52\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:07<00:00,  6.30it/s]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([129, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 2158])\n\n\n\n_edge_attr.shape\n\ntorch.Size([2158])\n\n\n\n_y.shape\n\ntorch.Size([129])\n\n\n\ny_hat.shape\n\ntorch.Size([129, 1])\n\n\n\n_x.shape\n\ntorch.Size([129, 4])\n\n\nx\n\n\n\ny\n\n\n\nThe node features correspond to the number of COVID-19 cases in the region in the past window days.\nThe task is to predict the number of cases in each node after 1 day\n\n_x[0:3]\n\ntensor([[-1.4697, -1.9283, -1.6990, -1.8137],\n        [-1.2510, -1.1812, -1.3208, -1.1812],\n        [-1.0934, -1.0934, -1.0934, -1.0934]])\n\n\n\n_y[:3]\n\ntensor([-1.8137, -1.3208, -1.0934])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#montevideobusdatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#montevideobusdatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "MontevideoBusDatasetLoader",
    "text": "MontevideoBusDatasetLoader\nhttps://www.fing.edu.uy/~renzom/msc/uploads/msc-thesis.pdf\nMontevideo Buses\n\nA dataset about the hourly passenger inflow at bus stop level for eleven bus lines from the city of Montevideo. Nodes are bus stops and edges represent connections between the stops; the dataset covers a whole month of traffic patterns.\n\në°ì´í„°ì •ë¦¬\n\nT = 739\nV = ë²„ìŠ¤ì •ë¥˜ì¥\nN = 675 # number of nodes\nE = 101761 = N^2 # edges\n\\(f(v,t)\\)ì˜ ì°¨ì›? (1,) # passenger inflow\nì‹œê°„ì— ë”°ë¼ì„œ Number of nodesê°€ ë³€í•˜ëŠ”ì§€? False\nì‹œê°„ì— ë”°ë¼ì„œ Number of nodesê°€ ë³€í•˜ëŠ”ì§€? False\nX: (675,4) (N,4), \\(f(v,t_0),f(v,t_1),f(v,t_2),f(v,t_3)\\)\ny: (675,,) (N,), \\(f(v,t_4)\\)\nì˜ˆì œì½”ë“œì ìš©ê°€ëŠ¥ì—¬ë¶€: Yes\n\n- Nodes : 675\n\nvertices are bus stops\n\n-Edges : 690\n\nedges are links between bus stops when a bus line connects them and the weight represent the road distance\n\n- Time : 739\n\nhourly inflow passenger data at bus stop level for 11 bus lines during October 2020 from Montevideo city (Uruguay).\n\n\nfrom torch_geometric_temporal.dataset import MontevideoBusDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = MontevideoBusDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n739\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([675, 4]), torch.Size([2, 690]), torch.Size([690]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(675)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(739):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(675, 690)\n\n\n\nnx.draw(G,node_color='green',node_size=50,font_color='white',width=1)\n\n\n\n\n\ntimeë³„ ê°™ì€ edge ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆë‚˜ í™•ì¸\n\nnp.where(data[0][1].edge_index != data[10][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import MontevideoBusDatasetLoader\nloader = MontevideoBusDatasetLoader()\n\ndataset = loader.get_dataset(lags=4)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([-0.4200, -0.4200, -0.4200, -0.4200]), tensor(-0.4200))\n\n\n\\(t=0\\)ì—ì„œ \\(X\\)ì™€ \\(y\\)ë¥¼ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ.\n\nX:= \\(x_0,x_1,x_2,x_3\\)\ny:= \\(x_4\\)\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([-0.4200, -0.4200, -0.4200, -0.4200]), tensor(-0.4200))\n\n\n\nX:= \\(x_1,x_2,x_3,x_4\\)\ny:= \\(x_5\\)\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([-0.4200, -0.4200, -0.4200, -0.4200]), tensor(-0.4200))\n\n\n\nX:=\\(x_2,x_3,x_4,x_5\\)\ny:=\\(x_6\\)\n\ní•˜ë‚˜ì˜ ë…¸ë“œì— ê¸¸ì´ê°€ \\(T\\)ì¸ ì‹œê³„ì—´ì´ ë§µí•‘ë˜ì–´ ìˆìŒ. (ë…¸ë“œëŠ” ì´ 675ê°œ)\nê° ë…¸ë“œë§ˆë‹¤ ì•„ë˜ì™€ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ì˜ˆì¸¡ì´ ë¨\n\n\\((x_0,x_1,x_2,x_3) \\to (x_4)\\)\n\\((x_1,x_2,x_3,x_4) \\to (x_5)\\)\n\n\\(f(v,t), v \\in \\{v_1,\\dots,v_{675}\\}, t=1,2,\\dots,739\\)\n\\[{\\bf X}_{t=1} = \\begin{bmatrix}\nf(v_1,t=1) & f(v_1,t=2) & f(v_1,t=3)&  f(v_1,t=4) \\\\\nf(v_2,t=1) & f(v_2,t=2) & f(v_2,t=3)&  f(v_2,t=4) \\\\\n\\dots & \\dots  & \\dots & \\dots  \\\\\nf(v_{20},t=1) & f(v_{20},t=2) & f(v_{20},t=3)&  f(v_{20},t=4)\n\\end{bmatrix}\\]\n\\[{\\bf y}_{t=1} = \\begin{bmatrix}\nf(v_1,t=5) \\\\\nf(v_2,t=5) \\\\\n\\dots  \\\\\nf(v_{20},t=5)\n\\end{bmatrix}\\]\n\nLearn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:51<00:00,  2.23s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([675, 4])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 690])\n\n\n\n_edge_attr.shape\n\ntorch.Size([690])\n\n\n\n_y.shape\n\ntorch.Size([675])\n\n\n\n_x.shape\n\ntorch.Size([675, 4])\n\n\nx\n\n\n\ny\n\nThe target is the passenger inflow.\nThis is a curated dataset made from different data sources of the Metropolitan Transportation System (STM) of Montevide\n\n\n_x[0:3]\n\ntensor([[-0.4200, -0.4200, -0.4200, -0.4200],\n        [-0.0367, -0.0367, -0.0367, -0.0367],\n        [-0.2655, -0.2655, -0.2655, -0.2655]])\n\n\n\n_y[:3]\n\ntensor([-0.4200, -0.0367, -0.2655])"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#twittertennisdatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#twittertennisdatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "TwitterTennisDatasetLoader",
    "text": "TwitterTennisDatasetLoader\nhttps://appliednetsci.springeropen.com/articles/10.1007/s41109-018-0080-5?ref=https://githubhelp.com\nTwitter Tennis RG and UO\n\nTwitter mention graphs of major tennis tournaments from 2017. Each snapshot contains the graph of popular player or sport news accounts and mentions between them [5, 6]. Node labels encode the number of mentions received and vertex features are structural properties\n\në°ì´í„°ì •ë¦¬\n\nT = 52081\nV = íŠ¸ìœ„í„°ê³„ì •\n\nN = 1000 # number of nodes\nE = 119 = N^2 # edges\n\\(f(v,t)\\)ì˜ ì°¨ì›? (1,) # passenger inflow\nì‹œê°„ì— ë”°ë¼ì„œ Nì´ ë³€í•˜ëŠ”ì§€? ??\nì‹œê°„ì— ë”°ë¼ì„œ Eê°€ ë³€í•˜ëŠ”ì§€? True\nX: ?\ny: ?\nì˜ˆì œì½”ë“œì ìš©ê°€ëŠ¥ì—¬ë¶€: No\n\n- Nodes : 1000\n\nvertices are Twitter accounts\n\n-Edges : 119\n\nedges are mentions between them\n\n- Time : 52081\n\nTwitter mention graphs related to major tennis tournaments from 2017\n\n\nfrom torch_geometric_temporal.dataset import TwitterTennisDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = TwitterTennisDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n119\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([1000, 16]), torch.Size([2, 89]), torch.Size([89]))\n\n\n\ndata[0][1].x[0]\n\ntensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\ndata[0][1].edge_index[0]\n\ntensor([ 42, 909, 909, 909, 233, 233, 450, 256, 256, 256, 256, 256, 434, 434,\n        434, 233, 233, 233, 233, 233, 233, 233,   9,   9, 355,  84,  84,  84,\n         84, 140, 140, 140, 140,   0, 140, 238, 238, 238, 649, 875, 875, 234,\n         73,  73, 341, 341, 341, 341, 341, 417, 293, 991,  74, 581, 282, 162,\n        144, 383, 383, 135, 135, 910, 910, 910, 910, 910,  87,  87,  87,  87,\n          9,   9, 934, 934, 162, 225,  42, 911, 911, 911, 911, 911, 911, 911,\n        911, 498, 498,  64, 435])\n\n\n\ndata[0][1].edge_attr\n\ntensor([2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 3., 2., 1., 1., 1., 1., 2., 2., 2., 1., 1., 1., 3.])\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(1000)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(119):\n    for j in range(40):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(1000, 2819)\n\n\n\nnx.draw(G,node_color='green',node_size=50,width=1)\n\n\n\n\n\ntimeë³„ ê°™ì€ edge ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆë‚˜ í™•ì¸\n\nlen(data[2][1].edge_index[0])\n\n67\n\n\n\nlen(data[0][1].edge_index[0])\n\n89\n\n\në‹¤ë¦„..\n\n\nfrom torch_geometric_temporal.dataset import TwitterTennisDatasetLoader\nloader = TwitterTennisDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n tensor(4.8363))\n\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n tensor(4.9200))\n\n\n\n(data[2][1]).x[0],(data[2][1]).y[0]\n\n(tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n tensor(6.5539))\n\n\n\n(data[3][1]).x[0],(data[3][1]).y[0]\n\n(tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n tensor(6.9651))\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([1000, 16])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 89])\n\n\n\n_edge_attr.shape\n\ntorch.Size([89])\n\n\n\n_y.shape\n\ntorch.Size([1000])\n\n\n\n_x.shape\n\ntorch.Size([1000, 16])\n\n\nx\n\n\n\ny\n\n\n\n\n_x[0:3]\n\ntensor([[0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n\n\n\n_y[0]\n\ntensor(4.8363)"
  },
  {
    "objectID": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#mtmdatasetloader",
    "href": "posts/GCN/2022-12-21-ST-GCN_Dataset.html#mtmdatasetloader",
    "title": "PyTorch ST-GCN Dataset",
    "section": "MTMDatasetLoader",
    "text": "MTMDatasetLoader\nMTM-1 Hand Motions\n\nA temporal dataset of MethodsTime Measurement-1 [36] motions, signalled as consecutive graph frames of 21 3D hand key points that were acquired via MediaPipe Hands [64] from original RGB-Video material. Node features encode the normalized xyz-coordinates of each finger joint and the vertices are connected according to the human hand structure.\n\në°ì´í„°ì •ë¦¬\n\nT = 14452\nV = ì†ì˜ shapeì— ëŒ€ì‘í•˜ëŠ” dot\n\nN = 325 # number of nodes\nE = 19 = N^2 # edges\n\\(f(v,t)\\)ì˜ ì°¨ì›? (Grasp, Release, Move, Reach, Poision, -1)\nì‹œê°„ì— ë”°ë¼ì„œ Nì´ ë³€í•˜ëŠ”ì§€? ??\nì‹œê°„ì— ë”°ë¼ì„œ Eê°€ ë³€í•˜ëŠ”ì§€? ??\nX: ?\ny: ?\nì˜ˆì œì½”ë“œì ìš©ê°€ëŠ¥ì—¬ë¶€: No\n\n- Nodes : 325\n\nvertices are are the finger joints of the human hand\n\n-Edges : 19\n\nedges are the bones connecting them\n\n- Time : 14452\n\nfrom torch_geometric_temporal.dataset import MTMDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = MTMDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=1)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\ntime\n\n14452\n\n\n\n(data[0][1]).x.shape,(data[0][1]).edge_index.shape,(data[0][1]).edge_attr.shape\n\n(torch.Size([3, 21, 16]), torch.Size([2, 19]), torch.Size([19]))\n\n\n\nG = nx.Graph()\n\n\nnode_list = torch.tensor(range(21)).tolist()\n\n\nG.add_nodes_from(node_list)\n\n\nedge_list=[]\nfor i in range(14452):\n    for j in range(len(data[0][1].edge_index[0])):\n        edge_list.append([data[i][1].edge_index[0][j].tolist(),data[i][1].edge_index[1][j].tolist()])\n\n\nG.add_edges_from(edge_list)\n\n\nG.number_of_nodes(),G.number_of_edges()\n\n(21, 19)\n\n\n\nnx.draw(G,with_labels=True,font_weight='bold',node_color='green',node_size=350,font_color='white',width=1)\n\n\n\n\n\ntimeë³„ ê°™ì€ edge ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆë‚˜ í™•ì¸\n\nnp.where(data[0][1].edge_index != data[12][1].edge_index)\n\n(array([], dtype=int64), array([], dtype=int64))\n\n\n\n\nfrom torch_geometric_temporal.dataset import MTMDatasetLoader\nloader = MTMDatasetLoader()\n\ndataset = loader.get_dataset()\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\ndata=[]\nfor time, snapshot in enumerate(train_dataset):\n    data.append([time,snapshot])\n\n\n(data[0][1]).x[0], (data[0][1]).y[0]\n\n(tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n tensor([0., 0., 1., 0., 0., 0.]))\n\n\n\n(data[1][1]).x[0],(data[1][1]).y[0]\n\n(tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n tensor([0., 0., 1., 0., 0., 0.]))\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([3, 21, 16])\n\n\n\n_edge_index.shape\n\ntorch.Size([2, 19])\n\n\n\n_edge_attr.shape\n\ntorch.Size([19])\n\n\n\n_y.shape\n\ntorch.Size([16, 6])\n\n\n\n_x.shape\n\ntorch.Size([3, 21, 16])\n\n\nx\n\nThe data x is returned in shape (3, 21, T),\n\ny\n\nThe targets are manually labeled for each frame, according to one of the five MTM-1 motions (classes ): Grasp, Release, Move, Reach, Position plus a negative class for frames without graph signals (no hand present).\nthe target is returned one-hot-encoded in shape (T, 6).\n\n\n_x[0]\n\ntensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n\n\n\n_y[0]\n\ntensor([0., 0., 1., 0., 0., 0.])"
  },
  {
    "objectID": "posts/GCN/index.html",
    "href": "posts/GCN/index.html",
    "title": "GCN",
    "section": "",
    "text": "About GCN Study"
  },
  {
    "objectID": "posts/GCN/2023-01-26-guebin.html",
    "href": "posts/GCN/2023-01-26-guebin.html",
    "title": "Class of Method",
    "section": "",
    "text": "Class"
  },
  {
    "objectID": "posts/GCN/2023-01-26-guebin.html#ì‹œë‚˜ë¦¬ì˜¤1-baseline",
    "href": "posts/GCN/2023-01-26-guebin.html#ì‹œë‚˜ë¦¬ì˜¤1-baseline",
    "title": "Class of Method",
    "section": "ì‹œë‚˜ë¦¬ì˜¤1 (Baseline)",
    "text": "ì‹œë‚˜ë¦¬ì˜¤1 (Baseline)\nì‹œë‚˜ë¦¬ì˜¤1\n\nmissing rate: 0%\në³´ê°„ë°©ë²•: None\n\n\nSTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:39<00:00,  2.00s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nGNAR ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n-\n\n\nê²°ê³¼ì‹œê°í™”\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n mse(train) = {1:.2f}, mse(test) = {2:.2f}'.format(i,train_mse_eachnode[i],test_mse_eachnode[i]))\n    a.plot(range(1,160),stgcn_train[:,i],label='STCGCN (train)',color='C0')\n    a.plot(range(161,200),stgcn_test[:,i],label='STCGCN (test)',color='C1')\n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario1: STGCN \\n missing=0% \\n interpolation=None \\n mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n\".format(train_mse_total,test_mse_total),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-01-26-guebin.html#ì‹œë‚˜ë¦¬ì˜¤2",
    "href": "posts/GCN/2023-01-26-guebin.html#ì‹œë‚˜ë¦¬ì˜¤2",
    "title": "Class of Method",
    "section": "ì‹œë‚˜ë¦¬ì˜¤2",
    "text": "ì‹œë‚˜ë¦¬ì˜¤2\nì‹œë‚˜ë¦¬ì˜¤2\n\nmissing rate: 50%\në³´ê°„ë°©ë²•: linear\n\n- ê²°ì¸¡ì¹˜ìƒì„± + ë³´ê°„\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.5)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:31<00:00,  1.82s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:29<00:00,  1.80s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nestgcn_test = yyhat.squeeze() \n\n\n\nê²°ê³¼ì‹œê°í™”\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(161,200),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(161,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-01-26-guebin.html#ì‹œë‚˜ë¦¬ì˜¤3",
    "href": "posts/GCN/2023-01-26-guebin.html#ì‹œë‚˜ë¦¬ì˜¤3",
    "title": "Class of Method",
    "section": "ì‹œë‚˜ë¦¬ì˜¤3",
    "text": "ì‹œë‚˜ë¦¬ì˜¤3\nì‹œë‚˜ë¦¬ì˜¤3\n\nmissing rate: 80%\në³´ê°„ë°©ë²•: linear\n\n- ê²°ì¸¡ì¹˜ìƒì„± + ë³´ê°„\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:27<00:00,  1.76s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:32<00:00,  1.86s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nestgcn_test = yyhat.squeeze() \n\n\n\nê²°ê³¼ì‹œê°í™”\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(161,200),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(161,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario3: \\n missing=80% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.html",
    "href": "posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.html",
    "title": "MontevideoBus lag 4 Randomly Missing comparison Table",
    "section": "",
    "text": "ST-GCN Dataset MontevideoBus"
  },
  {
    "objectID": "posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.html#train",
    "href": "posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.html#train",
    "title": "MontevideoBus lag 4 Randomly Missing comparison Table",
    "section": "Train",
    "text": "Train\n\ndata_train=[]\nfor time, snapshot in enumerate(train_dataset):\n    data_train.append([time,snapshot])\n\n\ndata_train[0][1].x.shape,data_train[0][1].y.shape,data_train[0][1].edge_index.shape,data_train[0][1].edge_attr.shape\n\n(torch.Size([675, 4]),\n torch.Size([675]),\n torch.Size([2, 690]),\n torch.Size([690]))\n\n\n\ntime\n\n591\n\n\n\nT_train = time\nN = len(data_train[0][1].x)\n\n\nedge_index = data_train[0][1].edge_index\nedge_attr = data_train[0][1].edge_attr\n\n\nx_train = []\nfor i in range(time):\n    x_train.append(data_train[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_train = data_tensor.reshape(time,N,-1)\nx_train.shape\n\ntorch.Size([591, 675, 4])\n\n\n\ny_train = []\nfor i in range(time):\n    y_train.append(data_train[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_train = data_tensor.reshape(time,N)\ny_train.shape\n\ntorch.Size([591, 675])\n\n\n\nx_train.shape, y_train.shape\n\n(torch.Size([591, 675, 4]), torch.Size([591, 675]))"
  },
  {
    "objectID": "posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.html#test",
    "href": "posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.html#test",
    "title": "MontevideoBus lag 4 Randomly Missing comparison Table",
    "section": "Test",
    "text": "Test\n\ndata_test=[]\nfor time, snapshot in enumerate(test_dataset):\n    data_test.append([time,snapshot])\n\n\ndata_test[0][1].x.shape,data_test[0][1].y.shape,data_test[0][1].edge_index.shape,data_test[0][1].edge_attr.shape\n\n(torch.Size([675, 4]),\n torch.Size([675]),\n torch.Size([2, 690]),\n torch.Size([690]))\n\n\n\ntime\n\n147\n\n\n\nT_test = time\n\n\nx_test = []\nfor i in range(time):\n    x_test.append(data_test[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_test = data_tensor.reshape(time,N,-1)\nx_test.shape\n\ntorch.Size([147, 675, 4])\n\n\n\ny_test = []\nfor i in range(time):\n    y_test.append(data_test[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_test = data_tensor.reshape(time,N)\ny_test.shape\n\ntorch.Size([147, 675])\n\n\n\nx_test.shape, y_test.shape\n\n(torch.Size([147, 675, 4]), torch.Size([147, 675]))"
  },
  {
    "objectID": "posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.html#stgcn",
    "href": "posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.html#stgcn",
    "title": "MontevideoBus lag 4 Randomly Missing comparison Table",
    "section": "STGCN",
    "text": "STGCN\n\nclass STGCN_Missing:\n    def __init__(self,Dataset, df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n        self.iterable = iterable\n    def iter(self):\n        self.XX = x_test\n        self.yy = y_test\n\n        self.real_y = y_train\n        for i in range(self.iterable):\n    \n            _zero = Missing(x_train_f)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(np.stack([interpolated_signal[j:(len(interpolated_signal) - self.lag + 1 +j),:] for j in range(self.lag)],axis = -1)).float()\n            y = torch.tensor(interpolated_signal[self.lag:,:]).float()\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            for epoch in range(50):\n                for time, (xt,yt) in enumerate(zip(X,y)):\n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_stgcn = (((self.real_y-yhat.squeeze()).squeeze())**2).mean()\n            test_mse_total_stgcn = (((self.yy-yyhat.squeeze()).squeeze())**2).mean()\n    \n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_stgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_stgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.html#enhencement-of-stgcn",
    "href": "posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.html#enhencement-of-stgcn",
    "title": "MontevideoBus lag 4 Randomly Missing comparison Table",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nclass ESTGCN_Missing:\n    def __init__(self,Dataset, df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n        self.iterable = iterable\n    def iter(self):\n        self.XX = x_test\n        self.yy = y_test\n\n        self.real_y = y_train\n        for i in range(self.iterable):\n    \n            _zero = Missing(x_train_f)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            signal = interpolated_signal.copy()\n            for epoch in range(50):\n                signal = update_from_freq_domain(signal,missing_index)\n                X = torch.tensor(np.stack([signal[i:(T_train+epoch+i),:] for i in range(self.lag)],axis = -1)).reshape(-1,N,self.lag).float()\n                y = torch.tensor(signal).reshape(-1,N,1).float()[self.lag:,:,:]\n                for time, (xt,yt) in enumerate(zip(X,y)):        \n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().reshape(1,-1)]).squeeze()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_estgcn = (((self.real_y-yhat[:T_train,:].squeeze()).squeeze())**2).mean()\n            test_mse_total_estgcn = (((self.yy-yyhat.squeeze()).squeeze())**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_estgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_estgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.html#gnar",
    "href": "posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.html#gnar",
    "title": "MontevideoBus lag 4 Randomly Missing comparison Table",
    "section": "GNAR",
    "text": "GNAR\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\n\nR[write to console]: Loading required package: igraph\n\nR[write to console]: \nAttaching package: â€˜igraphâ€™\n\n\nR[write to console]: The following objects are masked from â€˜package:statsâ€™:\n\n    decompose, spectrum\n\n\nR[write to console]: The following object is masked from â€˜package:baseâ€™:\n\n    union\n\n\nR[write to console]: Loading required package: wordcloud\n\nR[write to console]: Loading required package: RColorBrewer\n\n\n\n\nGNAR = importr('GNAR') # import GNAR \nigraph = importr('igraph') # import igraph \n\n\nw=np.zeros((N,N))\n\n\nfor k in range(len(edge_index[0])):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\nm = robjects.r.matrix(FloatVector(w), nrow = N, ncol = N)\n\n\nclass GNAR_Missing:\n    def __init__(self,Dataset, df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n        self.iterable = iterable\n    def iter(self):\n        self.yy = torch.tensor(y_test).float()\n        for i in range(self.iterable):\n\n            _zero = Missing(x_train_f)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(np.stack([interpolated_signal[i:(T_train+i),:] for i in range(self.lag)],axis = -1)).float()\n\n            answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = T_train, ncol = N),net = GNAR.matrixtoGNAR(m), alphaOrder = 4, betaOrder = FloatVector([1, 1, 1, 1]))             \n            predict = GNAR.predict_GNARfit(answer,n_ahead=T_test)\n\n\n            train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,N))**2).mean()\n            test_mse_total_gnar = ((self.yy - pd.DataFrame(predict).values.reshape(-1,N))**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_gnar.tolist()\n            df_row['MSE_test'] = test_mse_total_gnar.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.html#stgcn-1",
    "href": "posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.html#stgcn-1",
    "title": "MontevideoBus lag 4 Randomly Missing comparison Table",
    "section": "STGCN",
    "text": "STGCN\n\nDataset = 'MontevideoBus'\nMethod = 'stgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 4 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_stgcn= pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    stgcn = STGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    stgcn.iter()\n    df_add = stgcn.df.copy()\n    df_stgcn = pd.concat([df_stgcn,df_add],axis=0)\n\n\nsave_data(df_stgcn, './data/EnglandCovid_stgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.html#enhencement-of-stgcn-1",
    "href": "posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.html#enhencement-of-stgcn-1",
    "title": "MontevideoBus lag 4 Randomly Missing comparison Table",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nDataset = 'MontevideoBus'\nMethod = 'estgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 4 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_estgcn = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    estgcn = ESTGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    estgcn.iter()\n    df_add = estgcn.df.copy()\n    df_estgcn = pd.concat([df_estgcn,df_add],axis=0)\n\n\nsave_data(df_estgcn, './data/EnglandCovid_estgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.html#gnar-1",
    "href": "posts/GCN/2023-02-19-ESTGCN_MONTEVIDEOBUS_DATA_1.html#gnar-1",
    "title": "MontevideoBus lag 4 Randomly Missing comparison Table",
    "section": "GNAR",
    "text": "GNAR\n\nDataset = 'MontevideoBus'\nMethod = 'gnar' # 'stgcn','estgcn','gnar'  \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 4 # 1,2,3,4 ... \nNumber_of_filters = None # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_gnar = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    gnar = GNAR_Missing(Dataset, df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    gnar.iter()\n    df_add = gnar.df.copy()\n    df_gnar = pd.concat([df_gnar,df_add],axis=0)\n\n\nsave_data(df_gnar, './data/EnglandCovid_gnar_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html",
    "href": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html",
    "title": "GNAR lag 1 Block Missing comparison Table",
    "section": "",
    "text": "GNAR fiveNet,fivenodes lag 1"
  },
  {
    "objectID": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#stgcn",
    "href": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#stgcn",
    "title": "GNAR lag 1 Block Missing comparison Table",
    "section": "STGCN",
    "text": "STGCN\n\nclass STGCN_Block:\n    def __init__(self,Dataset, start,end,node,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n        self.iterable = iterable\n        self.start,self.end,self.node = start,end,node\n    def iter(self):\n        _zero = Missing(fiveVTS_train)\n        _zero.miss(self.start,self.end,self.node)\n        _zero.second_linear()\n\n        missing_index = _zero.number\n        interpolated_signal = _zero.train_linear\n\n        X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n        y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n        XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            for epoch in range(50):\n                for time, (xt,yt) in enumerate(zip(X,y)):\n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n            train_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\n            test_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_stgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_stgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#enhencement-of-stgcn",
    "href": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#enhencement-of-stgcn",
    "title": "GNAR lag 1 Block Missing comparison Table",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nclass ESTGCN_Block:\n    def __init__(self,Dataset, start,end,node,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n        self.start,self.end,self.node = start,end,node\n        self.iterable = iterable\n    def iter(self):\n        _zero = Missing(fiveVTS_train)\n        _zero.miss(self.start,self.end,self.node)\n        _zero.second_linear()\n\n        missing_index = _zero.number\n        interpolated_signal = _zero.train_linear\n\n        X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n        y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n        XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n        for i in range(self.iterable):\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            signal = interpolated_signal.copy()\n            for epoch in range(50):\n                signal = update_from_freq_domain(signal,self.start,self.end,self.node)\n                X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n                y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n                for time, (xt,yt) in enumerate(zip(X,y)):        \n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n            train_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\n            test_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_estgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_estgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#gnar",
    "href": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#gnar",
    "title": "GNAR lag 1 Block Missing comparison Table",
    "section": "GNAR",
    "text": "GNAR\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\nlibrary(zoo)\n\nR[write to console]: Loading required package: igraph\n\nR[write to console]: \nAttaching package: â€˜igraphâ€™\n\n\nR[write to console]: The following objects are masked from â€˜package:statsâ€™:\n\n    decompose, spectrum\n\n\nR[write to console]: The following object is masked from â€˜package:baseâ€™:\n\n    union\n\n\nR[write to console]: Loading required package: wordcloud\n\nR[write to console]: Loading required package: RColorBrewer\n\nR[write to console]: \nAttaching package: â€˜zooâ€™\n\n\nR[write to console]: The following objects are masked from â€˜package:baseâ€™:\n\n    as.Date, as.Date.numeric\n\n\n\n\n\nGNAR = importr('GNAR') # import GNAR \nigraph = importr('igraph') # import igraph \n\n\nm = robjects.r.matrix(FloatVector([0,0,0,1,1,0,0,1,1,0,0,1,0,1,0,1,1,1,0,0,1,0,0,0,0]), nrow = 5, ncol = 5)\n\n\nclass GNAR_Block:\n    def __init__(self,Dataset, start,end,node,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n        self.iterable = iterable\n        self.start,self.end,self.node = start,end,node\n    def iter(self):\n        _zero = Missing(fiveVTS_train)\n        _zero.miss(self.start,self.end,self.node)\n        _zero.second_linear()\n\n        missing_index = _zero.number\n        interpolated_signal = _zero.train_linear\n\n        X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-2),:,:]\n\n        yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n        \n        for i in range(self.iterable):\n    \n            answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = 160, ncol = 5),net = GNAR.matrixtoGNAR(m), alphaOrder = 2, betaOrder = FloatVector([1, 1]))             \n            predict = GNAR.predict_GNARfit(answer,n_ahead=40)\n\n\n            train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,5))**2).mean()\n            test_mse_total_gnar = ((yy.squeeze() - pd.DataFrame(predict).values.reshape(-1,5)[:-1,:])**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_gnar.tolist()\n            df_row['MSE_test'] = test_mse_total_gnar.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#stgcn-1",
    "href": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#stgcn-1",
    "title": "GNAR lag 1 Block Missing comparison Table",
    "section": "STGCN",
    "text": "STGCN\n\nDataset = 'fivenodes'\nMethod = 'stgcn' # 'stgcn','estgcn','gnar' \nMissingrate = None # 0.0, 0.2, 0.4, 0.6, 0.8 \nMissingtype = 'block'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ncol = ['Dataset', 'iteration', 'method', 'missingrate', 'missingtype', 'lag', 'number_of_filters', 'interpolation','MSE_train', 'MSE_test']\n\n\ndf = pd.DataFrame(columns=col)\n\n\nstgcn = STGCN_Block(Dataset, 50,150,2,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n\n\nstgcn.iter()\n\n\ndf_stgcn = stgcn.df.copy()\n\n\nsave_data(df_stgcn, './data/GNAT_DATA_stgcn_block_node2.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#enhencement-of-stgcn-1",
    "href": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#enhencement-of-stgcn-1",
    "title": "GNAR lag 1 Block Missing comparison Table",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nDataset = 'fivenodes'\nMethod = 'estgcn' # 'stgcn','estgcn','gnar' \nMissingrate = None # 0.0, 0.2, 0.4, 0.6, 0.8 \nMissingtype = 'block'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ncol = ['Dataset', 'iteration', 'method', 'missingrate', 'missingtype', 'lag', 'number_of_filters', 'interpolation','MSE_train', 'MSE_test']\n\n\ndf = pd.DataFrame(columns=col)\n\n\nestgcn = ESTGCN_Block(Dataset, 50,150,2,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n\n\nestgcn.iter()\n\n\ndf_estgcn = estgcn.df.copy()\n\n\nsave_data(df_estgcn, './data/GNAR_DATA_estgcn_block_node2.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#gnar-1",
    "href": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#gnar-1",
    "title": "GNAR lag 1 Block Missing comparison Table",
    "section": "GNAR",
    "text": "GNAR\n\nDataset = 'fivedodes'\nMethod = 'gnar' # 'stgcn','estgcn','gnar' \nMissingrate = None # 0.0, 0.2, 0.4, 0.6, 0.8 \nMissingtype = 'block'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = None # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ncol = ['Dataset', 'iteration', 'method', 'missingrate', 'missingtype', 'lag', 'number_of_filters', 'interpolation','MSE_train', 'MSE_test']\n\n\ndf = pd.DataFrame(columns=col)\n\n\ngnar = GNAR_Block(Dataset, 50,150,2,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n\n\ngnar.iter()\n\n\ndf_gnar = gnar.df.copy()\n\n\nsave_data(df_gnar, './data/GNAR_DATA_gnar_block_node2.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#stgcn-2",
    "href": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#stgcn-2",
    "title": "GNAR lag 1 Block Missing comparison Table",
    "section": "STGCN",
    "text": "STGCN\n\nclass STGCN_Block:\n    def __init__(self,Dataset, df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n        self.iterable = iterable\n    def iter(self):\n \n        for i in range(self.iterable):\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            for epoch in range(50):\n                for time, (xt,yt) in enumerate(zip(X,y)):\n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n            train_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\n            test_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_stgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_stgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#estgcn",
    "href": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#estgcn",
    "title": "GNAR lag 1 Block Missing comparison Table",
    "section": "ESTGCN",
    "text": "ESTGCN\n\nclass ESTGCN_Block:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n        self.iterable = iterable\n    def iter(self):\n        for i in range(self.iterable):\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            signal = interpolated_signal.copy()\n            for epoch in range(50):\n                signal = update_from_freq_domain(signal,self.start,self.end,self.node)\n                X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n                y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n                for time, (xt,yt) in enumerate(zip(X,y)):        \n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n            train_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\n            test_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_estgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_estgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#gnar-2",
    "href": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#gnar-2",
    "title": "GNAR lag 1 Block Missing comparison Table",
    "section": "GNAR",
    "text": "GNAR\n\nclass GNAR_Block:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n        self.iterable = iterable\n    def iter(self):        \n        for i in range(self.iterable):\n    \n            answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = 160, ncol = 5),net = GNAR.matrixtoGNAR(m), alphaOrder = 2, betaOrder = FloatVector([1, 1]))             \n            predict = GNAR.predict_GNARfit(answer,n_ahead=40)\n\n\n            train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,5))**2).mean()\n            test_mse_total_gnar = ((yy.squeeze() - pd.DataFrame(predict).values.reshape(-1,5)[:-1,:])**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_gnar.tolist()\n            df_row['MSE_test'] = test_mse_total_gnar.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#stgcn-3",
    "href": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#stgcn-3",
    "title": "GNAR lag 1 Block Missing comparison Table",
    "section": "STGCN",
    "text": "STGCN\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(50,150,2)\n_zero1 = Missing(_zero.missing)\n_zero1.miss(40,140,3)\n_zero1.second_linear()\n\nmissing_index = _zero1.number\ninterpolated_signal = _zero1.train_linear\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nDataset = 'fivenodes'\nMethod = 'stgcn' # 'stgcn','estgcn','gnar' \nMissingrate = None # 0.0, 0.2, 0.4, 0.6, 0.8 \nMissingtype = 'block'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ncol = ['Dataset', 'iteration', 'method', 'missingrate', 'missingtype', 'lag', 'number_of_filters', 'interpolation','MSE_train', 'MSE_test']\n\n\ndf = pd.DataFrame(columns=col)\n\n\nstgcn = STGCN_Block(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n\n\nstgcn.iter()\n\n\ndf_stgcn = stgcn.df.copy()\n\n\nsave_data(df_stgcn, './data/GNAR_DATA_stgcn_block_twododes.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#enhencement-of-stgcn-2",
    "href": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#enhencement-of-stgcn-2",
    "title": "GNAR lag 1 Block Missing comparison Table",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(50,150,2)\n_zero1 = Missing(_zero.missing)\n_zero1.miss(40,140,3)\n_zero1.second_linear()\n\nmissing_index = _zero1.number\ninterpolated_signal = _zero1.train_linear\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-2),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nDataset = 'fivenodes'\nMethod = 'estgcn' # 'stgcn','estgcn','gnar' \nMissingrate = None # 0.0, 0.2, 0.4, 0.6, 0.8 \nMissingtype = 'block'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ncol = ['Dataset', 'iteration', 'method', 'missingrate', 'missingtype', 'lag', 'number_of_filters', 'interpolation','MSE_train', 'MSE_test']\n\n\ndf = pd.DataFrame(columns=col)\n\n\nestgcn = ESTGCN_Block(Dataset, 50,150,2,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n\n\nestgcn.iter()\n\n\ndf_estgcn = estgcn.df.copy()\n\n\nsave_data(df_estgcn, './data/GNAR_DATA_estgcn_block_twonodes.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#gnar-3",
    "href": "posts/GCN/2023-02-14-ESTGCN_GNAR_DATA_5.html#gnar-3",
    "title": "GNAR lag 1 Block Missing comparison Table",
    "section": "GNAR",
    "text": "GNAR\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(50,150,2)\n_zero1 = Missing(_zero.missing)\n_zero1.miss(40,140,3)\n_zero1.second_linear()\n\nmissing_index = _zero1.number\ninterpolated_signal = _zero1.train_linear\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-2),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nDataset = 'fivedodes'\nMethod = 'gnar' # 'stgcn','estgcn','gnar' \nMissingrate = None # 0.0, 0.2, 0.4, 0.6, 0.8 \nMissingtype = 'block'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = None # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ncol = ['Dataset', 'iteration', 'method', 'missingrate', 'missingtype', 'lag', 'number_of_filters', 'interpolation','MSE_train', 'MSE_test']\n\n\ndf = pd.DataFrame(columns=col)\n\n\ngnar = GNAR_Block(Dataset, 50,150,2,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n\n\ngnar.iter()\n\n\ndf_gnar = gnar.df.copy()\n\n\nsave_data(df_gnar, './data/GNAR_DATA_gnar_block_twonodes.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html",
    "href": "posts/GCN/2023-01-05-GNAR.html",
    "title": "GNAR data",
    "section": "",
    "text": "GNAR\nGNAR"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#gnar-network-example",
    "href": "posts/GCN/2023-01-05-GNAR.html#gnar-network-example",
    "title": "GNAR data",
    "section": "2.3 GNAR network example",
    "text": "2.3 GNAR network example\n\nedge(list)\ndist(list)\n\n\n%%R\nplot(fiveNet, vertex.label = c(\"A\", \"B\", \"C\", \"D\", \"E\"))\n\n\n\n\n\n%%R\nsummary(\"fiveNet\")\n\n   Length     Class      Mode \n        1 character character \n\n\nother examples\n\nigraphtoGNAR or GNARtoigraphì“°ëŠ” ì˜ˆì œ\n\n\n%%R\nfiveNet2 <- GNARtoigraph(net = fiveNet)\nsummary(fiveNet2)\n\nIGRAPH 2b4460d U-W- 5 5 -- \n+ attr: weight (e/n)\n\n\n\n%%R\nfiveNet3 <- igraphtoGNAR(fiveNet2)\nall.equal(fiveNet, fiveNet3)\n\n[1] TRUE\n\n\n\n%%R\nprint(igraphtoGNAR(fiveNet2))\n\nGNARnet with 5 nodes \nedges:1--4 1--5 2--3 2--4 3--2 3--4 4--1 4--2 4--3 5--1 \n     \n edges of each of length  1 \n\n\nedgeë“¤ ë³´ê³  ì‹¶ì„ ë•Œ\nwhereas the reverse conversion would be performed as\n\n%%R\ng <- make_ring(10)\nprint(igraphtoGNAR(g))\n\nGNARnet with 10 nodes \nedges:1--2 1--10 2--1 2--3 3--2 3--4 4--3 4--5 5--4 5--6 \n     6--5 6--7 7--6 7--8 8--7 8--9 9--8 9--10 10--1 10--9 \n     \n edges of each of length  1 \n\n\n\n%%R\nmake_ring(10)\n\nIGRAPH 22f6be5 U--- 10 10 -- Ring graph\n+ attr: name (g/c), mutual (g/l), circular (g/l)\n+ edges from 22f6be5:\n [1] 1-- 2 2-- 3 3-- 4 4-- 5 5-- 6 6-- 7 7-- 8 8-- 9 9--10 1--10\n\n\nì´ì–´ì§„ ë°©í–¥ìœ¼ë¡œ ê°ê°ì˜ edgeë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” ê²Œ igrapphtoGNARì´ë‹¤\nGNARtoigraph functionìœ¼ë¡œ ë†’ì€ ìˆ˜ì¤€ì˜ ì´ì›ƒ êµ¬ì¡°ë¥¼ í¬í•¨í•œ ê·¸ë˜í”„ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆë‹¤.\n\nas.matrix or matrixtoGNARë¡œ ì¸ì ‘ í–‰ë ¬ êµ¬í•  ìˆ˜ ìˆìŒ\n\nwe can prosucean adjacency matrix for the fiveNet obeject with\n\n%%R\nas.matrix(fiveNet)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    1    1\n[2,]    0    0    1    1    0\n[3,]    0    1    0    1    0\n[4,]    1    1    1    0    0\n[5,]    1    0    0    0    0\n\n\nand an example converting a weighted adjacency matrix to a GNARnet object is\n\n%%R\nadj <- matrix(runif(9), ncol = 3, nrow = 3)\nadj[adj < 0.3] <- 0\nprint(matrixtoGNAR(adj))\n\nWARNING: diagonal entries present in original matrix, these will be removed\nGNARnet with 3 nodes \nedges:1--2 1--3 2--1 2--3 3--1 3--2 \n edges of unequal lengths"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#example-gnar-model-fitting",
    "href": "posts/GCN/2023-01-05-GNAR.html#example-gnar-model-fitting",
    "title": "GNAR data",
    "section": "2.4. Example: GNAR model fitting",
    "text": "2.4. Example: GNAR model fitting\n\nGNARë¡œ fitê³¼ predict ê°€ëŠ¥\n\n\n%%R\ndata(\"fiveNode\")\nanswer <- GNARfit(vts = fiveVTS, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nanswer\n\nModel: \nGNAR(2,[1,1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\n dmatalpha1  dmatbeta1.1   dmatalpha2  dmatbeta2.1  \n    0.20624      0.50277      0.02124     -0.09523  \n\n\n\n\níŒŒë¼ë©”í„° 4ê°œ ê°€ì§€ê³  ìˆìŒ\n\n\n%%R\nlayout(matrix(c(1, 2), 2, 1))\nplot(fiveVTS[, 1], ylab = \"Node A Time Series\")\nlines(fitted(answer)[, 1], col = 2)\nplot(fiveVTS[, 2], ylab = \"Node B Time Series\")\nlines(fitted(answer)[, 2], col = 2)\n\n\n\n\n\n%%R\nlayout(matrix(c(1, 2), 2, 1))\nplot(fiveVTS[, 3], ylab = \"Node C Time Series\")\nlines(fitted(answer)[, 3], col = 2)\nplot(fiveVTS[, 4], ylab = \"Node D Time Series\")\nlines(fitted(answer)[, 4], col = 2)\n\n\n\n\n\nê° ë…¸ë“œì˜ time series(ê²€ì •), fitted values from â€˜answerâ€™ model overlaid in red\n\n\n%%R\nmyresiduals <- residuals(answer)[, 1]\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(residuals(answer)[, 1]), ylab = \"`answer' model residuals\")\nhist(residuals(answer)[, 1], main = \"\", xlab = \"`answer' model residuals\")\n\n\n\n\n\n%%R\nmyresiduals <- residuals(answer)[, 2]\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(residuals(answer)[, 1]), ylab = \"`answer' model residuals\")\nhist(residuals(answer)[, 2], main = \"\", xlab = \"`answer' model residuals\")\n\n\n\n\n\n%%R\nmyresiduals <- residuals(answer)[, 3]\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(residuals(answer)[, 1]), ylab = \"`answer' model residuals\")\nhist(residuals(answer)[, 3], main = \"\", xlab = \"`answer' model residuals\")\n\n\n\n\n\n%%R\nmyresiduals <- residuals(answer)[, 4]\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(residuals(answer)[, 1]), ylab = \"`answer' model residuals\")\nhist(residuals(answer)[, 4], main = \"\", xlab = \"`answer' model residuals\")\n\n\n\n\n\nresidual plots from â€˜answerâ€™ model fit. Top: time sereies, Bottom: Histogram"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#example-gnar-data-simulation-on-a-given-network",
    "href": "posts/GCN/2023-01-05-GNAR.html#example-gnar-data-simulation-on-a-given-network",
    "title": "GNAR data",
    "section": "2.5. Example: GNAR data simulation on a given network",
    "text": "2.5. Example: GNAR data simulation on a given network\n\nfiveNet ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ ì‹œê³„ì—´ ì‹œë®¬ë ˆì´ì…˜ ì§„í–‰\në‘ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë‘ sigma argumentë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œì¤€ í¸ì°¨ê°€ ì œì–´ë˜ëŠ” í‘œì¤€ ì •ê·œ ë…¸ì´ì¦ˆë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœë‹¤.\n\n\n%%R\nset.seed(10)\nfiveVTS2 <- GNARsim(n = 200, net = fiveNet, alphaParams = list(c(0.4, 0, -0.6, 0, 0)), betaParams = list(c(0.3)))\n\n\nfiveVTS2 ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œë®¬ë ˆì´ì…˜ ëœ ê²ƒì´ë‹¤ë³´ë‹ˆ íŒŒë¼ë©”í„° ê³„ìˆ˜ ë¹„ìŠ·\n\n\n%%R\nprint(GNARfit(vts = fiveVTS2, net = fiveNet, alphaOrder = 1, betaOrder = 1, globalalpha = FALSE))\n\nModel: \nGNAR(1,[1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\ndmatalpha1node1  dmatalpha1node2  dmatalpha1node3  dmatalpha1node4  \n        0.45902          0.13133         -0.49166          0.03828  \ndmatalpha1node5      dmatbeta1.1  \n        0.02249          0.24848  \n\n\n\n\n%%R\nset.seed(10)\nfiveVTS3 <- GNARsim(n = 200, net = fiveNet, alphaParams = list(rep(0.2, 5), rep(0.3, 5)), betaParams = list(c(0.2, 0.3), c(0)))\nprint(GNARfit(vts = fiveVTS3, net = fiveNet, alphaOrder = 2, betaOrder = c(2,0)))\n\nModel: \nGNAR(2,[2,0]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\n dmatalpha1  dmatbeta1.1  dmatbeta1.2   dmatalpha2  \n     0.2537       0.1049       0.3146       0.2907  \n\n\n\n\n%%R\nfiveVTS4 <- simulate(GNARfit(vts = fiveVTS2, net = fiveNet, alphaOrder = 1, betaOrder = 1, globalalpha = FALSE), n = 200)\nprint(GNARfit(vts = fiveVTS4, net = fiveNet, alphaOrder = 1, betaOrder = 1, globalalpha = FALSE))\n\nModel: \nGNAR(1,[1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\ndmatalpha1node1  dmatalpha1node2  dmatalpha1node3  dmatalpha1node4  \n      0.4478300       -0.0008695       -0.4822675        0.0523652  \ndmatalpha1node5      dmatbeta1.1  \n     -0.0063702        0.2249530  \n\n\n\n\nìœ„ì™€ ê°™ì´ GNAR ëª¨ë¸ì— ìˆëŠ” ì‹œê³„ì—´ì„ simulateí•˜ê¸° ìœ„í•´ GNARfit objectì— ëŒ€í•´ simulate S3 method ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#missing-data-and-changing-connection-weights-with-gnar-models",
    "href": "posts/GCN/2023-01-05-GNAR.html#missing-data-and-changing-connection-weights-with-gnar-models",
    "title": "GNAR data",
    "section": "2.6 Missing data and changing connection weights with GNAR models",
    "text": "2.6 Missing data and changing connection weights with GNAR models\n\nThe flexibility of GNAR modellingì´ ì˜ë¯¸í•˜ëŠ” ê²ƒì€ ì—°ê²° ê°€ì¤‘ì¹˜ë¥¼ ë°”ê¾¸ì§€ ì•Šê³  ë³€í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ë¡œ missing data ë¥¼ ëª¨ë¸ë§ í•  ìˆ˜ ìˆë‹¤.\ní•œ ë…¸ë“œê°€ missing data êµ¬ê°„ì´ ìƒê¸°ë©´ ê·¸ êµ¬ê°„ì—ì„œë§Œ ë„¤íŠ¸ì›Œí¬ë¥¼ ë³€í™”í•˜ì—¬ weightê°€ ë³€ê²½ëœë‹¤.\n\n\n%%R\nfiveVTS0 <- fiveVTS\nfiveVTS0[50:150, 3] <- NA\nnafit <- GNARfit(vts = fiveVTS0, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nlayout(matrix(c(1, 2), 2, 1))\nplot(ts(fitted(nafit)[, 3]), ylab = \"Node C fitted values\")\nplot(ts(fitted(nafit)[, 4]), ylab = \"Node D fitted values\")"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#stationary-conditions-for-a-gnar-process-with-fixed-network",
    "href": "posts/GCN/2023-01-05-GNAR.html#stationary-conditions-for-a-gnar-process-with-fixed-network",
    "title": "GNAR data",
    "section": "2.7. Stationary conditions for a GNAR process with fixed network",
    "text": "2.7. Stationary conditions for a GNAR process with fixed network\nTheorem 1\n\nGiven an unchanging network, \\(\\mathcal{G}\\) a sufficient condition for the GNAT model (1) to be stationary is\n\n\\[\\sum^p_{j=1}(|\\alpha_{i,j}| + \\sum^{C}_{c=1} \\sum^{s_j}_{r=1} |\\beta_{j,t,c}|)<1 , \\forall_i \\in 1,\\dots, N\\]\nìœ„ ì¡°ê±´ì„ GNARsimì„ ì´ìš©í•˜ì—¬ í™•ì¸ í•  ìˆ˜ ìˆë‹¤.\n\n%%R\nset.seed(10)\nfiveVTS4 <- GNARsim(n = 200, net = fiveNet, alphaParams = list(rep(0.2, 5)), betaParams = list(c(0.85)))\nc(mean(fiveVTS4[1:50, ]), mean(fiveVTS4[51:100, ]), mean(fiveVTS4[101:150, ]), mean(fiveVTS4[151:200, ]))\n\n[1]    -120.511   -1370.216  -15725.884 -180319.140\n\n\n\nThe mean increases rapidly indicating nonstationarity.\ní‰ê· ì´ ë¹ ë¥´ê²Œ ì¦ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì•„ ì •ìƒì„±ì„ ë„ê³  ìˆì§€ ì•Šë‹¤."
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#benefits-of-our-model-and-comparisons-to-others",
    "href": "posts/GCN/2023-01-05-GNAR.html#benefits-of-our-model-and-comparisons-to-others",
    "title": "GNAR data",
    "section": "2.8. Benefits of our model and comparisons to others",
    "text": "2.8. Benefits of our model and comparisons to others"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#order-selection",
    "href": "posts/GCN/2023-01-05-GNAR.html#order-selection",
    "title": "GNAR data",
    "section": "3.1. Order selection",
    "text": "3.1. Order selection\nBayesian information criterion\n\\[BIC(p,s) = ln|\\sum^{\\hat{}}_{p,s}| + T^{-1} M ln(T)\\]\n\n%%R\nBIC(GNARfit())\n\n[1] -0.003953124\n\n\n\n%%R\nBIC(GNARfit(betaOrder = c(2, 1)))\n\n[1] 0.02251406\n\n\nAkaike information criterion\n\\[AIC(p,s) = ln|\\sum^{\\hat{}}_{p,s}| + 2T^{-1} M\\]\n\n%%R\nAIC(GNARfit())\n\n[1] -0.06991947\n\n\n\n%%R\nAIC(GNARfit(betaOrder = c(2, 1)))\n\n[1] -0.05994387"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#model-selection-on-a-wind-network-time-series",
    "href": "posts/GCN/2023-01-05-GNAR.html#model-selection-on-a-wind-network-time-series",
    "title": "GNAR data",
    "section": "3.2. Model selection on a wind network time series",
    "text": "3.2. Model selection on a wind network time series\nthe data suite vswind that contains a number of R objects pertaining to 721 wind speeds taken at each of 102 weather stations in England and Wales. The suite contains the vector time series vswindts, the associated network vswindnet, a character vector of the weather station location names in vswindnames and coordinates of the stations in the two column matrix vswindcoords. The data originate from the UK Met Office site http://wow.metoffice.gov.uk and full details can be found in the vswind help file in the GNAR package.\n\nnodes : 102\ntime step : 721\n\n\n%%R\noldpar <- par(cex = 0.75)\nwindnetplot()\npar(oldpar)\n\n\n\n\nPlot of the wind speed network\n\nblue numbers are relative distance between sites\nlabels are the site name\n\n\n%%R\nBIC(GNARfit(vts = vswindts, net = vswindnet, alphaOrder = 1, betaOrder = 0))\n\n[1] -233.3848\n\n\n\n%%R \nfiveFit <- GNARfit(fiveVTS[1:160,],net=fiveNet, alphaOrder=2, betaOrder=c(2,0)) #learn \ndim(fitted(fiveFit))\n\n[1] 158   5\n\n\n\n%%R\ndummyFit <- GNARfit(fiveVTS,net=fiveNet, alphaOrder=2, betaOrder=c(2,0)) #learn \ndummyFit$mod$coefficients <- fiveFit$mod$coefficients\n\n\n%%R\nfitted(dummyFit)[161:200]\n\n [1]  0.01093152  0.07611113  0.50989356  0.84380803  0.90488185 -0.12703505\n [7] -0.57721780 -0.36681689 -0.26281975 -0.47712098 -0.62293008 -0.58121816\n[13] -0.81149078 -0.45403821 -0.60487041  0.28617606  0.20580455  0.19341988\n[19]  0.35296420  0.15628117  0.68350847  0.49043974  0.29627168 -0.35666858\n[25] -0.47565960  0.06692171 -0.14924170 -0.36616239 -0.49994894  0.22625500\n[31] -0.08023045  0.25371268 -0.47415540 -0.99390660 -1.16821429 -0.18438203\n[37] -1.10766872 -0.76969390  0.71828989  0.69737474\n\n\n\n%%R\nfitted(fiveFit)\n\n               [,1]         [,2]         [,3]          [,4]         [,5]\n  [1,]  0.300764573  0.731759572  0.645747192  0.6590441235  0.329408573\n  [2,]  0.584155581  0.709939766  0.504838042  0.4006542005  0.002404727\n  [3,]  0.248206643 -0.053672929  0.089912292  0.3569760840  0.886027241\n  [4,]  0.431422029 -0.596332784 -0.445500333 -0.1950437084  0.998677531\n  [5,]  0.369251545 -0.131646096 -0.151852774 -0.0307057532  0.602574799\n  [6,] -0.281163998  0.372761568  0.470612441  0.0640593261 -0.793452225\n  [7,] -0.629184394  0.122210560 -0.064188223  0.2614228537  0.027403171\n  [8,]  0.176102146  0.054605602  0.001654112 -0.0679337406 -0.041235943\n  [9,] -0.168610998 -0.212095723 -0.204125726 -0.2125261302 -0.010579734\n [10,] -0.161421278 -0.007465513  0.149274060 -0.0249484567 -0.327041759\n [11,]  0.359655447 -0.077039236 -0.039580520 -0.0457447866  0.112500734\n [12,] -0.015390804 -0.224700395 -0.236137628 -0.1566997305 -0.032236173\n [13,] -0.254595294 -0.052425427 -0.222029463 -0.1955623614 -0.090852345\n [14,] -0.133378083 -0.231175869 -0.262111004 -0.4557269171 -0.613390251\n [15,] -0.299492590 -1.329777415 -1.157380728 -0.7395871328  0.367735930\n [16,] -0.092939379 -0.649485715 -0.649077093 -0.5677985865  0.018220159\n [17,] -0.197734475  0.006055154  0.003767933  0.0142878952  0.017169293\n [18,] -0.593128663 -0.029057099  0.019930836 -0.1995170085 -0.699678586\n [19,] -0.406803183 -0.080878255 -0.076916012 -0.0142207300  0.084031465\n [20,]  0.195661742  0.636973567  0.514808183  0.1521298877 -0.565262518\n [21,] -0.646798007  0.034595763 -0.038468526 -0.2844049647 -1.200687294\n [22,] -0.858660467 -0.279197317 -0.100300563 -0.1774862659 -0.600631944\n [23,]  0.072500668  0.171516441  0.137895886  0.1253750409  0.109017776\n [24,]  0.361169443 -0.134591857 -0.172907391 -0.0276254736  0.454886946\n [25,] -0.555837928  0.021772877  0.086135588 -0.5401601843 -1.758757275\n [26,] -0.907025497 -0.516889787 -0.601369437 -0.4907248867 -0.518976355\n [27,] -0.577788165 -0.883706611 -0.990274375 -0.5655498386  0.219126828\n [28,] -0.381253476 -1.020667212 -0.974967606 -0.8163078225 -0.224880458\n [29,] -0.418889129 -1.136651964 -1.192973176 -0.8278078873  0.075705061\n [30,] -0.430848760 -0.711242749 -0.645456670 -0.7835485785 -0.902099485\n [31,] -0.408762558 -0.330133901 -0.313267801 -0.2950574688 -0.516672981\n [32,] -0.397456978 -0.177503203 -0.123190622  0.0851676997  0.175790185\n [33,] -0.321504797  0.137826136  0.185516044 -0.0469442492 -0.541534195\n [34,] -0.446890348  0.425984103  0.440639199  0.0859296090 -0.911239457\n [35,] -0.409664931  0.661582101  0.615592428  0.4648149587 -0.620671093\n [36,] -0.178853576  1.134703079  1.170584884  0.7788603566 -0.614547790\n [37,] -0.187107405  0.975055141  0.894986103  0.7556786981  0.299963760\n [38,] -0.421512891  0.230443777  0.311288224  0.3783662241  0.481463020\n [39,]  0.551866636  0.010017627 -0.031712684 -0.0291730137  0.299117468\n [40,] -0.415705371 -0.084974037 -0.102059950  0.0729357811 -0.004680301\n [41,] -0.596582277 -0.181138675 -0.037759798 -0.0710576429 -0.300907209\n [42,] -0.264388277  0.136784757  0.077185718  0.0270845136 -0.311590329\n [43,]  0.413516614  0.932841430  0.935079115  0.7161523403 -0.158898407\n [44,]  0.095801809  0.920995408  0.955252555  0.8459697803 -0.127453246\n [45,]  0.975373534  1.209886345  1.185240223  1.0015804496  0.169827608\n [46,]  0.618868253 -0.058442807 -0.058935046  0.2370795943  0.914954875\n [47,]  0.366854292  0.267881271  0.247344186  0.2400219950  0.479987689\n [48,] -0.150157867 -0.777932308 -0.717286974 -0.5794468369  0.195377648\n [49,]  0.075029321 -0.299959011 -0.355342761 -0.1920419709  0.334423437\n [50,]  0.386593451 -0.135877992 -0.072542184  0.2518678243  0.984484742\n [51,]  0.697822864 -0.311896474 -0.295669625 -0.2502382955  0.585030936\n [52,]  0.062654477 -0.191584061 -0.107314176  0.2208996954  1.056837103\n [53,]  0.728773427 -0.477836693 -0.494169917 -0.4843342717  0.451358938\n [54,]  0.585069845  0.665543312  0.719452401  0.5820396956  0.545931719\n [55,]  1.079661317  0.127297862  0.038038488  0.0677187351  0.907180658\n [56,]  0.651000271  0.443573833  0.461237609  0.2318123507  0.355931847\n [57,]  0.047340441  0.061483244 -0.142562719 -0.0074741198  0.203598849\n [58,]  0.020722674 -0.855956709 -0.772986200 -0.7280158105 -0.072139463\n [59,] -0.197216261 -0.894976325 -0.740954981 -0.7972459012 -0.237856418\n [60,] -0.335837112 -0.488349114 -0.651565734 -0.7163113371 -0.703717928\n [61,] -0.573434288 -1.566963776 -1.470953366 -0.9358838106  0.340002593\n [62,] -0.019967493 -1.006103326 -1.203028246 -0.7206978782  0.426166645\n [63,]  0.289165755 -0.676589963 -0.529091393 -0.3863962807  0.580747013\n [64,] -0.063683833 -0.320450285 -0.325613255 -0.2974046265 -0.105675029\n [65,] -0.112032969 -0.527479112 -0.523398055 -0.1590403400  0.496544412\n [66,] -0.286173235 -0.053400092  0.018863005 -0.0124208740 -0.118724115\n [67,] -0.499744957  0.041067235  0.076504951 -0.2048682261 -0.787362589\n [68,] -0.547870145 -0.732546310 -0.770409578 -0.7369449312 -0.331821251\n [69,] -0.905056316 -0.829491825 -0.850502405 -0.7607730212 -0.656264847\n [70,] -0.246440697 -0.279850678 -0.269853546 -0.5660758221 -1.111827018\n [71,] -0.186335645  0.010799425 -0.077495418  0.0601490253  0.102437820\n [72,]  0.010493539 -0.479453043 -0.453954591 -0.3426073144 -0.040090049\n [73,] -0.673580794 -0.734494634 -0.564315445 -0.8190023610 -1.019947630\n [74,] -0.640032299 -0.558498277 -0.596977420 -0.5456733025 -0.090276605\n [75,] -0.464960459 -0.529199551 -0.552331950 -0.8884973812 -1.066744453\n [76,] -0.887182659 -0.141258133 -0.110786546  0.0237603066  0.003998690\n [77,] -0.073105627 -0.207336194 -0.187193599 -0.6930066146 -1.105251019\n [78,] -0.689981040  0.027486354  0.151691133  0.2650861359 -0.203709649\n [79,]  0.258382277  0.652801148  0.545006830  0.3925781271 -0.249181067\n [80,]  0.751377138  0.239900459  0.276168396  0.6057405726  1.262601102\n [81,]  0.631390471  0.411822991  0.543334476  0.2438621480  0.112347717\n [82,] -0.034969641 -0.104184784 -0.097705561  0.1498464790  0.895833940\n [83,]  0.120113065  0.049356967  0.046034277 -0.0721309377  0.217972070\n [84,] -0.200386785 -0.203495710 -0.239586254 -0.1172887981  0.323191787\n [85,]  0.185644181 -0.395187380 -0.370733604 -0.5009300085 -0.211102667\n [86,] -0.237732516  0.276454796  0.287754635  0.2080093644 -0.223951494\n [87,]  0.216163467  0.192130141  0.177949800 -0.0753244143 -0.351364565\n [88,] -0.142649364 -0.186692214 -0.168987298 -0.1166303744  0.011404103\n [89,]  0.548618117  0.747707315  0.770581155  0.5221810333  0.172075628\n [90,]  0.028104354  0.688465275  0.636215214  0.5500772784  0.059464762\n [91,] -0.495363724 -0.071919605 -0.008154852 -0.4353087653 -1.271143508\n [92,] -1.107134875 -0.172815824 -0.303687490 -0.0844525666 -0.236711607\n [93,] -0.149354271  0.009431534  0.092873697 -0.4137230613 -1.079758945\n [94,] -0.480127162 -0.759207617 -0.733556252 -0.1155717520  0.856016011\n [95,] -0.068768871 -0.262250231 -0.276510238 -0.4068034977 -0.281317568\n [96,] -0.204766674 -0.466093264 -0.516820813 -0.4012743931 -0.036227691\n [97,]  0.450365055 -0.345449271 -0.277951368 -0.0735386028  0.626942400\n [98,]  0.825252322 -0.102070954  0.029346323  0.2224587296  0.891243876\n [99,]  0.918255191  0.974393754  0.785132509  0.9588950601  0.747082329\n[100,]  1.161471681  0.803193156  1.046464965  1.2474096963  1.536524225\n[101,]  0.796513581  0.943461644  0.698756745  0.5968311778  0.275247231\n[102,]  0.189772234  0.088167353  0.193522913 -0.1055246595 -0.205361362\n[103,] -0.486312684  0.528260131  0.609425778  0.4533837262 -0.448633125\n[104,] -0.780106474  0.619638040  0.487082196  0.2699333424 -1.074329863\n[105,]  0.001495171  0.043686101  0.071429149 -0.0068869393  0.109059794\n[106,]  0.412811020  0.589258087  0.534530373  0.1827023688 -0.169191222\n[107,]  0.230033356  0.369815780  0.441499631  0.4808802972  0.619074058\n[108,]  0.887957665  0.299276196  0.242694214  0.2120811348  0.437908215\n[109,] -0.003445154 -0.934051132 -0.730038077 -0.3220444395  0.707556500\n[110,] -0.138607950 -0.723059594 -0.801323554 -0.7383525593 -0.366013353\n[111,] -0.761794858 -0.776583246 -0.767390838 -0.6281202398 -0.286591883\n[112,] -0.143995206 -0.994654745 -0.891931956 -0.8145750541  0.194529857\n[113,] -0.158840376 -0.789802525 -0.720611341 -0.3434589195  0.705106074\n[114,]  0.213701938  0.094954893 -0.066337346 -0.1715778242 -0.301015012\n[115,]  0.353147878 -0.162438159 -0.035453134  0.2417453098  0.692188468\n[116,] -0.259026322 -0.062564122 -0.069539905  0.0494333669  0.016725122\n[117,]  0.439567933  0.334238798  0.123452083  0.2899531832  0.536404389\n[118,]  0.243694767 -0.136731679 -0.062368473 -0.0505163356  0.329751821\n[119,] -0.221760052 -0.034088224 -0.073079140 -0.0719693132  0.080492952\n[120,] -0.639870092 -0.346958117 -0.350162454 -0.6158933852 -1.052705642\n[121,]  0.191514800  0.198936857  0.188570749  0.0576140540 -0.098783032\n[122,]  0.497040376  0.104911046  0.183534206  0.3233013200  0.725368718\n[123,]  0.912741276  0.906110114  0.847102372  0.9050828507  0.840894148\n[124,]  0.761509505  0.486180954  0.657445208  0.6485641139  0.813518998\n[125,]  0.316821416 -0.083720193 -0.173493262 -0.1007134067  0.356082268\n[126,] -0.067185056 -0.753094489 -0.702750670 -0.2621385119  1.046954137\n[127,] -0.091228436 -0.675109353 -0.703565297 -0.5564405787  0.258988050\n[128,] -0.109253436 -0.444143304 -0.399640932 -0.3580561559 -0.035762392\n[129,] -0.232521560 -0.039990247 -0.095241200 -0.0443450553 -0.430664437\n[130,]  0.123446724  0.200420996  0.173518282  0.5184071044  0.313636742\n[131,]  0.285159008 -0.032242166  0.027550780  0.1048459050 -0.132334038\n[132,] -0.029094326  0.291860928  0.394652562  0.4297945912  0.077465921\n[133,]  0.607314455  0.854271821  0.946947611  0.7446008823  0.229008010\n[134,]  0.114274696  0.395094364  0.372741933  0.2933528915  0.073135295\n[135,]  0.058334948  0.020883512 -0.064495939 -0.0002935146  0.194116690\n[136,] -0.485203794  0.162128088  0.243616443  0.0173331790 -0.588154182\n[137,]  0.361414229  0.497203208  0.361158226  0.5018799742  0.573470360\n[138,]  0.819903948  0.366313425  0.363516294  0.3796662479  0.699684917\n[139,] -0.216165855 -0.335708047 -0.251520773 -0.2682301442  0.062768414\n[140,] -0.454294227 -0.359066363 -0.294969357 -0.4097376437 -0.479590775\n[141,] -0.062418172  0.009466708 -0.051169781  0.1398314442  0.275918751\n[142,]  0.546927988  0.424770686  0.388169659  0.1288546957 -0.347653081\n[143,] -0.311508529  0.345041027  0.500511866  0.3303953879 -0.189245985\n[144,]  0.156321321  0.623518356  0.518171727  0.4513620372  0.284295098\n[145,]  0.039622964  0.299954776  0.356381605  0.3469508290  0.326305082\n[146,]  0.579791654  0.695810253  0.652099702  0.5011115910  0.023065334\n[147,]  0.554655907  0.093490122  0.153079897  0.4182759318  0.767121199\n[148,]  0.384057236 -0.065678272 -0.023948715  0.0451867298  0.454508452\n[149,]  0.447874514  0.392017250  0.266888939  0.3039632644  0.540801889\n[150,]  0.497810670  0.980726335  1.014061377  0.9632510637  0.560362395\n[151,]  0.044270389  0.550595322  0.526083974  0.5090146051 -0.039702900\n[152,]  0.128989148  0.501717816  0.494582107  0.2658346703 -0.283886186\n[153,]  0.003640933  0.471547028  0.593579063  0.5105159483  0.179982706\n[154,]  0.207081614  0.420074580  0.265684904  0.2159979026 -0.161101019\n[155,] -0.478137997  0.240399554  0.349195339  0.3926704926 -0.290283502\n[156,] -0.368038032  0.588505317  0.539132547  0.2403574312 -0.831187194\n[157,] -0.452959327  0.212753187  0.402307299  0.1713284561 -0.415735323\n[158,]  0.013726373  0.132970935  0.077022243  0.0243587773 -0.211000744\n\n\n\n%%R \nfiveVTS\n#gdpVTSn2[52,]\n\n               [,1]         [,2]         [,3]         [,4]        [,5]\n  [1,] -0.106526553  1.077613724 -0.244694569  0.933710066  0.44443593\n  [2,]  0.664495737  0.935476457  1.402823610  0.826656526  0.02097885\n  [3,] -0.255977521 -1.478171940  2.160938890  1.746276180  0.80188586\n  [4,]  1.684436464  0.180662387  0.398879113 -0.418094544  0.28037722\n  [5,]  1.451205104  0.584157057 -1.700218367 -1.219724913  1.63810802\n  [6,]  0.728846398  0.536253148 -0.730310794 -0.297010221  1.04864255\n  [7,] -1.526780869  1.957817106 -0.506088728  0.470747730 -0.63167929\n  [8,]  1.269035583 -0.519870107  0.773038576 -0.176065931 -3.13354575\n  [9,] -0.403340381 -0.398760476  1.358212255 -0.755544421  1.58592625\n [10,] -0.057805476 -0.932876792  0.163580296 -0.299486667 -0.31232748\n [11,] -0.498505318  1.005323196 -0.762045202  0.136725053 -0.43412619\n [12,]  0.057551667  0.252414834 -1.585146661  0.892773199  0.64534882\n [13,] -0.235969810  0.168738145 -0.927906887 -0.201182245  0.28394360\n [14,] -0.574265201 -0.451671164  1.523378623 -1.466113999  0.88143283\n [15,] -1.120369546 -2.167955049 -0.290951250  0.951438750 -0.63430591\n [16,]  0.734315543 -1.074175821 -2.550164018 -1.843026440  0.16372069\n [17,] -0.300708705 -0.215965234 -1.251829204 -1.147084336  0.81200151\n [18,]  0.168111491  0.617846030 -0.067553835 -0.316813889 -0.57552628\n [19,] -1.130660662  1.049382642 -0.219923688 -0.838450813 -0.84289855\n [20,]  0.491603006  0.911202204 -0.037939755 -1.146905077 -0.70878526\n [21,] -1.282680651 -0.037695723  1.273889958  1.222120228  0.48792144\n [22,] -1.977860407 -1.476586870  0.618312041  0.541447907 -1.59494685\n [23,] -0.566157633  1.007129193 -0.421914435 -1.255494830 -1.57780757\n [24,]  0.427751755  0.446864227 -0.006556415  0.235046101 -0.13251371\n [25,]  0.588963964 -0.510977075 -0.208215798 -0.038859202  1.03505405\n [26,] -3.577108873  0.063814248 -0.764773488  0.750897963 -0.57677829\n [27,] -0.762998219 -0.715903539  0.070974165 -1.767866460 -0.85886671\n [28,]  1.154873570 -2.505711766 -0.235496918 -1.464120269 -1.74689908\n [29,] -0.272693870 -1.857460255 -1.012583601 -1.478438760 -0.20065611\n [30,]  0.395708110 -2.601309968 -1.102332663 -1.418525692 -0.67074519\n [31,] -1.632021790 -1.389226367 -1.406639927 -0.164463613 -0.57992151\n [32,] -0.593868925 -0.604746854 -0.889344852  0.170550009 -1.19152282\n [33,]  0.902385866  0.859337939 -0.216488189 -1.019066503 -1.20331830\n [34,] -0.755599082  1.205454399 -0.315142793 -0.182216870 -0.76219579\n [35,] -1.402617202  1.327218071  0.030109089  0.508358610 -1.19204930\n [36,] -0.248901260  0.835443000  0.673836075  1.249100802 -2.50681088\n [37,] -0.821889741  2.683690391  1.991361276  0.628557722 -0.63183483\n [38,]  0.509052231  2.387571425  3.198218488 -1.313720751  0.41614740\n [39,]  1.760264680  1.329747672  0.589082315 -0.959062662 -2.17248032\n [40,]  0.407692882 -0.494160904 -0.336544078  0.518871030  1.17098422\n [41,]  0.529542953 -0.109371157  0.276546799 -0.441917032 -1.77465267\n [42,]  0.071422121  1.390859571 -0.656166400 -1.112883997 -1.53889722\n [43,] -0.038685141  0.343631869 -0.121075284  0.189519172 -1.28519302\n [44,]  0.032507365  1.105775541  0.745569895  2.313785056 -0.54606867\n [45,]  0.326298291  1.672376452  0.867801558  1.596593083 -1.38530798\n [46,]  0.493735214  0.828445070  0.665517459  3.545540325  0.17892418\n [47,]  1.046193130  0.182022439  0.076358398 -0.564071723  2.31894117\n [48,]  0.487801423  0.402192928  0.660265277  0.160859127  0.85916399\n [49,]  0.012036883 -0.575802156 -1.375502706 -1.555497631  0.78253503\n [50,]  0.566249612 -0.679123506 -0.342116629 -0.293370938  0.16791547\n [51,]  1.983899460  0.325870466 -0.462247924 -0.144103964  0.20428669\n [52,]  0.082477869 -0.015707410 -0.827420937 -0.533338717  3.03848854\n [53,]  2.025321199  1.260059506 -0.667274520 -0.983423623 -0.25069462\n [54,] -0.182207953 -0.492329932 -1.572486971 -0.318050791  3.11590615\n [55,]  0.310016519  2.163157480  0.539075569  0.775034293  1.55599197\n [56,]  0.344219807 -0.289984010  0.019376746  0.352604618  3.88803701\n [57,] -0.628460347  0.908826573  0.839215153  0.387888048  2.79852674\n [58,]  0.219514752 -1.949662023  1.297952270  0.245509848 -0.10715958\n [59,] -0.703231049 -1.872499373 -0.948118550 -0.956147553  1.46327573\n [60,] -1.037484518 -0.353586885 -2.197040404 -1.022195776  1.03019923\n [61,] -1.671104120 -1.807816077 -0.456726722 -0.275561042  0.23349098\n [62,]  1.082454650 -2.163914674 -2.772699788 -1.867484810 -1.15276058\n [63,]  0.733526181 -2.901286567 -0.144692022 -1.533368920  0.64538823\n [64,]  0.607622782 -0.564034684 -1.024446572 -0.860302664  1.46464103\n [65,] -0.249447697 -0.622497600 -0.860157861  0.059602510 -0.28629810\n [66,]  1.242991882 -0.553213122 -0.784868076 -0.866755329 -0.50726271\n [67,]  0.025808026  1.134045937 -0.194833475 -0.766397319 -0.66714031\n [68,] -1.280038469  0.969797971 -0.529341149 -0.274648131 -0.88451275\n [69,] -0.899437152 -0.455454165 -0.889218336 -2.081201461  0.63782940\n [70,] -0.485225451 -1.579881065 -1.542956678 -0.728773133 -2.63529295\n [71,] -2.232840088 -0.666470900 -0.802215707  0.306471316  0.36000506\n [72,]  0.263283350 -0.473210196  0.858443996 -0.330691066 -0.23528971\n [73,]  0.194868023 -1.673792947 -1.157081284  0.464290012 -0.67196384\n [74,] -2.374209088  0.746939110 -1.641811447 -1.805924541  0.66216561\n [75,] -0.319164807  0.368646446 -0.675529010 -2.103798682  0.06414596\n [76,] -2.347006749 -0.926133386 -1.280596322 -0.435692565  0.21824222\n [77,]  0.446994651  1.473521493  0.734103122 -2.231131628 -1.36931119\n [78,] -2.402766578 -0.673636271 -1.514644362  0.766704957  0.57180703\n [79,]  0.591165442  1.638210347 -0.954659887  0.008337226 -2.90507678\n [80,] -0.284550885  0.812008860  0.745209777  1.099796776  0.12632330\n [81,]  2.404372924  0.494780967 -0.137672593  0.738809147  0.71597343\n [82,] -0.732204526  2.210038243 -0.355431422  0.339909653  2.46725315\n [83,]  1.437145004  1.547713162 -0.345952409 -1.578834604  0.52850356\n [84,] -0.084957358  1.234464257 -0.032386550 -0.956802127  1.29018233\n [85,]  0.630013261  0.172600953 -0.127816246 -1.056826347 -0.23795324\n [86,] -0.893457396 -0.660542921 -1.265061454  0.015612038  1.27234265\n [87,] -0.287301062  0.963686285  0.359594848  0.203580641 -0.80393259\n [88,] -1.133229025  0.151632482 -0.040554491  0.515841576  1.28839412\n [89,] -0.023953670 -0.117144878 -0.537169309 -0.192949617 -0.19885872\n [90,] -0.238994400  1.725878556  1.063475592  0.828811176  1.72692730\n [91,]  0.299232378  0.799624873  1.181753039  0.831726730 -0.84857446\n [92,] -2.481179710  0.113529319 -0.650053446  0.011672273 -0.33558946\n [93,]  0.312713660 -0.079515578  1.502108851 -2.180229770 -2.24161884\n [94,] -1.996496328 -0.407588562 -0.652585366  0.923280995 -0.15832811\n [95,]  2.250167112 -0.220512659 -0.805730284 -2.066199413 -1.28057608\n [96,] -0.763689773 -0.031020794 -0.208759418 -0.867474784  0.72626616\n [97,]  0.014431435 -1.336163148 -0.640654650 -0.333381100 -0.44435745\n [98,]  0.957691513 -0.361694475 -0.967050395  0.060214711  1.10246612\n [99,]  1.469745730  0.554733531 -1.990318074  1.277950650  0.91469843\n[100,]  1.356192755  0.363980993  1.646543813  2.153855538  0.49502358\n[101,]  3.001712443  2.666746291 -0.368684582  1.849950555  0.63675201\n[102,] -0.281234965  0.456235005  2.292473984  0.854928226  2.22619190\n[103,] -1.140727890  0.202030354 -0.219402824  0.310787559  1.38376301\n[104,] -0.101523413  1.693526806  0.088496042  0.849098718 -2.58113447\n[105,] -1.122014513  0.674017745  1.696074464  0.065284585 -2.46042966\n[106,] -0.050503692  0.476461163  0.741019471 -1.033544711  1.30742678\n[107,] -0.977589842  0.119899249  1.188456997  1.081889577  1.41965239\n[108,]  0.909422801  1.255343375  0.842509208 -0.192153747  0.71392520\n[109,]  0.441028047 -0.896440332 -0.270300473  1.985698469  1.24604331\n[110,]  1.237351903  0.285784091 -2.573210003 -1.364613642  0.30715435\n[111,] -0.812621272 -1.216797535 -1.946208168 -0.293442424  0.01613793\n[112,] -0.569590223 -0.000629766 -0.657769831 -2.422099355 -0.23850071\n[113,]  0.013135413 -0.269047393 -2.386679520 -1.612243344  1.04220642\n[114,]  1.497050345  0.473005141 -2.121414725 -1.449151157 -0.36453856\n[115,] -0.826489548 -0.635578799  0.160746530  0.588310936  0.64139235\n[116,]  1.714414681 -0.069917852 -1.565924860  1.120471391 -0.78042307\n[117,]  0.090005879  1.200734542  0.270198304 -1.404710351 -0.03628579\n[118,]  1.236752409 -1.518803758  1.237344754  1.129076932 -0.20967169\n[119,] -0.087766705  0.109758177  0.830463404 -1.215657793  2.10369911\n[120,]  0.171155851 -0.458798255  0.673636072 -0.491339577 -0.55057403\n[121,] -2.015292503 -0.870762059 -0.029309395 -0.793820162 -0.46857349\n[122,] -0.362231614 -0.364385616  0.389845581  0.862317056  0.58137591\n[123,]  1.134908449  0.528983153 -0.215140490  0.381732812  0.95051905\n[124,]  1.457741194  0.847372382  1.281677103  1.882417946  0.73169697\n[125,]  1.108355891  2.153722703 -0.310021511  0.663789759  1.50520899\n[126,]  0.071438586 -0.058006918 -0.325876606 -0.374044712  1.44845100\n[127,]  1.793527240  0.106114670 -0.893934806 -2.276541783  0.63834022\n[128,]  0.315117203 -0.717323756 -0.955019446 -1.370627155  0.39051426\n[129,] -0.043340209 -0.312945747 -1.197591049 -0.317055511 -0.20153262\n[130,] -0.102618844 -0.880170227 -0.604543790  1.138606188 -2.06784587\n[131,]  1.740909147 -0.637353850 -0.010562729  1.561563247 -2.32157478\n[132,]  0.188345255 -0.469433386 -0.998242748  1.333047029 -0.61688863\n[133,]  0.478267920  1.814870921 -0.214383410  0.211463195 -0.68793981\n[134,]  0.520210151  2.448452650 -0.463708150  1.952197392  0.18188706\n[135,] -0.269362229  1.911952464  0.242272223 -0.444547682  1.07677467\n[136,]  0.423190923 -0.410880998 -0.195993929  0.257239829 -0.28741994\n[137,] -1.026903990  1.526871656  0.360358335 -0.772639199 -0.55385790\n[138,]  1.450078506 -0.349330702  1.034884543  1.105119122 -0.44915787\n[139,]  0.550436921  0.247527774  1.257978907  0.192344787  2.62949378\n[140,] -0.191655801  0.134136826 -0.521197521 -1.048113725  0.16877213\n[141,] -0.791900380  0.321889000 -1.170523314 -0.640876817 -0.67475039\n[142,]  0.971146450  0.021298110 -0.039896748  0.046666821 -0.88362522\n[143,] -0.982608481 -0.377262589  0.171316614  1.854723760  0.99834845\n[144,] -0.495627812  2.483176264  0.354246741 -0.684488580 -0.02501307\n[145,]  0.561938527  0.993967044  1.211151634  0.257789007  0.12399797\n[146,]  0.709311790  1.260796758  0.500729309 -0.317030392 -0.10472747\n[147,]  0.242781056  0.024085820  0.200161589  2.499565048 -0.35615091\n[148,]  1.391448147  0.433674956 -0.406153463  0.535043661  0.73457838\n[149,]  0.412636662  0.657392857 -0.602350630 -0.222007914  1.28589205\n[150,]  0.464724442  0.241309629  1.384164088 -0.007823340  1.51425328\n[151,]  1.387500492  1.166650199  1.425112356  1.867514447 -0.82096695\n[152,]  0.324362289  0.546923900  1.123115609  0.598062895 -0.86820404\n[153,] -0.642847662  0.488927474  1.005998211  0.623605903  0.36832761\n[154,]  0.580613501  1.826643913  0.215912542  0.360628058 -0.61370894\n[155,] -0.144307876 -0.646151661  0.724633376  1.186826797 -0.31028196\n[156,]  0.546715317  0.754951418 -0.046634000  0.644574743 -3.02529497\n[157,] -1.292512590  1.254904828  1.493495999 -0.128867226 -0.51111891\n[158,] -0.178360098  1.914596966 -0.981080025  0.228541535 -1.75337640\n[159,] -0.292207540  0.598820101 -0.516651587  0.274304089  0.03373131\n[160,] -2.308524149  0.208852278  1.461230272 -0.500911249 -2.27810147\n[161,] -1.379791032  0.145882158  0.915799429  1.484776097 -3.50041126\n[162,] -0.687780533 -0.916656103  1.270414228  1.016692379 -0.33067553\n[163,]  0.719998169  2.802816864  1.671148575 -0.623046401  0.58186844\n[164,]  0.720895216 -0.441022235  1.355521153  0.169312739  1.37392204\n[165,]  1.255917246  1.237433250 -1.261445845  1.156903886  1.34803048\n[166,]  1.705742677 -2.082080117  1.178644006  0.035092003  2.31810280\n[167,] -1.241297515 -1.810899663  0.429577096 -1.605573403  1.82238571\n[168,] -1.733594362 -1.512615033 -0.660630798 -0.541034771 -0.51548359\n[169,]  0.792480615  1.111823458 -0.087410649 -0.471304848 -1.46894677\n[170,] -1.308490289 -0.820921717 -1.060340478 -0.848828375  0.61581232\n[171,] -0.151371781 -1.755296255 -2.178401224 -1.133293893 -0.73148664\n[172,] -1.440949271 -1.701461336  0.086620787 -0.574783622 -0.96759066\n[173,] -0.602659336  0.088764382 -1.505800731 -1.165425847 -0.71480221\n[174,]  1.275259704 -0.410086120  0.494580410 -2.991666233 -1.27793672\n[175,] -1.404478046 -0.271241519  0.442854733 -0.222198583 -0.71844124\n[176,] -0.656202533  1.203821570  0.978017687  0.126331498 -1.96978752\n[177,] -0.394150009 -0.161910841  2.176312978  0.695232617  0.89852498\n[178,]  0.942635640  0.627373149  1.433906899 -0.365531957  0.60603188\n[179,] -0.023229999  1.377273430 -0.409007667  1.061813530 -0.29478037\n[180,]  0.365704922  1.211613739 -1.923486162  0.212759987  0.96275639\n[181,] -0.005420862 -0.765759383 -1.517079711  0.902219156 -0.34235656\n[182,]  0.024961781  0.098786009 -0.242548348  0.230247435  2.58670863\n[183,]  0.827409379 -0.019279260  1.645100501 -0.517327835  1.97867114\n[184,]  0.446528306 -0.641671765  0.183926490  0.194955794  0.61934460\n[185,] -0.852794309 -0.150650614 -1.175556623 -0.749290811 -0.16935739\n[186,]  0.545053341 -0.164000579 -0.705192833 -2.243859141 -0.09017822\n[187,] -1.237547900  0.639970678 -0.071693146  0.802596589  0.36885061\n[188,] -0.400597906 -0.490175656  1.285472941  0.566619402 -0.76045074\n[189,] -0.346030030 -0.031140831  0.468958960 -0.626935003 -0.58954715\n[190,] -0.242598074 -0.643482354  0.637618122  0.729482578 -2.59565491\n[191,]  1.906927330  1.295421628 -0.473359922  0.009433931 -0.45087521\n[192,]  0.247556013  0.380624945  0.511373051 -0.303734068 -0.35216521\n[193,]  0.109182478 -1.095266046 -1.117244679  0.260695357  0.63022764\n[194,] -0.428068355  0.957493744  1.147709215 -1.806721502  0.20508136\n[195,] -2.391696093  1.006982896  0.469035946 -1.029263987 -1.23612823\n[196,] -2.210774865 -1.403493412  0.556772208 -0.600485513 -2.42048578\n[197,] -0.722601932 -0.414053540  0.988636770 -0.610321642  0.58772892\n[198,] -2.938590715 -0.559749691  0.078615069  0.010633188 -2.36841490\n[199,] -0.985951916  0.215394351 -0.119048423  0.779733910 -2.98799845\n[200,]  0.447242609  0.404155027  0.719959458 -0.442525233 -1.78159592\n\n\n\n%%R\nBIC(GNARfit(vts = vswindts, net = vswindnet, alphaOrder = 1, betaOrder = 0, globalalpha = FALSE))\n\n[1] -233.1697\n\n\n\n%%R\nBIC.Alpha2.Beta <- matrix(0, ncol = 15, nrow = 15)\nfor(b1 in 0:14)\n    for(b2 in 0:14)\n        BIC.Alpha2.Beta[b1 + 1, b2 + 1] <- BIC(GNARfit(vts = vswindts,\n                    net = vswindnet, alphaOrder = 2, betaOrder = c(b1, b2)))\ncontour(0:14, 0:14, log(251 + BIC.Alpha2.Beta), xlab = \"Lag 1 Neighbour Order\", ylab = \"Lag 2 Neighbour Order\")\n\nException ignored from cffi callback <function _processevents at 0x7f1829767f70>:\nTraceback (most recent call last):\n  File \"/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/rpy2/rinterface_lib/callbacks.py\", line 277, in _processevents\n    try:\nKeyboardInterrupt: \nException ignored from cffi callback <function _processevents at 0x7f1829767f70>:\nTraceback (most recent call last):\n  File \"/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/rpy2/rinterface_lib/callbacks.py\", line 277, in _processevents\n    try:\nKeyboardInterrupt: \nException ignored from cffi callback <function _processevents at 0x7f1829767f70>:\nTraceback (most recent call last):\n  File \"/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/rpy2/rinterface_lib/callbacks.py\", line 277, in _processevents\n    try:\nKeyboardInterrupt: \n\n\n\na set of GNAR(2,[b1,b2]) models with b1, b2 ranging from zero to 14\nContour plot of BIC values for the two-lag autoregressive model incorporating b1-stage and b2-stage neighbours at time lags one and two. Values shown are log(251 + BIC) to display clearer contours.\n\nì´í•´ ëœ ë¨..\n\nincreasing the lag two neighbour sets beyond first stage neighbours would appear to increase the BIC for those lag one neighbour stages greater than five\n\nchatGPT\nì´ ë¬¸ì¥ì„ ì¡°ê¸ˆ ë” ìì„¸íˆ ì„¤ëª…í•˜ë©´, BIC(Bayesian Information Criterion)ëŠ” ëª¨ë¸ì„ ì„ íƒí•  ë•Œ ì‚¬ìš©í•˜ëŠ” ì§€í‘œë¡œì„œ, ìš°ë¦¬ê°€ ì„ íƒí•œ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì í•©í•œì§€ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. ì´ ë¬¸ì¥ì—ì„œëŠ”, ì´ì›ƒ ì§‘í•©ì˜ ëŒ€ê¸° ì‹œê°„ì´ ì¦ê°€í• ìˆ˜ë¡ BIC ê°’ì´ ì¦ê°€í•  ê²ƒì´ë¼ê³  ì–¸ê¸‰í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ìš°ë¦¬ê°€ ì„ íƒí•œ ëª¨ë¸ì´ ì í•©í•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ìˆë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ê·¸ë˜í”„ë¥¼ ë³´ê³  ìˆì„ ë•Œ, ìˆ˜í‰ ìœ¤ê³½ì„ ì€ BIC ê°’ì´ 0ì¸ ìŠ¤í…Œì´ì§€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŠ” ìš°ë¦¬ê°€ ì„ íƒí•œ ëª¨ë¸ì´ ì™„ë²½í•˜ê²Œ ì í•©í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\n\n%%R\ngoodmod <- GNARfit(vts = vswindts, net = vswindnet, alphaOrder = 2, betaOrder = c(5, 1))\ngoodmod\n\nModel: \nGNAR(2,[5,1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\n dmatalpha1  dmatbeta1.1  dmatbeta1.2  dmatbeta1.3  dmatbeta1.4  dmatbeta1.5  \n    0.56911      0.10932      0.03680      0.02332      0.02937      0.04709  \n dmatalpha2  dmatbeta2.1  \n    0.23424     -0.04872"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#constructing-a-network-to-aid-prediction",
    "href": "posts/GCN/2023-01-05-GNAR.html#constructing-a-network-to-aid-prediction",
    "title": "GNAR data",
    "section": "3.3. Constructing a network to aid prediction",
    "text": "3.3. Constructing a network to aid prediction\nWe propose a network construction method that uses prediction error, but note here that our scope is not to estimate an underlying network, but merely to find a structure that is useful in the task of prediction.\nwe use a prediction error measure, understood as the sum of squared differences between the observations and the estimates:\n\\[\\sum^N_{i=1} (X_{i,t} - \\hat{X}_{i,t})^2\\]\n\n%%R\nprediction <- predict(GNARfit(vts = fiveVTS[1:199,], net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1)))\nprediction\n\nTime Series:\nStart = 1 \nEnd = 1 \nFrequency = 1 \n    Series 1  Series 2  Series 3  Series 4   Series 5\n1 -0.6427718 0.2060671 0.2525534 0.1228404 -0.8231921"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#oecd-gdp-network-structure-aids-prediction",
    "href": "posts/GCN/2023-01-05-GNAR.html#oecd-gdp-network-structure-aids-prediction",
    "title": "GNAR data",
    "section": "4. OECD GDP: Network structure aids prediction",
    "text": "4. OECD GDP: Network structure aids prediction\nGOP growth rate time series\n\n35 countries from the OECD website\ntime series : 1961 - 2013\nT = 52\nNodes = 35\nIn this data set 20.8% (379 out of 1820) of the observations were missing due to some nodes not being included from the start.\nwe do not uese covariate information, so C=1\n\n\n%%R\nlibrary(\"fields\")\nlayout(matrix(c(1, 2), nrow = 1, ncol = 2), widths = c(4.5, 1))\nimage(t(apply(gdpVTS, 1, rev)), xaxt = \"n\", yaxt = \"n\", col = gray.colors(14), xlab = \"Year\", ylab = \"Country\")\naxis(side = 1, at = seq(from = 0, to = 1, length = 52), labels = FALSE, col.ticks = \"grey\")\naxis(side = 1, at = seq(from = 0, to = 1, length = 52)[5*(1:11)], labels = (1:52)[5*(1:11)])\naxis(side = 2, at = seq(from = 1, to = 0, length = 35), labels = colnames(gdpVTS), las = 1, cex = 0.8)\nlayout(matrix(1))\nimage.plot(zlim = range(gdpVTS, na.rm = TRUE), legend.only = TRUE, col = gray.colors(14))\n\nR[write to console]: Loading required package: spam\n\nR[write to console]: Spam version 2.8-0 (2022-01-05) is loaded.\nType 'help( Spam)' or 'demo( spam)' for a short introduction \nand overview of this package.\nHelp for individual functions is also obtained by adding the\nsuffix '.spam' to the function name, e.g. 'help( chol.spam)'.\n\nR[write to console]: \nAttaching package: â€˜spamâ€™\n\n\nR[write to console]: The following objects are masked from â€˜package:baseâ€™:\n\n    backsolve, forwardsolve\n\n\nR[write to console]: Loading required package: viridis\n\nR[write to console]: Loading required package: viridisLite\n\nR[write to console]: \nTry help(fields) to get started.\n\n\n\n\n\n\nHeat plot(grey scale) of the differenced time series,\n\nwhite space indicates missing time series observations"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#finding-a-network-to-aid-prediction",
    "href": "posts/GCN/2023-01-05-GNAR.html#finding-a-network-to-aid-prediction",
    "title": "GNAR data",
    "section": "4.1. Finding a network to aid prediction",
    "text": "4.1. Finding a network to aid prediction\n\n%%R\nnet1 <- seedToNet(seed.no = seed.nos[1], nnodes = 35, graph.prob = 0.15)\nnet2 <- seedToNet(seed.no = seed.nos[2], nnodes = 35, graph.prob = 0.15)\n\n\n%%R\nlayout(matrix(c(2, 1), 1, 2))\npar(mar=c(0,1,0,1))\nplot(net1, vertex.label = colnames(gdpVTS), vertex.size = 0)\nplot(net2, vertex.label = colnames(gdpVTS), vertex.size = 0)\n\n\n\n\n\nErdos-Renyi random graphs xonstructed from the first two elements of the seed.nos variable with 35 nodes and connection probability 0.15.\nìê¸°íšŒê·€ ëª¨ë¸ì¸ GNAR ëª¨ë¸ì„ ì˜ˆì¸¡ì— ì‚¬ìš©í•  ë•Œ, ì–´ë–¤ ë„¤íŠ¸ì›Œí¬ê°€ ê°€ì¥ ì í•©í•œì§€ ì¡°ì‚¬í•´ì•¼ í•¨.\nì´ë•Œ ê° ë…¸ë“œì˜ ìê¸° ìƒê´€ í•¨ìˆ˜ë¥¼ ì´ìš©í•œ ì´ˆê¸° ë¶„ì„ ê²°ê³¼, 2ì°¨ ìê¸°íšŒê·€ êµ¬ì„± ìš”ì†Œê°€ ì¶©ë¶„í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ì–´ p = 2ê¹Œì§€ì˜ GNAR ëª¨ë¸ì„ ì‹œí—˜í•¨.\nê° ì‹œê°„ ì§€ì—°ì—ì„œ ìµœëŒ€ 2ê°œì˜ ì´ì›ƒ ì§‘í•©ì„ í¬í•¨í•¨.\nì´ì— ë”°ë¼ ì•„ë˜ì™€ ê°™ì€ GNAR ëª¨ë¸ì´ ì‹œí—˜ë¨.\n\nGNAR(1, [0]), GNAR(1, [1]), GNAR(2, [0, 0]), GNAR(2, [1, 0]), GNAR(2, [1, 1]), GNAR(2, [2, 0]), GNAR(2, [2, 1]), ê·¸ë¦¬ê³  GNAR(2, [2, 2])ê°€ ì‹œí—˜ë˜ë©°, ê°ê° individual-\\(\\alpha\\)ì™€ global-\\(\\alpha\\) GNAR ëª¨ë¸ë¡œ ì í•©í•¨.\nì´ 16ê°œì˜ ëª¨ë¸ì´ ìƒì„±ë¨.\nì´ ì¤‘ì—ì„œ ì „ì²´ GDP ì˜ˆì¸¡ì— ì‚¬ìš©í•  GNAR ëª¨ë¸ì„ ì„ íƒí•  ê²ƒ.\nì—°ê²° í™•ë¥ ì´ 0.15ì¸ 10,000ê°œì˜ ì„ì˜ì˜ ì–‘ë°©í–¥ ë„¤íŠ¸ì›Œí¬ë¥¼ ìƒì„±í•˜ê³ , ìœ„ì—ì„œ ì–¸ê¸‰í•œ GNAR ëª¨ë¸ì„ ì´ìš©í•´ ì˜ˆì¸¡í•  ê²ƒ.\nê·¸ë˜ì„œ ì´ ì˜ˆì œëŠ” ìƒë‹¹í•œ ê³„ì‚° ì‹œê°„ì´ í•„ìš”(ë°ìŠ¤í¬íƒ‘ PCì—ì„œ ì•½ 90ë¶„).\nì´ë¥¼ ìœ„í•´ ì•„ë˜ ì½”ë“œì—ëŠ” ì¼ë¶€ ë¶„ì„ë§Œ í¬í•¨.\nê³„ì‚° ìƒì˜ ì´ìœ ë¡œ, ìš°ì„  ê° ë…¸ë“œì—ì„œ í‘œì¤€ í¸ì°¨ë¡œ ë‚˜ëˆ ì„œ ì”ì°¨ê°€ ê° ë…¸ë“œì—ì„œ ë™ì¼í•œ ë¶„ì‚°ì„ ê°€ì§€ê²Œ í•¨.\nseedSim í•¨ìˆ˜ëŠ” ì˜ˆì¸¡ê°’ê³¼ ì›ë˜ ê°’ì˜ ì œê³± ì°¨ì´ì˜ í•©ì„ ì¶œë ¥í•˜ê³ , ì´ë¥¼ ì˜ˆì¸¡ ì •í™•ë„ì˜ ì¸¡ì • ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©\n\n\n\n%%R\ngdpVTSn <- apply(gdpVTS, 2, function(x){x / sd(x[1:50], na.rm = TRUE)})\nalphas <- c(rep(1, 2), rep(2, 6))\nbetas <- list(c(0), c(1), c(0, 0), c(1, 0), c(1, 1), c(2, 0), c(2, 1), c(2, 2))\nseedSim <- function(seedNo, modelNo, globalalpha){\n    net1 <- seedToNet(seed.no = seedNo, nnodes = 35, graph.prob = 0.15)\n    gdpPred <- predict(GNARfit(vts = gdpVTSn[1:50, ], net = net1,\n                               alphaOrder = alphas[modelNo], betaOrder = betas[[modelNo]],\n                               globalalpha = globalalpha))\n    return(sum((gdpPred - gdpVTSn[51, ])^2))\n    }\n\n\n%%R\nseedSim(seedNo = seed.nos[1], modelNo = 1, globalalpha = TRUE)\n\n[1] 23.36913\n\n\n\n%%R\nseedSim(seed.nos[1], modelNo = 3, globalalpha = TRUE)\n\n[1] 11.50739\n\n\n\n%%R\nseedSim(seed.nos[1], modelNo = 3, globalalpha = FALSE)\n\n[1] 18.96766\n\n\n\n\n\nimage.png\n\n\n\n10,000ê°œì˜ ì„ì˜ì˜ ë„¤íŠ¸ì›Œí¬ì™€ 16ê°œì˜ ëª¨ë¸ë¡œë¶€í„° ì‹œë®¬ë ˆì´ì…˜í•œ ì˜ˆì¸¡ ì˜¤ë¥˜ì˜ ë°•ìŠ¤ ê·¸ë˜í”„\n(ê³„ì‚° ì‹œê°„ì´ ê¸¸ì–´(90ë¶„) ì½”ë“œëŠ” ìƒëµ).\nì¼ë°˜ì ìœ¼ë¡œ global-Î± ëª¨ë¸ì€ ë” ë‚®ì€ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ì¼ìœ¼í‚´.\nê·¸ë˜ì„œ ì´ ë²„ì „ì˜ GNAR ëª¨ë¸ì„ ì‚¬ìš©í•  ê²ƒ.\nê·¸ë¦¼ 9ì—ì„œ ì²« ë²ˆì§¸ ëª¨ë¸ì¸ GNAR(1, [0])ê³¼ ì„¸ ë²ˆì§¸ ëª¨ë¸ì¸ GNAR(2, [0, 0])ì˜ ê²½ìš°, â€œë°•ìŠ¤ ê·¸ë˜í”„â€ëŠ” ì¸ì ‘í•œ ë§¤ê°œë³€ìˆ˜ê°€ ì í•©ë˜ì§€ ì•Šì•„ ê²°ê³¼ê°€ ì „ë¶€ ë™ì¼í•´ ì§§ì€ ìˆ˜í‰ì„ ìœ¼ë¡œ í‘œì‹œë¨.\në‹¤ë¥¸ global-Î± ëª¨ë¸ë“¤ì€ ì´ ì•ˆì— í¬í•¨ë˜ì–´ ìˆê¸° ë•Œë¬¸ì—, global-Î± GNAR(2, [2, 2])ì˜ ì˜ˆì¸¡ ì˜¤ë¥˜ê°€ ìµœì†Œê°€ ë˜ëŠ” ì„ì˜ì˜ ê·¸ë˜í”„ë¥¼ ì„ íƒí•  ê²ƒ.\nì´ëŠ” seed.nos[921]ì—ì„œ ìƒì„±ëœ ë„¤íŠ¸ì›Œí¬ê°€ ì„ íƒë˜ê²Œ ë©ë‹ˆë‹¤.\n\n\n%%R\nnet921 <- seedToNet(seed.no = seed.nos[921], nnodes = 35, graph.prob = 0.15)\nlayout(matrix(c(1), 1, 1))\nplot(net921, vertex.label = colnames(gdpVTS), vertex.size = 0)\n\n\n\n\nRandomly generated un-weighted and un-directed graph over the OECD ountries that minimises the prediction error at t = 51 using GNAR(2, [2, 2]).\n\nseed.nos[921]ì—ì„œ ìƒì„±ëœ ë„¤íŠ¸ì›Œí¬\në„¤íŠ¸ì›Œí¬ì—ëŠ” ì „ë¶€ 2ê°œ ì´ìƒì˜ ì´ì›ƒì„ ê°€ì§€ê³  ìˆëŠ” countriesë“¤ì´ ìˆê³ , ì´ 97ê°œì˜ edgesì´ ìˆìŒ.\nì´ â€œ921â€ ë„¤íŠ¸ì›Œí¬ëŠ” GDP ì˜ˆì¸¡ì„ ìœ„í•´ ìƒì„±ë˜ì—ˆê¸° ë•Œë¬¸ì—, ì°¾ì€ ë„¤íŠ¸ì›Œí¬ì— ì¸ì‹ ê°€ëŠ¥í•œ êµ¬ì¡°ê°€ ìˆì§€ ì•Šì„ ê²ƒì´ë¼ê³  ì˜ˆìƒí•  ìˆ˜ ìˆìŒ\nê·¸ëŸ¬ë‚˜ ë¯¸êµ­, ë©•ì‹œì½”, ìºë‚˜ë‹¤ëŠ” ê°ê° 8ê°œ, 8ê°œ, 6ê°œì˜ edgeì„ ê°€ì§€ê³  ìˆì–´ ë§¤ìš° ì˜ ì—°ê²°ë˜ì–´ ìˆìŒ.\nìŠ¤ì›¨ë´ê³¼ ì¹ ë ˆë„ ì˜ ì—°ê²°ë˜ì–´ ìˆìœ¼ë©°, ê°ê° 8ê°œì™€ 7ê°œì˜ edgeì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\nì˜ˆì¸¡ ì„±ëŠ¥ì´ ìœ ì‚¬í•œ ì ì€ ê°œìˆ˜ì˜ edgeë¥¼ ê°€ì§„ ë„¤íŠ¸ì›Œí¬ë¥¼ ì°¾ê¸° ìœ„í•´ í…ŒìŠ¤íŠ¸ ë  ìˆ˜ ìˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì „ì²´ ì„ íƒëœ ë„¤íŠ¸ì›Œí¬ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©.\nì´ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ë©´ BICë¥¼ ì´ìš©í•´ ìµœì ì˜ GNAR ìˆœì„œë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŒ.\n\n\n%%R\nres <- rep(NA, 8)\nfor(i in 1:8){\n    res[i] <- BIC(GNARfit(gdpVTSn[1:50, ],\n                          net = seedToNet(seed.nos[921], nnodes = 35, graph.prob = 0.15),\n                          alphaOrder = alphas[i], betaOrder = betas[[i]]))}\norder(res)\n\n[1] 6 3 4 7 8 5 1 2\n\n\n\n%%R\nsort(res)\n\n[1] -64.44811 -64.32155 -64.18751 -64.12683 -64.09656 -63.86919 -60.67858\n[8] -60.54207"
  },
  {
    "objectID": "posts/GCN/2023-01-05-GNAR.html#results-and-comparisons",
    "href": "posts/GCN/2023-01-05-GNAR.html#results-and-comparisons",
    "title": "GNAR data",
    "section": "4.2. Results and comparisons",
    "text": "4.2. Results and comparisons\n\nì´ì „ ì„¹ì…˜ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•´ t=52ì¼ ë•Œì˜ ê°’ì„ ì˜ˆì¸¡\nì´ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ í‘œì¤€ ARê³¼ VAR ëª¨ë¸ì„ ì‚¬ìš©í•´ ì°¾ì€ ì˜ˆì¸¡ ì˜¤ë¥˜ì™€ ë¹„êµ\nGNAR ì˜ˆì¸¡ì€ ì„ íƒëœ ë„¤íŠ¸ì›Œí¬(seed.nos[921]ì— í•´ë‹¹)ë¥¼ ê°€ì§„ GNAR(2, [2, 0]) ëª¨ë¸ì„ t=51ê¹Œì§€ì˜ ë°ì´í„°ì— ì í•©ì‹œí‚¤ê³ , t=52ì¼ ë•Œì˜ ê°’ì„ ì˜ˆì¸¡\nìš°ì„  seriesë¥¼ ì •ê·œí™”í•œ ë‹¤ìŒ, ëª¨ë¸ ì í•©ìœ¼ë¡œë¶€í„° SSEë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n\n\n%%R\ngdpVTSn2 <- apply(gdpVTS, 2, function(x){x / sd(x[1:51], na.rm = TRUE)})\ngdpFit <- GNARfit(gdpVTSn2[1:51,], net = net921, alphaOrder = 2, betaOrder = c(2, 0))\nsummary(gdpFit)\n\n\nCall:\nlm(formula = yvec2 ~ dmat2 + 0)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4806 -0.5491 -0.0121  0.5013  3.1208 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \ndmat2alpha1  -0.41693    0.03154 -13.221  < 2e-16 ***\ndmat2beta1.1 -0.12662    0.05464  -2.317   0.0206 *  \ndmat2beta1.2  0.28044    0.06233   4.500  7.4e-06 ***\ndmat2alpha2  -0.33282    0.02548 -13.064  < 2e-16 ***\n---\nSignif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1\n\nResidual standard error: 0.8926 on 1332 degrees of freedom\n  (23 observations deleted due to missingness)\nMultiple R-squared:  0.1859,    Adjusted R-squared:  0.1834 \nF-statistic: 76.02 on 4 and 1332 DF,  p-value: < 2.2e-16\n\nGNAR BIC: -62.86003\n\n\n\n%%R\nsum((predict(gdpFit) - gdpVTSn2[52, ])^2)\n\n[1] 5.737203\n\n\nì´ GNAR ëª¨ë¸ì˜ ì í•©ëœ ë§¤ê°œë³€ìˆ˜ëŠ” \\(\\alpha^1 = - 0.42, \\beta^1,1 = - 0.13, \\beta^1,2 = 0.28\\), ê·¸ë¦¬ê³  \\(\\alpha^2 = - 0.33\\)ì…ë‹ˆë‹¤.\n\n\n\nModel\nparameters\nprediction error\n\n\n\n\nGNAR(2,[2,0])\n4\n5.7\n\n\nIndividual AR(2)\n38\n8.1\n\n\nVAR(1)\n199\n26.2\n\n\n\nEstimated prediction error of differenced real GDP change at t = 52 for all 35 countries.\nìš°ë¦¬ì˜ ë°©ë²•ê³¼ CRAN forecast íŒ¨í‚¤ì§€ì˜ ë²„ì „ 8.0ì—ì„œì˜ forecast.ar()ê³¼ auto.arima() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ê° ë…¸ë“œë³„ë¡œ AR ëª¨ë¸ì„ ì í•©í•œ ê²°ê³¼ë¥¼ ë¹„êµ\n\nì„¹ì…˜ 4.1ì˜ ìê¸°ìƒê´€ ë¶„ì„ì„ ê³ ë ¤í•´ ê°ê° 35ê°œì˜ ê°œë³„ ëª¨ë¸ì˜ ìµœëŒ€ AR ìˆœì„œë¥¼ p=2ë¡œ ì„¤ì •\n\n\n%%R\nlibrary(\"forecast\")\narforecast <- apply(gdpVTSn2[1:51, ], 2, function(x){\n            forecast(auto.arima(x[!is.na(x)], d= ,D=0,max.p = 2,max.q=0,\n                                max.P=0,max.Q = 0,stationary = TRUE, seasonal = FALSE), ic = \"bic\",\n                     allowmean = FALSE, allowdraft = FALSE, trace = FALSE, h=1)$mean\n})\nsum((arforecast - gdpVTSn2[52, ])^2)\n\nR[write to console]: Registered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n\n[1] 7.8974\n\n\nWe fit the model using the VAR function and then use the restrict function to reduce dimensionality further, by setting to zero any coefficient whose associated absolute t-statistic value is less than two.\n\n%%R\nlibrary(\"vars\")\ngdpVTSn2.0 <- gdpVTSn2\ngdpVTSn2.0[is.na(gdpVTSn2.0)] <- 0\nvarforecast <- predict(restrict(VAR(gdpVTSn2.0[1:51, ], p = 1, type = \"none\")), n.ahead = 1)\n\ncompute the prediction error\n\n%%R\ngetfcst <- function(x){return(x[1])}\nvarforecastpt <- unlist(lapply(varforecast$fcst, getfcst))\nsum((varforecastpt - gdpVTSn2.0[52, ])^2)\n\n[1] 26.19805\n\n\nGNAR ëª¨ë¸ì€ ARê³¼ VAR ê²°ê³¼ë³´ë‹¤ ì ì€ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” ARê³¼ ë¹„êµí–ˆì„ ë•Œ 29%ê°€ ì¤„ì–´ë“¤ê³ , VARê³¼ ë¹„êµí–ˆì„ ë•Œ 78%ê°€ ì¤„ì–´ë“­ë‹ˆë‹¤.\nìœ„ ì ˆì°¨ë¥¼ ë°˜ë³µí•´ 2ë‹¨ê³„ ì•ìœ¼ë¡œì˜ ì˜ˆì¸¡ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\nì´ ê²½ìš° ë‹¤ë¥¸ ë„¤íŠ¸ì›Œí¬ê°€ GNAR(2,[2,2]) ëª¨ë¸ì˜ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤.\nê·¸ëŸ¬ë‚˜ BIC ë‹¨ê³„ì—ì„œ GNAR(2,[0,0]) ëª¨ë¸ì´ ìµœì ìœ¼ë¡œ ì í•©ëœ ê²ƒì„ ì‹ë³„í•˜ì˜€ê³ , ì´ëŠ” ë„¤íŠ¸ì›Œí¬ íšŒê·€ ë§¤ê°œë³€ìˆ˜ë¥¼ í¬í•¨í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.\n\n%%R\nlibrary(\"vars\")\ngdpVTSn2.0 <- gdpVTSn2\ngdpVTSn2.0[is.na(gdpVTSn2.0)] <- 0\nvarforecast <- predict(restrict(VAR(gdpVTSn2.0[1:51, ], p = 1, type = \"none\")), n.ahead = 40)"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html",
    "href": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html",
    "title": "Class of Method(GNAR) lag 1",
    "section": "",
    "text": "GNAR fiveNet,fivenodes"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html#ì‹œë‚˜ë¦¬ì˜¤1-baseline",
    "href": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html#ì‹œë‚˜ë¦¬ì˜¤1-baseline",
    "title": "Class of Method(GNAR) lag 1",
    "section": "ì‹œë‚˜ë¦¬ì˜¤1 (Baseline)",
    "text": "ì‹œë‚˜ë¦¬ì˜¤1 (Baseline)\nì‹œë‚˜ë¦¬ì˜¤1\n\nmissing rate: 0%\në³´ê°„ë°©ë²•: None\n\n\nSTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:34<00:00,  1.43it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nGNAR ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n-\n\n%load_ext rpy2.ipython\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython\n\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\nlibrary(tidyverse)\n\n\n%R -i fiveVTS_train\n\n\n%%R\nanswer <- GNARfit(vts = fiveVTS_train, net = fiveNet, alphaOrder = 1, betaOrder = c(1))\nprediction <- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((fiveVTS_test - gnar_test.reshape(-1,5))**2).mean(axis=0)\ntest_mse_total_gnar = ((fiveVTS_test - gnar_test.reshape(-1,5))**2).mean()\n\n\n\nê²°ê³¼ì‹œê°í™”\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.9994323113693153, 1.2692101967317866)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n mse(train) = {3:.2f}, mse(test) = {4:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(range(1,160),stgcn_train[:,i],label='STCGCN (train)',color='C0')\n    a.plot(range(160,199),stgcn_test[:,i],label='STCGCN (test)',color='C0')\n    a.plot(range(1,160),gnar_train.reshape(-1,5)[:,i],label='GNAR (train)',color='C1')\n    a.plot(range(161,201),gnar_test.reshape(-1,5)[:,i],label='GNAR (test)',color='C1')\n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario1: STGCN \\n missing=0% \\n interpolation=None \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n GNAR: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html#ì‹œë‚˜ë¦¬ì˜¤2",
    "href": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html#ì‹œë‚˜ë¦¬ì˜¤2",
    "title": "Class of Method(GNAR) lag 1",
    "section": "ì‹œë‚˜ë¦¬ì˜¤2",
    "text": "ì‹œë‚˜ë¦¬ì˜¤2\nì‹œë‚˜ë¦¬ì˜¤2\n\nmissing rate: 50%\në³´ê°„ë°©ë²•: linear\n\n- ê²°ì¸¡ì¹˜ìƒì„± + ë³´ê°„\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.5)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:35<00:00,  1.41it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:37<00:00,  1.33it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n-\n\nX_train1 = np.array(X).squeeze()\nX_test1 =  np.array(XX).squeeze()\n\n\n%R -i X_train1\n\n\n%%R\nanswer <- GNARfit(vts = X_train1, net = fiveNet, alphaOrder = 1, betaOrder = c(1))\nprediction <- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((X_test1 - gnar_test[1:,:])**2).mean(axis=0)\ntest_mse_total_gnar = ((X_test1 - gnar_test[1:,:])**2).mean()\n\n\n\nê²°ê³¼ì‹œê°í™”\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.7473098322871093, 1.3231643342748722)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(160,199),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(161,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(1,159),gnar_train[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(161,201),gnar_test[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html#ì‹œë‚˜ë¦¬ì˜¤3",
    "href": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html#ì‹œë‚˜ë¦¬ì˜¤3",
    "title": "Class of Method(GNAR) lag 1",
    "section": "ì‹œë‚˜ë¦¬ì˜¤3",
    "text": "ì‹œë‚˜ë¦¬ì˜¤3\nì‹œë‚˜ë¦¬ì˜¤3\n\nmissing rate: 80%\në³´ê°„ë°©ë²•: linear\n\n- ê²°ì¸¡ì¹˜ìƒì„± + ë³´ê°„\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:35<00:00,  1.39it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:37<00:00,  1.33it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n-\n\nX_train1 = np.array(X).squeeze()\nX_test1 =  np.array(XX).squeeze()\n\n\n%R -i X_train1\n\n\n%%R\nanswer <- GNARfit(vts = X_train1, net = fiveNet, alphaOrder = 1, betaOrder = c(1))\nprediction <- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((X_test1 - gnar_test[1:,:])**2).mean(axis=0)\ntest_mse_total_gnar = ((X_test1 - gnar_test[1:,:])**2).mean()\n\n\n\nê²°ê³¼ì‹œê°í™”\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.38358787816283946, 1.3239931193379793)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(160,199),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(161,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(1,159),gnar_train[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(161,201),gnar_test[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario3: \\n missing=80% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html#ì‹œë‚˜ë¦¬ì˜¤4",
    "href": "posts/GCN/2023-01-26-ESTGCN_GNAR_DATA.html#ì‹œë‚˜ë¦¬ì˜¤4",
    "title": "Class of Method(GNAR) lag 1",
    "section": "ì‹œë‚˜ë¦¬ì˜¤4",
    "text": "ì‹œë‚˜ë¦¬ì˜¤4\nì‹œë‚˜ë¦¬ì˜¤4\n\nmissing rate: 30%\në³´ê°„ë°©ë²•: linear\n\n- ê²°ì¸¡ì¹˜ìƒì„± + ë³´ê°„\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.3)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:27<00:00,  1.85it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:27<00:00,  1.79it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n-\n\nX_train1 = np.array(X).squeeze()\nX_test1 =  np.array(XX).squeeze()\n\n\n%R -i X_train1\n\n\n%%R\nanswer <- GNARfit(vts = X_train1, net = fiveNet, alphaOrder = 1, betaOrder = c(1))\nprediction <- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((X_test1 - gnar_test[1:,:])**2).mean(axis=0)\ntest_mse_total_gnar = ((X_test1 - gnar_test[1:,:])**2).mean()\n\n\n\nê²°ê³¼ì‹œê°í™”\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.7978462123549198, 1.3146463350699074)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(160,199),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(161,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(1,159),gnar_train[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(160,200),gnar_test[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario3: \\n missing=80% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html",
    "title": "ESTGCN Comparison Table",
    "section": "",
    "text": "all dataset for method"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#stgcn-randomly-missing-by-missing-rate",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#stgcn-randomly-missing-by-missing-rate",
    "title": "ESTGCN Comparison Table",
    "section": "STGCN randomly missing by Missing rate",
    "text": "STGCN randomly missing by Missing rate\n\ngnar_stgcn_1 = load_data('./data/GNAR_stgcn_randomly_by_rate.pkl')\n\n\ngnar_stgcn_1.plot.box(backend='plotly',x='missingrate',color='method',y='MSE_test')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#estgcn-randomly-missing-by-missing-rate",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#estgcn-randomly-missing-by-missing-rate",
    "title": "ESTGCN Comparison Table",
    "section": "ESTGCN randomly missing by Missing rate",
    "text": "ESTGCN randomly missing by Missing rate\n\ngnar_estgcn_1 = load_data('./data/GNAR_estgcn_randomly_by_rate.pkl')\n\n\ngnar_estgcn_1.plot.box(backend='plotly',x='missingrate',color='method',y='MSE_test')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#gna-randomly-missing-by-missing-rate",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#gna-randomly-missing-by-missing-rate",
    "title": "ESTGCN Comparison Table",
    "section": "GNA randomly missing by Missing rate",
    "text": "GNA randomly missing by Missing rate\n\ngnar_gnar_1 = load_data('./data/GANR_gnar_randomly_by_rate.pkl')\n\n\ngnar_gnar_1.plot.box(backend='plotly',x='missingrate',color='method',y='MSE_test')\n\n\n                                                \n\n\n\ndf= pd.concat([gnar_stgcn_1,gnar_estgcn_1,gnar_gnar_1]).reset_index()\n\n\ndf.plot.box(backend='plotly',x='missingrate',color='method',y='MSE_test')\n\n\n                                                \n\n\n\ndf.query(\"method!='gnar'\").plot.box(backend='plotly',x='missingrate',color='method',y='MSE_test')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#stgcn-block-missing",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#stgcn-block-missing",
    "title": "ESTGCN Comparison Table",
    "section": "STGCN block missing",
    "text": "STGCN block missing\n\ngnar_stgcn_2 = load_data('./data/GNAR_stgcn_block_node2.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#estgcn-block-missing",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#estgcn-block-missing",
    "title": "ESTGCN Comparison Table",
    "section": "ESTGCN block missing",
    "text": "ESTGCN block missing\n\ngnar_estgcn_2 = load_data('./data/GNAR_DATA_estgcn_block_node2.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#gnar-block-missing",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#gnar-block-missing",
    "title": "ESTGCN Comparison Table",
    "section": "GNAR block missing",
    "text": "GNAR block missing\n\ngnar_gnar_2 = load_data('./data/GNAR_DATA_gnar_block_node2.pkl')\n\n\ndf= pd.concat([gnar_e\\stgcn_2,gnar_estgcn_2,gnar_gnar_2]).reset_index()\n\n\ndf.plot.box(backend='plotly',x='missingrate',color='method',y='MSE_test')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#stgcn-by-filter",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#stgcn-by-filter",
    "title": "ESTGCN Comparison Table",
    "section": "STGCN by filter",
    "text": "STGCN by filter\n\ngnar_stgcn_3 = load_data('./data/GNAR_stgcn_randomly_by_filter_30.pkl')\n\n\ngnar_stgcn_3.plot.box(backend='plotly',x='number_of_filters',color='method',y='MSE_test')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#estgcn-by-filter",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#estgcn-by-filter",
    "title": "ESTGCN Comparison Table",
    "section": "ESTGCN by filter",
    "text": "ESTGCN by filter\n\ngnar_estgcn_3 = load_data('./data/GNAR_estgcn_randomly_by_filter_30.pkl')\n\n\ngnar_estgcn_3.plot.box(backend='plotly',x='number_of_filters',color='method',y='MSE_test')\n\n\n                                                \n\n\n\ndf= pd.concat([gnar_stgcn_3,gnar_estgcn_3]).reset_index()\n\n\ndf.plot.box(backend='plotly',x='number_of_filters',color='method',y='MSE_test')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#gnar-randomly-missing-forecast-updating-by-1",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#gnar-randomly-missing-forecast-updating-by-1",
    "title": "ESTGCN Comparison Table",
    "section": "GNAR randomly missing forecast updating by 1",
    "text": "GNAR randomly missing forecast updating by 1\n\ngnar_gnar_3 = load_data('./data/GANR_gnar_one_randomly_by_rate.pkl')\n\n\ngnar_gnar_3.plot.box(backend='plotly',x='missingrate',color='method',y='MSE_test')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#stgcn-missing-randomly-by-missing-rate",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#stgcn-missing-randomly-by-missing-rate",
    "title": "ESTGCN Comparison Table",
    "section": "STGCN missing randomly by Missing rate",
    "text": "STGCN missing randomly by Missing rate\n\npedal_stgcn_1 = load_data('./data/Pedal_stgcn_randomly_by_rate.pkl')\n\n\npedal_stgcn_1.plot.box(backend='plotly',x='missingrate',y='MSE_test')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#estgcn-missing-randomly-by-missing-rate",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#estgcn-missing-randomly-by-missing-rate",
    "title": "ESTGCN Comparison Table",
    "section": "ESTGCN missing randomly by Missing rate",
    "text": "ESTGCN missing randomly by Missing rate\n\npedal_estgcn_1 = load_data('./data/Pedal_estgcn_randomly_by_rate.pkl')\n\n\npedal_estgcn_1.plot.box(backend='plotly',x='missingrate',y='MSE_test')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#gnar-missing-randomly-by-missing-rate",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#gnar-missing-randomly-by-missing-rate",
    "title": "ESTGCN Comparison Table",
    "section": "GNAR missing randomly by Missing rate",
    "text": "GNAR missing randomly by Missing rate\n\npedal_gnar_1 = load_data('./data/Pedal_gnar_randomly_by_rate.pkl')\n\n\npedal_gnar_1.plot.box(backend='plotly',x='missingrate',y='MSE_test')\n\n\n                                                \n\n\n\ndf= pd.concat([pedal_stgcn_1,pedal_estgcn_1,pedal_gnar_1]).reset_index()\n\n\ndf.plot.box(backend='plotly',x='missingrate',color='method',y='MSE_test')\n\n\n                                                \n\n\n\ndf.query(\"method!='gnar'\").plot.box(backend='plotly',x='missingrate',color='method',y='MSE_test')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#stgcn-missing-randomly-by-missing-rate-1",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#stgcn-missing-randomly-by-missing-rate-1",
    "title": "ESTGCN Comparison Table",
    "section": "STGCN missing randomly by Missing rate",
    "text": "STGCN missing randomly by Missing rate\n\nEnglandCovid_stgcn_1 = load_data('./data/EnglandCovid_stgcn_randomly_by_rate.pkl')\n\n\nEnglandCovid_stgcn_1.plot.box(backend='plotly',x='missingrate',color='method',y='MSE_test')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#estgcn-missing-randomly-by-missing-rate-1",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#estgcn-missing-randomly-by-missing-rate-1",
    "title": "ESTGCN Comparison Table",
    "section": "ESTGCN missing randomly by Missing rate",
    "text": "ESTGCN missing randomly by Missing rate\n\nEnglandCovid_estgcn_1 = load_data('./data/EnglandCovid_estgcn_randomly_by_rate.pkl')\n\n\nEnglandCovid_estgcn_1.plot.box(backend='plotly',x='missingrate',color='method',y='MSE_test')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#gnar-missing-randomly-by-missing-rate-1",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#gnar-missing-randomly-by-missing-rate-1",
    "title": "ESTGCN Comparison Table",
    "section": "GNAR missing randomly by Missing rate",
    "text": "GNAR missing randomly by Missing rate\n\nEnglandCovid_gnar_1 = load_data('./data/EnglandCovid_gnar_randomly_by_rate.pkl')\n\n\nEnglandCovid_gnar_1.plot.box(backend='plotly',x='missingrate',color='method',y='MSE_test')\n\n\n                                                \n\n\n\ndf= pd.concat([EnglandCovid_stgcn_1,EnglandCovid_estgcn_1,EnglandCovid_gnar_1]).reset_index()\n\n\ndf.plot.box(backend='plotly',x='missingrate',color='method',y='MSE_test')\n\n\n                                                \n\n\n\ndf.query(\"method!='gnar'\").plot.box(backend='plotly',x='missingrate',color='method',y='MSE_test')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#stgcn-missing-randomly-by-missing-rate-2",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#stgcn-missing-randomly-by-missing-rate-2",
    "title": "ESTGCN Comparison Table",
    "section": "STGCN missing randomly by Missing rate",
    "text": "STGCN missing randomly by Missing rate\n\nMontevideoBus_stgcn_1 = load_data('./data/MontevideoBus_stgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#estgcn-missing-randomly-by-missing-rate-2",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#estgcn-missing-randomly-by-missing-rate-2",
    "title": "ESTGCN Comparison Table",
    "section": "ESTGCN missing randomly by Missing rate",
    "text": "ESTGCN missing randomly by Missing rate\n\nMontevideoBus_estgcn_1 = load_data('./data/MontevideoBus_estgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#gnar-missing-randomly-by-missing-rate-2",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#gnar-missing-randomly-by-missing-rate-2",
    "title": "ESTGCN Comparison Table",
    "section": "GNAR missing randomly by Missing rate",
    "text": "GNAR missing randomly by Missing rate\n\nMontevideoBus_gnar_1 = load_data('./data/MontevideoBus_gnar_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#stgcn-missing-randomly-by-missing-rate-3",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#stgcn-missing-randomly-by-missing-rate-3",
    "title": "ESTGCN Comparison Table",
    "section": "STGCN missing randomly by Missing rate",
    "text": "STGCN missing randomly by Missing rate\n\nwiki_stgcn_1 = load_data('./data/Wiki_stgcn_randomly_by_rate.pkl')\n\n\nwiki_stgcn_1.plot.box(backend='plotly',x='missingrate',color='method',y='MSE_test')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#estgcn-missing-randomly-by-missing-rate-3",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#estgcn-missing-randomly-by-missing-rate-3",
    "title": "ESTGCN Comparison Table",
    "section": "ESTGCN missing randomly by Missing rate",
    "text": "ESTGCN missing randomly by Missing rate\n\nwiki_estgcn_1 = load_data('./data/Wiki_estgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#gnar-missing-randomly-by-missing-rate-3",
    "href": "posts/GCN/2023-02-15-ESTGCN_DATASET.html#gnar-missing-randomly-by-missing-rate-3",
    "title": "ESTGCN Comparison Table",
    "section": "GNAR missing randomly by Missing rate",
    "text": "GNAR missing randomly by Missing rate\n\nwiki_gnar_1 = load_data('./data/Wiki_gnar_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "",
    "text": "Try to divide train and test(GNAR fivenet)"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "1) ST-GCN",
    "text": "1) ST-GCN\n\nmean_f_fiveVTS_train = torch.tensor(fiveVTS_train_mean).reshape(160,5,1).float()\n\n\nmean_X_fiveVTS = mean_f_fiveVTS_train[:159,:,:]\nmean_y_fiveVTS = mean_f_fiveVTS_train[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(mean_X_fiveVTS,mean_y_fiveVTS)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:27<00:00,  1.84it/s]\n\n\n\nmean_fhat_fiveVTS = torch.stack([model(xt, edge_index, edge_attr) for xt in mean_X_fiveVTS]).detach().numpy()\n\n\nxt_test = torch.tensor(fiveVTS_test.reshape(40,5,1)[:-1,:,:]).float()\n\n\nmean_fhat_fiveVTS_forecast = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],mean_fhat_fiveVTS_forecast);\n\n\n\n\n\nvis2(fiveVTS_train_mean,mean_fhat_fiveVTS);"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "2) Fourier transform",
    "text": "2) Fourier transform\n\nw=np.zeros((159*N,159*N))\n\n\nfor i in range(159*N):\n    for j in range(159*N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\n# np.fft(mean_fhat_fiveVTS[:,0,0])\n\n\n# mean_fhat_fiveVTS.shape\n\n\n# fft_result =np.stack([np.fft.fft(mean_fhat_fiveVTS[:,n,0]) for n in range(N)]).T\n\n\n# plt.plot(abs(fft_result[:,0])**2)\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\n\n\nfhatbar = Psi.T @ mean_fhat_fiveVTS.reshape(159*N,1)"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "3) Ebayes",
    "text": "3) Ebayes\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\nfhatbar_threshed = ebayesthresh(FloatVector(fhatbar))\n\n\nplt.plot(fhatbar)\nplt.plot(fhatbar_threshed)"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "4) Inverse Fourier transform",
    "text": "4) Inverse Fourier transform\n\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_mean_spatio_temporal = fhatbarhat.reshape(159,N,1)\n\n\nvis2(mean_fhat_fiveVTS,fhatbarhat_mean_spatio_temporal.reshape(159,5));"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-1",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-1",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "5) ST-GCN",
    "text": "5) ST-GCN\n\nfiveVTS_train_mean[seed_number1,0] = fhatbarhat_mean_spatio_temporal[seed_number1,0,0]\nfiveVTS_train_mean[seed_number2,1] = fhatbarhat_mean_spatio_temporal[seed_number2,1,0]\nfiveVTS_train_mean[seed_number3,2] = fhatbarhat_mean_spatio_temporal[seed_number3,2,0]\nfiveVTS_train_mean[seed_number4,3] = fhatbarhat_mean_spatio_temporal[seed_number4,3,0]\nfiveVTS_train_mean[seed_number5,4] = fhatbarhat_mean_spatio_temporal[seed_number5,4,0]\nvis(fiveVTS_train_mean);\n\n\n\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(mean_X_fiveVTS,mean_y_fiveVTS)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:26<00:00,  1.88it/s]\n\n\n\nmean_fhat_spatio_temporal = torch.stack([model(xt, edge_index, edge_attr) for xt in mean_X_fiveVTS]).detach().numpy()\n\n\nmean_fhat_spatio_temporal_test = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],mean_fhat_spatio_temporal_test);\n\n\n\n\n\nvis2(fhatbarhat_mean_spatio_temporal,mean_fhat_spatio_temporal);\n\n\n\n\n\n\nfor i in tqdm(range(50)):\n    ## GFT \n    fhatbar = Psi.T @ mean_fhat_fiveVTS.reshape(159*N,1)\n\n    ## Ebayes\n    ebayesthresh = importr('EbayesThresh').ebayesthresh\n    fhatbar_threshed = ebayesthresh(FloatVector(fhatbar))\n    #plt.plot(fhatbar)\n    #plt.plot(fhatbar_threshed)\n\n    ## inverse GFT \n    fhatbarhat = Psi @ fhatbar_threshed\n    fhatbarhat_mean_spatio_temporal = fhatbarhat.reshape(159,N,1)\n    #vis2(mean_fhat_fiveVTS,fhatbarhat_mean_spatio_temporal.reshape(159,5));\n\n    ## STGCN \n    fiveVTS_train_mean[seed_number1,0] = fhatbarhat_mean_spatio_temporal[seed_number1,0,0]\n    fiveVTS_train_mean[seed_number2,1] = fhatbarhat_mean_spatio_temporal[seed_number1,1,0]\n    fiveVTS_train_mean[seed_number3,2] = fhatbarhat_mean_spatio_temporal[seed_number1,2,0]\n    fiveVTS_train_mean[seed_number4,3] = fhatbarhat_mean_spatio_temporal[seed_number1,3,0]\n    fiveVTS_train_mean[seed_number5,4] = fhatbarhat_mean_spatio_temporal[seed_number1,4,0]\n    #vis(fiveVTS_train_mean);\n\n    #model = RecurrentGCN(node_features=1, filters=4)\n\n    #optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n    #model.train()\n    for epoch in range(1):\n        for time, (xt,yt) in enumerate(zip(mean_X_fiveVTS,mean_y_fiveVTS)):\n            y_hat = model(xt, edge_index, edge_attr)\n            cost = torch.mean((y_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n    mean_fhat_spatio_temporal = torch.stack([model(xt, edge_index, edge_attr) for xt in mean_X_fiveVTS]).detach().numpy()\n    mean_fhat_spatio_temporal_test = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n    #vis2(fiveVTS_test[1:],mean_fhat_spatio_temporal_test);\n    #vis2(fiveVTS_train_backup,mean_fhat_spatio_temporal);\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:55<00:00,  1.10s/it]\n\n\n\nvis2(fiveVTS_train_backup,mean_fhat_spatio_temporal);\n\n\n\n\n\nvis2(fiveVTS_train_backup,mean_fhat_spatio_temporal);"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-1",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-1",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "6) Fourier transform",
    "text": "6) Fourier transform\n\nw=np.zeros((159*N,159*N))\n\n\nfor i in range(159*N):\n    for j in range(159*N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\n\n\nfhatbar = Psi.T @ mean_fhat_spatio_temporal.reshape(159*N,1)\npower = fhatbar**2"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-1",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-1",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "7) Ebayes",
    "text": "7) Ebayes\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\nfhatbar_threshed = ebayesthresh(FloatVector(fhatbar))\n\n\nplt.plot(fhatbar)\n\n\n\n\n\nplt.plot(fhatbar)\nplt.plot(fhatbar_threshed)"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-1",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-1",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "8) Inverse Fourier transform",
    "text": "8) Inverse Fourier transform\n\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_mean_spatio_temporal2 = fhatbarhat.reshape(159,N,1)\n\n\nvis2(mean_fhat_spatio_temporal,fhatbarhat_mean_spatio_temporal2.reshape(159,5));"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-2",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-2",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "9) ST-GCN",
    "text": "9) ST-GCN\n\nfiveVTS_train_mean[seed_number1,0] = fhatbarhat_mean_spatio_temporal[seed_number1,0,0]\nfiveVTS_train_mean[seed_number2,1] = fhatbarhat_mean_spatio_temporal[seed_number2,1,0]\nfiveVTS_train_mean[seed_number3,2] = fhatbarhat_mean_spatio_temporal[seed_number3,2,0]\nfiveVTS_train_mean[seed_number4,3] = fhatbarhat_mean_spatio_temporal[seed_number4,3,0]\nfiveVTS_train_mean[seed_number5,4] = fhatbarhat_mean_spatio_temporal[seed_number5,4,0]\nvis(fiveVTS_train_mean);\n\n\n\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(mean_X_fiveVTS,mean_y_fiveVTS)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:27<00:00,  1.84it/s]\n\n\n\nmean_fhat_spatio_temporal2 = torch.stack([model(xt, edge_index, edge_attr) for xt in mean_X_fiveVTS]).detach().numpy()\n\n\nmean_fhat_spatio_temporal_test2 = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],mean_fhat_spatio_temporal_test2);\n\n\n\n\n\nvis2(fhatbarhat_mean_spatio_temporal2,mean_fhat_spatio_temporal2);"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-2",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-2",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "10) Fourier transform",
    "text": "10) Fourier transform\n\nw=np.zeros((159*N,159*N))\n\n\nfor i in range(159*N):\n    for j in range(159*N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\n\n\nfhatbar = Psi.T @ mean_fhat_spatio_temporal.reshape(159*N,1)\npower = fhatbar**2"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-2",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-2",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "11) Ebayes",
    "text": "11) Ebayes\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\nfhatbar_threshed = ebayesthresh(FloatVector(fhatbar))"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-2",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-2",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "12) Inverse Fourier transform",
    "text": "12) Inverse Fourier transform\n\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_mean_spatio_temporal3 = fhatbarhat.reshape(159,N,1)"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-3",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-3",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "13) ST-GCN",
    "text": "13) ST-GCN\n\nfiveVTS_train_mean[seed_number1,0] = fhatbarhat_mean_spatio_temporal[seed_number1,0,0]\nfiveVTS_train_mean[seed_number2,1] = fhatbarhat_mean_spatio_temporal[seed_number2,1,0]\nfiveVTS_train_mean[seed_number3,2] = fhatbarhat_mean_spatio_temporal[seed_number3,2,0]\nfiveVTS_train_mean[seed_number4,3] = fhatbarhat_mean_spatio_temporal[seed_number4,3,0]\nfiveVTS_train_mean[seed_number5,4] = fhatbarhat_mean_spatio_temporal[seed_number5,4,0]\nvis(fiveVTS_train_mean);\n\n\n\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(mean_X_fiveVTS,mean_y_fiveVTS)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:26<00:00,  1.86it/s]\n\n\n\nmean_fhat_spatio_temporal3 = torch.stack([model(xt, edge_index, edge_attr) for xt in mean_X_fiveVTS]).detach().numpy()\n\n\nmean_fhat_spatio_temporal_test3 = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],mean_fhat_spatio_temporal_test3);\n\n\n\n\n\nvis2(fhatbarhat_mean_spatio_temporal3,mean_fhat_spatio_temporal3);\n\n\n\n\n\none = []\nfor i in range(N):\n    one.append(np.mean((fiveVTS_test[1:,i] - mean_fhat_fiveVTS_forecast.reshape(39,5)[:,i])))\n\n\ntwo = []\nfor i in range(N):\n    two.append(np.mean((fiveVTS_test[1:,i] - mean_fhat_spatio_temporal_test.reshape(39,5)[:,i])))\n\n\nthree = []\nfor i in range(N):\n    three.append(np.mean((fiveVTS_test[1:,i] - mean_fhat_spatio_temporal_test2.reshape(39,5)[:,i])))\n\n\nfour = []\nfor i in range(N):\n    four.append(np.mean((fiveVTS_test[1:,i] - mean_fhat_spatio_temporal_test3.reshape(39,5)[:,i])))\n\n\npd.DataFrame({'one':one,'two':two,'three':three,'four':four})\n\n\n\n\n\n  \n    \n      \n      one\n      two\n      three\n      four\n    \n  \n  \n    \n      0\n      -0.196310\n      -0.189000\n      -0.173563\n      -0.200559\n    \n    \n      1\n      -0.161632\n      -0.135003\n      -0.142250\n      -0.159892\n    \n    \n      2\n      0.079347\n      0.106893\n      0.108179\n      0.079011\n    \n    \n      3\n      -0.267653\n      -0.244438\n      -0.248220\n      -0.269292\n    \n    \n      4\n      -0.162464\n      -0.135709\n      -0.130221\n      -0.167336"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-4",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-4",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "1) ST-GCN",
    "text": "1) ST-GCN\n\nlinear_f_fiveVTS_train = torch.tensor(linear_fiveVTS_train).reshape(160,5,1).float()\n\n\nlinear_X_fiveVTS = linear_f_fiveVTS_train[:159,:,:]\nlinear_y_fiveVTS = linear_f_fiveVTS_train[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(linear_X_fiveVTS,linear_y_fiveVTS)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nlinear_fhat_fiveVTS = torch.stack([model(xt, edge_index, edge_attr) for xt in linear_X_fiveVTS]).detach().numpy()\n\n\nxt_test = torch.tensor(fiveVTS_test.reshape(40,5,1)[:-1,:,:]).float()\n\n\nlinear_fhat_fiveVTS_forecast = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],linear_fhat_fiveVTS_forecast);\n\n\nvis2(linear_fiveVTS_train,linear_f_fiveVTS_train);"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-3",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-3",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "2) Fourier transform",
    "text": "2) Fourier transform\n\nw=np.zeros((159*N,159*N))\n\n\nfor i in range(159*N):\n    for j in range(159*N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\n\n\nfhatbar = Psi.T @ linear_fhat_fiveVTS.reshape(159*N,1)\npower = fhatbar**2"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-3",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-3",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "3) Ebayes",
    "text": "3) Ebayes\n\nplt.plot(fhatbar.reshape(159,5)[:,0]**2)\n\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\nfhatbar_threshed = ebayesthresh(FloatVector(fhatbar))\n\n\nplt.plot(fhatbar)\nplt.plot(fhatbar_threshed)"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-3",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-3",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "4) Inverse Fourier transform",
    "text": "4) Inverse Fourier transform\n\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_linear_spatio_temporal = fhatbarhat.reshape(159,N,1)\n\n\nvis2(linear_fhat_fiveVTS,fhatbarhat_linear_spatio_temporal.reshape(159,5));"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-5",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-5",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "5) ST-GCN",
    "text": "5) ST-GCN\n\nlinear_spatio_temporal = torch.tensor(fhatbarhat_linear_spatio_temporal).reshape(159,5,1).float()\n\n\nlinear_X_spatio_temporal = linear_spatio_temporal[:158,:,:]\nlinear_y_spatio_temporal = linear_spatio_temporal[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(linear_X_spatio_temporal,linear_y_spatio_temporal)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nlinear_fhat_spatio_temporal = torch.stack([model(xt, edge_index, edge_attr) for xt in linear_X_spatio_temporal]).detach().numpy()\n\n\nlinear_fhat_spatio_temporal_test = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],linear_fhat_spatio_temporal_test);\n\n\nvis2(fhatbarhat_linear_spatio_temporal,linear_fhat_spatio_temporal);"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-4",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-4",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "6) Fourier transform",
    "text": "6) Fourier transform\n\nw=np.zeros((158*N,158*N))\n\n\nfor i in range(158*N):\n    for j in range(158*N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\n\n\nfhatbar = Psi.T @ linear_fhat_spatio_temporal.reshape(158*N,1)\npower = fhatbar**2"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-4",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-4",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "7) Ebayes",
    "text": "7) Ebayes\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\nfhatbar_threshed = ebayesthresh(FloatVector(fhatbar))\n\n\nplt.plot(fhatbar)\nplt.plot(fhatbar_threshed)"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-4",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-4",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "8) Inverse Fourier transform",
    "text": "8) Inverse Fourier transform\n\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_linear_spatio_temporal2 = fhatbarhat.reshape(158,N,1)\n\n\nvis2(linear_fhat_spatio_temporal,fhatbarhat_linear_spatio_temporal2.reshape(158,5));"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-6",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-6",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "9) ST-GCN",
    "text": "9) ST-GCN\n\nlinear_spatio_temporal2 = torch.tensor(fhatbarhat_linear_spatio_temporal2).reshape(158,5,1).float()\n\n\nlinear_X_spatio_temporal2 = linear_spatio_temporal2[:157,:,:]\nlinear_y_spatio_temporal2 = linear_spatio_temporal2[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(linear_X_spatio_temporal2,linear_y_spatio_temporal2)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nlinear_fhat_spatio_temporal2 = torch.stack([model(xt, edge_index, edge_attr) for xt in linear_X_spatio_temporal2]).detach().numpy()\n\n\nlinear_fhat_spatio_temporal_test2 = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],linear_fhat_spatio_temporal_test2);\n\n\nvis2(fhatbarhat_linear_spatio_temporal2,linear_fhat_spatio_temporal2);\n\n\none = []\nfor i in range(N):\n    one.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_fiveVTS_forecast.reshape(39,5)[:,i])))\n\n\ntwo = []\nfor i in range(N):\n    two.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_spatio_temporal_test.reshape(39,5)[:,i])))\n\n\nthree = []\nfor i in range(N):\n    three.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_spatio_temporal_test2.reshape(39,5)[:,i])))\n\n\npd.DataFrame({'one':one,'two':two,'three':three})"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-5",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#fourier-transform-5",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "10) Fourier transform",
    "text": "10) Fourier transform\n\nw=np.zeros((157*N,157*N))\n\n\nfor i in range(157*N):\n    for j in range(157*N):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\n\n\nfhatbar = Psi.T @ linear_fhat_spatio_temporal2.reshape(157*N,1)"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-5",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#ebayes-5",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "11) Ebayes",
    "text": "11) Ebayes\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\nfhatbar_threshed = ebayesthresh(FloatVector(fhatbar))\n\n\nplt.plot(fhatbar)\nplt.plot(fhatbar_threshed)"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-5",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#inverse-fourier-transform-5",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "12) Inverse Fourier transform",
    "text": "12) Inverse Fourier transform\n\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_linear_spatio_temporal3 = fhatbarhat.reshape(157,N,1)\n\n\nvis2(linear_fhat_spatio_temporal2,fhatbarhat_linear_spatio_temporal3.reshape(157,5));"
  },
  {
    "objectID": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-7",
    "href": "posts/GCN/2023-01-20-Algorithm_traintest.html#st-gcn-7",
    "title": "1st ST-GCN Example dividing train and test",
    "section": "13) ST-GCN",
    "text": "13) ST-GCN\n\nlinear_spatio_temporal3 = torch.tensor(fhatbarhat_linear_spatio_temporal3).reshape(157,5,1).float()\n\n\nlinear_X_spatio_temporal3 = linear_spatio_temporal3[:156,:,:]\nlinear_y_spatio_temporal3 = linear_spatio_temporal3[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(linear_X_spatio_temporal3,linear_y_spatio_temporal3)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n\nlinear_fhat_spatio_temporal3 = torch.stack([model(xt, edge_index, edge_attr) for xt in linear_X_spatio_temporal3]).detach().numpy()\n\n\nlinear_fhat_spatio_temporal_test3 = torch.stack([model(xt, edge_index, edge_attr) for xt in xt_test]).detach().numpy()\n\n\nvis2(fiveVTS_test[1:],linear_fhat_spatio_temporal_test3);\n\n\nvis2(fhatbarhat_linear_spatio_temporal3,linear_fhat_spatio_temporal3);\n\n\none = []\nfor i in range(N):\n    one.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_fiveVTS_forecast.reshape(39,5)[:,i])))\n\n\ntwo = []\nfor i in range(N):\n    two.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_spatio_temporal_test.reshape(39,5)[:,i])))\n\n\nthree = []\nfor i in range(N):\n    three.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_spatio_temporal_test2.reshape(39,5)[:,i])))\n\n\nfour = []\nfor i in range(N):\n    four.append(np.mean((fiveVTS_test[1:,i] - linear_fhat_spatio_temporal_test3.reshape(39,5)[:,i])))\n\n\npd.DataFrame({'one':one,'two':two,'three':three,'four':four})"
  },
  {
    "objectID": "posts/GCN/2023-01-18-Algorithm_traintest_2.html",
    "href": "posts/GCN/2023-01-18-Algorithm_traintest_2.html",
    "title": "2nd ST-GCN Example dividing train and test",
    "section": "",
    "text": "Try to divide train and test(ST-GCN WikiMathsDatasetLoader)"
  },
  {
    "objectID": "posts/GCN/2023-01-18-Algorithm_traintest_2.html#train",
    "href": "posts/GCN/2023-01-18-Algorithm_traintest_2.html#train",
    "title": "2nd ST-GCN Example dividing train and test",
    "section": "Train",
    "text": "Train\n\ndata_train=[]\nfor time, snapshot in enumerate(train_dataset):\n    data_train.append([time,snapshot])\n\n\ndata_train[0][1].x.shape,data_train[0][1].y.shape,data_train[0][1].edge_index.shape,data_train[0][1].edge_attr.shape\n\n(torch.Size([1068, 1]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n583\n\n\n\nT_train = time\nN = len(data[0][1].x)\n\n\nedge_index = data_train[0][1].edge_index\nedge_attr = data_train[0][1].edge_attr\n\n\nx_train = []\nfor i in range(time):\n    x_train.append(data_train[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_train = data_tensor.reshape(time,1068,-1)\nx_train.shape\n\ntorch.Size([583, 1068, 1])\n\n\n\ny_train = []\nfor i in range(time):\n    y_train.append(data_train[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_train = data_tensor.reshape(time,1068)\ny_train.shape\n\ntorch.Size([583, 1068])\n\n\n\nx_train.shape, y_train.shape\n\n(torch.Size([583, 1068, 1]), torch.Size([583, 1068]))"
  },
  {
    "objectID": "posts/GCN/2023-01-18-Algorithm_traintest_2.html#test",
    "href": "posts/GCN/2023-01-18-Algorithm_traintest_2.html#test",
    "title": "2nd ST-GCN Example dividing train and test",
    "section": "Test",
    "text": "Test\n\ndata_test=[]\nfor time, snapshot in enumerate(test_dataset):\n    data_test.append([time,snapshot])\n\n\ndata_test[0][1].x.shape,data_test[0][1].y.shape,data_test[0][1].edge_index.shape,data_test[0][1].edge_attr.shape\n\n(torch.Size([1068, 1]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n145\n\n\n\nT_test = time\n\n\nx_test = []\nfor i in range(time):\n    x_test.append(data_test[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_test = data_tensor.reshape(time,1068,-1)\nx_test.shape\n\ntorch.Size([145, 1068, 1])\n\n\n\ny_test = []\nfor i in range(time):\n    y_test.append(data_test[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_test = data_tensor.reshape(time,1068)\ny_test.shape\n\ntorch.Size([145, 1068])\n\n\n\nx_test.shape, y_test.shape\n\n(torch.Size([145, 1068, 1]), torch.Size([145, 1068]))"
  },
  {
    "objectID": "posts/GCN/2023-01-18-Algorithm_traintest_2.html#st-gcn",
    "href": "posts/GCN/2023-01-18-Algorithm_traintest_2.html#st-gcn",
    "title": "2nd ST-GCN Example dividing train and test",
    "section": "ST-GCN",
    "text": "ST-GCN\n\nmean_f_train = x_train_mean.reshape(T_train,N,1).float()\n\n\nmean_X = mean_f_train[:438,:,:]\nmean_y = mean_f_train[145:,:,:]\n\n\nmean_X.shape,mean_y.shape\n\n(torch.Size([438, 1068, 1]), torch.Size([438, 1068, 1]))\n\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(mean_X,mean_y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [04:17<00:00,  5.15s/it]\n\n\n\nmean_X_fore = mean_f_train[438:,:]\n\n\nmean_fhat = torch.stack([model(xt, edge_index, edge_attr) for xt in mean_X_fore]).detach().numpy()\n\n\nmean_X_fore.shape,x_test.shape\n\n(torch.Size([145, 1068, 1]), torch.Size([145, 1068, 1]))"
  },
  {
    "objectID": "posts/GCN/2023-01-18-Algorithm_traintest_2.html#st-gcn-1",
    "href": "posts/GCN/2023-01-18-Algorithm_traintest_2.html#st-gcn-1",
    "title": "2nd ST-GCN Example dividing train and test",
    "section": "ST-GCN",
    "text": "ST-GCN\n\nlinear_f_train = x_train_linear.clone()\n\n\nlinear_X = linear_f_train[:438,:,:]\nlinear_y = linear_f_train[145:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(linear_X,linear_y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [04:20<00:00,  5.22s/it]\n\n\n\nlinear_X_fore = linear_f_train[438:,:]\n\n\nlinear_X_fore.shape\n\ntorch.Size([145, 1068, 1])\n\n\n\nlinear_fhat = torch.stack([model(xt, edge_index, edge_attr) for xt in linear_X_fore]).detach().numpy()\n\n\nlinear_X_fore.shape,x_test.shape\n\n(torch.Size([145, 1068, 1]), torch.Size([145, 1068, 1]))"
  },
  {
    "objectID": "posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.html",
    "href": "posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.html",
    "title": "EnglandCovidDatasetLoader lag 4 Randomly Missing comparison Table",
    "section": "",
    "text": "ST-GCN Dataset EnglandCovidDatasetLoader"
  },
  {
    "objectID": "posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.html#train",
    "href": "posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.html#train",
    "title": "EnglandCovidDatasetLoader lag 4 Randomly Missing comparison Table",
    "section": "Train",
    "text": "Train\n\ndata_train=[]\nfor time, snapshot in enumerate(train_dataset):\n    data_train.append([time,snapshot])\n\n\ndata_train[0][1].x.shape,data_train[0][1].y.shape,data_train[0][1].edge_index.shape,data_train[0][1].edge_attr.shape\n\n(torch.Size([129, 4]),\n torch.Size([129]),\n torch.Size([2, 2158]),\n torch.Size([2158]))\n\n\n\ntime\n\n44\n\n\n\nT_train = time\nN = len(data_train[0][1].x)\n\n\nedge_index = data_train[0][1].edge_index\nedge_attr = data_train[0][1].edge_attr\n\n\nx_train = []\nfor i in range(time):\n    x_train.append(data_train[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_train = data_tensor.reshape(time,N,-1)\nx_train.shape\n\ntorch.Size([44, 129, 4])\n\n\n\ny_train = []\nfor i in range(time):\n    y_train.append(data_train[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_train = data_tensor.reshape(time,N)\ny_train.shape\n\ntorch.Size([44, 129])\n\n\n\nx_train.shape, y_train.shape\n\n(torch.Size([44, 129, 4]), torch.Size([44, 129]))"
  },
  {
    "objectID": "posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.html#test",
    "href": "posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.html#test",
    "title": "EnglandCovidDatasetLoader lag 4 Randomly Missing comparison Table",
    "section": "Test",
    "text": "Test\n\ndata_test=[]\nfor time, snapshot in enumerate(test_dataset):\n    data_test.append([time,snapshot])\n\n\ndata_test[0][1].x.shape,data_test[0][1].y.shape,data_test[0][1].edge_index.shape,data_test[0][1].edge_attr.shape\n\n(torch.Size([129, 4]),\n torch.Size([129]),\n torch.Size([2, 1417]),\n torch.Size([1417]))\n\n\n\ntime\n\n11\n\n\n\nT_test = time\n\n\nx_test = []\nfor i in range(time):\n    x_test.append(data_test[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_test = data_tensor.reshape(time,N,-1)\nx_test.shape\n\ntorch.Size([11, 129, 4])\n\n\n\ny_test = []\nfor i in range(time):\n    y_test.append(data_test[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_test = data_tensor.reshape(time,N)\ny_test.shape\n\ntorch.Size([11, 129])\n\n\n\nx_test.shape, y_test.shape\n\n(torch.Size([11, 129, 4]), torch.Size([11, 129]))"
  },
  {
    "objectID": "posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.html#stgcn",
    "href": "posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.html#stgcn",
    "title": "EnglandCovidDatasetLoader lag 4 Randomly Missing comparison Table",
    "section": "STGCN",
    "text": "STGCN\n\nclass STGCN_Missing:\n    def __init__(self,Dataset, df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n        self.iterable = iterable\n    def iter(self):\n        self.XX = x_test\n        self.yy = y_test\n\n        self.real_y = y_train\n        for i in range(self.iterable):\n    \n            _zero = Missing(x_train_f)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(np.stack([interpolated_signal[j:(T_train+j),:] for j in range(self.lag)],axis = -1)).float()\n            y = torch.tensor(interpolated_signal[self.lag:,:]).float()\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            for epoch in range(50):\n                for time, (xt,yt) in enumerate(zip(X,y)):\n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_stgcn = (((self.real_y-yhat.squeeze()).squeeze())**2).mean()\n            test_mse_total_stgcn = (((self.yy-yyhat.squeeze()).squeeze())**2).mean()\n    \n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_stgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_stgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.html#enhencement-of-stgcn",
    "href": "posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.html#enhencement-of-stgcn",
    "title": "EnglandCovidDatasetLoader lag 4 Randomly Missing comparison Table",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nclass ESTGCN_Missing:\n    def __init__(self,Dataset, df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n        self.iterable = iterable\n    def iter(self):\n        self.XX = x_test\n        self.yy = y_test\n\n        self.real_y = y_train\n        for i in range(self.iterable):\n    \n            _zero = Missing(x_train_f)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            signal = interpolated_signal.copy()\n            for epoch in range(50):\n                signal = update_from_freq_domain(signal,missing_index)\n                X = torch.tensor(np.stack([signal[i:(T_train+epoch+i),:] for i in range(self.lag)],axis = -1)).reshape(-1,N,self.lag).float()\n                y = torch.tensor(signal).reshape(-1,N,1).float()[self.lag:,:,:]\n                for time, (xt,yt) in enumerate(zip(X,y)):        \n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().reshape(1,-1)]).squeeze()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_estgcn = (((self.real_y-yhat[:T_train,:].squeeze()).squeeze())**2).mean()\n            test_mse_total_estgcn = (((self.yy-yyhat.squeeze()).squeeze())**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_estgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_estgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.html#gnar",
    "href": "posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.html#gnar",
    "title": "EnglandCovidDatasetLoader lag 4 Randomly Missing comparison Table",
    "section": "GNAR",
    "text": "GNAR\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\n\nR[write to console]: Loading required package: igraph\n\nR[write to console]: \nAttaching package: â€˜igraphâ€™\n\n\nR[write to console]: The following objects are masked from â€˜package:statsâ€™:\n\n    decompose, spectrum\n\n\nR[write to console]: The following object is masked from â€˜package:baseâ€™:\n\n    union\n\n\nR[write to console]: Loading required package: wordcloud\n\nR[write to console]: Loading required package: RColorBrewer\n\n\n\n\nGNAR = importr('GNAR') # import GNAR \nigraph = importr('igraph') # import igraph \n\n\nw=np.zeros((129,129))\n\n\nfor k in range(2158):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\nm = robjects.r.matrix(FloatVector(w), nrow = 129, ncol = 129)\n\n\nclass GNAR_Missing:\n    def __init__(self,Dataset, df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n        self.iterable = iterable\n    def iter(self):\n        self.yy = torch.tensor(y_test).float()\n        for i in range(self.iterable):\n\n            _zero = Missing(x_train_f)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(np.stack([interpolated_signal[i:(T_train+i),:] for i in range(self.lag)],axis = -1)).float()\n\n            answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = T_train, ncol = N),net = GNAR.matrixtoGNAR(m), alphaOrder = 4, betaOrder = FloatVector([1, 1, 1, 1]))             \n            predict = GNAR.predict_GNARfit(answer,n_ahead=T_test)\n\n\n            train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,N))**2).mean()\n            test_mse_total_gnar = ((self.yy - pd.DataFrame(predict).values.reshape(-1,N))**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_gnar.tolist()\n            df_row['MSE_test'] = test_mse_total_gnar.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.html#stgcn-1",
    "href": "posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.html#stgcn-1",
    "title": "EnglandCovidDatasetLoader lag 4 Randomly Missing comparison Table",
    "section": "STGCN",
    "text": "STGCN\n\nDataset = 'EnglandCovid'\nMethod = 'stgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 4 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_stgcn= pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    stgcn = STGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    stgcn.iter()\n    df_add = stgcn.df.copy()\n    df_stgcn = pd.concat([df_stgcn,df_add],axis=0)\n\n\nsave_data(df_stgcn, './data/EnglandCovid_stgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.html#enhencement-of-stgcn-1",
    "href": "posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.html#enhencement-of-stgcn-1",
    "title": "EnglandCovidDatasetLoader lag 4 Randomly Missing comparison Table",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nDataset = 'EnglandCovid'\nMethod = 'estgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 4 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_estgcn = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    estgcn = ESTGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    estgcn.iter()\n    df_add = estgcn.df.copy()\n    df_estgcn = pd.concat([df_estgcn,df_add],axis=0)\n\n\nsave_data(df_estgcn, './data/EnglandCovid_estgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.html#gnar-1",
    "href": "posts/GCN/2023-02-16-ESTGCN_EnglandCovid_DATA_1.html#gnar-1",
    "title": "EnglandCovidDatasetLoader lag 4 Randomly Missing comparison Table",
    "section": "GNAR",
    "text": "GNAR\n\nDataset = 'EnglandCovid'\nMethod = 'gnar' # 'stgcn','estgcn','gnar'  \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 4 # 1,2,3,4 ... \nNumber_of_filters = None # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_gnar = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    gnar = GNAR_Missing(Dataset, df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    gnar.iter()\n    df_add = gnar.df.copy()\n    df_gnar = pd.concat([df_gnar,df_add],axis=0)\n\n\nsave_data(df_gnar, './data/EnglandCovid_gnar_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_7.html",
    "href": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_7.html",
    "title": "GNAR lag 1 Randomly Missing comparison Table by Number of Filter and gnar forecast updating",
    "section": "",
    "text": "GNAR fiveNet,fivenodes lag 1 (Missing rate 80%)"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_7.html#stgcn",
    "href": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_7.html#stgcn",
    "title": "GNAR lag 1 Randomly Missing comparison Table by Number of Filter and gnar forecast updating",
    "section": "STGCN",
    "text": "STGCN\n\nclass STGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            for epoch in range(50):\n                for time, (xt,yt) in enumerate(zip(X,y)):\n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_stgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_stgcn = (((self.yy-yyhat).squeeze())**2).mean() \n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_stgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_stgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_7.html#enhencement-of-stgcn",
    "href": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_7.html#enhencement-of-stgcn",
    "title": "GNAR lag 1 Randomly Missing comparison Table by Number of Filter and gnar forecast updating",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nclass ESTGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n    \n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            signal = interpolated_signal.copy()\n            for epoch in range(50):\n                signal = update_from_freq_domain(signal,missing_index)\n                X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n                y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n                for time, (xt,yt) in enumerate(zip(X,y)):        \n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])               \n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_estgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_estgcn = (((self.yy-yyhat).squeeze())**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_estgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_estgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_7.html#stgcn-1",
    "href": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_7.html#stgcn-1",
    "title": "GNAR lag 1 Randomly Missing comparison Table by Number of Filter and gnar forecast updating",
    "section": "STGCN",
    "text": "STGCN\n\nDataset = 'fivenodes'\nMethod = 'stgcn' # 'stgcn','estgcn','gnar' \nMissingrate = 0.3 # 0.0, 0.2, 0.4, 0.6, 0.8 \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_stgcn= pd.DataFrame(columns=col)\n\n\nfor Number_of_filters in filter_num:\n    df = pd.DataFrame(columns=col)\n    stgcn = STGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    stgcn.iter()\n    df_add = stgcn.df.copy()\n    df_stgcn = pd.concat([df_stgcn,df_add],axis=0)\n\n\nsave_data(df_stgcn, './data/GNAR_stgcn_randomly_by_filter_30.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_7.html#enhencement-of-stgcn-1",
    "href": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_7.html#enhencement-of-stgcn-1",
    "title": "GNAR lag 1 Randomly Missing comparison Table by Number of Filter and gnar forecast updating",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nDataset = 'fivenodes'\nMethod = 'estgcn' # 'stgcn','estgcn','gnar' \nMissingrate = 0.3 # 0.0, 0.2, 0.4, 0.6, 0.8 \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_estgcn = pd.DataFrame(columns=col)\n\n\nfor Number_of_filters in filter_num:\n    df = pd.DataFrame(columns=col)\n    estgcn = ESTGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    estgcn.iter()\n    df_add = estgcn.df.copy()\n    df_estgcn = pd.concat([df_estgcn,df_add],axis=0)\n\n\nsave_data(df_estgcn, './data/GNAR_estgcn_randomly_by_filter_30.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "",
    "text": "GNAR fiveNet,fivenodes lag 1"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-ì¼ì •",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-ì¼ì •",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "missing ì¼ì •",
    "text": "missing ì¼ì •\n\nstgcn_train1 = []\nstgcn_test1 = []\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nfor i in range(100):\n    net = RecurrentGCN(node_features=1, filters=4)\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n    net.train()\n    for epoch in range(50):\n        for time, (xt,yt) in enumerate(zip(X,y)):\n            yt_hat = net(xt, edge_index, edge_attr)\n            cost = torch.mean((yt_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n    yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n    yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n    \n    train_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\n    test_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n    \n    stgcn_train1.append(train_mse_total_stgcn.tolist())\n    stgcn_test1.append(test_mse_total_stgcn.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(stgcn_train1);\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(stgcn_test1);"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-ë‹¤ë¥´ê²Œ",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-ë‹¤ë¥´ê²Œ",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "missing ë‹¤ë¥´ê²Œ",
    "text": "missing ë‹¤ë¥´ê²Œ\n\nstgcn_train2 = []\nstgcn_test2 = []\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nfor i in range(100):\n    \n    _zero = Missing(fiveVTS_train)\n    _zero.miss(percent = 0.8)\n    _zero.second_linear()\n\n    missing_index = _zero.number\n    interpolated_signal = _zero.train_linear\n\n\n    X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n    net = RecurrentGCN(node_features=1, filters=4)\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n    net.train()\n    for epoch in range(50):\n        for time, (xt,yt) in enumerate(zip(X,y)):\n            yt_hat = net(xt, edge_index, edge_attr)\n            cost = torch.mean((yt_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n    yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n    yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n    \n    train_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\n    test_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean() \n    \n    stgcn_train2.append(train_mse_total_stgcn.tolist())\n    stgcn_test2.append(test_mse_total_stgcn.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(stgcn_train2);\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(stgcn_test2);"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-ì¼ì •-1",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-ì¼ì •-1",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "missing ì¼ì •",
    "text": "missing ì¼ì •\n\nestgcn_train1 = []\nestgcn_test1 = []\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nfor i in range(100):\n    net = RecurrentGCN(node_features=1, filters=4)\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n    net.train()\n    signal = interpolated_signal.copy()\n    for epoch in range(50):\n        signal = update_from_freq_domain(signal,missing_index)\n        X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n        y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for time, (xt,yt) in enumerate(zip(X,y)):        \n            yt_hat = net(xt, edge_index, edge_attr)\n            cost = torch.mean((yt_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n    yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n    yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n    train_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\n    test_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n    \n    estgcn_train1.append(train_mse_total_estgcn.tolist())\n    estgcn_test1.append(test_mse_total_estgcn.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(estgcn_train1); \n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(estgcn_test1);"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-ë§¤ë²ˆ-ë‹¤ë¥´ê²Œ",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-ë§¤ë²ˆ-ë‹¤ë¥´ê²Œ",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "missing ë§¤ë²ˆ ë‹¤ë¥´ê²Œ",
    "text": "missing ë§¤ë²ˆ ë‹¤ë¥´ê²Œ\n\nestgcn_train2 = []\nestgcn_test2 = []\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\nfor i in range(100):\n    \n    _zero = Missing(fiveVTS_train)\n    _zero.miss(percent = 0.8)\n    _zero.second_linear()\n\n    missing_index = _zero.number\n    interpolated_signal = _zero.train_linear\n\n    X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n    y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\n    net = RecurrentGCN(node_features=1, filters=4)\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n    net.train()\n    signal = interpolated_signal.copy()\n    for epoch in range(50):\n        signal = update_from_freq_domain(signal,missing_index)\n        X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n        y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for time, (xt,yt) in enumerate(zip(X,y)):        \n            yt_hat = net(xt, edge_index, edge_attr)\n            cost = torch.mean((yt_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])               \n\n    yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n    yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n    train_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\n    test_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n    \n    estgcn_train2.append(train_mse_total_estgcn.tolist())\n    estgcn_test2.append(test_mse_total_estgcn.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(estgcn_train2);\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(estgcn_test2);"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-ì¼ì •-2",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-ì¼ì •-2",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "missing ì¼ì •",
    "text": "missing ì¼ì •\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\nX = np.array(torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:].squeeze())\ny = np.array(torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:].squeeze())\n\nXX = np.array(torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float().squeeze())\nyy = np.array(torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float().squeeze())\n\nreal_y = np.array(torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:])\n\n\n%R -i X\n%R -i XX\n\n\n%%R\ngnar_train1 <- matrix(ncol=1,nrow=100)\ngnar_test1 <- matrix(ncol=1,nrow=100)\nfor(i in 1:100){\n  answer <- GNARfit(vts = X, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\n  prediction <- predict(answer,n.ahead=40)\n  \n  train_mse_total_gnar <- mean(residuals(answer)**2)\n  test_mse_total_gnar <- mean((XX - prediction[1:40])**2)\n  \n  gnar_train1[i] <- train_mse_total_gnar\n  gnar_test1[i] <- train_mse_total_gnar\n}\n\n\n%R -o gnar_train1\n%R -o gnar_test1\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(gnar_train1);\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(gnar_test1);"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-ë‹¤ë¥´ê²Œ-1",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_3.html#missing-ë‹¤ë¥´ê²Œ-1",
    "title": "Class of Method(GNAR) lag 1 80% Missing repeat",
    "section": "missing ë‹¤ë¥´ê²Œ",
    "text": "missing ë‹¤ë¥´ê²Œ\n\nm = robjects.r.matrix(FloatVector([0,0,0,1,1,0,0,1,1,0,0,1,0,1,0,1,1,1,0,0,1,0,0,0,0]), nrow = 5, ncol = 5)\nprint(m)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    1    1\n[2,]    0    0    1    1    0\n[3,]    0    1    0    1    0\n[4,]    1    1    1    0    0\n[5,]    1    0    0    0    0\n\n\n\n\ngnar_train2 = []\ngnar_test2 = []\n\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nfor i in range(100):\n    \n    _zero = Missing(fiveVTS_train)\n    _zero.miss(percent = 0.8)\n    _zero.second_linear()\n\n    missing_index = _zero.number\n    interpolated_signal = _zero.train_linear\n\n    X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-2),:,:]\n\n    answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = 160, ncol = 5),net = GNAR.matrixtoGNAR(m), alphaOrder = 2, betaOrder = FloatVector([1, 1]))             \n    predict = GNAR.predict_GNARfit(answer,n_ahead=40)\n\n    \n    train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,5))**2).mean()\n    test_mse_total_gnar = ((yy.squeeze() - pd.DataFrame(predict).values.reshape(-1,5)[:-1,:])**2).mean()\n    \n    gnar_train2.append(train_mse_total_gnar.tolist())\n    gnar_test2.append(test_mse_total_gnar.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(gnar_train2);\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(gnar_test2);"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html",
    "title": "2nd ITSTGCN",
    "section": "",
    "text": "GNAR fiveNet,fivenodes lag 1"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#stgcn",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#stgcn",
    "title": "2nd ITSTGCN",
    "section": "STGCN",
    "text": "STGCN\n\nclass STGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            for epoch in range(50):\n                for time, (xt,yt) in enumerate(zip(X,y)):\n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_stgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_stgcn = (((self.yy-yyhat).squeeze())**2).mean() \n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset, \n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_stgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_stgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#enhencement-of-stgcn",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#enhencement-of-stgcn",
    "title": "2nd ITSTGCN",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nclass ESTGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n    \n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            signal = interpolated_signal.copy()\n            for epoch in range(50):\n                signal = update_from_freq_domain(signal,missing_index)\n                X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n                y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n                for time, (xt,yt) in enumerate(zip(X,y)):        \n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])               \n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_estgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_estgcn = (((self.yy-yyhat).squeeze())**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_estgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_estgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#gnar",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#gnar",
    "title": "2nd ITSTGCN",
    "section": "GNAR",
    "text": "GNAR\n\nm = robjects.r.matrix(FloatVector([0,0,0,1,1,0,0,1,1,0,0,1,0,1,0,1,1,1,0,0,1,0,0,0,0]), nrow = 5, ncol = 5)\n\n\nclass GNAR_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-2),:,:]\n\n            answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = 160, ncol = 5),net = GNAR.matrixtoGNAR(m), alphaOrder = 2, betaOrder = FloatVector([1, 1]))             \n            predict = GNAR.predict_GNARfit(answer,n_ahead=40)\n\n\n            train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,5))**2).mean()\n            test_mse_total_gnar = ((self.yy.squeeze() - pd.DataFrame(predict).values.reshape(-1,5)[:-1,:])**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_gnar.tolist()\n            df_row['MSE_test'] = test_mse_total_gnar.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#stgcn-1",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#stgcn-1",
    "title": "2nd ITSTGCN",
    "section": "STGCN",
    "text": "STGCN\n\nDataset = 'fivenodes'\nMethod = 'stgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_stgcn= pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    stgcn = STGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    stgcn.iter()\n    df_add = stgcn.df.copy()\n    df_stgcn = pd.concat([df_stgcn,df_add],axis=0)\n\n\nsave_data(df_stgcn, './data/GNAR_stgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#enhencement-of-stgcn-1",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#enhencement-of-stgcn-1",
    "title": "2nd ITSTGCN",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nDataset = 'fivenodes'\nMethod = 'estgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_estgcn = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    estgcn = ESTGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    estgcn.iter()\n    df_add = estgcn.df.copy()\n    df_estgcn = pd.concat([df_estgcn,df_add],axis=0)\n\n\nsave_data(df_estgcn, './data/GNAR_estgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#gnar-1",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin2.html#gnar-1",
    "title": "2nd ITSTGCN",
    "section": "GNAR",
    "text": "GNAR\n\nDataset = 'fivenodes'\nMethod = 'gnar' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = None # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_gnar = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    gnar = GNAR_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    gnar.iter()\n    df_add = gnar.df.copy()\n    df_gnar = pd.concat([df_gnar,df_add],axis=0)\n\n\nsave_data(df_gnar, './data/GANR_gnar_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html",
    "href": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "",
    "text": "ST-GCN Dataset WikiMathsDatasetLoader"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#train",
    "href": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#train",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "Train",
    "text": "Train\n\ndata_train=[]\nfor time, snapshot in enumerate(train_dataset):\n    data_train.append([time,snapshot])\n\n\ndata_train[0][1].x.shape,data_train[0][1].y.shape,data_train[0][1].edge_index.shape,data_train[0][1].edge_attr.shape\n\n(torch.Size([1068, 4]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n580\n\n\n\nT_train = time\nN = len(data_train[0][1].x)\n\n\nedge_index = data_train[0][1].edge_index\nedge_attr = data_train[0][1].edge_attr\n\n\nx_train = []\nfor i in range(time):\n    x_train.append(data_train[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_train = data_tensor.reshape(time,1068,-1)\nx_train.shape\n\ntorch.Size([580, 1068, 4])\n\n\n\ny_train = []\nfor i in range(time):\n    y_train.append(data_train[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_train = data_tensor.reshape(time,1068)\ny_train.shape\n\ntorch.Size([580, 1068])\n\n\n\nx_train.shape, y_train.shape\n\n(torch.Size([580, 1068, 4]), torch.Size([580, 1068]))"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#test",
    "href": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#test",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "Test",
    "text": "Test\n\ndata_test=[]\nfor time, snapshot in enumerate(test_dataset):\n    data_test.append([time,snapshot])\n\n\ndata_test[0][1].x.shape,data_test[0][1].y.shape,data_test[0][1].edge_index.shape,data_test[0][1].edge_attr.shape\n\n(torch.Size([1068, 4]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n145\n\n\n\nT_test = time\n\n\nx_test = []\nfor i in range(time):\n    x_test.append(data_test[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_test = data_tensor.reshape(time,1068,-1)\nx_test.shape\n\ntorch.Size([145, 1068, 4])\n\n\n\ny_test = []\nfor i in range(time):\n    y_test.append(data_test[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_test = data_tensor.reshape(time,1068)\ny_test.shape\n\ntorch.Size([145, 1068])\n\n\n\nx_test.shape, y_test.shape\n\n(torch.Size([145, 1068, 4]), torch.Size([145, 1068]))"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#ì‹œë‚˜ë¦¬ì˜¤1-baseline",
    "href": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#ì‹œë‚˜ë¦¬ì˜¤1-baseline",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "ì‹œë‚˜ë¦¬ì˜¤1 (Baseline)",
    "text": "ì‹œë‚˜ë¦¬ì˜¤1 (Baseline)\nì‹œë‚˜ë¦¬ì˜¤1\n\nmissing rate: 0%\në³´ê°„ë°©ë²•: None\n\n\nSTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(np.stack([x_train_f[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = x_train_f[4:T_train,:].reshape(T_train-4,N,-1).float()\n\n\nXX = x_test\nyy = y_test\n\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [05:47<00:00,  6.96s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y.squeeze()-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y.squeeze()-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\n\nGNAR ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n-\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\n\nR[write to console]: Loading required package: igraph\n\nR[write to console]: \nAttaching package: â€˜igraphâ€™\n\n\nR[write to console]: The following objects are masked from â€˜package:statsâ€™:\n\n    decompose, spectrum\n\n\nR[write to console]: The following object is masked from â€˜package:baseâ€™:\n\n    union\n\n\nR[write to console]: Loading required package: wordcloud\n\nR[write to console]: Loading required package: RColorBrewer\n\n\n\n\nEdge = np.array(edge_index)\nX_gnar = np.array(x_train_f)\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet <- matrixtoGNAR(w)\n\n\n%%R\nanswer <- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 4, betaOrder = c(1,1,1,1))\nprediction <- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((yy-gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((yy-gnar_test)**2).mean()\n\n\n\nê²°ê³¼ì‹œê°í™”\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,h=5)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n GNAR: mse(train) = {3:.2f}, mse(test) = {4:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(range(4,580),stgcn_train[:,i],label='STCGCN (train)',color='C0')\n    a.plot(range(583,728),stgcn_test[:,i],label='STCGCN (test)',color='C0')\n    a.plot(range(4,583),gnar_train[:,i],label='GNAR (train)',color='C1')\n    a.plot(range(583,728),gnar_test[:,i],label='GNAR (test)',color='C1')\n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario1: STGCN \\n missing=0% \\n interpolation=None \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n GNAR: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#ì‹œë‚˜ë¦¬ì˜¤2",
    "href": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#ì‹œë‚˜ë¦¬ì˜¤2",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "ì‹œë‚˜ë¦¬ì˜¤2",
    "text": "ì‹œë‚˜ë¦¬ì˜¤2\nì‹œë‚˜ë¦¬ì˜¤2\n\nmissing rate: 50%\në³´ê°„ë°©ë²•: linear\n\n- ê²°ì¸¡ì¹˜ìƒì„± + ë³´ê°„\n\n_zero = Missing(x_train_f)\n_zero.miss(percent = 0.5)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],x_train[:,:,0][:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\nimport numpy as np\n\nT= 100\nN= 5\nlag =4 \n\nsignal=np.arange(T*N).reshape(T,N)\n\nX= np.stack([signal[i:(T-lag+i),:] for i in range(lag)],axis=-1)\nX.shape\n\ny=signal[lag:].reshape(T-lag,N,1)\ny.shape\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\n\nXX = x_test\nyy = y_test\n\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [06:23<00:00,  7.67s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train[4:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\n\nXX = x_test\nyy = y_test\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(np.stack([signal[i:(T_train+i),:] for i in range(4)],axis = -1)).reshape(T_train,N,4).float()\n    y = torch.tensor(signal).reshape(-1,N,1).float()[4:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().reshape(1,-1)]).squeeze()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [07:11<00:00,  8.63s/it]\n\n\n- ESTGCN\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\ny_train.shape,T_train\n\n(torch.Size([580, 1068]), 580)\n\n\n\nreal_y = y_train\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n-\n\nX_gnar = np.array(torch.concat([X[:-1,:,0], x_train[-1,:,:].T]))\nEdge = np.array(edge_index)\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet <- matrixtoGNAR(w)\n\n\n%%R\nanswer <- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 4, betaOrder = c(1,1,1,1))\nprediction <- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((y_test-gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((y_test-gnar_test)**2).mean()\n\n\n\nê²°ê³¼ì‹œê°í™”\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,h=5)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],x_train[missing_index[i],i,0],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(4,580),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(583,728),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(4,584),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(582,727),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(4,583),gnar_train.squeeze()[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(583,728),gnar_test.squeeze()[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#ì‹œë‚˜ë¦¬ì˜¤3",
    "href": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#ì‹œë‚˜ë¦¬ì˜¤3",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "ì‹œë‚˜ë¦¬ì˜¤3",
    "text": "ì‹œë‚˜ë¦¬ì˜¤3\nì‹œë‚˜ë¦¬ì˜¤3\n\nmissing rate: 80%\në³´ê°„ë°©ë²•: linear\n\n- ê²°ì¸¡ì¹˜ìƒì„± + ë³´ê°„\n\n_zero = Missing(x_train_f)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,h=5)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],x_train_f[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\n\nXX = x_test\nyy = y_test\n\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [06:40<00:00,  8.01s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train[4:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\n\nXX = x_test\nyy = y_test\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(np.stack([signal[i:(T_train+i),:] for i in range(4)],axis = -1)).reshape(T_train,N,4).float()\n    y = torch.tensor(signal).reshape(-1,N,1).float()[4:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().reshape(1,-1)]).squeeze()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [07:18<00:00,  8.77s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n-\n\nX_gnar = np.array(torch.concat([X[:-1,:,0], x_train[-1,:,:].T]))\nEdge = np.array(edge_index)\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet <- matrixtoGNAR(w)\n\n\n%%R\nanswer <- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 4, betaOrder = c(1,1,1,1))\nprediction <- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((y_test-gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((y_test-gnar_test)**2).mean()\n\n\n\nê²°ê³¼ì‹œê°í™”\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,h=5)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],x_train[missing_index[i],i,0],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(4,580),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(583,728),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(4,584),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(582,727),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(4,583),gnar_train.squeeze()[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(583,728),gnar_test.squeeze()[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#ì‹œë‚˜ë¦¬ì˜¤4",
    "href": "posts/GCN/2023-01-26-ESTGCN_WIKI_DATA.html#ì‹œë‚˜ë¦¬ì˜¤4",
    "title": "Class of Method(WikiMath) lag 4",
    "section": "ì‹œë‚˜ë¦¬ì˜¤4",
    "text": "ì‹œë‚˜ë¦¬ì˜¤4\nì‹œë‚˜ë¦¬ì˜¤4\n\nmissing rate: 30%\në³´ê°„ë°©ë²•: linear\n\n- ê²°ì¸¡ì¹˜ìƒì„± + ë³´ê°„\n\n_zero = Missing(x_train_f)\n_zero.miss(percent = 0.3)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,h=5)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],x_train_f[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\n\nXX = x_test\nyy = y_test\n\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [06:32<00:00,  7.86s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train[4:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\n\nXX = x_test\nyy = y_test\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=4, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(np.stack([signal[i:(T_train+i),:] for i in range(4)],axis = -1)).reshape(T_train,N,4).float()\n    y = torch.tensor(signal).reshape(-1,N,1).float()[4:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().reshape(1,-1)]).squeeze()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [07:13<00:00,  8.66s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n-\n\nX_gnar = np.array(torch.concat([X[:-1,:,0], x_train[-1,:,:].T]))\nEdge = np.array(edge_index)\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet <- matrixtoGNAR(w)\n\n\n%%R\nanswer <- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 4, betaOrder = c(1,1,1,1))\nprediction <- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((y_test-gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((y_test-gnar_test)**2).mean()\n\n\n\nê²°ê³¼ì‹œê°í™”\n\nfig = plot(x_train_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,h=5)\nfig = plot_add(fig,x_test_f[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,729))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],x_train[missing_index[i],i,0],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(4,580),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(583,728),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(4,584),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(582,727),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(4,583),gnar_train.squeeze()[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(583,728),gnar_test.squeeze()[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario4: \\n missing=30% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html",
    "title": "SY 1st ITSTGCN",
    "section": "",
    "text": "edit"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#random-missing-values",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#random-missing-values",
    "title": "SY 1st ITSTGCN",
    "section": "Random Missing Values",
    "text": "Random Missing Values\n\nclass Missing:\n    def __init__(self,df):\n        self.df = df\n        self.N = N\n        self.number = []\n    def miss(self,percent=0.5):\n        self.missing = self.df.copy()\n        self.percent = percent\n        for i in range(self.N):\n            #self.seed = np.random.choice(1000,1,replace=False)\n            #np.random.seed(self.seed)\n            self.number.append(np.random.choice(int(len(self.df))-1,int(len(self.df)*self.percent),replace=False))\n            self.missing[self.number[i],i] = float('nan')\n    def first_mean(self):\n        self.train_mean = self.missing.copy()\n        for i in range(self.N):\n            self.train_mean[self.number[i],i] = np.nanmean(self.missing[:,i])\n    def second_linear(self):\n        self.train_linear = pd.DataFrame(self.missing)\n        self.train_linear.interpolate(method='linear', inplace=True)\n        self.train_linear = self.train_linear.fillna(0)\n        self.train_linear = np.array(self.train_linear).reshape(int(len(self.df)),N)\n\n\ncol = ['Dataset','iteration', 'method', 'missingrate', 'missingtype', 'lag', 'number_of_filters', 'interpolation','MSE_train', 'MSE_test']\n\nrate = [i/10 for i in range(10)]"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#class-code-by-method",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#class-code-by-method",
    "title": "SY 1st ITSTGCN",
    "section": "Class code by Method",
    "text": "Class code by Method"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#stgcn",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#stgcn",
    "title": "SY 1st ITSTGCN",
    "section": "STGCN",
    "text": "STGCN\n\nclass STGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            for epoch in range(50):\n                for time, (xt,yt) in enumerate(zip(X,y)):\n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_stgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_stgcn = (((self.yy-yyhat).squeeze())**2).mean() \n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset, \n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_stgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_stgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#enhencement-of-stgcn",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#enhencement-of-stgcn",
    "title": "SY 1st ITSTGCN",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nclass ESTGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n    \n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            signal = interpolated_signal.copy()\n            for epoch in range(50):\n                signal = update_from_freq_domain(signal,missing_index)\n                X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n                y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n                for time, (xt,yt) in enumerate(zip(X,y)):        \n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])               \n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_estgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_estgcn = (((self.yy-yyhat).squeeze())**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_estgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_estgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#gnar",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#gnar",
    "title": "SY 1st ITSTGCN",
    "section": "GNAR",
    "text": "GNAR\n\nm = robjects.r.matrix(FloatVector([0,0,0,1,1,0,0,1,1,0,0,1,0,1,0,1,1,1,0,0,1,0,0,0,0]), nrow = 5, ncol = 5)\n\n\nclass GNAR_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-2),:,:]\n\n            answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = 160, ncol = 5),net = GNAR.matrixtoGNAR(m), alphaOrder = 2, betaOrder = FloatVector([1, 1]))             \n            predict = GNAR.predict_GNARfit(answer,n_ahead=40)\n\n\n            train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,5))**2).mean()\n            test_mse_total_gnar = ((self.yy.squeeze() - pd.DataFrame(predict).values.reshape(-1,5)[:-1,:])**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_gnar.tolist()\n            df_row['MSE_test'] = test_mse_total_gnar.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#stgcn-1",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#stgcn-1",
    "title": "SY 1st ITSTGCN",
    "section": "STGCN",
    "text": "STGCN\n\nDataset = 'fivenodes'\nMethod = 'stgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_stgcn= pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    stgcn = STGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    stgcn.iter()\n    df_add = stgcn.df.copy()\n    df_stgcn = pd.concat([df_stgcn,df_add],axis=0)\n\n\nsave_data(df_stgcn, './data/GNAR_stgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#enhencement-of-stgcn-1",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#enhencement-of-stgcn-1",
    "title": "SY 1st ITSTGCN",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nDataset = 'fivenodes'\nMethod = 'estgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_estgcn = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    estgcn = ESTGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    estgcn.iter()\n    df_add = estgcn.df.copy()\n    df_estgcn = pd.concat([df_estgcn,df_add],axis=0)\n\n\nsave_data(df_estgcn, './data/GNAR_estgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#gnar-1",
    "href": "posts/GCN/2023-03-03-ESTGCN_GNAR_edit_guebin2_seoyeon.html#gnar-1",
    "title": "SY 1st ITSTGCN",
    "section": "GNAR",
    "text": "GNAR\n\nDataset = 'fivenodes'\nMethod = 'gnar' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = None # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_gnar = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    gnar = GNAR_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    gnar.iter()\n    df_add = gnar.df.copy()\n    df_gnar = pd.concat([df_gnar,df_add],axis=0)\n\n\nsave_data(df_gnar, './data/GANR_gnar_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.html",
    "href": "posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.html",
    "title": "Class of Method(WikiMath) lag 4 80% repeat",
    "section": "",
    "text": "ST-GCN Dataset WikiMathsDatasetLoader"
  },
  {
    "objectID": "posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.html#train",
    "href": "posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.html#train",
    "title": "Class of Method(WikiMath) lag 4 80% repeat",
    "section": "Train",
    "text": "Train\n\ndata_train=[]\nfor time, snapshot in enumerate(train_dataset):\n    data_train.append([time,snapshot])\n\n\ndata_train[0][1].x.shape,data_train[0][1].y.shape,data_train[0][1].edge_index.shape,data_train[0][1].edge_attr.shape\n\n(torch.Size([1068, 4]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n580\n\n\n\nT_train = time\nN = len(data_train[0][1].x)\n\n\nedge_index = data_train[0][1].edge_index\nedge_attr = data_train[0][1].edge_attr\n\n\nx_train = []\nfor i in range(time):\n    x_train.append(data_train[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_train = data_tensor.reshape(time,1068,-1)\nx_train.shape\n\ntorch.Size([580, 1068, 4])\n\n\n\ny_train = []\nfor i in range(time):\n    y_train.append(data_train[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_train = data_tensor.reshape(time,1068)\ny_train.shape\n\ntorch.Size([580, 1068])\n\n\n\nx_train.shape, y_train.shape\n\n(torch.Size([580, 1068, 4]), torch.Size([580, 1068]))"
  },
  {
    "objectID": "posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.html#test",
    "href": "posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.html#test",
    "title": "Class of Method(WikiMath) lag 4 80% repeat",
    "section": "Test",
    "text": "Test\n\ndata_test=[]\nfor time, snapshot in enumerate(test_dataset):\n    data_test.append([time,snapshot])\n\n\ndata_test[0][1].x.shape,data_test[0][1].y.shape,data_test[0][1].edge_index.shape,data_test[0][1].edge_attr.shape\n\n(torch.Size([1068, 4]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n145\n\n\n\nT_test = time\n\n\nx_test = []\nfor i in range(time):\n    x_test.append(data_test[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_test = data_tensor.reshape(time,1068,-1)\nx_test.shape\n\ntorch.Size([145, 1068, 4])\n\n\n\ny_test = []\nfor i in range(time):\n    y_test.append(data_test[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_test = data_tensor.reshape(time,1068)\ny_test.shape\n\ntorch.Size([145, 1068])\n\n\n\nx_test.shape, y_test.shape\n\n(torch.Size([145, 1068, 4]), torch.Size([145, 1068]))"
  },
  {
    "objectID": "posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.html#missing-ì¼ì •",
    "href": "posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.html#missing-ì¼ì •",
    "title": "Class of Method(WikiMath) lag 4 80% repeat",
    "section": "missing ì¼ì •",
    "text": "missing ì¼ì •\n\nstgcn_train1 = []\nstgcn_test1 = []\n\n_zero = Missing(x_train_f)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\nXX = x_test\nyy = y_test\n\nreal_y = y_train[4:,:]\n\n\nfor i in range(100):\n    net = RecurrentGCN(node_features=4, filters=4)\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n    net.train()\n    for epoch in range(50):\n        for time, (xt,yt) in enumerate(zip(X,y)):\n            yt_hat = net(xt, edge_index, edge_attr)\n            cost = torch.mean((yt_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n    yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n    yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n    train_mse_total_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\n    test_mse_total_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n    stgcn_train1.append(train_mse_total_stgcn.tolist())\n    stgcn_test1.append(test_mse_total_stgcn.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(stgcn_train1);\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(stgcn_test1);"
  },
  {
    "objectID": "posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.html#missing-ë‹¤ë¥´ê²Œ",
    "href": "posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.html#missing-ë‹¤ë¥´ê²Œ",
    "title": "Class of Method(WikiMath) lag 4 80% repeat",
    "section": "missing ë‹¤ë¥´ê²Œ",
    "text": "missing ë‹¤ë¥´ê²Œ\n\nstgcn_train2 = []\nstgcn_test2 = []\n\nXX = x_test\nyy = y_test\n\nreal_y = y_train[4:,:]\n\n\nfor i in range(100):\n    \n    _zero = Missing(x_train_f)\n    _zero.miss(percent = 0.8)\n    _zero.second_linear()\n\n    missing_index = _zero.number\n    interpolated_signal = _zero.train_linear\n\n    X = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\n    y = torch.tensor(interpolated_signal[4:,:]).float()\n\n    net = RecurrentGCN(node_features=4, filters=4)\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n    net.train()\n    for epoch in range(50):\n        for time, (xt,yt) in enumerate(zip(X,y)):\n            yt_hat = net(xt, edge_index, edge_attr)\n            cost = torch.mean((yt_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n    yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n    yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n    train_mse_total_stgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\n    test_mse_total_stgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n    stgcn_train2.append(train_mse_total_stgcn.tolist())\n    stgcn_test2.append(test_mse_total_stgcn.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(stgcn_train2);\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(stgcn_test2);"
  },
  {
    "objectID": "posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.html#missing-ì¼ì •-1",
    "href": "posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.html#missing-ì¼ì •-1",
    "title": "Class of Method(WikiMath) lag 4 80% repeat",
    "section": "missing ì¼ì •",
    "text": "missing ì¼ì •\n\nestgcn_train1 = []\nestgcn_test1 = []\n\n_zero = Missing(x_train_f)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\nX = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[4:,:]).float()\n\nXX = x_test\nyy = y_test\n\nreal_y = y_train\n\n\nfor i in range(100):\n    net = RecurrentGCN(node_features=4, filters=4)\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n    net.train()\n    signal = interpolated_signal.copy()\n    for epoch in range(50):\n        signal = update_from_freq_domain(signal,missing_index)\n        X = torch.tensor(np.stack([signal[i:(T_train+i),:] for i in range(4)],axis = -1)).reshape(T_train,N,4).float()\n        y = torch.tensor(signal).reshape(-1,N,1).float()[4:,:,:]\n        for time, (xt,yt) in enumerate(zip(X,y)):        \n            yt_hat = net(xt, edge_index, edge_attr)\n            cost = torch.mean((yt_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().reshape(1,-1)]).squeeze()\n\n    yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n    yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n    train_mse_total_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\n    test_mse_total_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n    estgcn_train1.append(train_mse_total_estgcn.tolist())\n    estgcn_test1.append(test_mse_total_estgcn.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(estgcn_train1); \n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(estgcn_test1);"
  },
  {
    "objectID": "posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.html#missing-ë§¤ë²ˆ-ë‹¤ë¥´ê²Œ",
    "href": "posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.html#missing-ë§¤ë²ˆ-ë‹¤ë¥´ê²Œ",
    "title": "Class of Method(WikiMath) lag 4 80% repeat",
    "section": "missing ë§¤ë²ˆ ë‹¤ë¥´ê²Œ",
    "text": "missing ë§¤ë²ˆ ë‹¤ë¥´ê²Œ\n\nestgcn_train2 = []\nestgcn_test2 = []\n\nXX = x_test\nyy = y_test\n\nreal_y = y_train\n\n\nfor i in range(100):\n    \n    _zero = Missing(x_train_f)\n    _zero.miss(percent = 0.8)\n    _zero.second_linear()\n\n    missing_index = _zero.number\n    interpolated_signal = _zero.train_linear\n    \n    X = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\n    y = torch.tensor(interpolated_signal[4:,:]).float()\n\n    net = RecurrentGCN(node_features=4, filters=4)\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n    net.train()\n    signal = interpolated_signal.copy()\n    for epoch in range(50):\n        signal = update_from_freq_domain(signal,missing_index)\n        X = torch.tensor(np.stack([signal[i:(T_train+i),:] for i in range(4)],axis = -1)).reshape(T_train,N,4).float()\n        y = torch.tensor(signal).reshape(-1,N,1).float()[4:,:,:]\n        for time, (xt,yt) in enumerate(zip(X,y)):        \n            yt_hat = net(xt, edge_index, edge_attr)\n            cost = torch.mean((yt_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().reshape(1,-1)]).squeeze()\n\n    yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n    yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n    train_mse_total_estgcn = (((real_y-yhat.squeeze()).squeeze())**2).mean()\n    test_mse_total_estgcn = (((yy-yyhat.squeeze()).squeeze())**2).mean()\n\n    estgcn_train2.append(train_mse_total_estgcn.tolist())\n    estgcn_test2.append(test_mse_total_estgcn.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(estgcn_train2); \n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(estgcn_test2);"
  },
  {
    "objectID": "posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.html#missing-ì¼ì •-2",
    "href": "posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.html#missing-ì¼ì •-2",
    "title": "Class of Method(WikiMath) lag 4 80% repeat",
    "section": "missing ì¼ì •",
    "text": "missing ì¼ì •\n\n%%R\ngnar_train1 <- matrix(ncol=1,nrow=100)\ngnar_test1 <- matrix(ncol=1,nrow=100)\n\nfor(i in 1:100){\n    answer <- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 4, betaOrder = c(1,1,1,1))\n    prediction <- predict(answer,n.ahead=T_test)\n\n    train_mse_total_gnar = mean(residuals(answer)**2)\n    test_mse_total_gnar = mean((y_gnar-prediction[1:T_test])**2)\n\n    gnar_train1[i] <- train_mse_total_gnar\n    gnar_test1[i] <- train_mse_total_gnar\n}\n\n\n%R -o gnar_train1\n%R -o gnar_test1\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(gnar_train1);\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(gnar_test1);"
  },
  {
    "objectID": "posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.html#missing-ë§¤ë²ˆ-ë‹¤ë¥´ê²Œ-1",
    "href": "posts/GCN/2023-02-11-ESTGCN_WIKI_DATA_3.html#missing-ë§¤ë²ˆ-ë‹¤ë¥´ê²Œ-1",
    "title": "Class of Method(WikiMath) lag 4 80% repeat",
    "section": "missing ë§¤ë²ˆ ë‹¤ë¥´ê²Œ",
    "text": "missing ë§¤ë²ˆ ë‹¤ë¥´ê²Œ\n\nm = robjects.r.matrix(FloatVector(w), nrow = 1068, ncol = 1068)\n\n\ngnar_train2 = []\ngnar_test2 = []\n\nyy = torch.tensor(y_test).float()\n\n/tmp/ipykernel_3456764/2230476182.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  yy = torch.tensor(y_test).float()\n\n\n\nfor i in range(100):\n    \n    _zero = Missing(x_train_f)\n    _zero.miss(percent = 0.8)\n    _zero.second_linear()\n\n    missing_index = _zero.number\n    interpolated_signal = _zero.train_linear\n\n    X = torch.tensor(np.stack([interpolated_signal[i:(T_train-4+i),:] for i in range(4)],axis = -1)).float()\n\n    answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = T_train, ncol = N),net = GNAR.matrixtoGNAR(m), alphaOrder = 4, betaOrder = FloatVector([1, 1, 1, 1]))             \n    predict = GNAR.predict_GNARfit(answer,n_ahead=T_test)\n\n    \n    train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,N))**2).mean()\n    test_mse_total_gnar = ((yy - pd.DataFrame(predict).values.reshape(-1,N))**2).mean()\n    \n    gnar_train2.append(train_mse_total_gnar.tolist())\n    gnar_test2.append(test_mse_total_gnar.tolist())\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(gnar_train2);\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(gnar_test2);"
  },
  {
    "objectID": "posts/GCN/2023-02-08-ESTGCN_GNAR_DATA_4.html",
    "href": "posts/GCN/2023-02-08-ESTGCN_GNAR_DATA_4.html",
    "title": "Class of Method(GNAR) lag 1 Block Missing repeat",
    "section": "",
    "text": "GNAR fiveNet,fivenodes lag 1\n\n\nimport\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\n\n# torch\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvGRU\n\n# scipy \nfrom scipy.interpolate import interp1d\n\n# utils\nimport time\nimport pickle\nfrom tqdm import tqdm\n\n# rpy2\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr\n\nimport rpy2.robjects.numpy2ri as rpyn\nimport rpy2.robjects as robjects\n\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvGRU(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\n\nmy functions\n\ndef load_data(fname):\n    with open(fname, 'rb') as outfile:\n        data_dict = pickle.load(outfile)\n    return data_dict\n\n\ndef save_data(data_dict,fname):\n    with open(fname,'wb') as outfile:\n        pickle.dump(data_dict,outfile)\n\n\ndef plot(f,*args,t=None,h=2.5,**kwargs):\n    T,N = f.shape\n    if t == None: t = range(T)\n    fig = plt.figure()\n    ax = fig.subplots(N,1)\n    for n in range(N):\n        ax[n].plot(t,f[:,n],*args,**kwargs)\n        ax[n].set_title('node='+str(n))\n    fig.set_figheight(N*h)\n    fig.tight_layout()\n    plt.close()\n    return fig\n\n\ndef plot_add(fig,f,*args,t=None,**kwargs):\n    T = f.shape[0]\n    N = f.shape[1] \n    if t == None: t = range(T)   \n    ax = fig.get_axes()\n    for n in range(N):\n        ax[n].plot(t,f[:,n],*args,**kwargs)\n    return fig\n\n\ndef make_Psi(T):\n    W = np.zeros((T,T))\n    for i in range(T):\n        for j in range(T):\n            if i==j :\n                W[i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                W[i,j] = 1\n    d = np.array(W.sum(axis=1))\n    D = np.diag(d)\n    L = np.array(np.diag(1/np.sqrt(d)) @ (D-W) @ np.diag(1/np.sqrt(d)))\n    lamb, Psi = np.linalg.eigh(L)\n    return Psi\n\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\n\n\ndef trim(f):\n    f = np.array(f)\n    if len(f.shape)==1: f = f.reshape(-1,1)\n    T,N = f.shape\n    Psi = make_Psi(T)\n    fbar = Psi.T @ f # apply dft \n    fbar_threshed = np.stack([ebayesthresh(FloatVector(fbar[:,i])) for i in range(N)],axis=1)\n    fhat = Psi @ fbar_threshed # inverse dft \n    return fhat\n\n\ndef update_from_freq_domain(signal, start, end, node):\n    signal = np.array(signal)\n    T,N = signal.shape \n    signal_trimed = trim(signal)\n    signal[start:end,node] = signal_trimed[start:end,node]\n    return signal\n\n\n\ndata ì •ë¦¬\n- ë°ì´í„°ì •ë¦¬\n\ndata = load_data('./data/fivenodes.pkl')\n\n\nedges_tensor = torch.tensor(data['edges'])\nfiveVTS = np.array(data['f'])\nnonzero_indices = edges_tensor.nonzero()\nfiveNet_edge = np.array(nonzero_indices).T\nT = 200\nN = 5 # number of Nodes\nE = fiveNet_edge\nV = np.array([1,2,3,4,5])\nt = np.arange(0,T)\nnode_features = 1\nedge_index = torch.tensor(E)\nedge_attr = torch.tensor(np.array([1,1,1,1,1,1,1,1,1,1]),dtype=torch.float32)\n\n- train / test\n\nfiveVTS_train = fiveVTS[:int(len(fiveVTS)*0.8)]\nfiveVTS_test = fiveVTS[int(len(fiveVTS)*0.8):]\n\n\n\nBLock Missing Values\n\nclass Missing:\n    def __init__(self,df):\n        self.df = df\n        self.number = []\n    def miss(self,start = 50,end = 150,node = 2):\n        self.missing = self.df.copy()\n        self.start = start\n        self.end = end\n        self.node = node\n        self.missing[self.start:self.end,self.node] = float('nan')\n    def first_mean(self):\n        self.train_mean = self.missing.copy()\n        self.train_mean[self.start-1:self.end,self.node] = np.mean(self.df[:(self.start-1),self.node].tolist()+self.df[self.end:,self.node].tolist())\n    def second_linear(self):\n        self.train_linear = pd.DataFrame(self.missing)\n        self.train_linear.interpolate(method='linear', inplace=True)\n        self.train_linear = self.train_linear.fillna(0)\n        self.train_linear = np.array(self.train_linear).reshape(int(len(self.df)),N)\n\n\n\nSTGCN\n\nmissing rate: 80%\në³´ê°„ë°©ë²•: linear\n\n\nstgcn_train1 = []\nstgcn_test1 = []\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(50,150,2)\n_zero.second_linear()\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\nfor i in range(100):\n    net = RecurrentGCN(node_features=1, filters=4)\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n    net.train()\n    for epoch in range(50):\n        for time, (xt,yt) in enumerate(zip(X,y)):\n            yt_hat = net(xt, edge_index, edge_attr)\n            cost = torch.mean((yt_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n    yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n    yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n    \n    train_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\n    test_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n    \n    stgcn_train1.append(train_mse_total_stgcn.tolist())\n    stgcn_test1.append(test_mse_total_stgcn.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(stgcn_train1);\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(stgcn_test1);\n\n\n\n\n\n\nEnhencement of STGCN\n\nmissing rate: 80%\në³´ê°„ë°©ë²•: linear\n\n- ê²°ì¸¡ì¹˜ìƒì„± + ë³´ê°„\n\nestgcn_train1 = []\nestgcn_test1 = []\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(50,150,2)\n_zero.second_linear()\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\ny = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\nXX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\nfor i in range(100):\n    net = RecurrentGCN(node_features=1, filters=4)\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n    net.train()\n    signal = interpolated_signal.copy()\n    for epoch in range(50):\n        signal = update_from_freq_domain(signal,50,150,2)\n        X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n        y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for time, (xt,yt) in enumerate(zip(X,y)):        \n            yt_hat = net(xt, edge_index, edge_attr)\n            cost = torch.mean((yt_hat-yt)**2)\n            cost.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n    yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n    yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n    train_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\n    test_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n    \n    estgcn_train1.append(train_mse_total_estgcn.tolist())\n    estgcn_test1.append(test_mse_total_estgcn.tolist())\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(estgcn_train1); \n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(estgcn_test1); \n\n\n\n\n\n\nGNAR\n\n%load_ext rpy2.ipython\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython\n\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\nlibrary(zoo)\n\n\nGNAR = importr('GNAR') # import GNAR \nigraph = importr('igraph') # import igraph \n\n\nm = robjects.r.matrix(FloatVector([0,0,0,1,1,0,0,1,1,0,0,1,0,1,0,1,1,1,0,0,1,0,0,0,0]), nrow = 5, ncol = 5)\nprint(m)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    1    1\n[2,]    0    0    1    1    0\n[3,]    0    1    0    1    0\n[4,]    1    1    1    0    0\n[5,]    1    0    0    0    0\n\n\n\n\ngnar_train1 = []\ngnar_test1 = []\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(50,150,2)\n_zero.second_linear()\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-2),:,:]\n\nyy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n\nfor i in range(100):\n    \n    answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = 160, ncol = 5),net = GNAR.matrixtoGNAR(m), alphaOrder = 2, betaOrder = FloatVector([1, 1]))             \n    predict = GNAR.predict_GNARfit(answer,n_ahead=40)\n\n    \n    train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,5))**2).mean()\n    test_mse_total_gnar = ((yy.squeeze() - pd.DataFrame(predict).values.reshape(-1,5)[:-1,:])**2).mean()\n    \n    gnar_train1.append(train_mse_total_gnar.tolist())\n    gnar_test1.append(test_mse_total_gnar.tolist())\n\n\n%R -o gnar_train1\n%R -o gnar_test1\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(gnar_train1);\n\n\n\n\n\nplt.figure(figsize=(12, 8))\nplt.boxplot(gnar_test1);\n\n\n\n\n\n\nVisualization\n\nfig = plt.figure(figsize = (12,10))\nax = fig.add_subplot(111)\n\nbp1 = ax.boxplot(stgcn_train1, positions=[1], notch=True, widths=0.35, patch_artist=True, boxprops=dict(facecolor=\"C0\"))\nbp2 = ax.boxplot(estgcn_train1, positions=[2], notch=True, widths=0.35, patch_artist=True, boxprops=dict(facecolor=\"C1\"))\nbp3 = ax.boxplot(gnar_train1, positions=[3], notch=True, widths=0.35, patch_artist=True, boxprops=dict(facecolor=\"C2\"))\nax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0], bp3[\"boxes\"][0]], [\"STGCN\", \"ESTGCN\", \"GNAR\"], loc='upper right')\n\nplt.text(x=0.5, y=0.94, s=\"TRAIN_Fix missing number_Block Missing\", fontsize=25, ha=\"center\", transform=fig.transFigure)\nfig.tight_layout()\nplt.show()\n\n\n\n\n\nfig = plt.figure(figsize = (12,10))\nax = fig.add_subplot(111)\n\nbp1 = ax.boxplot(stgcn_test1, positions=[1], notch=True, widths=0.35, patch_artist=True, boxprops=dict(facecolor=\"C0\"))\nbp2 = ax.boxplot(estgcn_test1, positions=[2], notch=True, widths=0.35, patch_artist=True, boxprops=dict(facecolor=\"C1\"))\nbp3 = ax.boxplot(gnar_test1, positions=[3], notch=True, widths=0.35, patch_artist=True, boxprops=dict(facecolor=\"C2\"))\nax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0], bp3[\"boxes\"][0]], [\"STGCN\", \"ESTGCN\", \"GNAR\"], loc='upper right')\n\nplt.text(x=0.5, y=0.94, s=\"TEST_Fix missing number_Block Missing\", fontsize=25, ha=\"center\", transform=fig.transFigure)\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html",
    "title": "Class of Method(GNAR) lag 2",
    "section": "",
    "text": "GNAR fiveNet,fivenodes lag 2"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html#ì‹œë‚˜ë¦¬ì˜¤1-baseline",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html#ì‹œë‚˜ë¦¬ì˜¤1-baseline",
    "title": "Class of Method(GNAR) lag 2",
    "section": "ì‹œë‚˜ë¦¬ì˜¤1 (Baseline)",
    "text": "ì‹œë‚˜ë¦¬ì˜¤1 (Baseline)\nì‹œë‚˜ë¦¬ì˜¤1\n\nmissing rate: 0%\në³´ê°„ë°©ë²•: None\n\n\nSTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(np.stack([fiveVTS_train[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(fiveVTS_train[2:int(T*0.8),:].reshape(int(T*0.8)-2,N,-1)).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:26<00:00,  1.87it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nGNAR ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n-\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\nlibrary(tidyverse)\n\n\n%R -i fiveVTS_train\n\n\n%%R\nanswer <- GNARfit(vts = fiveVTS_train, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nprediction <- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((fiveVTS_test - gnar_test.reshape(-1,5))**2).mean(axis=0)\ntest_mse_total_gnar = ((fiveVTS_test - gnar_test.reshape(-1,5))**2).mean()\n\n\n\nê²°ê³¼ì‹œê°í™”\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.995256618187614, 1.2577286248028454)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n mse(train) = {3:.2f}, mse(test) = {4:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(range(2,160),stgcn_train[:,i],label='STCGCN (train)',color='C0')\n    a.plot(range(162,200),stgcn_test[:,i],label='STCGCN (test)',color='C0')\n    a.plot(range(2,160),gnar_train.reshape(-1,5)[:,i],label='GNAR (train)',color='C1')\n    a.plot(range(160,200),gnar_test.reshape(-1,5)[:,i],label='GNAR (test)',color='C1')\n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario1: STGCN \\n missing=0% \\n interpolation=None \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n GNAR: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html#ì‹œë‚˜ë¦¬ì˜¤2",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html#ì‹œë‚˜ë¦¬ì˜¤2",
    "title": "Class of Method(GNAR) lag 2",
    "section": "ì‹œë‚˜ë¦¬ì˜¤2",
    "text": "ì‹œë‚˜ë¦¬ì˜¤2\nì‹œë‚˜ë¦¬ì˜¤2\n\nmissing rate: 50%\në³´ê°„ë°©ë²•: linear\n\n- ê²°ì¸¡ì¹˜ìƒì„± + ë³´ê°„\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.5)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(np.stack([interpolated_signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[2:,:]).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:26<00:00,  1.85it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[2:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(np.stack([interpolated_signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[2:,:]).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(np.stack([signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\n    y = torch.tensor(signal[2:,:]).float()\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:28<00:00,  1.78it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[2:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n-\n\nX_train1 = np.array(interpolated_signal).squeeze()\nX_test1 =  np.array(fiveVTS_test).squeeze()\n\n\n%R -i X_train1\n\n\n%%R\nanswer <- GNARfit(vts = X_train1, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nprediction <- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((X_test1 - gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((X_test1 - gnar_test)**2).mean()\n\n\n\nê²°ê³¼ì‹œê°í™”\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.6280648350096797, 1.3222499750457097)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(2,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(162,200),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(2,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(162,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(2,160),gnar_train[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(162,202),gnar_test[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html#ì‹œë‚˜ë¦¬ì˜¤3",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html#ì‹œë‚˜ë¦¬ì˜¤3",
    "title": "Class of Method(GNAR) lag 2",
    "section": "ì‹œë‚˜ë¦¬ì˜¤3",
    "text": "ì‹œë‚˜ë¦¬ì˜¤3\nì‹œë‚˜ë¦¬ì˜¤3\n\nmissing rate: 80%\në³´ê°„ë°©ë²•: linear\n\n- ê²°ì¸¡ì¹˜ìƒì„± + ë³´ê°„\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(np.stack([interpolated_signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[2:,:]).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:27<00:00,  1.85it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[2:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(np.stack([interpolated_signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[2:,:]).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(np.stack([signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\n    y = torch.tensor(signal[2:,:]).float()\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:28<00:00,  1.77it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[2:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n-\n\nX_train1 = np.array(interpolated_signal).squeeze()\nX_test1 =  np.array(fiveVTS_test).squeeze()\n\n\n%R -i X_train1\n\n\n%%R\nanswer <- GNARfit(vts = X_train1, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nprediction <- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((X_test1 - gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((X_test1 - gnar_test)**2).mean()\n\n\n\nê²°ê³¼ì‹œê°í™”\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.2092691948436627, 1.5191113001100904)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(2,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(162,200),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(2,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(162,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(2,160),gnar_train[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(162,202),gnar_test[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario3: \\n missing=80% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html#ì‹œë‚˜ë¦¬ì˜¤4",
    "href": "posts/GCN/2023-02-06-ESTGCN_GNAR_DATA_2.html#ì‹œë‚˜ë¦¬ì˜¤4",
    "title": "Class of Method(GNAR) lag 2",
    "section": "ì‹œë‚˜ë¦¬ì˜¤4",
    "text": "ì‹œë‚˜ë¦¬ì˜¤4\nì‹œë‚˜ë¦¬ì˜¤4\n\nmissing rate: 30%\në³´ê°„ë°©ë²•: linear\n\n- ê²°ì¸¡ì¹˜ìƒì„± + ë³´ê°„\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.3)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(fiveVTS,'--o',h=4,color='gray',label='complete data',alpha=0.2)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(np.stack([interpolated_signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[2:,:]).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:26<00:00,  1.86it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[2:,:,:]\n\ntrain_mse_eachnode_stgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nstgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nstgcn_test = yyhat.squeeze() \n\n\n\nESTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = torch.tensor(np.stack([interpolated_signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\ny = torch.tensor(interpolated_signal[2:,:]).float()\n\n\nXX = torch.tensor(np.stack([fiveVTS_test[i:(int(T*0.2)-2+i),:] for i in range(2)],axis = -1)).float()\nyy = fiveVTS_test[2:int(T*0.2),:].reshape(int(T*0.2)-2,N,-1)\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=2, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(np.stack([signal[i:(int(T*0.8)-2+i),:] for i in range(2)],axis = -1)).float()\n    y = torch.tensor(signal[2:,:]).float()\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:27<00:00,  1.79it/s]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[2:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n-\n\nX_train1 = np.array(interpolated_signal).squeeze()\nX_test1 =  np.array(fiveVTS_test).squeeze()\n\n\n%R -i X_train1\n\n\n%%R\nanswer <- GNARfit(vts = X_train1, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nprediction <- predict(answer,n.ahead=40)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((X_test1 - gnar_test)**2).mean(axis=0)\ntest_mse_total_gnar = ((X_test1 - gnar_test)**2).mean()\n\n\n\nê²°ê³¼ì‹œê°í™”\n\ntrain_mse_total_gnar,test_mse_total_gnar\n\n(0.7594163080873634, 1.2656937545324825)\n\n\n\nfig = plot(fiveVTS,'--.',h=4,color='gray',label='complete data',alpha=0.5)\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],fiveVTS_train[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(2,160),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(162,200),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(2,160),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(162,200),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(2,160),gnar_train[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(162,202),gnar_test[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario3: \\n missing=80% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2022-12-07-torchgcn.html",
    "href": "posts/GCN/2022-12-07-torchgcn.html",
    "title": "TORCH_GEOMETRIC.NN",
    "section": "",
    "text": "221207\nhttps://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html\n\nimport torch\nfrom torch_geometric.data import Data\n\n\nedge_index = torch.tensor([[0, 1, 1, 2],\n                           [1, 0, 2, 1]], dtype=torch.long)\nx = torch.tensor([[-1], [0], [1]], dtype=torch.float)\ndata = Data(x=x, edge_index=edge_index)\n\n\ndata\n\nData(x=[3, 1], edge_index=[2, 4])\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n\nG=nx.Graph()\nG.add_node('0')\nG.add_node('1')\nG.add_node('2')\nG.add_edge('0','1')\nG.add_edge('1','2')\npos = {}\npos['0'] = (0,0)\npos['1'] = (1,1)\npos['2'] = (2,0)\nnx.draw(G,pos,with_labels=True)\nplt.show()\n\n\n\n\n\nfrom torch.nn import Linear, ReLU\nfrom torch_geometric.nn import Sequential, GCNConv\n\nex\nmodel = Sequential('x, edge_index', [\n    (GCNConv(in_channels, 64), 'x, edge_index -> x'),\n    ReLU(inplace=True),\n    (GCNConv(64, 64), 'x, edge_index -> x'),\n    ReLU(inplace=True),\n    Linear(64, out_channels),\n])\n\nmodel = Sequential('x, edge_index', [\n    (GCNConv(3, 64), 'x, edge_index -> x'),\n    ReLU(inplace=True),\n    (GCNConv(64, 64), 'x, edge_index -> x'),\n    ReLU(inplace=True),\n    Linear(64, 3),\n])\n\n\nmodel(x,edge_index)\n\n\nfrom torch.nn import Linear, ReLU, Dropout\nfrom torch_geometric.nn import Sequential, GCNConv, JumpingKnowledge\nfrom torch_geometric.nn import global_mean_pool\n\nmodel = Sequential('x, edge_index, batch', [\n    (Dropout(p=0.5), 'x -> x'),\n    (GCNConv(dataset.num_features, 64), 'x, edge_index -> x1'),\n    ReLU(inplace=True),\n    (GCNConv(64, 64), 'x1, edge_index -> x2'),\n    ReLU(inplace=True),\n    (lambda x1, x2: [x1, x2], 'x1, x2 -> xs'),\n    (JumpingKnowledge(\"cat\", 64, num_layers=2), 'xs -> x'),\n    (global_mean_pool, 'x, batch -> x'),\n    Linear(2 * 64, dataset.num_classes),\n])\n\nmodel = Sequential('x, edge_index, batch', [\n    (Dropout(p=0.5), 'x -> x'),\n    (GCNConv(dataset.num_features, 64), 'x, edge_index -> x1'),\n    ReLU(inplace=True),\n    (GCNConv(64, 64), 'x1, edge_index -> x2'),\n    ReLU(inplace=True),\n    (lambda x1, x2: [x1, x2], 'x1, x2 -> xs'),\n    (JumpingKnowledge(\"cat\", 64, num_layers=2), 'xs -> x'),\n    (global_mean_pool, 'x, batch -> x'),\n    Linear(2 * 64, dataset.num_classes),\n])\n\n\ntorch_geometric.nn.Linear()"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html",
    "href": "posts/GCN/2023-01-21-Class.html",
    "title": "Class of Method",
    "section": "",
    "text": "Class"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#mean",
    "href": "posts/GCN/2023-01-21-Class.html#mean",
    "title": "Class of Method",
    "section": "Mean",
    "text": "Mean\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero.train_mean\nc = ___zero.number\nd = train_X_mean\nf = train_y_mean\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n\n\n3282.4832243919373\n\n\n\nmean_mse100_10 = pd.DataFrame(_mse)\n\n\nmean_mae100_10 = pd.DataFrame(_mae)\n\n\n_train_result_mean10 = _train_result.copy()\n\n\n_test_result_mean10 = _test_result.copy()\n\n\nplt.plot(mean_mse100_10.T);\n\n\n\n\n\nplt.plot(mean_mae100_10.T);\n\n\n\n\n\nvis2(_zero.train_mean,_train_result[59]);\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result[0]);"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#linear",
    "href": "posts/GCN/2023-01-21-Class.html#linear",
    "title": "Class of Method",
    "section": "Linear",
    "text": "Linear\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero.second_linear\nc = ___zero.number\nd = train_X_linear\nf = train_y_linear\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.83it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n\n\n3295.478456020355\n\n\n\nlinear_mse100_10 = pd.DataFrame(_mse)\n\n\nlinear_mae100_10 = pd.DataFrame(_mae)\n\n\n_train_result_linear10 = _train_result.copy()\n\n\n_test_result_linear10 = _test_result.copy()\n\n\nplt.plot(linear_mse100_10.T);\n\n\n\n\n\nplt.plot(linear_mae100_10.T);\n\n\n\n\n\nvis2(_zero.train_mean,_train_result_linear10[59]);\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_linear10[0]);"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#mean-1",
    "href": "posts/GCN/2023-01-21-Class.html#mean-1",
    "title": "Class of Method",
    "section": "Mean",
    "text": "Mean\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero20.train_mean\nc = ___zero20.number\nd = train_X_mean\nf = train_y_mean\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:52<00:00,  1.89it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:52<00:00,  1.89it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n\n\n3277.0478909015656\n\n\n\nmean_mse100_20 = pd.DataFrame(_mse)\n\n\nmean_mae100_20 = pd.DataFrame(_mae)\n\n\n_train_result_mean20 = _train_result.copy()\n\n\n_test_result_mean20 = _test_result.copy()\n\n\nplt.plot(mean_mse100_20.T);\n\n\n\n\n\nplt.plot(mean_mae100_20.T);\n\n\n\n\n\nvis2(___zero20.train_mean,_train_result_mean20[59]);\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_mean20[0]);\n\n\n\n\n\nshow_lrpr(fiveVTS_test[1:,0],fiveVTS_test[1:,1],fiveVTS_test[1:,2],fiveVTS_test[1:,3],fiveVTS_test[1:,4],_test_result_mean20)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#linear-1",
    "href": "posts/GCN/2023-01-21-Class.html#linear-1",
    "title": "Class of Method",
    "section": "Linear",
    "text": "Linear\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero20.second_linear\nc = ___zero20.number\nd = train_X_linear\nf = train_y_linear\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.83it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n\n\n3298.6988050937653\n\n\n\nlinear_mse100_20 = pd.DataFrame(_mse)\n\n\nlinear_mae100_20 = pd.DataFrame(_mae)\n\n\n_train_result_linear20 = _train_result.copy()\n\n\n_test_result_linear20 = _test_result.copy()\n\n\nplt.plot(linear_mse100_20.T);\n\n\n\n\n\nplt.plot(linear_mae100_20.T);\n\n\n\n\n\nvis2(___zero20.train_mean,_train_result_linear20[59]);\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_linear20[0]);\n\n\n\n\n\nshow_lrpr(fiveVTS_test[1:,0],fiveVTS_test[1:,1],fiveVTS_test[1:,2],fiveVTS_test[1:,3],fiveVTS_test[1:,4],_test_result_linear20)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#mean-2",
    "href": "posts/GCN/2023-01-21-Class.html#mean-2",
    "title": "Class of Method",
    "section": "Mean",
    "text": "Mean\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero30.train_mean\nc = ___zero30.number\nd = train_X_mean\nf = train_y_mean\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n\n\n3280.9767186641693\n\n\n\nmean_mse_30 = pd.DataFrame(_mse)\n\n\nmean_mae_30 = pd.DataFrame(_mae)\n\n\n_train_result_mean30 = _train_result.copy()\n\n\n_test_result_mean30 = _test_result.copy()\n\n\nplt.plot(mean_mse_30.T);\n\n\n\n\n\nplt.plot(mean_mae_30.T);\n\n\n\n\n\nvis2(___zero30.train_mean,_train_result_mean30[59]);\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_mean30[0]);\n\n\n\n\n\nshow_lrpr(fiveVTS_test[1:,0],fiveVTS_test[1:,1],fiveVTS_test[1:,2],fiveVTS_test[1:,3],fiveVTS_test[1:,4],_test_result_mean30)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#linear-2",
    "href": "posts/GCN/2023-01-21-Class.html#linear-2",
    "title": "Class of Method",
    "section": "Linear",
    "text": "Linear\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero30.second_linear\nc = ___zero30.number\nd = train_X_linear\nf = train_y_linear\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.83it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.83it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.83it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.83it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.83it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.83it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.83it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n\n\n3305.375289440155\n\n\n\nlinear_mse_30 = pd.DataFrame(_mse)\n\n\nlinear_mae_30 = pd.DataFrame(_mae)\n\n\n_train_result_linear30 = _train_result.copy()\n\n\n_test_result_linear30 = _test_result.copy()\n\n\nplt.plot(linear_mse_30.T);\n\n\n\n\n\nplt.plot(linear_mae_30.T);\n\n\n\n\n\nvis2(___zero30.train_mean,_train_result_linear30[59]);\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_linear30[0]);\n\n\n\n\n\nshow_lrpr(fiveVTS_test[1:,0],fiveVTS_test[1:,1],fiveVTS_test[1:,2],fiveVTS_test[1:,3],fiveVTS_test[1:,4],_test_result_linear30)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#mean-3",
    "href": "posts/GCN/2023-01-21-Class.html#mean-3",
    "title": "Class of Method",
    "section": "Mean",
    "text": "Mean\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero40.train_mean\nc = ___zero40.number\nd = train_X_mean\nf = train_y_mean\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.88it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n\n\n3287.529237985611\n\n\n\nmean_mse_40 = pd.DataFrame(_mse)\n\n\nmean_mae_40 = pd.DataFrame(_mae)\n\n\n_train_result_mean40 = _train_result.copy()\n\n\n_test_result_mean40 = _test_result.copy()\n\n\nplt.plot(mean_mse_40.T);\n\n\n\n\n\nplt.plot(mean_mae_40.T);\n\n\n\n\n\nvis2(___zero40.train_mean,_train_result_mean40[59]);\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_mean40[0]);\n\n\n\n\n\nshow_lrpr(fiveVTS_test[1:,0],fiveVTS_test[1:,1],fiveVTS_test[1:,2],fiveVTS_test[1:,3],fiveVTS_test[1:,4],_test_result_mean40)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#linear-3",
    "href": "posts/GCN/2023-01-21-Class.html#linear-3",
    "title": "Class of Method",
    "section": "Linear",
    "text": "Linear\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero40.second_linear\nc = ___zero40.number\nd = train_X_linear\nf = train_y_linear\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.83it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.83it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.83it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.82it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.83it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.83it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.83it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.86it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.83it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:54<00:00,  1.84it/s]\n\n\n3303.96302652359\n\n\n\nlinear_mse_40 = pd.DataFrame(_mse)\n\n\nlinear_mae_40 = pd.DataFrame(_mae)\n\n\n_train_result_linear40 = _train_result.copy()\n\n\n_test_result_linear40 = _test_result.copy()\n\n\nplt.plot(linear_mse_40.T);\n\n\n\n\n\nplt.plot(linear_mae_40.T);\n\n\n\n\n\nvis2(___zero40.train_mean,_train_result_linear40[59]);\n\n\n\n\n\nvis2(fiveVTS_test[1:],_test_result_linear40[0]);\n\n\n\n\n\nshow_lrpr(fiveVTS_test[1:,0],fiveVTS_test[1:,1],fiveVTS_test[1:,2],fiveVTS_test[1:,3],fiveVTS_test[1:,4],_test_result_linear40)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#mean-4",
    "href": "posts/GCN/2023-01-21-Class.html#mean-4",
    "title": "Class of Method",
    "section": "Mean",
    "text": "Mean\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero50.train_mean\nc = ___zero50.number\nd = train_X_mean\nf = train_y_mean\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n\nmean_mse_50 = pd.DataFrame(_mse)\n\n\nmean_mae_50 = pd.DataFrame(_mae)\n\n\n_train_result_mean50 = _train_result.copy()\n\n\n_test_result_mean50 = _test_result.copy()\n\n\nplt.plot(mean_mse_50.T);\n\n\nplt.plot(mean_mae_50.T);\n\n\nvis2(_zero.train_mean,_train_result_mean50[59]);\n\n\nvis2(fiveVTS_test[1:],_test_result_mean50[0]);"
  },
  {
    "objectID": "posts/GCN/2023-01-21-Class.html#linear-4",
    "href": "posts/GCN/2023-01-21-Class.html#linear-4",
    "title": "Class of Method",
    "section": "Linear",
    "text": "Linear\n\nt1= time.time()\nttt = 160\n_mse = []\n_mae = []\n_train_result = []\n_test_result = []\nb = ___zero50.second_linear\nc = ___zero50.number\nd = train_X_linear\nf = train_y_linear\nfor i in range(60):\n    a = Method(b,ttt,c,d,f)\n    a.FT()\n    _mse.append(a.mse)\n    _mae.append(a.mae)\n    _train_result.append(a.train_result)\n    _test_result.append(a.test_result)\n    b = a.FT_result\nt2 = time.time()\nt2-t1\n\n\nlinear_mse_50 = pd.DataFrame(_mse)\n\n\nlinear_mae_50 = pd.DataFrame(_mae)\n\n\n_train_result_linear50 = _train_result.copy()\n\n\n_test_result_linear50 = _test_result.copy()\n\n\nplt.plot(linear_mse_50.T);\n\n\nplt.plot(linear_mae_50.T);\n\n\nvis2(_zero.train_mean,_train_result_linear50[59]);\n\n\nvis2(fiveVTS_test[1:],_test_result_linear50[0]);"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html",
    "title": "1st ITSTGCN",
    "section": "",
    "text": "GNAR fiveNet,fivenodes lag 1"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#stgcn",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#stgcn",
    "title": "1st ITSTGCN",
    "section": "STGCN",
    "text": "STGCN\n\nclass STGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            for epoch in range(50):\n                for time, (xt,yt) in enumerate(zip(X,y)):\n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_stgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_stgcn = (((self.yy-yyhat).squeeze())**2).mean() \n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset, \n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_stgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_stgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#enhencement-of-stgcn",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#enhencement-of-stgcn",
    "title": "1st ITSTGCN",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nclass ESTGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n    \n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            signal = interpolated_signal.copy()\n            for epoch in range(50):\n                signal = update_from_freq_domain(signal,missing_index)\n                X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n                y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n                for time, (xt,yt) in enumerate(zip(X,y)):        \n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])               \n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_estgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_estgcn = (((self.yy-yyhat).squeeze())**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_estgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_estgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#gnar",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#gnar",
    "title": "1st ITSTGCN",
    "section": "GNAR",
    "text": "GNAR\n\nm = robjects.r.matrix(FloatVector([0,0,0,1,1,0,0,1,1,0,0,1,0,1,0,1,1,1,0,0,1,0,0,0,0]), nrow = 5, ncol = 5)\n\n\nclass GNAR_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-2),:,:]\n\n            answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = 160, ncol = 5),net = GNAR.matrixtoGNAR(m), alphaOrder = 2, betaOrder = FloatVector([1, 1]))             \n            predict = GNAR.predict_GNARfit(answer,n_ahead=40)\n\n\n            train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,5))**2).mean()\n            test_mse_total_gnar = ((self.yy.squeeze() - pd.DataFrame(predict).values.reshape(-1,5)[:-1,:])**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_gnar.tolist()\n            df_row['MSE_test'] = test_mse_total_gnar.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#stgcn-1",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#stgcn-1",
    "title": "1st ITSTGCN",
    "section": "STGCN",
    "text": "STGCN\n\nDataset = 'fivenodes'\nMethod = 'stgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_stgcn= pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    stgcn = STGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    stgcn.iter()\n    df_add = stgcn.df.copy()\n    df_stgcn = pd.concat([df_stgcn,df_add],axis=0)\n\n\nsave_data(df_stgcn, './data/GNAR_stgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#enhencement-of-stgcn-1",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#enhencement-of-stgcn-1",
    "title": "1st ITSTGCN",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nDataset = 'fivenodes'\nMethod = 'estgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_estgcn = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    estgcn = ESTGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    estgcn.iter()\n    df_add = estgcn.df.copy()\n    df_estgcn = pd.concat([df_estgcn,df_add],axis=0)\n\n\nsave_data(df_estgcn, './data/GNAR_estgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#gnar-1",
    "href": "posts/GCN/2023-02-20-ESTGCN_GNAR_edit_guebin.html#gnar-1",
    "title": "1st ITSTGCN",
    "section": "GNAR",
    "text": "GNAR\n\nDataset = 'fivenodes'\nMethod = 'gnar' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = None # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_gnar = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    gnar = GNAR_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    gnar.iter()\n    df_add = gnar.df.copy()\n    df_gnar = pd.concat([df_gnar,df_add],axis=0)\n\n\nsave_data(df_gnar, './data/GANR_gnar_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_6.html",
    "href": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_6.html",
    "title": "GNAR lag 1 Randomly Missing comparison Table by Missing Rate",
    "section": "",
    "text": "GNAR fiveNet,fivenodes lag 1"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_6.html#stgcn",
    "href": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_6.html#stgcn",
    "title": "GNAR lag 1 Randomly Missing comparison Table by Missing Rate",
    "section": "STGCN",
    "text": "STGCN\n\nclass STGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            for epoch in range(50):\n                for time, (xt,yt) in enumerate(zip(X,y)):\n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_stgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_stgcn = (((self.yy-yyhat).squeeze())**2).mean() \n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset, \n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_stgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_stgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_6.html#enhencement-of-stgcn",
    "href": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_6.html#enhencement-of-stgcn",
    "title": "GNAR lag 1 Randomly Missing comparison Table by Missing Rate",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nclass ESTGCN_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.XX = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[:-1,:,:]).float()\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n\n        self.real_y = torch.tensor(fiveVTS_train).reshape(int(T*0.8),N,1).float()[1:,:,:]\n        for i in range(self.iterable):\n    \n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n            y = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            signal = interpolated_signal.copy()\n            for epoch in range(50):\n                signal = update_from_freq_domain(signal,missing_index)\n                X = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-1),:,:]\n                y = torch.tensor(signal).reshape(int(T*0.8),N,1).float()[1:,:,:]\n                for time, (xt,yt) in enumerate(zip(X,y)):        \n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])               \n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_estgcn = (((self.real_y-yhat).squeeze())**2).mean()\n            test_mse_total_estgcn = (((self.yy-yyhat).squeeze())**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_estgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_estgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_6.html#gnar",
    "href": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_6.html#gnar",
    "title": "GNAR lag 1 Randomly Missing comparison Table by Missing Rate",
    "section": "GNAR",
    "text": "GNAR\n\nm = robjects.r.matrix(FloatVector([0,0,0,1,1,0,0,1,1,0,0,1,0,1,0,1,1,1,0,0,1,0,0,0,0]), nrow = 5, ncol = 5)\n\n\nclass GNAR_Missing:\n    def __init__(self,Dataset,df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n    def iter(self):\n        self.yy = torch.tensor(fiveVTS_test.reshape(int(T*0.2),N,1)[1:,:,:]).float()\n        for i in range(self.iterable):\n\n            _zero = Missing(fiveVTS_train)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-2),:,:]\n\n            answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = 160, ncol = 5),net = GNAR.matrixtoGNAR(m), alphaOrder = 2, betaOrder = FloatVector([1, 1]))             \n            predict = GNAR.predict_GNARfit(answer,n_ahead=40)\n\n\n            train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,5))**2).mean()\n            test_mse_total_gnar = ((self.yy.squeeze() - pd.DataFrame(predict).values.reshape(-1,5)[:-1,:])**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_gnar.tolist()\n            df_row['MSE_test'] = test_mse_total_gnar.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_6.html#stgcn-1",
    "href": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_6.html#stgcn-1",
    "title": "GNAR lag 1 Randomly Missing comparison Table by Missing Rate",
    "section": "STGCN",
    "text": "STGCN\n\nDataset = 'fivenodes'\nMethod = 'stgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_stgcn= pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    stgcn = STGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    stgcn.iter()\n    df_add = stgcn.df.copy()\n    df_stgcn = pd.concat([df_stgcn,df_add],axis=0)\n\n\nsave_data(df_stgcn, './data/GNAR_stgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_6.html#enhencement-of-stgcn-1",
    "href": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_6.html#enhencement-of-stgcn-1",
    "title": "GNAR lag 1 Randomly Missing comparison Table by Missing Rate",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nDataset = 'fivenodes'\nMethod = 'estgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_estgcn = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    estgcn = ESTGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    estgcn.iter()\n    df_add = estgcn.df.copy()\n    df_estgcn = pd.concat([df_estgcn,df_add],axis=0)\n\n\nsave_data(df_estgcn, './data/GNAR_estgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_6.html#gnar-1",
    "href": "posts/GCN/2023-02-15-ESTGCN_GNAR_DATA_6.html#gnar-1",
    "title": "GNAR lag 1 Randomly Missing comparison Table by Missing Rate",
    "section": "GNAR",
    "text": "GNAR\n\nDataset = 'fivenodes'\nMethod = 'gnar' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 1 # 1,2,3,4 ... \nNumber_of_filters = None # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_gnar = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    gnar = GNAR_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    gnar.iter()\n    df_add = gnar.df.copy()\n    df_gnar = pd.concat([df_gnar,df_add],axis=0)\n\n\nsave_data(df_gnar, './data/GANR_gnar_randomly_by_rate.pkl')\n\n\n_zero = Missing(fiveVTS_train)\n_zero.miss(percent = 0.5)\n_zero.second_linear()\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\nX = torch.tensor(interpolated_signal).reshape(int(T*0.8),N,1).float()[:int(T*0.8-2),:,:]\n\n\nanswer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = 160, ncol = 5),net = GNAR.matrixtoGNAR(m), alphaOrder = 2, betaOrder = FloatVector([1, 1]))             \npredict = GNAR.predict_GNARfit(answer,n_ahead=40)\n\n\nanswer\n\n\n\n        ListVector with 4 elements.\n        \n        \n        \n          \n            \n            mod\n            \n            \n             [RTYPES.VECSXP]\n            \n          \n        \n          \n            \n            y\n            \n            \n             [RTYPES.REALSXP]\n            \n          \n        \n          \n            \n            dd\n            \n            \n             [RTYPES.REALSXP]\n            \n          \n        \n          \n            \n            frbic\n            \n            \n             [RTYPES.VECSXP]"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.html",
    "href": "posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.html",
    "title": "WikiMath lag 4 Randomly Missing comparison Table",
    "section": "",
    "text": "ST-GCN Dataset WikiMathsDatasetLoader"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.html#train",
    "href": "posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.html#train",
    "title": "WikiMath lag 4 Randomly Missing comparison Table",
    "section": "Train",
    "text": "Train\n\ndata_train=[]\nfor time, snapshot in enumerate(train_dataset):\n    data_train.append([time,snapshot])\n\n\ndata_train[0][1].x.shape,data_train[0][1].y.shape,data_train[0][1].edge_index.shape,data_train[0][1].edge_attr.shape\n\n(torch.Size([1068, 4]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n580\n\n\n\nT_train = time\nN = len(data_train[0][1].x)\n\n\nedge_index = data_train[0][1].edge_index\nedge_attr = data_train[0][1].edge_attr\n\n\nx_train = []\nfor i in range(time):\n    x_train.append(data_train[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_train = data_tensor.reshape(time,1068,-1)\nx_train.shape\n\ntorch.Size([580, 1068, 4])\n\n\n\ny_train = []\nfor i in range(time):\n    y_train.append(data_train[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_train = data_tensor.reshape(time,1068)\ny_train.shape\n\ntorch.Size([580, 1068])\n\n\n\nx_train.shape, y_train.shape\n\n(torch.Size([580, 1068, 4]), torch.Size([580, 1068]))"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.html#test",
    "href": "posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.html#test",
    "title": "WikiMath lag 4 Randomly Missing comparison Table",
    "section": "Test",
    "text": "Test\n\ndata_test=[]\nfor time, snapshot in enumerate(test_dataset):\n    data_test.append([time,snapshot])\n\n\ndata_test[0][1].x.shape,data_test[0][1].y.shape,data_test[0][1].edge_index.shape,data_test[0][1].edge_attr.shape\n\n(torch.Size([1068, 4]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n145\n\n\n\nT_test = time\n\n\nx_test = []\nfor i in range(time):\n    x_test.append(data_test[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_test = data_tensor.reshape(time,1068,-1)\nx_test.shape\n\ntorch.Size([145, 1068, 4])\n\n\n\ny_test = []\nfor i in range(time):\n    y_test.append(data_test[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_test = data_tensor.reshape(time,1068)\ny_test.shape\n\ntorch.Size([145, 1068])\n\n\n\nx_test.shape, y_test.shape\n\n(torch.Size([145, 1068, 4]), torch.Size([145, 1068]))"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.html#stgcn",
    "href": "posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.html#stgcn",
    "title": "WikiMath lag 4 Randomly Missing comparison Table",
    "section": "STGCN",
    "text": "STGCN\n\nclass STGCN_Missing:\n    def __init__(self,Dataset, df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n        self.iterable = iterable\n    def iter(self):\n        self.XX = x_test\n        self.yy = y_test\n\n        self.real_y = y_train\n        for i in range(self.iterable):\n    \n            _zero = Missing(x_train_f)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(np.stack([interpolated_signal[j:(T_train+j),:] for j in range(self.lag)],axis = -1)).float()\n            y = torch.tensor(interpolated_signal[self.lag:,:]).float()\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            for epoch in range(50):\n                for time, (xt,yt) in enumerate(zip(X,y)):\n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_stgcn = (((self.real_y-yhat.squeeze()).squeeze())**2).mean()\n            test_mse_total_stgcn = (((self.yy-yyhat.squeeze()).squeeze())**2).mean()\n    \n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_stgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_stgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.html#enhencement-of-stgcn",
    "href": "posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.html#enhencement-of-stgcn",
    "title": "WikiMath lag 4 Randomly Missing comparison Table",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nclass ESTGCN_Missing:\n    def __init__(self,Dataset, df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n        self.iterable = iterable\n    def iter(self):\n        self.XX = x_test\n        self.yy = y_test\n\n        self.real_y = y_train\n        for i in range(self.iterable):\n    \n            _zero = Missing(x_train_f)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            signal = interpolated_signal.copy()\n            for epoch in range(50):\n                signal = update_from_freq_domain(signal,missing_index)\n                X = torch.tensor(np.stack([signal[i:(T_train+epoch+i),:] for i in range(self.lag)],axis = -1)).reshape(-1,N,self.lag).float()\n                y = torch.tensor(signal).reshape(-1,N,1).float()[self.lag:,:,:]\n                for time, (xt,yt) in enumerate(zip(X,y)):        \n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().reshape(1,-1)]).squeeze()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_estgcn = (((self.real_y-yhat[:T_train,:].squeeze()).squeeze())**2).mean()\n            test_mse_total_estgcn = (((self.yy-yyhat.squeeze()).squeeze())**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_estgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_estgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.html#gnar",
    "href": "posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.html#gnar",
    "title": "WikiMath lag 4 Randomly Missing comparison Table",
    "section": "GNAR",
    "text": "GNAR\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\n\nR[write to console]: Loading required package: igraph\n\nR[write to console]: \nAttaching package: â€˜igraphâ€™\n\n\nR[write to console]: The following objects are masked from â€˜package:statsâ€™:\n\n    decompose, spectrum\n\n\nR[write to console]: The following object is masked from â€˜package:baseâ€™:\n\n    union\n\n\nR[write to console]: Loading required package: wordcloud\n\nR[write to console]: Loading required package: RColorBrewer\n\n\n\n\nGNAR = importr('GNAR') # import GNAR \nigraph = importr('igraph') # import igraph \n\n\nw=np.zeros((N,N))\n\n\nfor k in range(len(edge_index[0])):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\nm = robjects.r.matrix(FloatVector(w), nrow = N, ncol = N)\n\n\nclass GNAR_Missing:\n    def __init__(self,Dataset, df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n        self.iterable = iterable\n    def iter(self):\n        self.yy = torch.tensor(y_test).float()\n        for i in range(self.iterable):\n\n            _zero = Missing(x_train_f)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(np.stack([interpolated_signal[i:(T_train-self.lag+i),:] for i in range(self.lag)],axis = -1)).float()\n\n            answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = T_train, ncol = N),net = GNAR.matrixtoGNAR(m), alphaOrder = 4, betaOrder = FloatVector([1, 1, 1, 1]))             \n            predict = GNAR.predict_GNARfit(answer,n_ahead=T_test)\n\n\n            train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,N))**2).mean()\n            test_mse_total_gnar = ((self.yy - pd.DataFrame(predict).values.reshape(-1,N))**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_gnar.tolist()\n            df_row['MSE_test'] = test_mse_total_gnar.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.html#stgcn-1",
    "href": "posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.html#stgcn-1",
    "title": "WikiMath lag 4 Randomly Missing comparison Table",
    "section": "STGCN",
    "text": "STGCN\n\nDataset = 'WikiMaths'\nMethod = 'stgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 4 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 30\n\n\ndf_stgcn= pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    stgcn = STGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    stgcn.iter()\n    df_add = stgcn.df.copy()\n    df_stgcn = pd.concat([df_stgcn,df_add],axis=0)\n\n\nsave_data(df_stgcn, './data/Wiki_stgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.html#enhencement-of-stgcn-1",
    "href": "posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.html#enhencement-of-stgcn-1",
    "title": "WikiMath lag 4 Randomly Missing comparison Table",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nDataset = 'WikiMaths'\nMethod = 'estgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 4 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 30\n\n\ndf_estgcn = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    estgcn = ESTGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    estgcn.iter()\n    df_add = estgcn.df.copy()\n    df_estgcn = pd.concat([df_estgcn,df_add],axis=0)\n\n\nsave_data(df_estgcn, './data/Wiki_estgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.html#gnar-1",
    "href": "posts/GCN/2023-02-15-ESTGCN_WIKI_DATA_4.html#gnar-1",
    "title": "WikiMath lag 4 Randomly Missing comparison Table",
    "section": "GNAR",
    "text": "GNAR\n\nDataset = 'WikiMaths'\nMethod = 'gnar' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 4 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 30\n\n\ndf_gnar = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    gnar = GNAR_Missing(Dataset, df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    gnar.iter()\n    df_add = gnar.df.copy()\n    df_gnar = pd.concat([df_gnar,df_add],axis=0)\n\n\nsave_data(df_gnar, './data/Wiki_gnar_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2022-12-28-gcn_simulation.html",
    "href": "posts/GCN/2022-12-28-gcn_simulation.html",
    "title": "Simulation of geometric-temporal",
    "section": "",
    "text": "Simulation\n\nhttps://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/dataset.html#module-torch_geometric_temporal.dataset.chickenpox\n\nimport\n\nimport networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\n\n\n\nê³µì‹ í™ˆí˜ì´ì§€ ì˜ˆì œ\n\ndata\n\nfrom torch_geometric_temporal.dataset import WikiMathsDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset(lags=14)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)\n\n\n\nRecurrentGCN\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvGRU\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvGRU(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n\n\n\nLearn\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=14, filters=32)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(1)):\n    for time, snapshot in enumerate(train_dataset):\n        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((y_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:11<00:00, 11.93s/it]\n\n\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([1068, 14])\n\n\n\nsnapshot.y.shape\n\ntorch.Size([1068])\n\n\n\n1068ê°œì˜ nodes\ní•œ ê°œì˜ nodeì— mappingëœ ì°¨ì›ì˜ ìˆ˜\n\n\n_edge_index.shape\n\ntorch.Size([2, 27079])\n\n\n\n_edge_attr.shape\n\ntorch.Size([27079])\n\n\n\n_y.shape\n\ntorch.Size([1068])\n\n\n\n\n\nìš°ë¦¬ ì˜ˆì œ\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\n\n\nT = 100\nN = 4 # number of Nodes\nE = np.array([[0,1],[1,2],[2,3],[3,0]]).T\nV = np.array([1,2,3,4])\nAMP = np.array([3,2,1,2.2])\nt = np.arange(0,T)\nnode_features = 1\n\n\nf = np.stack([a*np.sin(2*t**2/1000)+np.random.normal(loc=0,scale=0.2,size=T) for a in AMP],axis=1).reshape(T,N,node_features)\nf = torch.tensor(f).float()\n\n\nf.shape\n\ntorch.Size([100, 4, 1])\n\n\n\nX = f[:99,:,:]\ny = f[1:,:,:]\n\n\nplt.plot(y[:,0,0],label=\"v1\")\nplt.plot(y[:,1,0],label=\"v2\")\nplt.plot(y[:,2,0],label=\"v3\")\nplt.plot(y[:,3,0],label=\"v4\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efc48673490>\n\n\n\n\n\n\nedge_index = torch.tensor(E)\nedge_attr = torch.tensor(np.array([1,1,1,1]),dtype=torch.float32)\n\n\n_ee = enumerate(zip(X,y))\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:16<00:00,  3.01it/s]\n\n\n\nyhat = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nplt.plot(y[:,0,0],label=\"y in V1\")\nplt.plot(yhat[:,0,0],label=\"yhat in V1\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efc48524730>\n\n\n\n\n\n\nplt.plot(y[:,1,0],label=\"y in V2\")\nplt.plot(yhat[:,1,0],label=\"yhat in V2\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efc4849c730>\n\n\n\n\n\n\nplt.plot(y[:,2,0],label=\"y in V3\")\nplt.plot(yhat[:,2,0],label=\"yhat in V3\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efc484098e0>\n\n\n\n\n\n\nplt.plot(y[:,3,0],label=\"y in V4\")\nplt.plot(yhat[:,3,0],label=\"yhat in V4\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efc483f5880>\n\n\n\n\n\n\n\nGNAR\n\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr\n\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\n\nR[write to console]: Loading required package: igraph\n\nR[write to console]: \nAttaching package: â€˜igraphâ€™\n\n\nR[write to console]: The following objects are masked from â€˜package:statsâ€™:\n\n    decompose, spectrum\n\n\nR[write to console]: The following object is masked from â€˜package:baseâ€™:\n\n    union\n\n\nR[write to console]: Loading required package: wordcloud\n\nR[write to console]: Loading required package: RColorBrewer\n\n\n\n\n%%R\nsummary(fiveNet)\n\nGNARnet with 5 nodes and 10 edges\n of equal length  1\n\n\n\n%%R\nedges <- as.matrix(fiveNet)\nedges\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    1    1\n[2,]    0    0    1    1    0\n[3,]    0    1    0    1    0\n[4,]    1    1    1    0    0\n[5,]    1    0    0    0    0\n\n\n\n%%R\nprint(fiveNet)\n\nGNARnet with 5 nodes \nedges:1--4 1--5 2--3 2--4 3--2 3--4 4--1 4--2 4--3 5--1 \n     \n edges of each of length  1 \n\n\n\n%%R\ndata(\"fiveNode\")\nanswer <- GNARfit(vts = fiveVTS, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nanswer\n\nModel: \nGNAR(2,[1,1]) \n\nCall:\nlm(formula = yvec ~ dmat + 0)\n\nCoefficients:\n dmatalpha1  dmatbeta1.1   dmatalpha2  dmatbeta2.1  \n    0.20624      0.50277      0.02124     -0.09523  \n\n\n\n\n%%R\nlayout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))\nplot(fiveVTS[, 1], ylab = \"Node A Time Series\")\nlines(fitted(answer)[, 1], col = 2)\nplot(fiveVTS[, 2], ylab = \"Node B Time Series\")\nlines(fitted(answer)[, 2], col = 2)\nplot(fiveVTS[, 3], ylab = \"Node C Time Series\")\nlines(fitted(answer)[, 3], col = 2)\nplot(fiveVTS[, 4], ylab = \"Node D Time Series\")\nlines(fitted(answer)[, 4], col = 2)\n\n\n\n\n\n%R -o fiveVTS\n%R -o edges\n\n\nnode: 5\ntime 200\n\n\nedges_tensor = torch.tensor(edges)\n\n\nnonzero_indices = edges_tensor.nonzero()\n\n\nfiveNet_edge = np.array(nonzero_indices).T\nfiveNet_edge\n\narray([[0, 0, 1, 1, 2, 2, 3, 3, 3, 4],\n       [3, 4, 2, 3, 1, 3, 0, 1, 2, 0]])\n\n\n\nfiveVTS.shape\n\n(200, 5)\n\n\n\nT = 200\nN = 5 # number of Nodes\nE = fiveNet_edge\nV = np.array([1,2,3,4,5])\nt = np.arange(0,T)\nnode_features = 1\n\n\nf = torch.tensor(fiveVTS).reshape(200,5,1).float()\n\n\nX = f[:199,:,:]\ny = f[1:,:,:]\n\n\nedge_index = torch.tensor(E)\nedge_attr = torch.tensor(np.array([1,1,1,1,1,1,1,1,1,1]),dtype=torch.float32)\n\n\n_ee = enumerate(zip(X,y))\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=1, filters=8)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆ| 50/50 [00:34<00:00,  1.45it/\n\n\n\nyhat = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nyhat.shape\n\n(199, 5, 1)\n\n\n\nplt.plot(y[:,1])\nplt.plot(yhat[:,1].data)\n\n\n\n\n\n\nWind network time series\nthe data suite vswind that contains a number of R objects pertaining to 721 wind speeds taken at each of 102 weather stations in England and Wales. The suite contains the vector time series vswindts, the associated network vswindnet, a character vector of the weather station location names in vswindnames and coordinates of the stations in the two column matrix vswindcoords. The data originate from the UK Met Office site http://wow.metoffice.gov.uk and full details can be found in the vswind help file in the GNAR package.\n\n%%R\noldpar <- par(cex = 0.75)\nwindnetplot()\npar(oldpar)\n\n\n\n\n\n%%R\nedges_wind <- as.matrix(vswindnet)\n\n\n%R -o vswindts\n%R -o edges_wind\n\n\nnodes : 102\ntime step : 721\n\n\nvswindts.shape\n\n(721, 102)\n\n\n\nedges_wind.shape\n\n(102, 102)\n\n\n\nedges_winds = torch.tensor(edges_wind)\n\n\nnonzero_indices_wind = edges_winds.nonzero()\n\n\nvswindnet_edge = np.array(nonzero_indices_wind).T\nvswindnet_edge.shape\n\n(2, 202)\n\n\n\nT = 721\nN = 102 # number of Nodes\nE = vswindnet_edge\nV = np.array(range(101))\nt = np.arange(0,T)\nnode_features = 1\n\n\nf = torch.tensor(vswindts).reshape(721,102,1).float()\n\n\nX = f[:720,:,:]\ny = f[1:,:,:]\n\n\nedge_index = torch.tensor(E)\nedge_attr = torch.tensor(np.array([1]*202),dtype=torch.float32)\n\n\n_ee = enumerate(zip(X,y))\n\n\nfrom tqdm import tqdm\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:16<00:00,  2.73s/it]\n\n\n\nyhat = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nyhat.shape\n\n(720, 102, 1)\n\n\n\nplt.plot(y[:,1])\nplt.plot(yhat[:,1].data)\n\n\n\n\n\n\nOECD GDP\ní•´ë‹¹ì˜ˆì œëŠ” GNAR íŒ¨í‚¤ì§€ì—ì„œ ë„¤íŠ¸ì›Œí¬(ì—£ì§€)ë¥¼ ë§ì¶”ëŠ” ì˜ˆì œë¡œì„œ ë‚˜ì˜´, ê·¸ë ‡ê¸°ì— ë„¤íŠ¸ì›Œí¬ ì¡´ì¬í•˜ì§€ ì•Šì•„ ì—°êµ¬ ì˜ˆì œë¡œì„œ ì‚¬ìš©í•˜ì§€ ì•Šì„ ì˜ˆì •\nì´ ë°ì´í„°ëŠ” ë„¤íŠ¸ì›Œí¬ë¥¼ ì¶”ì •í•˜ì—¬ fit ë° predictí•¨\nGOP growth rate time series\n\n35 countries from the OECD website\ntime series : 1961 - 2013\nT = 52\nNodes = 35\nIn this data set 20.8% (379 out of 1820) of the observations were missing due to some nodes not being included from the start.\n\n\n%%R\nlibrary(\"fields\")\n\n\n%R -o gdpVTS\n\n\ngdpVTS.shape\n\n(52, 35)\n\n\n\nplt.plot(gdpVTS[:,1])"
  },
  {
    "objectID": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html",
    "href": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "",
    "text": "ST-GCN Dataset WikiMathsDatasetLoader"
  },
  {
    "objectID": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#train",
    "href": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#train",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "Train",
    "text": "Train\n\ndata_train=[]\nfor time, snapshot in enumerate(train_dataset):\n    data_train.append([time,snapshot])\n\n\ndata_train[0][1].x.shape,data_train[0][1].y.shape,data_train[0][1].edge_index.shape,data_train[0][1].edge_attr.shape\n\n(torch.Size([1068, 1]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n583\n\n\n\nT_train = time\nN = len(data_train[0][1].x)\n\n\nedge_index = data_train[0][1].edge_index\nedge_attr = data_train[0][1].edge_attr\n\n\nx_train = []\nfor i in range(time):\n    x_train.append(data_train[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_train = data_tensor.reshape(time,1068,-1)\nx_train.shape\n\ntorch.Size([583, 1068, 1])\n\n\n\ny_train = []\nfor i in range(time):\n    y_train.append(data_train[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_train = data_tensor.reshape(time,1068)\ny_train.shape\n\ntorch.Size([583, 1068])\n\n\n\nx_train.shape, y_train.shape\n\n(torch.Size([583, 1068, 1]), torch.Size([583, 1068]))"
  },
  {
    "objectID": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#test",
    "href": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#test",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "Test",
    "text": "Test\n\ndata_test=[]\nfor time, snapshot in enumerate(test_dataset):\n    data_test.append([time,snapshot])\n\n\ndata_test[0][1].x.shape,data_test[0][1].y.shape,data_test[0][1].edge_index.shape,data_test[0][1].edge_attr.shape\n\n(torch.Size([1068, 1]),\n torch.Size([1068]),\n torch.Size([2, 27079]),\n torch.Size([27079]))\n\n\n\ntime\n\n145\n\n\n\nT_test = time\n\n\nx_test = []\nfor i in range(time):\n    x_test.append(data_test[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_test = data_tensor.reshape(time,1068,-1)\nx_test.shape\n\ntorch.Size([145, 1068, 1])\n\n\n\ny_test = []\nfor i in range(time):\n    y_test.append(data_test[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_test = data_tensor.reshape(time,1068)\ny_test.shape\n\ntorch.Size([145, 1068])\n\n\n\nx_test.shape, y_test.shape\n\n(torch.Size([145, 1068, 1]), torch.Size([145, 1068]))"
  },
  {
    "objectID": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#ì‹œë‚˜ë¦¬ì˜¤1-baseline",
    "href": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#ì‹œë‚˜ë¦¬ì˜¤1-baseline",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "ì‹œë‚˜ë¦¬ì˜¤1 (Baseline)",
    "text": "ì‹œë‚˜ë¦¬ì˜¤1 (Baseline)\nì‹œë‚˜ë¦¬ì˜¤1\n\nmissing rate: 0%\në³´ê°„ë°©ë²•: None\n\n\nSTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [05:31<00:00,  6.62s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nGNAR ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n-\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\n\n\nEdge = np.array(edge_index)\nX_gnar = np.array(x_train.squeeze())\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet <- matrixtoGNAR(w)\n\n\n%%R\nanswer <- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 1, betaOrder = c(1))\nprediction <- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean(axis=0)\ntest_mse_total_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean()\n\n\n\nê²°ê³¼ì‹œê°í™”\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n GNAR: mse(train) = {3:.2f}, mse(test) = {4:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(range(1,583),stgcn_train[:,i],label='STCGCN (train)',color='C0')\n    a.plot(range(584,728),stgcn_test[:,i],label='STCGCN (test)',color='C0')\n    a.plot(range(1,583),gnar_train[:,i],label='GNAR (train)',color='C1')\n    a.plot(range(583,728),gnar_test[:,i],label='GNAR (test)',color='C1')\n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario1: STGCN \\n missing=0% \\n interpolation=None \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n GNAR: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#ì‹œë‚˜ë¦¬ì˜¤2",
    "href": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#ì‹œë‚˜ë¦¬ì˜¤2",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "ì‹œë‚˜ë¦¬ì˜¤2",
    "text": "ì‹œë‚˜ë¦¬ì˜¤2\nì‹œë‚˜ë¦¬ì˜¤2\n\nmissing rate: 50%\në³´ê°„ë°©ë²•: linear\n\n- ê²°ì¸¡ì¹˜ìƒì„± + ë³´ê°„\n\n_zero = Missing(x_train.squeeze())\n_zero.miss(percent = 0.5)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],x_train[:,:,0][:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [05:34<00:00,  6.68s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nESTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(T_train,N,1).float()[:-1,:,:]\n    y = torch.tensor(signal).reshape(T_train,N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [06:56<00:00,  8.33s/it]\n\n\n- ESTGCN\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nx_test.shape,y_test.shape\n\n(torch.Size([145, 1068, 1]), torch.Size([145, 1068]))\n\n\n\nreal_y = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n-\n\nEdge = np.array(edge_index)\nX_gnar = np.array(x_train.squeeze())\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet <- matrixtoGNAR(w)\n\n\n%%R\nanswer <- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 1, betaOrder = c(1))\nprediction <- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean(axis=0)\ntest_mse_total_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean()\n\n\n\nê²°ê³¼ì‹œê°í™”\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],x_train[missing_index[i],i,0],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,583),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(584,728),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,583),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(584,728),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(1,583),gnar_train.squeeze()[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(584,729),gnar_test.squeeze()[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#ì‹œë‚˜ë¦¬ì˜¤3",
    "href": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#ì‹œë‚˜ë¦¬ì˜¤3",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "ì‹œë‚˜ë¦¬ì˜¤3",
    "text": "ì‹œë‚˜ë¦¬ì˜¤3\nì‹œë‚˜ë¦¬ì˜¤3\n\nmissing rate: 80%\në³´ê°„ë°©ë²•: linear\n\n- ê²°ì¸¡ì¹˜ìƒì„± + ë³´ê°„\n\n_zero = Missing(x_train.squeeze())\n_zero.miss(percent = 0.8)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],x_train.squeeze()[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [05:51<00:00,  7.04s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nESTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(T_train,N,1).float()[:-1,:,:]\n    y = torch.tensor(signal).reshape(T_train,N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [07:00<00:00,  8.40s/it]\n\n\n- ESTGCN\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n-\n\nEdge = np.array(edge_index)\nX_gnar = np.array(x_train.squeeze())\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet <- matrixtoGNAR(w)\n\n\n%%R\nanswer <- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 1, betaOrder = c(1))\nprediction <- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean(axis=0)\ntest_mse_total_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean()\n\n\n\nê²°ê³¼ì‹œê°í™”\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],x_train[missing_index[i],i,0],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,583),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(584,728),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,583),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(584,728),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(1,583),gnar_train.squeeze()[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(584,729),gnar_test.squeeze()[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#ì‹œë‚˜ë¦¬ì˜¤4",
    "href": "posts/GCN/2023-02-07-ESTGCN_WIKI_DATA_2.html#ì‹œë‚˜ë¦¬ì˜¤4",
    "title": "Class of Method(WikiMath) lag 1",
    "section": "ì‹œë‚˜ë¦¬ì˜¤4",
    "text": "ì‹œë‚˜ë¦¬ì˜¤4\nì‹œë‚˜ë¦¬ì˜¤4\n\nmissing rate: 30%\në³´ê°„ë°©ë²•: linear\n\n- ê²°ì¸¡ì¹˜ìƒì„± + ë³´ê°„\n\n_zero = Missing(x_train.squeeze())\n_zero.miss(percent = 0.3)\n_zero.second_linear()\n\n\nmissing_index = _zero.number\ninterpolated_signal = _zero.train_linear\n\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.plot(missing_index[i],x_train.squeeze()[:,i][missing_index[i]],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.legend()\nfig.set_figwidth(15)\nfig\n\n\n\n\n\nSTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [05:54<00:00,  7.09s/it]\n\n\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nstgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nstgcn_test = yyhat.squeeze() \n\n\ntrain_mse_eachnode_stgcn = (((y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_stgcn = (((y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_stgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_stgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\n\nESTGCN ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n\nX = x_train.float()[:-1,:,:]\ny = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\n\nXX = x_test[:-1,:,:].float()\nyy = y_test.reshape(T_test,N,1)[1:,:,:].float()\n\n- ESTGCN\n\nnet = RecurrentGCN(node_features=1, filters=4)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\nnet.train()\nsignal = interpolated_signal.copy()\nfor epoch in tqdm(range(50)):\n    signal = update_from_freq_domain(signal,missing_index)\n    X = torch.tensor(signal).reshape(T_train,N,1).float()[:-1,:,:]\n    y = torch.tensor(signal).reshape(T_train,N,1).float()[1:,:,:]\n    for time, (xt,yt) in enumerate(zip(X,y)):        \n        yt_hat = net(xt, edge_index, edge_attr)\n        cost = torch.mean((yt_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    signal = torch.concat([X.squeeze(),yt_hat.detach().squeeze().reshape(1,-1)])        \n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [07:00<00:00,  8.40s/it]\n\n\n- ESTGCN\n\nyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\nyyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in XX]).detach().numpy()\n\n\nreal_y = y_train.reshape(T_train,N,1).float()[1:,:,:]\n\ntrain_mse_eachnode_estgcn = (((real_y-yhat).squeeze())**2).mean(axis=0)\ntrain_mse_total_estgcn = (((real_y-yhat).squeeze())**2).mean()\ntest_mse_eachnode_estgcn = (((yy-yyhat).squeeze())**2).mean(axis=0)\ntest_mse_total_estgcn = (((yy-yyhat).squeeze())**2).mean()\n\n\nestgcn_train = yhat.squeeze() # stgcnì€ stgcnì— ì˜í•œ ì í•©ê²°ê³¼ë¥¼ ì˜ë¯¸í•¨\nestgcn_test = yyhat.squeeze() \n\n\n\nGNAR ìœ¼ë¡œ ì í•© + ì˜ˆì¸¡\n-\n\nEdge = np.array(edge_index)\nX_gnar = np.array(x_train.squeeze())\n\n\nw=np.zeros((1068,1068))\n\n\nfor k in range(27079):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\n%R -i X_gnar\n%R -i w\n%R -i T_test\n\n\n%%R\nwikiNet <- matrixtoGNAR(w)\n\n\n%%R\nanswer <- GNARfit(vts = X_gnar, net = wikiNet, alphaOrder = 1, betaOrder = c(1))\nprediction <- predict(answer,n.ahead=T_test)\n\n\n%%R\ngnar_train <- residuals(answer)\ngnar_test <- prediction\n\n\n%R -o gnar_train\n%R -o gnar_test\n\n\ntrain_mse_eachnode_gnar = (gnar_train**2).mean(axis=0)\ntrain_mse_total_gnar = (gnar_train**2).mean()\ntest_mse_eachnode_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean(axis=0)\ntest_mse_total_gnar = ((yy.squeeze()-gnar_test[1:])**2).mean()\n\n\n\nê²°ê³¼ì‹œê°í™”\n\nfig = plot(x_train.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2)\nfig = plot_add(fig,x_test.squeeze()[:,:5],'--o',color='gray',label='complete data',alpha=0.2,t=range(581,726))\nax = fig.get_axes()\nfor i,a in enumerate(ax):     \n    a.set_title('node{0} \\n STGCN: mse(train) = {1:.2f}, mse(test) = {2:.2f} \\n ESTGCN: mse(train) = {3:.2f}, mse(test) = {4:.2f}\\n GNAR: mse(train) = {5:.2f}, mse(test) = {6:.2f}'.format(i,train_mse_eachnode_stgcn[i],test_mse_eachnode_stgcn[i],train_mse_eachnode_estgcn[i],test_mse_eachnode_estgcn[i],train_mse_eachnode_gnar[i],test_mse_eachnode_gnar[i]))\n    a.plot(missing_index[i],x_train[missing_index[i],i,0],'xk',label='missing')\n    a.plot(interpolated_signal[:,i],'-',color='gray',label='linear interpolation')\n    a.plot(range(1,583),stgcn_train.squeeze()[:,i],'--.',label='STCGCN (train)',color='C0')\n    a.plot(range(584,728),stgcn_test.squeeze()[:,i],'--.',label='STCGCN (test)',color='C0')\n    a.plot(range(1,583),estgcn_train.squeeze()[:,i],label='ESTCGCN (train)',color='C1')\n    a.plot(range(584,728),estgcn_test.squeeze()[:,i],label='ESTCGCN (test)',color='C1')\n    a.plot(range(1,583),gnar_train.squeeze()[:,i],label='GNAR (train)',color='C2')\n    a.plot(range(584,729),gnar_test.squeeze()[:,i],label='GNAR (test)',color='C2')\n    \n    a.legend()\nfig.set_figwidth(14)\nfig.suptitle(\"Scenario2: \\n missing=50% \\n interpolation=linear \\n\\n STGCN: mse(train) = {0:.2f}, mse(test) = {1:.2f} \\n ESTGCN: mse(train) = {2:.2f}, mse(test) = {3:.2f} \\n GNAR: mse(train) = {4:.2f}, mse(test) = {5:.2f} \\n\".format(train_mse_total_stgcn,test_mse_total_stgcn,train_mse_total_estgcn,test_mse_total_estgcn,train_mse_total_gnar,test_mse_total_gnar),size=15)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.html",
    "href": "posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.html",
    "title": "PedalMeDatasetLoader lag 4 Randomly Missing comparison Table",
    "section": "",
    "text": "ST-GCN Dataset PedalMeDatasetLoader"
  },
  {
    "objectID": "posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.html#train",
    "href": "posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.html#train",
    "title": "PedalMeDatasetLoader lag 4 Randomly Missing comparison Table",
    "section": "Train",
    "text": "Train\n\ntrain_dataset.x\n\nAttributeError: 'StaticGraphTemporalSignal' object has no attribute 'x'\n\n\n\ndata_train=[]\nfor time, snapshot in enumerate(train_dataset):\n    data_train.append([time,snapshot])\n\n\ndata_train[0][1].x.shape,data_train[0][1].y.shape,data_train[0][1].edge_index.shape,data_train[0][1].edge_attr.shape\n\n(torch.Size([15, 4]),\n torch.Size([15]),\n torch.Size([2, 225]),\n torch.Size([225]))\n\n\n\ntime\n\n23\n\n\n\nT_train = time\nN = len(data_train[0][1].x)\n\n\nedge_index = data_train[0][1].edge_index\nedge_attr = data_train[0][1].edge_attr\n\n\nx_train = []\nfor i in range(time):\n    x_train.append(data_train[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_train = data_tensor.reshape(time,15,-1)\nx_train.shape\n\ntorch.Size([23, 15, 4])\n\n\n\ny_train = []\nfor i in range(time):\n    y_train.append(data_train[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_train:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_train = data_tensor.reshape(time,15)\ny_train.shape\n\ntorch.Size([23, 15])\n\n\n\nx_train.shape, y_train.shape\n\n(torch.Size([23, 15, 4]), torch.Size([23, 15]))"
  },
  {
    "objectID": "posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.html#test",
    "href": "posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.html#test",
    "title": "PedalMeDatasetLoader lag 4 Randomly Missing comparison Table",
    "section": "Test",
    "text": "Test\n\ndata_test=[]\nfor time, snapshot in enumerate(test_dataset):\n    data_test.append([time,snapshot])\n\n\ndata_test[0][1].x.shape,data_test[0][1].y.shape,data_test[0][1].edge_index.shape,data_test[0][1].edge_attr.shape\n\n(torch.Size([15, 4]),\n torch.Size([15]),\n torch.Size([2, 225]),\n torch.Size([225]))\n\n\n\ntime\n\n6\n\n\n\nT_test = time\n\n\nx_test = []\nfor i in range(time):\n    x_test.append(data_test[i][1].x)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in x_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\nx_test = data_tensor.reshape(time,15,-1)\nx_test.shape\n\ntorch.Size([6, 15, 4])\n\n\n\ny_test = []\nfor i in range(time):\n    y_test.append(data_test[i][1].y)\n\n\ndata_tensor = torch.Tensor()\n# Iterate over the data points of the dataset\nfor i in y_test:\n    # Concatenate the data point to the tensor\n    data_tensor = torch.cat((data_tensor, i), dim=0)\ny_test = data_tensor.reshape(time,15)\ny_test.shape\n\ntorch.Size([6, 15])\n\n\n\nx_test.shape, y_test.shape\n\n(torch.Size([6, 15, 4]), torch.Size([6, 15]))"
  },
  {
    "objectID": "posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.html#stgcn",
    "href": "posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.html#stgcn",
    "title": "PedalMeDatasetLoader lag 4 Randomly Missing comparison Table",
    "section": "STGCN",
    "text": "STGCN\n\nclass STGCN_Missing:\n    def __init__(self,Dataset, df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n        self.iterable = iterable\n    def iter(self):\n        self.XX = x_test\n        self.yy = y_test\n\n        self.real_y = y_train\n        for i in range(self.iterable):\n    \n            _zero = Missing(x_train_f)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(np.stack([interpolated_signal[j:(T_train+j),:] for j in range(self.lag)],axis = -1)).float()\n            y = torch.tensor(interpolated_signal[self.lag:,:]).float()\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            for epoch in range(50):\n                for time, (xt,yt) in enumerate(zip(X,y)):\n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_stgcn = (((self.real_y-yhat.squeeze()).squeeze())**2).mean()\n            test_mse_total_stgcn = (((self.yy-yyhat.squeeze()).squeeze())**2).mean()\n    \n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_stgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_stgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.html#enhencement-of-stgcn",
    "href": "posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.html#enhencement-of-stgcn",
    "title": "PedalMeDatasetLoader lag 4 Randomly Missing comparison Table",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nclass ESTGCN_Missing:\n    def __init__(self,Dataset, df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n        self.iterable = iterable\n    def iter(self):\n        self.XX = x_test\n        self.yy = y_test\n\n        self.real_y = y_train\n        for i in range(self.iterable):\n    \n            _zero = Missing(x_train_f)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            net = RecurrentGCN(node_features=self.lag, filters=self.Number_of_filters)\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n            net.train()\n            signal = interpolated_signal.copy()\n            for epoch in range(50):\n                signal = update_from_freq_domain(signal,missing_index)\n                X = torch.tensor(np.stack([signal[i:(T_train+epoch+i),:] for i in range(self.lag)],axis = -1)).reshape(-1,N,self.lag).float()\n                y = torch.tensor(signal).reshape(-1,N,1).float()[self.lag:,:,:]\n                for time, (xt,yt) in enumerate(zip(X,y)):        \n                    yt_hat = net(xt, edge_index, edge_attr)\n                    cost = torch.mean((yt_hat-yt)**2)\n                    cost.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                signal = torch.concat([X[:-1,:,0], X[-1,:,:].T, yt_hat.detach().reshape(1,-1)]).squeeze()\n\n            yhat = torch.stack([net(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n            yyhat = torch.stack([net(xt, edge_index, edge_attr) for xt in self.XX]).detach().numpy()\n\n            train_mse_total_estgcn = (((self.real_y-yhat[:T_train,:].squeeze()).squeeze())**2).mean()\n            test_mse_total_estgcn = (((self.yy-yyhat.squeeze()).squeeze())**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_estgcn.tolist()\n            df_row['MSE_test'] = test_mse_total_estgcn.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.html#gnar",
    "href": "posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.html#gnar",
    "title": "PedalMeDatasetLoader lag 4 Randomly Missing comparison Table",
    "section": "GNAR",
    "text": "GNAR\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(GNAR)\nlibrary(igraph)\n\nR[write to console]: Loading required package: igraph\n\nR[write to console]: \nAttaching package: â€˜igraphâ€™\n\n\nR[write to console]: The following objects are masked from â€˜package:statsâ€™:\n\n    decompose, spectrum\n\n\nR[write to console]: The following object is masked from â€˜package:baseâ€™:\n\n    union\n\n\nR[write to console]: Loading required package: wordcloud\n\nR[write to console]: Loading required package: RColorBrewer\n\n\n\n\nGNAR = importr('GNAR') # import GNAR \nigraph = importr('igraph') # import igraph \n\n\nw=np.zeros((N,N))\n\n\nfor k in range(len(edge_index[0])):\n    w[edge_index[0][k],edge_index[1][k]] = 1\n\n\nm = robjects.r.matrix(FloatVector(w), nrow = N, ncol = N)\n\n\nclass GNAR_Missing:\n    def __init__(self,Dataset, df, iterable, Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation):\n        self.Dataset = Dataset\n        self.df = df\n        self.iterable = iterable\n        self.Method = Method\n        self.Missingrate = Missingrate\n        self.Missingtype = Missingtype\n        self.lag = lag\n        self.Number_of_filters = Number_of_filters\n        self.Interpolation = Interpolation\n        self.iterable = iterable\n    def iter(self):\n        self.yy = torch.tensor(y_test).float()\n        for i in range(self.iterable):\n\n            _zero = Missing(x_train_f)\n            _zero.miss(percent = self.Missingrate)\n            _zero.second_linear()\n\n            missing_index = _zero.number\n            interpolated_signal = _zero.train_linear\n\n            X = torch.tensor(np.stack([interpolated_signal[i:(T_train+i),:] for i in range(self.lag)],axis = -1)).float()\n\n            answer = GNAR.GNARfit(vts=robjects.r.matrix(rpyn.numpy2rpy(np.array(X).squeeze()), nrow = T_train, ncol = N),net = GNAR.matrixtoGNAR(m), alphaOrder = 4, betaOrder = FloatVector([1, 1, 1, 1]))             \n            predict = GNAR.predict_GNARfit(answer,n_ahead=T_test)\n\n\n            train_mse_total_gnar = ((pd.DataFrame(GNAR.residuals_GNARfit(answer)).values.reshape(-1,N))**2).mean()\n            test_mse_total_gnar = ((self.yy - pd.DataFrame(predict).values.reshape(-1,N))**2).mean()\n\n            df_row = pd.DataFrame(columns=col)\n            df_row['Dataset'] = self.Dataset,\n            df_row['iteration'] = i+1, # 1,2,3,...,10 \n            df_row['method'] = self.Method, # 'stgcn','estgcn','gnar' \n            df_row['missingrate'] = self.Missingrate, # 0.0, 0.2, 0.4, 0.6, 0.8 \n            df_row['missingtype'] = self.Missingtype,  # None, 'randomly' and 'block' \n            df_row['lag'] = self.lag, # 1,2,3,4 ... \n            df_row['number_of_filters'] = self.Number_of_filters, # 16,24,32, ... \n            df_row['interpolation'] = self.Interpolation, # None, 'mean', 'linear'\n            df_row['MSE_train'] = train_mse_total_gnar.tolist()\n            df_row['MSE_test'] = test_mse_total_gnar.tolist()\n\n            self.df = pd.concat([self.df,df_row])"
  },
  {
    "objectID": "posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.html#stgcn-1",
    "href": "posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.html#stgcn-1",
    "title": "PedalMeDatasetLoader lag 4 Randomly Missing comparison Table",
    "section": "STGCN",
    "text": "STGCN\n\nDataset = 'PedalMeDatasetLoader'\nMethod = 'stgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 4 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_stgcn= pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    stgcn = STGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    stgcn.iter()\n    df_add = stgcn.df.copy()\n    df_stgcn = pd.concat([df_stgcn,df_add],axis=0)\n\n\nsave_data(df_stgcn, './data/Pedal_stgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.html#enhencement-of-stgcn-1",
    "href": "posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.html#enhencement-of-stgcn-1",
    "title": "PedalMeDatasetLoader lag 4 Randomly Missing comparison Table",
    "section": "Enhencement of STGCN",
    "text": "Enhencement of STGCN\n\nDataset = 'PedalMeDatasetLoader'\nMethod = 'estgcn' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 4 # 1,2,3,4 ... \nNumber_of_filters = 4 # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_estgcn = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    estgcn = ESTGCN_Missing(Dataset,df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    estgcn.iter()\n    df_add = estgcn.df.copy()\n    df_estgcn = pd.concat([df_estgcn,df_add],axis=0)\n\n\nsave_data(df_estgcn, './data/Pedal_estgcn_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.html#gnar-1",
    "href": "posts/GCN/2023-02-16-ESTGCN_PEDAL_DATA_1.html#gnar-1",
    "title": "PedalMeDatasetLoader lag 4 Randomly Missing comparison Table",
    "section": "GNAR",
    "text": "GNAR\n\nDataset = 'PedalMeDatasetLoader'\nMethod = 'gnar' # 'stgcn','estgcn','gnar' \nMissingtype = 'randomly'  # None, 'randomly' and 'block' \nlag = 4 # 1,2,3,4 ... \nNumber_of_filters = None # 16,24,32, ... \nInterpolation = 'Linear' # None, 'mean', 'linear'\niterable = 100\n\n\ndf_gnar = pd.DataFrame(columns=col)\n\n\nfor Missingrate in rate:\n    df = pd.DataFrame(columns=col)\n    gnar = GNAR_Missing(Dataset, df, iterable,Method, Missingrate, Missingtype, lag, Number_of_filters, Interpolation)\n    gnar.iter()\n    df_add = gnar.df.copy()\n    df_gnar = pd.concat([df_gnar,df_add],axis=0)\n\n\nsave_data(df_gnar, './data/Pedal_gnar_randomly_by_rate.pkl')"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html",
    "title": "GCN Algorithm Example 1",
    "section": "",
    "text": "Our method; GNAR Dataset Example(fiveVTS, fiveNet)"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#ë°ì´í„°-ì¼ë¶€-missing-ì²˜ë¦¬",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#ë°ì´í„°-ì¼ë¶€-missing-ì²˜ë¦¬",
    "title": "GCN Algorithm Example 1",
    "section": "ë°ì´í„° ì¼ë¶€ missing ì²˜ë¦¬",
    "text": "ë°ì´í„° ì¼ë¶€ missing ì²˜ë¦¬\n\n1) Block ì²˜ë¦¬\n\n[1] ST-GCN\n\n%%R\nfiveVTS0 <- fiveVTS\nfiveVTS0[50:150, 3] <- NA\n\n\nplt.plot(fiveVTS0[:,2])\n\n\n\n\n\nT = 200\nN = 5 # number of Nodes\nE = fiveNet_edge\nV = np.array([1,2,3,4,5])\nt = np.arange(0,T)\nnode_features = 1\n\n\nf = torch.tensor(fiveVTS0).reshape(200,5,1).float()\n\n\nX = f[:199,:,:]\ny = f[1:,:,:]\n\n\nedge_index = torch.tensor(E)\nedge_attr = torch.tensor(np.array([1,1,1,1,1,1,1,1,1,1]),dtype=torch.float32)\n\n\n_ee = enumerate(zip(X,y))\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:32<00:00,  1.53it/s]\n\n\n\nyhat = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nplt.plot(yhat[:,0].data)\nplt.plot(yhat[:,1].data)\nplt.plot(yhat[:,2].data)\nplt.plot(yhat[:,3].data)\n\n\n\n\n\n\n\n2) Random missing values\n\n%%R\nset.seed(1)\nfiveVTSrandom <- fiveVTS\nsampleindex = sort(sample(1:200, 100))\nfiveVTSrandom[sampleindex,3] <- NA\n\n\n%R -o fiveVTSrandom\n%R -o sampleindex\n\n\nplt.plot(fiveVTSrandom[:,2],'o')\n\n\n\n\n\n\n3) By 2\n\n%%R\nfiveVTStwo <- fiveVTS\nindextwo <- rep(seq(1, by = 2, 200))\nfiveVTStwo[indextwo, 3] <- NA\n\n\n%R -o fiveVTStwo\n%R -o indextwo\n\n\nplt.plot(fiveVTStwo[:,2],'o')"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#mean",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#mean",
    "title": "GCN Algorithm Example 1",
    "section": "1.1. Mean",
    "text": "1.1. Mean\n\n1) Block\n\nfiveVTS0_mean = fiveVTS0.copy()\n\n\nfiveVTS0_mean[49:150,2] = np.mean(fiveVTS0[:49,2].tolist()+fiveVTS0[150:,2].tolist())\n\n\nplt.plot(fiveVTS0_mean[:,2])\n\n\n\n\n\n\n2) Random missing values\n\nfiveVTSrandom_mean = fiveVTSrandom.copy()\n\n\ndf = pd.DataFrame(fiveVTSrandom[:,2])\nmean_value = df.mean() # finds the mean value of the column A\ndf = df.fillna(mean_value) # replace missing values with the mean value\n\n\nfiveVTSrandom_mean[:,2] = np.array(df).reshape(200,)\n\n\nplt.plot(fiveVTSrandom_mean[:,2])\n\n\n\n\n\n\n3) By 2\n\nfiveVTStwo_mean = fiveVTStwo.copy()\n\n\ndf = pd.DataFrame(fiveVTStwo[:,2])\nmean_value = df.mean() # finds the mean value of the column A\ndf = df.fillna(mean_value) # replace missing values with the mean value\n\n\nfiveVTStwo_mean[:,2] = np.array(df).reshape(200,)\n\n\nplt.plot(fiveVTStwo_mean[:,2])"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#linear-interpolation",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#linear-interpolation",
    "title": "GCN Algorithm Example 1",
    "section": "1.2. linear interpolation",
    "text": "1.2. linear interpolation\n\n1) Block\n\nfiveVTS0_linearinterpolation = fiveVTS0.copy()\n\n\n# Sample data points\nx = np.array([48,150])\ny = np.array([fiveVTS0_linearinterpolation[48,2],fiveVTS0_linearinterpolation[150,2]])\n\n# Create interpolating function\nf = interp1d(x, y, kind='linear')\n\n# Estimate y value for x = 2.5\ny_interp = f(range(49,150))\n\n\nfiveVTS0_linearinterpolation[49:150,2] = y_interp\n\n\nplt.plot(fiveVTS0_linearinterpolation[:,2])\n\n\n\n\n\n\n2) Random missing values\n\nfiveVTSrandom_linearinterpolation = fiveVTSrandom.copy()\n\n\n_df = pd.DataFrame(fiveVTSrandom_linearinterpolation[:,2])\n_df.interpolate(method='linear', inplace=True)\n_df = _df.fillna(0)\n\n\nfiveVTSrandom_linearinterpolation[:,2] = np.array(_df).reshape(200,)\n\n\nplt.plot(fiveVTSrandom_linearinterpolation[:,2])\n\n\n\n\n\n\n3) By 2\n\nfiveVTStwo_linearinterpolation = fiveVTStwo.copy()\n\n\n_df = pd.Series(fiveVTStwo_linearinterpolation[:,2])\n_df.interpolate(method='linear', inplace=True)\n_df = _df.fillna(0)\n\n\nfiveVTStwo_linearinterpolation[:,2] = _df\n\n\nplt.plot(fiveVTStwo_linearinterpolation[:,2])"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#mean-1",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#mean-1",
    "title": "GCN Algorithm Example 1",
    "section": "2.1. Mean",
    "text": "2.1. Mean\n\n1) Block\n\nf_mean = torch.tensor(fiveVTS0_mean).reshape(200,5,1).float()\n\n\nX_mean = f_mean[:199,:,:]\ny_mean = f_mean[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_mean,y_mean)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:32<00:00,  1.56it/s]\n\n\n\nfhat_mean = torch.stack([model(xt, edge_index, edge_attr) for xt in X_mean]).detach().numpy()\n\n\nplt.plot(fhat_mean[:,2].data)\n\n\n\n\n\n\n2) Random missing values\n\nf_fiveVTSrandom_mean = torch.tensor(fiveVTSrandom_mean).reshape(200,5,1).float()\n\n\nX_fiveVTSrandom_mean = f_fiveVTSrandom_mean[:199,:,:]\ny_fiveVTSrandom_mean = f_fiveVTSrandom_mean[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_fiveVTSrandom_mean,y_fiveVTSrandom_mean)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:32<00:00,  1.55it/s]\n\n\n\nfhat_fiveVTSrandom_mean = torch.stack([model(xt, edge_index, edge_attr) for xt in X_fiveVTSrandom_mean]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTSrandom_mean[:,2].data)\n\n\n\n\n\n\n3) By 2\n\nf_fiveVTStwo_mean = torch.tensor(fiveVTStwo_mean).reshape(200,5,1).float()\n\n\nX_fiveVTStwo_mean = f_fiveVTStwo_mean[:199,:,:]\ny_fiveVTStwo_mean = f_fiveVTStwo_mean[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_fiveVTStwo_mean,y_fiveVTStwo_mean)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:32<00:00,  1.54it/s]\n\n\n\nfhat_fiveVTStwo_mean = torch.stack([model(xt, edge_index, edge_attr) for xt in X_fiveVTStwo_mean]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTStwo_mean[:,2].data)"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#linear-interpolation-1",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#linear-interpolation-1",
    "title": "GCN Algorithm Example 1",
    "section": "2.2. linear interpolation",
    "text": "2.2. linear interpolation\n\n1) Block\n\nf_linearinterpolation = torch.tensor(fiveVTS0_linearinterpolation).reshape(200,5,1).float()\n\n\nX_linearinterpolation = f_linearinterpolation[:199,:,:]\ny_linearinterpolation = f_linearinterpolation[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_linearinterpolation,y_linearinterpolation)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:32<00:00,  1.55it/s]\n\n\n\nfhat_linearinterpolation = torch.stack([model(xt, edge_index, edge_attr) for xt in X_linearinterpolation]).detach().numpy()\n\n\nplt.plot(fhat_linearinterpolation[:,2].data)\n\n\n\n\n\n\n2) Random missing values\n\nf_fiveVTSrandom_linearinterpolation = torch.tensor(fiveVTSrandom_linearinterpolation).reshape(200,5,1).float()\n\n\nX_fiveVTSrandom_linearinterpolation = f_fiveVTSrandom_linearinterpolation[:199,:,:]\ny_fiveVTSrandom_linearinterpolation = f_fiveVTSrandom_linearinterpolation[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_fiveVTSrandom_linearinterpolation,y_fiveVTSrandom_linearinterpolation)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:32<00:00,  1.55it/s]\n\n\n\nfhat_fiveVTSrandom_linearinterpolation = torch.stack([model(xt, edge_index, edge_attr) for xt in X_fiveVTSrandom_linearinterpolation]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTSrandom_linearinterpolation[:,2].data)\n\n\n\n\n\n\n3) By 2\n\nf_fiveVTStwo_linearinterpolation = torch.tensor(fiveVTStwo_linearinterpolation).reshape(200,5,1).float()\n\n\nX_fiveVTStwo_linearinterpolation = f_fiveVTSrandom_linearinterpolation[:199,:,:]\ny_fiveVTStwo_linearinterpolation = f_fiveVTSrandom_linearinterpolation[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X_fiveVTStwo_linearinterpolation,y_fiveVTStwo_linearinterpolation)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:32<00:00,  1.55it/s]\n\n\n\nfhat_fiveVTStwo_linearinterpolation = torch.stack([model(xt, edge_index, edge_attr) for xt in X_fiveVTStwo_linearinterpolation]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTStwo_linearinterpolation[:,2].data)"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#ì›ë˜-f",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#ì›ë˜-f",
    "title": "GCN Algorithm Example 1",
    "section": "2.3. ì›ë˜ f",
    "text": "2.3. ì›ë˜ f\n\nf = torch.tensor(fiveVTS).reshape(200,5,1).float()\n\n\nX = f[:199,:,:]\ny = f[1:,:,:]\n\n\nmodel = RecurrentGCN(node_features=1, filters=4)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for time, (xt,yt) in enumerate(zip(X,y)):\n        y_hat = model(xt, edge_index, edge_attr)\n        cost = torch.mean((y_hat-yt)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:32<00:00,  1.55it/s]\n\n\n\nfhat_fiveVTS = torch.stack([model(xt, edge_index, edge_attr) for xt in X]).detach().numpy()\n\n\nplt.plot(fhat_fiveVTS[:,2].data)"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#mean-2",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#mean-2",
    "title": "GCN Algorithm Example 1",
    "section": "3.1. Mean",
    "text": "3.1. Mean\n\n3.1.1. Temporal\n\n1) Block\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])    \nfhatbar = np.hstack([Psi[i] @ fhat_mean[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_mean_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_mean_temporal[:,0])\nplt.plot(fhatbarhat_mean_temporal[:,1])\nplt.plot(fhatbarhat_mean_temporal[:,2])\nplt.plot(fhatbarhat_mean_temporal[:,3])\nplt.plot(fhatbarhat_mean_temporal[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])    \nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTSrandom_mean[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_random_mean_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_random_mean_temporal[:,0])\nplt.plot(fhatbarhat_random_mean_temporal[:,1])\nplt.plot(fhatbarhat_random_mean_temporal[:,2])\nplt.plot(fhatbarhat_random_mean_temporal[:,3])\nplt.plot(fhatbarhat_random_mean_temporal[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])    \nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTStwo_mean[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_twomean_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_twomean_temporal[:,0])\nplt.plot(fhatbarhat_twomean_temporal[:,1])\nplt.plot(fhatbarhat_twomean_temporal[:,2])\nplt.plot(fhatbarhat_twomean_temporal[:,3])\nplt.plot(fhatbarhat_twomean_temporal[:,4])\n\n\n\n\n\n\n\n3.1.2. Spatio\n\n1) Block\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_mean.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_mean_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_mean_spatio[:,0])\nplt.plot(fhatbarhat_mean_spatio[:,1])\nplt.plot(fhatbarhat_mean_spatio[:,2])\nplt.plot(fhatbarhat_mean_spatio[:,3])\nplt.plot(fhatbarhat_mean_spatio[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTSrandom_mean.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_random_mean_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_random_mean_spatio[:,0])\nplt.plot(fhatbarhat_random_mean_spatio[:,1])\nplt.plot(fhatbarhat_random_mean_spatio[:,2])\nplt.plot(fhatbarhat_random_mean_spatio[:,3])\nplt.plot(fhatbarhat_random_mean_spatio[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTStwo_mean.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_two_mean_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_two_mean_spatio[:,0])\nplt.plot(fhatbarhat_two_mean_spatio[:,1])\nplt.plot(fhatbarhat_two_mean_spatio[:,2])\nplt.plot(fhatbarhat_two_mean_spatio[:,3])\nplt.plot(fhatbarhat_two_mean_spatio[:,4])\n\n\n\n\n\n\n\n3.1.3. Spatio-Temporal\n\n1) Block\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_mean.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_mean_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhatbarhat_mean_spatio_temporal[:,0])\nplt.plot(fhatbarhat_mean_spatio_temporal[:,1])\nplt.plot(fhatbarhat_mean_spatio_temporal[:,2])\nplt.plot(fhatbarhat_mean_spatio_temporal[:,3])\nplt.plot(fhatbarhat_mean_spatio_temporal[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTSrandom_mean.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_random_mean_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,0])\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,1])\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,2])\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,3])\nplt.plot(fhatbarhat_random_mean_spatio_temporal[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTStwo_mean.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_two_mean_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,0])\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,1])\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,2])\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,3])\nplt.plot(fhatbarhat_two_mean_spatio_temporal[:,4])"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#linear-interpolation-2",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#linear-interpolation-2",
    "title": "GCN Algorithm Example 1",
    "section": "3.2.linear interpolation",
    "text": "3.2.linear interpolation\n\n3.2.1. Temporal\n\n1) Block\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])\nfhatbar = np.hstack([Psi[i] @ fhat_linearinterpolation[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_linearinterpolation_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,0])\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,1])\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,2])\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,3])\nplt.plot(fhatbarhat_linearinterpolation_temporal[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])\nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTSrandom_linearinterpolation[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_random_linearinterpolation_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,0])\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,1])\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,2])\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,3])\nplt.plot(fhatbarhat_random_linearinterpolation_temporal[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])\nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTStwo_linearinterpolation[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_two_linearinterpolation_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,0])\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,1])\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,2])\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,3])\nplt.plot(fhatbarhat_two_linearinterpolation_temporal[:,4])\n\n\n\n\n\n\n\n3.2.2. Spatio\n\n1) Block\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_linearinterpolation.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_linearinterpolation_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,0])\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,1])\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,2])\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,3])\nplt.plot(fhatbarhat_linearinterpolation_spatio[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTSrandom_linearinterpolation.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_random_linearinterpolation_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,0])\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,1])\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,2])\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,3])\nplt.plot(fhatbarhat_random_linearinterpolation_spatio[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTStwo_linearinterpolation.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_two_linearinterpolation_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,0])\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,1])\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,2])\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,3])\nplt.plot(fhatbarhat_two_linearinterpolation_spatio[:,4])\n\n\n\n\n\n\n\n3.2.3. Spatio-Temporal\n\n1) Block\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_linearinterpolation.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhat_linearinterpolation_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,0])\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,1])\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,2])\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,3])\nplt.plot(fhat_linearinterpolation_spatio_temporal[:,4])\n\n\n\n\n\n\n2) Random missing values\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTSrandom_linearinterpolation.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhat_random_linearinterpolation_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,0])\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,1])\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,2])\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,3])\nplt.plot(fhat_random_linearinterpolation_spatio_temporal[:,4])\n\n\n\n\n\n\n3) By 2\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTStwo_linearinterpolation.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhat_two_linearinterpolation_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,0])\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,1])\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,2])\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,3])\nplt.plot(fhat_two_linearinterpolation_spatio_temporal[:,4])"
  },
  {
    "objectID": "posts/GCN/2023-01-11-Algorithm_EX_1.html#original",
    "href": "posts/GCN/2023-01-11-Algorithm_EX_1.html#original",
    "title": "GCN Algorithm Example 1",
    "section": "3.3. original",
    "text": "3.3. original\n\n1) Temporal\n\nw=np.zeros((5,199,199))\n\n\nfor k in range(5):\n    for i in range(199):\n        for j in range(199):\n            if i==j :\n                w[k,i,j] = 0\n            elif np.abs(i-j) <= 1 : \n                w[k,i,j] = 1\n\n\nd = np.array([w[i].sum(axis=1) for i in range(5)])\nD= np.array([np.diag(d[i]) for i in range(5)])\nL = np.array([np.diag(1/np.sqrt(d[i])) @ (D[i]-w[i]) @ np.diag(1/np.sqrt(d[i])) for i in range(5)])\nlamb, Psi  = np.linalg.eigh(L)[0],np.linalg.eigh(L)[1]\nLamb = np.array([np.diag(lamb[i]) for i in range(5)])    \nfhatbar = np.hstack([Psi[i] @ fhat_fiveVTS[:,i] for i in range(5)])\n_fhatbar = fhatbar.reshape(5,199)\npower = _fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(_fhatbar[i]**2))) for i in range(5)])\nfhatbar_threshed = np.where(power_threshed>0,_fhatbar,0)\nfhatbarhat = np.array([Psi[i] @ fhatbar_threshed[i] for i in range(5)])    \nfhatbarhat_temporal = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_temporal[:,0])\nplt.plot(fhatbarhat_temporal[:,1])\nplt.plot(fhatbarhat_temporal[:,2])\nplt.plot(fhatbarhat_temporal[:,3])\nplt.plot(fhatbarhat_temporal[:,4])\n\n\n\n\n\n\n2) Spatio\n\nw=np.zeros((5,5))\n\n\nfor i in range(5):\n    for j in range(5):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTS.reshape(5,199)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(5)])    \nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhatbarhat_spatio = fhatbarhat.reshape(199,-1)\n\n\nplt.plot(fhatbarhat_spatio[:,0])\nplt.plot(fhatbarhat_spatio[:,1])\nplt.plot(fhatbarhat_spatio[:,2])\nplt.plot(fhatbarhat_spatio[:,3])\nplt.plot(fhatbarhat_spatio[:,4])\n\n\n\n\n\n\n3) Spatio-Temporal\n\nw=np.zeros((995,995))\n\n\nfor i in range(995):\n    for j in range(995):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nd = np.array(w.sum(axis=1))\nD = np.diag(d)\nL = np.array(np.diag(1/np.sqrt(d)) @ (D-w) @ np.diag(1/np.sqrt(d)))\nlamb, Psi = np.linalg.eigh(L)\nLamb = np.diag(lamb)\nfhatbar = Psi @ fhat_fiveVTS.reshape(995,1)\npower = fhatbar**2 \nebayesthresh = importr('EbayesThresh').ebayesthresh\npower_threshed=np.array([np.array(ebayesthresh(FloatVector(fhatbar[i]**2))) for i in range(995)])\nfhatbar_threshed = np.where(power_threshed>0,fhatbar,0)\nfhatbarhat = Psi @ fhatbar_threshed\nfhat_spatio_temporal = fhatbarhat.reshape(199,5,1)\n\n\nplt.plot(fhat_spatio_temporal[:,0])\nplt.plot(fhat_spatio_temporal[:,1])\nplt.plot(fhat_spatio_temporal[:,2])\nplt.plot(fhat_spatio_temporal[:,3])\nplt.plot(fhat_spatio_temporal[:,4])"
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html",
    "title": "Graph code",
    "section": "",
    "text": "Poster"
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#linear1",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#linear1",
    "title": "Graph code",
    "section": "Linear(1)",
    "text": "Linear(1)\n\n_x = np.linspace(0,2,1000)\n_y1 = 5*_x\n_y = _y1 + x # x is epsilon\n\n\ndf1=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\nw=np.zeros((1000,1000))\n\n\nfor i in range(1000):\n    for j in range(1000):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nclass SIMUL:\n    def __init__(self,df):\n        self.df = df\n        self.y = df.y.to_numpy()\n        self.y1 = df.y1.to_numpy()\n        self.x = df.x.to_numpy()\n        self.n = len(self.y)\n        self.W = w\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)      \n    def fit(self,sd=5,ref=30,ymin=-5,ymax=20,cuts=0,cutf=995): # fit with ebayesthresh\n        self._eigen()\n        self.ybar = self.Psi.T @ self.y # fbar := graph fourier transform of f\n        self.power = self.ybar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.ybar**2),sd=sd))\n        self.ybar_threshed = np.where(self.power_threshed>0,self.ybar,0)\n        self.yhat = self.Psi@self.ybar_threshed\n        self.df = self.df.assign(yHat = self.yhat)\n        self.df = self.df.assign(Residual = self.df.y- self.df.yHat)\n        self.differ=(np.abs(self.y-self.yhat)-np.min(np.abs(self.y-self.yhat)))/(np.max(np.abs(self.y-self.yhat))-np.min(np.abs(self.y-self.yhat))) #color í‘œí˜„ì€ ìœ„í•¸ í‘œì¤€í™”\n        self.df = self.df.assign(differ = self.differ)\n        \n        fig,ax = plt.subplots(figsize=(10,10))\n        ax.scatter(self.x,self.y,color='gray',s=50,alpha=0.7)\n        ax.scatter(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],color='red',s=50)\n        ax.plot(self.x[cuts:cutf],self.yhat[cuts:cutf], '--k',lw=3)\n        ax.scatter(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],color='red',s=550,facecolors='none', edgecolors='r')\n        fig.tight_layout()\n        fig.savefig('fig1.eps',format='eps')\n\n\n_simul = SIMUL(df1)\n\n\n_simul.fit(sd=20,ref=25,ymin=-10,ymax=15)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque."
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#linear2",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#linear2",
    "title": "Graph code",
    "section": "Linear(2)",
    "text": "Linear(2)\n\n_x = np.linspace(0,2,1000)\n_y1 = 5*_x**2\n_y = _y1 + x # x is epsilon\n\n\ndf2=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul2 = SIMUL(df2)\n\n\n_simul2.fit(sd=20,ref=20,ymin=-10,ymax=15)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque."
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#cos",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#cos",
    "title": "Graph code",
    "section": "COS",
    "text": "COS\n\n_x = np.linspace(0,2,1000)\n_y1 = -2+ 3*np.cos(_x) + 1*np.cos(2*_x) + 5*np.cos(5*_x)\n_y = _y1 + x\n\n\ndf4=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul4 = SIMUL(df4)\n\n\n_simul4.fit(sd=20,ref=20,ymin=-10,ymax=15)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque."
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#sin",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#sin",
    "title": "Graph code",
    "section": "SIN",
    "text": "SIN\n\n_x = np.linspace(0,2,1000)\n_y1 =  3*np.sin(_x) + 1*np.sin(_x**2) + 5*np.sin(5*_x) \n_y = _y1 + x # x is epsilon\n\n\ndf5=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul5 = SIMUL(df5)\n\n\n_simul5.fit(ref=15,ymin=-10,ymax=15,cuts=5)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque."
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#d-manifold",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#d-manifold",
    "title": "Graph code",
    "section": "1D manifold",
    "text": "1D manifold\n\nnp.random.seed(777)\npi=np.pi\nn=1000\nang=np.linspace(-pi,pi-2*pi/n,n)\nr=5+np.cos(np.linspace(0,12*pi,n))\nvx=r*np.cos(ang)\nvy=r*np.sin(ang)\nf1=10*np.sin(np.linspace(0,6*pi,n))\nf = f1 + x\n\n\ndf = pd.DataFrame({'x' : vx, 'y' : vy, 'f' : f, 'f1' : f1})\n\n\nclass SIMUL:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.f.to_numpy()\n        self.f1 = df.f1.to_numpy()\n        self.x = df.x.to_numpy()\n        self.y = df.y.to_numpy()\n        self.n = len(self.f)\n        self.theta= None\n    def get_distance(self):\n        self.D = np.zeros([self.n,self.n])\n        locations = np.stack([self.x, self.y],axis=1)\n        for i in tqdm.tqdm(range(self.n)):\n            for j in range(i,self.n):\n                self.D[i,j]=np.linalg.norm(locations[i]-locations[j])\n        self.D = self.D + self.D.T\n    def get_weightmatrix(self,theta=1,beta=0.5,kappa=4000):\n        self.theta = theta\n        dist = np.where(self.D < kappa,self.D,0)\n        self.W = np.exp(-(dist/self.theta)**2)\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)       \n    def fit(self,sd=5,ref=60): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.f # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2),sd=sd))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(fHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.f- self.df.fHat)\n        self.dif=(np.abs(self.f-self.fhat)-np.min(np.abs(self.f-self.fhat)))/(np.max(np.abs(self.f-self.fhat))-np.min(np.abs(self.f-self.fhat)))\n        self.df = self.df.assign(dif = self.dif)\n        self.bottom = np.zeros_like(self.f)\n        self.width=0.05\n        self.depth=0.05\n#         fig = plt.figure(figsize=(10,10))\n        # ax = fig.add_subplot(1,1,1, projection='3d')\n        #\n        fig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(30,15),subplot_kw={\"projection\":\"3d\"})\n        ax1.grid(False)\n        ax1.scatter3D(self.x,self.y,self.f,zdir='z',s=50,marker='.',color='gray')\n        ax1.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.f[index_of_trueoutlier_bool],zdir='z',s=50,marker='.',color='red')\n        ax1.scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['f'],edgecolors='red',zdir='z',s=50,facecolors='none')\n        ax1.plot3D(self.x,self.y,self.f1,'--k',lw=3)\n        ax2.view_init(elev=30., azim=60)\n        \n        ax2.grid(False)\n        ax2.scatter3D(self.x,self.y,self.f,zdir='z',s=50,marker='.',color='gray')\n        ax2.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.f[index_of_trueoutlier_bool],zdir='z',s=50,marker='.',color='red') \n        ax2.scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['f'],edgecolors='red',zdir='z',s=50,facecolors='none')\n        ax2.plot3D(self.x,self.y,self.f1,'--k',lw=3)\n        ax2.view_init(elev=30., azim=40)\n        \n        ax3.grid(False)\n        ax3.scatter3D(self.x,self.y,self.f,zdir='z',s=50,marker='.',color='gray')\n        ax3.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.f[index_of_trueoutlier_bool],zdir='z',s=50,marker='.',color='red') \n        ax3.scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['f'],edgecolors='red',zdir='z',s=50,facecolors='none')\n        ax3.plot3D(self.x,self.y,self.f1,'--k',lw=3)\n        ax3.view_init(elev=30., azim=10)\n        \n        fig.savefig('fig2.eps',format='eps')\n\n\n_simul3d = SIMUL(df)\n\n\n_simul3d.get_distance()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:01<00:00, 562.21it/s]\n\n\n\n_simul3d.get_weightmatrix(theta=(_simul3d.D[_simul3d.D>0].mean()),kappa=2500) \n\n\n%%capture --no-display\n_simul3d.fit(sd=15,ref=20)"
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#bunny",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#bunny",
    "title": "Graph code",
    "section": "Bunny",
    "text": "Bunny\n\nG = graphs.Bunny()\nn = G.N\n\n\ng = filters.Heat(G, tau=75) # ê¼¬ë¦¬ë¶€ë¶„ì˜ ë¹¨ê°„ì‹ í˜¸ë¥¼ í¼ì§€ê²Œí•˜ëŠ” ì •ë„\n\n\nnormal = np.random.randn(n)\nunif = np.concatenate([np.random.uniform(low=3,high=7,size=60), np.random.uniform(low=-7,high=-3,size=60),np.zeros(n-120)]); np.random.shuffle(unif)\nnoise = normal + unif\n\n\nindex_of_trueoutlier_bool = (unif!=0)\n\n\nf = np.zeros(n)\nf[1000] = -3234\nf = g.filter(f, method='chebyshev') \n\n2022-11-10 21:12:29,879:[WARNING](pygsp.graphs.graph.lmax): The largest eigenvalue G.lmax is not available, we need to estimate it. Explicitly call G.estimate_lmax() or G.compute_fourier_basis() once beforehand to suppress the warning.\n\n\n\nW = G.W.toarray()\nx = G.coords[:,0]\ny = G.coords[:,1]\nz = -G.coords[:,2]\n\n\ndf = pd.DataFrame({'x' : x, 'y' : y, 'z' : z, 'f' : f, 'noise' : noise})\n\n\nclass SIMUL:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.f.to_numpy()\n        self.z = df.z.to_numpy()\n        self.x = df.x.to_numpy()\n        self.y = df.y.to_numpy()\n        self.noise = df.noise.to_numpy()\n        self.fnoise = self.f + self.noise\n        self.W = W\n        self.n = len(self.f)\n        self.theta= None\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)       \n    def fit(self,sd=2.5,ref=6): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.fnoise # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2),sd=sd))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(fnoise = self.fnoise)\n        self.df = self.df.assign(fHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.f + self.df.noise - self.df.fHat)\n        self.bottom = np.zeros_like(self.f)\n        self.width=0.05\n        self.depth=0.05\n        \n        fig = plt.figure(figsize=(30,12),dpi=400)\n        ax1 = fig.add_subplot(251, projection='3d')\n        ax1.grid(False)\n        ax1.scatter3D(self.x,self.y,self.z,c='gray',zdir='z',alpha=0.5,marker='.')\n        ax1.view_init(elev=60., azim=-90)\n\n        ax2= fig.add_subplot(252, projection='3d')\n        ax2.grid(False)\n        ax2.scatter3D(self.x,self.y,self.z,c=self.f,cmap='hsv',zdir='z',marker='.',alpha=0.5,vmin=-12,vmax=10)\n        ax2.view_init(elev=60., azim=-90)\n\n        ax3= fig.add_subplot(253, projection='3d')\n        ax3.grid(False)\n        ax3.scatter3D(self.x,self.y,self.z,c=self.fnoise,cmap='hsv',zdir='z',marker='.',alpha=0.5,vmin=-12,vmax=10)\n        ax3.view_init(elev=60., azim=-90)\n        \n        ax4= fig.add_subplot(254, projection='3d')\n        ax4.grid(False)\n        ax4.scatter3D(self.x,self.y,self.z,c=self.fnoise,cmap='hsv',zdir='z',marker='.',vmin=-12,vmax=10,s=1)\n        ax4.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.z[index_of_trueoutlier_bool],c=self.fnoise[index_of_trueoutlier_bool],cmap='hsv',zdir='z',marker='.',s=50)\n        ax4.view_init(elev=60., azim=-90)\n\n        ax5= fig.add_subplot(255, projection='3d')\n        ax5.grid(False)\n        ax5.scatter3D(self.x,self.y,self.z,c=self.fnoise,cmap='hsv',zdir='z',marker='.',vmin=-12,vmax=10,s=1)\n        ax5.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.z[index_of_trueoutlier_bool],c=self.fnoise[index_of_trueoutlier_bool],cmap='hsv',zdir='z',marker='.',s=50)\n        ax5.scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['z'],zdir='z',s=550,marker='.',edgecolors='red',facecolors='none')\n        ax5.view_init(elev=60., azim=-90)\n        \n        ax6 = fig.add_subplot(256, projection='3d')\n        ax6.grid(False)\n        ax6.scatter3D(self.x,self.y,self.z,c='gray',zdir='z',alpha=0.5,marker='.')\n        ax6.view_init(elev=-60., azim=-90)\n\n        ax7= fig.add_subplot(257, projection='3d')\n        ax7.grid(False)\n        ax7.scatter3D(self.x,self.y,self.z,c=self.f,cmap='hsv',zdir='z',marker='.',alpha=0.5,vmin=-12,vmax=10)\n        ax7.view_init(elev=-60., azim=-90)\n\n        ax8= fig.add_subplot(258, projection='3d')\n        ax8.grid(False)\n        ax8.scatter3D(self.x,self.y,self.z,c=self.fnoise,cmap='hsv',zdir='z',marker='.',alpha=0.5,vmin=-12,vmax=10)\n        ax8.view_init(elev=-60., azim=-90)\n        \n        ax9= fig.add_subplot(259, projection='3d')\n        ax9.grid(False)\n        ax9.scatter3D(self.x,self.y,self.z,c=self.fnoise,cmap='hsv',zdir='z',marker='.',vmin=-12,vmax=10,s=1)\n        ax9.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.z[index_of_trueoutlier_bool],c=self.fnoise[index_of_trueoutlier_bool],cmap='hsv',zdir='z',marker='.',s=50)\n        ax9.view_init(elev=-60., azim=-90)\n\n        ax10= fig.add_subplot(2,5,10, projection='3d')\n        ax10.grid(False)\n        ax10.scatter3D(self.x,self.y,self.z,c=self.fnoise,cmap='hsv',zdir='z',marker='.',vmin=-12,vmax=10,s=1)\n        ax10.scatter3D(self.x[index_of_trueoutlier_bool],self.y[index_of_trueoutlier_bool],self.z[index_of_trueoutlier_bool],c=self.fnoise[index_of_trueoutlier_bool],cmap='hsv',zdir='z',marker='.',s=50)\n        ax10.scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['z'],zdir='z',s=550,marker='.',edgecolors='red',facecolors='none')\n        ax10.view_init(elev=-60., azim=-90)        \n        fig.savefig('fig_bunny.eps',format='eps')\n\n\n_simul = SIMUL(df)\n\n\nmax(_simul.f),max(_simul.fnoise)\n\n(-0.010827167666814895, 8.453057038638512)\n\n\n\nmin(_simul.f),min(_simul.fnoise)\n\n(-4.74620052476489, -11.196627043702925)\n\n\n\n%%capture --no-display\n_simul.fit(sd=20,ref=10)"
  },
  {
    "objectID": "posts/GODE/2022-12-01-graph_code_guebin.html#earthquake",
    "href": "posts/GODE/2022-12-01-graph_code_guebin.html#earthquake",
    "title": "Graph code",
    "section": "Earthquake",
    "text": "Earthquake\n\ndf= pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv')\n\n\ndf_global= pd.concat([pd.read_csv('00_05.csv'),pd.read_csv('05_10.csv'),pd.read_csv('10_15.csv'),pd.read_csv('15_20.csv')]).iloc[:,[0,1,2,4]].rename(columns={'latitude':'Latitude','longitude':'Longitude','mag':'Magnitude'}).reset_index().iloc[:,1:]\n\n\ndf_global = df_global.assign(Year=list(map(lambda x: x.split('-')[0], df_global.time))).iloc[:,1:]\n\n\ndf_global.Year = df_global.Year.astype(np.float64)\n\n\nclass MooYaHo:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.Magnitude.to_numpy()\n        self.year = df.Year.to_numpy()\n        self.lat = df.Latitude.to_numpy()\n        self.long = df.Longitude.to_numpy()\n        self.n = len(self.f)\n        \n        self.theta= None\n    def get_distance(self):\n        self.D = np.zeros([self.n,self.n])\n        locations = np.stack([self.lat, self.long],axis=1)\n        for i in tqdm.tqdm(range(self.n)):\n            for j in range(i,self.n): \n                self.D[i,j]=haversine(locations[i],locations[j])\n        self.D = self.D+self.D.T\n    def get_weightmatrix(self,theta=1,beta=0.5,kappa=4000):\n        self.theta = theta\n        dist = np.where(self.D<kappa,self.D,0)\n        self.W = np.exp(-(dist/self.theta)**2)\n\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)        \n    def fit(self,m):\n        self._eigen()\n        self.fhat = self.Psi[:,0:m]@self.Psi[:,0:m].T@self.f\n        self.df = self.df.assign(MagnitudeHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.Magnitude- self.df.MagnitudeHat)\n        plt.plot(self.f,'.')\n        plt.plot(self.fhat,'x')\n\n\nclass MooYaHo2(MooYaHo): # ebayesthresh ê¸°ëŠ¥ì¶”ê°€\n    def fit2(self,ref=0.5): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.f # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2)))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(MagnitudeHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.Magnitude- self.df.MagnitudeHat)\n        self.con = np.where(self.df.Residual>0.7,1,0)\n\n\nclass eachlocation(MooYaHo2):\n    def haiti(self,MagThresh=7,ResThresh=1,adjzoom=5,adjmarkersize = 40):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=15,\n                        center=dict(lat=18.4430, lon=-72.5710), \n                        zoom= adjzoom,\n                        height=900,\n                        opacity = 0.8,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-3,3])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 5,\n                      marker_color= 'blue',\n                      opacity = 0.1\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= adjmarkersize,\n                      marker_color= 'red',\n                      opacity = 0.8\n                      )\n        fig.add_trace(go.Scattermapbox(\n                    lat=self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                    lon=self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                    mode='markers',\n                    marker=go.scattermapbox.Marker(\n                        size=20,\n                        color='rgb(255, 255, 255)',\n                        opacity=0.4\n                    )\n                ))\n        return fig \n    def lquique(self,MagThresh=7,ResThresh=1,adjzoom=5, adjmarkersize= 40):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=15,\n                        center=dict(lat=-32.6953, lon=-71.4416), \n                        zoom=adjzoom,\n                        height=900,\n                        opacity = 0.8,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 5,\n                      marker_color= 'blue',\n                      opacity = 0.1\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= adjmarkersize,\n                      marker_color= 'red',\n                      opacity = 0.8\n                      )\n        fig.add_trace(go.Scattermapbox(\n                    lat=self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                    lon=self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                    mode='markers',\n                    marker=go.scattermapbox.Marker(\n                        size=20,\n                        color='rgb(255, 255, 255)',\n                        opacity=0.8\n                    )\n                ))\n        return fig \n    def sichuan(self,MagThresh=7,ResThresh=1,adjzoom=5,adjmarkersize=40):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=15,\n                        center=dict(lat=30.3080, lon=102.8880), \n                        zoom=adjzoom,\n                        height=900,\n                        opacity = 0.6,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 5,\n                      marker_color= 'blue',\n                      opacity = 0.1\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= adjmarkersize,\n                      marker_color= 'red',\n                      opacity = 0.8\n                      )\n        fig.add_trace(go.Scattermapbox(\n                    lat=self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                    lon=self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                    mode='markers',\n                    marker=go.scattermapbox.Marker(\n                        size=20,\n                        color='rgb(255, 255, 255)',\n                        opacity=0.8\n                    )\n                ))\n        return fig \n\n\neach_location=eachlocation(df_global.query(\"2010 <= Year < 2015\"))\n\n- get distance\n\neach_location.get_distance()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12498/12498 [03:24<00:00, 61.15it/s] \n\n\n\neach_location.D[each_location.D>0].mean()\n\n8810.865423093777\n\n\n\nplt.hist(each_location.D[each_location.D>0])\n\n(array([14176290., 16005894., 21186674., 22331128., 19394182., 17548252.,\n        16668048., 13316436., 12973260.,  2582550.]),\n array([8.97930163e-02, 2.00141141e+03, 4.00273303e+03, 6.00405465e+03,\n        8.00537626e+03, 1.00066979e+04, 1.20080195e+04, 1.40093411e+04,\n        1.60106627e+04, 1.80119844e+04, 2.00133060e+04]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n- weight matrix\n\neach_location.get_weightmatrix(theta=(8810.865423093777),kappa=2500) \n\n- fit\n\neach_location.fit2()\n\n\neach_location.haiti(MagThresh=6.9,ResThresh=0.5,adjzoom=5,adjmarkersize=40)\nfig = each_location.haiti(MagThresh=6.9,ResThresh=0.5,adjzoom=5,adjmarkersize=40)\nfig.write_image('fig_haiti.png',scale=3)\n\n\neach_location.lquique(MagThresh=6.4,ResThresh=0.4,adjzoom=5,adjmarkersize=40)\n# fig = each_location.lquique(MagThresh=6.4,ResThresh=0.4,adjzoom=5,adjmarkersize=20)\n# fig.write_image('fig_lquique.svg',scale=3)\n\n\neach_location.sichuan(MagThresh=6.5,ResThresh=0.4,adjzoom=5,adjmarkersize=40)\n# fig = each_location.sichuan(MagThresh=6.5,ResThresh=0.4,adjzoom=5,adjmarkersize=20)\n# fig.write_image('fig_sichuan.svg',scale=3)"
  },
  {
    "objectID": "posts/GODE/index.html",
    "href": "posts/GODE/index.html",
    "title": "GODE",
    "section": "",
    "text": "About GODE paper"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html",
    "title": "Class code for Comparison Study",
    "section": "",
    "text": "Simulation\nex - The Stanford bunny data generated using the pygsp package is a common graphics 3D test model and NN-graph. It has 2503 data. We use filter.Heat in this package and it calculate data by \\(\\hat{g}(x) = \\exp(\\frac{-\\tau x}{\\lambda_{max}})\\) and \\(\\tau\\) is 75. We use Chebyshev polynomial approximation on this filter. We make zero vector whixh size is 2503, and put -3000 to one value to use a Lanczos approximation. A Lanczos approximation will resize signals by flattened.\nref: https://pygsp.readthedocs.io/en/v0.5.1/reference/filters.html\n\\(r = 5 + \\cos(\\frac{12\\pi - (-\\pi)}{n})\\times i , (i=1,2,\\dots , n)\\)\n\\(r \\cos(\\frac{\\pi - 2\\times \\pi /n - (-\\pi) }{n}\\times i)),(i=1,2,\\dots ,n)\\)\n\\(r \\sin((\\frac{\\pi - 2\\times \\pi /n - (-\\pi) }{n}\\times i)),(i=1,2,\\dots ,n)\\)\n\\(f = 10 \\times (\\frac{6 \\pi}{n} \\times i),(i=1,2, \\dots , n)\\)"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#import",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#import",
    "title": "Class code for Comparison Study",
    "section": "Import",
    "text": "Import\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.linear_model import SGDOneClassSVM\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.pipeline import make_pipeline\n\nimport pandas as pd\nfrom sklearn.neighbors import LocalOutlierFactor\n\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr\n\nfrom sklearn.datasets import fetch_kddcup99, fetch_covtype, fetch_openml\nfrom sklearn.preprocessing import LabelBinarizer\n\nimport tqdm\n\nfrom pygsp import graphs, filters, plotting, utils\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\nimport plotly.graph_objects as go\nfrom IPython.display import HTML\n\nimport plotly.express as px\n\nfrom sklearn.covariance import EmpiricalCovariance, MinCovDet\n\nfrom alibi_detect.od import IForest\n# from pyod.models.iforest import IForest\n\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nimport seaborn as sns\n\nfrom PyNomaly import loop\n\nfrom sklearn import svm\n\nfrom pyod.models.lscp import LSCP\nfrom pyod.models.hbos import HBOS\n\nfrom pyod.models.so_gaal import SO_GAAL\nfrom pyod.models.mcd import MCD\nfrom pyod.models.mo_gaal import MO_GAAL\nfrom pyod.models.knn import KNN\nfrom pyod.models.lof import LOF\nfrom pyod.models.ocsvm import OCSVM\n\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.sos import SOS"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#class-code",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#class-code",
    "title": "Class code for Comparison Study",
    "section": "Class Code",
    "text": "Class Code\n\ntab_linear = pd.DataFrame(columns=[\"Accuracy\",\"Precision\",\"Recall\",\"F1\"])\ntab_orbit = pd.DataFrame(columns=[\"Accuracy\",\"Precision\",\"Recall\",\"F1\"])\ntab_bunny = pd.DataFrame(columns=[\"Accuracy\",\"Precision\",\"Recall\",\"F1\"])\n\n\nclass Conf_matrx:\n    def __init__(self,original,compare,tab):\n        self.original = original\n        self.compare = compare\n        self.tab = tab\n    def conf(self,name):\n        self.conf_matrix = confusion_matrix(self.original, self.compare)\n        \n        fig, ax = plt.subplots(figsize=(5, 5))\n        ax.matshow(self.conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n        for i in range(self.conf_matrix.shape[0]):\n            for j in range(self.conf_matrix.shape[1]):\n                ax.text(x=j, y=i,s=self.conf_matrix[i, j], va='center', ha='center', size='xx-large')\n        plt.xlabel('Predictions', fontsize=18)\n        plt.ylabel('Actuals', fontsize=18)\n        plt.title('Confusion Matrix', fontsize=18)\n        plt.show()\n        \n        self.acc = accuracy_score(self.original, self.compare)\n        self.pre = precision_score(self.original, self.compare)\n        self.rec = recall_score(self.original, self.compare)\n        self.f1 = f1_score(self.original, self.compare)\n        \n        print('Accuracy: %.3f' % self.acc)\n        print('Precision: %.3f' % self.pre)\n        print('Recall: %.3f' % self.rec)\n        print('F1 Score: %.3f' % self.f1)\n        \n        self.tab = self.tab.append(pd.DataFrame({\"Accuracy\":[self.acc],\"Precision\":[self.pre],\"Recall\":[self.rec],\"F1\":[self.f1]},index = [name]))\n\n\nclass Linear:\n    def __init__(self,df):\n        self.df = df\n        self.y = df.y.to_numpy()\n        #self.y1 = df.y1.to_numpy()\n        self.x = df.x.to_numpy()\n        self.n = len(self.y)\n        self.W = w\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)      \n    def fit(self,sd=20): # fit with ebayesthresh\n        self._eigen()\n        self.ybar = self.Psi.T @ self.y # fbar := graph fourier transform of f\n        self.power = self.ybar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.ybar**2),sd=sd))\n        self.ybar_threshed = np.where(self.power_threshed>0,self.ybar,0)\n        self.yhat = self.Psi@self.ybar_threshed\n        self.df = self.df.assign(yHat = self.yhat)\n        self.df = self.df.assign(Residual = self.df.y- self.df.yHat)\n\n\nclass Orbit:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.f.to_numpy()\n        self.x = df.x.to_numpy()\n        self.y = df.y.to_numpy()\n        self.n = len(self.f)\n        self.theta= None\n    def get_distance(self):\n        self.D = np.zeros([self.n,self.n])\n        locations = np.stack([self.x, self.y],axis=1)\n        for i in tqdm.tqdm(range(self.n)):\n            for j in range(i,self.n):\n                self.D[i,j]=np.linalg.norm(locations[i]-locations[j])\n        self.D = self.D + self.D.T\n    def get_weightmatrix(self,theta=1,beta=0.5,kappa=4000):\n        self.theta = theta\n        dist = np.where(self.D < kappa,self.D,0)\n        self.W = np.exp(-(dist/self.theta)**2)\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)       \n    def fit(self,sd=5,ref=20): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.f # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2),sd=sd))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(fHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.f- self.df.fHat)\n        self.bottom = np.zeros_like(self.f)\n        self.width=0.05\n        self.depth=0.05\n\n\nclass BUNNY:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.f.to_numpy()\n        self.z = df.z.to_numpy()\n        self.x = df.x.to_numpy()\n        self.y = df.y.to_numpy()\n        self.noise = df.noise.to_numpy()\n        self.fnoise = self.f + self.noise\n        self.W = _W\n        self.n = len(self.f)\n        self.theta= None\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)       \n    def fit(self,sd=5,ref=6): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.fnoise # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2),sd=sd))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(fnoise = self.fnoise)\n        self.df = self.df.assign(fHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.f + self.df.noise - self.df.fHat)\n        self.bottom = np.zeros_like(self.f)\n        self.width=0.05\n        self.depth=0.05"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#linear-ebayesthresh",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#linear-ebayesthresh",
    "title": "Class code for Comparison Study",
    "section": "Linear EbayesThresh",
    "text": "Linear EbayesThresh\n\n%load_ext rpy2.ipython\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython\n\n\n\n%%R\nlibrary(EbayesThresh)\nset.seed(1)\nepsilon = rnorm(1000)\nsignal_1 = sample(c(runif(25,-2,-1.5), runif(25,1.5,2), rep(0,950)))\nindex_of_trueoutlier_1 = which(signal_1!=0)\nindex_of_trueoutlier_1\nx_1=signal_1+epsilon\n\n\n%R -o x_1\n%R -o index_of_trueoutlier_1\n%R -o signal_1\n\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\n\n\noutlier_true_index_1 = index_of_trueoutlier_1\n\n\noutlier_true_value_1 = x_1[index_of_trueoutlier_1]\n\n\noutlier_true_one_1 = signal_1.copy()\n\n\noutlier_true_one_1 = list(map(lambda x: -1 if x!=0 else 1,outlier_true_one_1))"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#linear",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#linear",
    "title": "Class code for Comparison Study",
    "section": "Linear",
    "text": "Linear\n\n_x_1 = np.linspace(0,2,1000)\n_y1_1 = 5*_x_1\n_y_1 = _y1_1 + x_1 # x is epsilon\n\n\n_df=pd.DataFrame({'x':_x_1, 'y':_y_1})\n\n\nX = np.array(_df)\n\n\nGODE\n\nw=np.zeros((1000,1000))\n\n\nfor i in range(1000):\n    for j in range(1000):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\n_Linear = Linear(_df)\n\n\n_Linear.fit(sd=5)\n\n\noutlier_simul_one = (_Linear.df['Residual']**2).tolist()\n\n\noutlier_simul_one = list(map(lambda x: -1 if x > 20 else 1,outlier_simul_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_simul_one,tab_linear)\n\n\n_conf.conf(\"GODE\")\n\n\n\n\nAccuracy: 0.950\nPrecision: 0.950\nRecall: 1.000\nF1 Score: 0.974\n\n\n\none = _conf.tab\n\n\n\nLOF\n\nclf = LocalOutlierFactor(n_neighbors=2)\n\n\n_conf = Conf_matrx(outlier_true_one_1,clf.fit_predict(X),tab_linear)\n\n\n_conf.conf(\"LOF (Breunig et al., 2000)\")\n\n\n\n\nAccuracy: 0.890\nPrecision: 0.973\nRecall: 0.909\nF1 Score: 0.940\n\n\n\ntwo = one.append(_conf.tab)\n\n\n\nKNN\n\nfrom pyod.models.knn import KNN\n\n\nclf = KNN()\nclf.fit(_df[['x', 'y']])\n_df['knn_Clf'] = clf.labels_\n\n\noutlier_KNN_one = list(clf.labels_)\n\n\noutlier_KNN_one = list(map(lambda x: 1 if x==0  else -1,outlier_KNN_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_KNN_one,tab_linear)\n\n\n_conf.conf(\"kNN (Ramaswamy et al., 2000)\")\n\n\n\n\nAccuracy: 0.912\nPrecision: 0.979\nRecall: 0.927\nF1 Score: 0.952\n\n\n\nthree = two.append(_conf.tab)\n\n\n\nCBLOF\n\nclf = CBLOF(contamination=0.05,check_estimator=False, random_state=77)\nclf.fit(_df[['x', 'y']])\n_df['CBLOF_Clf'] = clf.labels_\n\n\noutlier_CBLOF_one = list(clf.labels_)\n\n\noutlier_CBLOF_one = list(map(lambda x: 1 if x==0  else -1,outlier_CBLOF_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_CBLOF_one,tab_linear)\n\n\n_conf.conf(\"CBLOF (He et al., 2003)\")\n\n\n\n\nAccuracy: 0.920\nPrecision: 0.958\nRecall: 0.958\nF1 Score: 0.958\n\n\n\nfour = three.append(_conf.tab)\n\n\n\nOCSVM\n\nclf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n\n\nclf.fit(X)\n\nOneClassSVM(gamma=0.1, nu=0.1)\n\n\n\noutlier_OSVM_one = list(clf.predict(X))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_OSVM_one,tab_linear)\n\n\n_conf.conf(\"OCSVM (Sch Ìˆolkopf et al., 2001)\")\n\n\n\n\nAccuracy: 0.909\nPrecision: 0.978\nRecall: 0.925\nF1 Score: 0.951\n\n\n\nfive = four.append(_conf.tab)\n\n\n\nMCD\n\nclf = MCD()\nclf.fit(_df[['x', 'y']])\n_df['MCD_clf'] = clf.labels_\n\n\noutlier_MCD_one = list(clf.labels_)\n\n\noutlier_MCD_one = list(map(lambda x: 1 if x==0  else -1,outlier_MCD_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_MCD_one,tab_linear)\n\n\n_conf.conf(\"MCD (Hardin and Rocke, 2004)\")\n\n\n\n\nAccuracy: 0.918\nPrecision: 0.982\nRecall: 0.931\nF1 Score: 0.956\n\n\n\nsix = five.append(_conf.tab)\n\n\n\nFeature Bagging\n\nclf = FeatureBagging()\nclf.fit(_df[['x', 'y']])\n_df['FeatureBagging_clf'] = clf.labels_\n\n\noutlier_FeatureBagging_one = list(clf.labels_)\n\n\noutlier_FeatureBagging_one = list(map(lambda x: 1 if x==0  else -1,outlier_FeatureBagging_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_FeatureBagging_one,tab_linear)\n\n\n_conf.conf(\"Feature Bagging (Lazarevic and Kumar, 2005)\")\n\n\n\n\nAccuracy: 0.918\nPrecision: 0.982\nRecall: 0.931\nF1 Score: 0.956\n\n\n\nseven = six.append(_conf.tab)\n\n\n\nABOD\n\nclf = ABOD(contamination=0.05)\nclf.fit(_df[['x', 'y']])\n_df['ABOD_Clf'] = clf.labels_\n\n\noutlier_ABOD_one = list(clf.labels_)\n\n\noutlier_ABOD_one = list(map(lambda x: 1 if x==0  else -1,outlier_ABOD_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_ABOD_one,tab_linear)\n\n\n_conf.conf(\"ABOD (Kriegel et al., 2008)\")\n\n\n\n\nAccuracy: 0.946\nPrecision: 0.972\nRecall: 0.972\nF1 Score: 0.972\n\n\n\neight = seven.append(_conf.tab)\n\n\n\nIForest\n\nod = IForest(\n    threshold=0.,\n    n_estimators=100\n)\n\n\nod.fit(_df[['x', 'y']])\n\n\npreds = od.predict(\n    _df[['x', 'y']],\n    return_instance_score=True\n)\n\n\n_df['IF_alibi'] = preds['data']['is_outlier']\n\n\noutlier_alibi_one = _df['IF_alibi']\n\n\noutlier_alibi_one = list(map(lambda x: 1 if x==0  else -1,outlier_alibi_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_alibi_one,tab_linear)\n\n\n_conf.conf(\"Isolation Forest (Liu et al., 2008)\")\n\n\n\n\nAccuracy: 0.800\nPrecision: 0.984\nRecall: 0.802\nF1 Score: 0.884\n\n\n\nnine = eight.append(_conf.tab)\n\n\n\nHBOS\n\nclf = HBOS()\nclf.fit(_df[['x', 'y']])\n_df['HBOS_clf'] = clf.labels_\n\n\noutlier_HBOS_one = list(clf.labels_)\n\n\noutlier_HBOS_one = list(map(lambda x: 1 if x==0  else -1,outlier_HBOS_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_HBOS_one,tab_linear)\n\n\n_conf.conf(\"HBOS (Goldstein and Dengel, 2012)\")\n\n\n\n\nAccuracy: 0.889\nPrecision: 0.960\nRecall: 0.921\nF1 Score: 0.940\n\n\n\nten = nine.append(_conf.tab)\n\n\n\nSOS\n\noutlier_SOS_one = list(clf.labels_)\n\n\noutlier_SOS_one = list(map(lambda x: 1 if x==0  else -1,outlier_SOS_one))\n\n\nclf = SOS()\nclf.fit(_df[['x', 'y']])\n_df['SOS_clf'] = clf.labels_\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_SOS_one,tab_linear)\n\n\n_conf.conf(\"SOS (Janssens et al., 2012)\")\n\n\n\n\nAccuracy: 0.889\nPrecision: 0.960\nRecall: 0.921\nF1 Score: 0.940\n\n\n\neleven = ten.append(_conf.tab)\n\n\n\nSO_GAAL\n\nclf = SO_GAAL()\nclf.fit(_df[['x', 'y']])\n_df['SO_GAAL_clf'] = clf.labels_\n\nEpoch 1 of 60\n\nTesting for epoch 1 index 1:\n\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n\n\n\nTesting for epoch 1 index 2:\nEpoch 2 of 60\n\nTesting for epoch 2 index 1:\n\nTesting for epoch 2 index 2:\nEpoch 3 of 60\n\nTesting for epoch 3 index 1:\n\nTesting for epoch 3 index 2:\nEpoch 4 of 60\n\nTesting for epoch 4 index 1:\n\nTesting for epoch 4 index 2:\nEpoch 5 of 60\n\nTesting for epoch 5 index 1:\n\nTesting for epoch 5 index 2:\nEpoch 6 of 60\n\nTesting for epoch 6 index 1:\n\nTesting for epoch 6 index 2:\nEpoch 7 of 60\n\nTesting for epoch 7 index 1:\n\nTesting for epoch 7 index 2:\nEpoch 8 of 60\n\nTesting for epoch 8 index 1:\n\nTesting for epoch 8 index 2:\nEpoch 9 of 60\n\nTesting for epoch 9 index 1:\n\nTesting for epoch 9 index 2:\nEpoch 10 of 60\n\nTesting for epoch 10 index 1:\n\nTesting for epoch 10 index 2:\nEpoch 11 of 60\n\nTesting for epoch 11 index 1:\n\nTesting for epoch 11 index 2:\nEpoch 12 of 60\n\nTesting for epoch 12 index 1:\n\nTesting for epoch 12 index 2:\nEpoch 13 of 60\n\nTesting for epoch 13 index 1:\n\nTesting for epoch 13 index 2:\nEpoch 14 of 60\n\nTesting for epoch 14 index 1:\n\nTesting for epoch 14 index 2:\nEpoch 15 of 60\n\nTesting for epoch 15 index 1:\n\nTesting for epoch 15 index 2:\nEpoch 16 of 60\n\nTesting for epoch 16 index 1:\n\nTesting for epoch 16 index 2:\nEpoch 17 of 60\n\nTesting for epoch 17 index 1:\n\nTesting for epoch 17 index 2:\nEpoch 18 of 60\n\nTesting for epoch 18 index 1:\n\nTesting for epoch 18 index 2:\nEpoch 19 of 60\n\nTesting for epoch 19 index 1:\n\nTesting for epoch 19 index 2:\nEpoch 20 of 60\n\nTesting for epoch 20 index 1:\n\nTesting for epoch 20 index 2:\nEpoch 21 of 60\n\nTesting for epoch 21 index 1:\n\nTesting for epoch 21 index 2:\nEpoch 22 of 60\n\nTesting for epoch 22 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.3973\n\nTesting for epoch 22 index 2:\n16/16 [==============================] - 0s 679us/step - loss: 1.4180\nEpoch 23 of 60\n\nTesting for epoch 23 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4019\n\nTesting for epoch 23 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4210\nEpoch 24 of 60\n\nTesting for epoch 24 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4234\n\nTesting for epoch 24 index 2:\n16/16 [==============================] - 0s 691us/step - loss: 1.4552\nEpoch 25 of 60\n\nTesting for epoch 25 index 1:\n16/16 [==============================] - 0s 663us/step - loss: 1.4271\n\nTesting for epoch 25 index 2:\n16/16 [==============================] - 0s 767us/step - loss: 1.4613\nEpoch 26 of 60\n\nTesting for epoch 26 index 1:\n16/16 [==============================] - 0s 602us/step - loss: 1.4776\n\nTesting for epoch 26 index 2:\n16/16 [==============================] - 0s 715us/step - loss: 1.4349\nEpoch 27 of 60\n\nTesting for epoch 27 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4333\n\nTesting for epoch 27 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4840\nEpoch 28 of 60\n\nTesting for epoch 28 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5092\n\nTesting for epoch 28 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4956\nEpoch 29 of 60\n\nTesting for epoch 29 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5026\n\nTesting for epoch 29 index 2:\n16/16 [==============================] - 0s 831us/step - loss: 1.5576\nEpoch 30 of 60\n\nTesting for epoch 30 index 1:\n16/16 [==============================] - 0s 611us/step - loss: 1.5602\n\nTesting for epoch 30 index 2:\n16/16 [==============================] - 0s 638us/step - loss: 1.4791\nEpoch 31 of 60\n\nTesting for epoch 31 index 1:\n16/16 [==============================] - 0s 619us/step - loss: 1.5625\n\nTesting for epoch 31 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5635\nEpoch 32 of 60\n\nTesting for epoch 32 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5925\n\nTesting for epoch 32 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5807\nEpoch 33 of 60\n\nTesting for epoch 33 index 1:\n16/16 [==============================] - 0s 844us/step - loss: 1.5739\n\nTesting for epoch 33 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6122\nEpoch 34 of 60\n\nTesting for epoch 34 index 1:\n16/16 [==============================] - 0s 657us/step - loss: 1.6156\n\nTesting for epoch 34 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6021\nEpoch 35 of 60\n\nTesting for epoch 35 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6237\n\nTesting for epoch 35 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6302\nEpoch 36 of 60\n\nTesting for epoch 36 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6586\n\nTesting for epoch 36 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 1.6349\nEpoch 37 of 60\n\nTesting for epoch 37 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6708\n\nTesting for epoch 37 index 2:\n16/16 [==============================] - 0s 726us/step - loss: 1.7010\nEpoch 38 of 60\n\nTesting for epoch 38 index 1:\n16/16 [==============================] - 0s 826us/step - loss: 1.6865\n\nTesting for epoch 38 index 2:\n16/16 [==============================] - 0s 820us/step - loss: 1.6874\nEpoch 39 of 60\n\nTesting for epoch 39 index 1:\n16/16 [==============================] - 0s 680us/step - loss: 1.7410\n\nTesting for epoch 39 index 2:\n16/16 [==============================] - 0s 663us/step - loss: 1.7334\nEpoch 40 of 60\n\nTesting for epoch 40 index 1:\n16/16 [==============================] - 0s 637us/step - loss: 1.6871\n\nTesting for epoch 40 index 2:\n16/16 [==============================] - 0s 621us/step - loss: 1.7771\nEpoch 41 of 60\n\nTesting for epoch 41 index 1:\n16/16 [==============================] - 0s 629us/step - loss: 1.7724\n\nTesting for epoch 41 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.7815\nEpoch 42 of 60\n\nTesting for epoch 42 index 1:\n16/16 [==============================] - 0s 647us/step - loss: 1.7470\n\nTesting for epoch 42 index 2:\n16/16 [==============================] - 0s 612us/step - loss: 1.7897\nEpoch 43 of 60\n\nTesting for epoch 43 index 1:\n16/16 [==============================] - 0s 660us/step - loss: 1.8400\n\nTesting for epoch 43 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8351\nEpoch 44 of 60\n\nTesting for epoch 44 index 1:\n16/16 [==============================] - 0s 689us/step - loss: 1.8388\n\nTesting for epoch 44 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8174\nEpoch 45 of 60\n\nTesting for epoch 45 index 1:\n16/16 [==============================] - 0s 974us/step - loss: 1.8131\n\nTesting for epoch 45 index 2:\n16/16 [==============================] - 0s 712us/step - loss: 1.8391\nEpoch 46 of 60\n\nTesting for epoch 46 index 1:\n16/16 [==============================] - 0s 635us/step - loss: 1.7937\n\nTesting for epoch 46 index 2:\n16/16 [==============================] - 0s 971us/step - loss: 1.8550\nEpoch 47 of 60\n\nTesting for epoch 47 index 1:\n16/16 [==============================] - 0s 628us/step - loss: 1.8632\n\nTesting for epoch 47 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8457\nEpoch 48 of 60\n\nTesting for epoch 48 index 1:\n16/16 [==============================] - 0s 628us/step - loss: 1.8924\n\nTesting for epoch 48 index 2:\n16/16 [==============================] - 0s 640us/step - loss: 1.8481\nEpoch 49 of 60\n\nTesting for epoch 49 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8722\n\nTesting for epoch 49 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9405\nEpoch 50 of 60\n\nTesting for epoch 50 index 1:\n16/16 [==============================] - 0s 640us/step - loss: 1.9428\n\nTesting for epoch 50 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8585\nEpoch 51 of 60\n\nTesting for epoch 51 index 1:\n16/16 [==============================] - 0s 638us/step - loss: 1.8806\n\nTesting for epoch 51 index 2:\n16/16 [==============================] - 0s 608us/step - loss: 1.9145\nEpoch 52 of 60\n\nTesting for epoch 52 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9380\n\nTesting for epoch 52 index 2:\n16/16 [==============================] - 0s 615us/step - loss: 1.8934\nEpoch 53 of 60\n\nTesting for epoch 53 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9282\n\nTesting for epoch 53 index 2:\n16/16 [==============================] - 0s 651us/step - loss: 1.8956\nEpoch 54 of 60\n\nTesting for epoch 54 index 1:\n16/16 [==============================] - 0s 630us/step - loss: 1.8997\n\nTesting for epoch 54 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9230\nEpoch 55 of 60\n\nTesting for epoch 55 index 1:\n16/16 [==============================] - 0s 671us/step - loss: 1.9290\n\nTesting for epoch 55 index 2:\n16/16 [==============================] - 0s 885us/step - loss: 1.9631\nEpoch 56 of 60\n\nTesting for epoch 56 index 1:\n16/16 [==============================] - 0s 643us/step - loss: 1.9394\n\nTesting for epoch 56 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9368\nEpoch 57 of 60\n\nTesting for epoch 57 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9715\n\nTesting for epoch 57 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9327\nEpoch 58 of 60\n\nTesting for epoch 58 index 1:\n16/16 [==============================] - 0s 690us/step - loss: 1.9782\n\nTesting for epoch 58 index 2:\n16/16 [==============================] - 0s 612us/step - loss: 1.9637\nEpoch 59 of 60\n\nTesting for epoch 59 index 1:\n16/16 [==============================] - 0s 575us/step - loss: 1.9657\n\nTesting for epoch 59 index 2:\n16/16 [==============================] - 0s 890us/step - loss: 1.9923\nEpoch 60 of 60\n\nTesting for epoch 60 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9824\n\nTesting for epoch 60 index 2:\n16/16 [==============================] - 0s 658us/step - loss: 2.0536\n\n\n\noutlier_SO_GAAL_one = list(clf.labels_)\n\n\noutlier_SO_GAAL_one = list(map(lambda x: 1 if x==0  else -1,outlier_SO_GAAL_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_SO_GAAL_one,tab_linear)\n\n\n_conf.conf(\"SO-GAAL (Liu et al., 2019)\")\n\n\n\n\nAccuracy: 0.868\nPrecision: 0.954\nRecall: 0.904\nF1 Score: 0.929\n\n\n\ntwelve = eleven.append(_conf.tab)\n\n\n\nMO_GAAL\n\nclf = MO_GAAL()\nclf.fit(_df[['x', 'y']])\n_df['MO_GAAL_clf'] = clf.labels_\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n\n\nEpoch 1 of 60\n\nTesting for epoch 1 index 1:\n\nTesting for epoch 1 index 2:\nEpoch 2 of 60\n\nTesting for epoch 2 index 1:\n\nTesting for epoch 2 index 2:\nEpoch 3 of 60\n\nTesting for epoch 3 index 1:\n\nTesting for epoch 3 index 2:\nEpoch 4 of 60\n\nTesting for epoch 4 index 1:\n\nTesting for epoch 4 index 2:\nEpoch 5 of 60\n\nTesting for epoch 5 index 1:\n\nTesting for epoch 5 index 2:\nEpoch 6 of 60\n\nTesting for epoch 6 index 1:\n\nTesting for epoch 6 index 2:\nEpoch 7 of 60\n\nTesting for epoch 7 index 1:\n\nTesting for epoch 7 index 2:\nEpoch 8 of 60\n\nTesting for epoch 8 index 1:\n\nTesting for epoch 8 index 2:\nEpoch 9 of 60\n\nTesting for epoch 9 index 1:\n\nTesting for epoch 9 index 2:\nEpoch 10 of 60\n\nTesting for epoch 10 index 1:\n\nTesting for epoch 10 index 2:\nEpoch 11 of 60\n\nTesting for epoch 11 index 1:\n\nTesting for epoch 11 index 2:\nEpoch 12 of 60\n\nTesting for epoch 12 index 1:\n\nTesting for epoch 12 index 2:\nEpoch 13 of 60\n\nTesting for epoch 13 index 1:\n\nTesting for epoch 13 index 2:\nEpoch 14 of 60\n\nTesting for epoch 14 index 1:\n\nTesting for epoch 14 index 2:\nEpoch 15 of 60\n\nTesting for epoch 15 index 1:\n\nTesting for epoch 15 index 2:\nEpoch 16 of 60\n\nTesting for epoch 16 index 1:\n\nTesting for epoch 16 index 2:\nEpoch 17 of 60\n\nTesting for epoch 17 index 1:\n\nTesting for epoch 17 index 2:\nEpoch 18 of 60\n\nTesting for epoch 18 index 1:\n\nTesting for epoch 18 index 2:\nEpoch 19 of 60\n\nTesting for epoch 19 index 1:\n\nTesting for epoch 19 index 2:\nEpoch 20 of 60\n\nTesting for epoch 20 index 1:\n\nTesting for epoch 20 index 2:\nEpoch 21 of 60\n\nTesting for epoch 21 index 1:\n\nTesting for epoch 21 index 2:\n16/16 [==============================] - 0s 674us/step - loss: 0.5119\n16/16 [==============================] - 0s 1ms/step - loss: 0.8174\n16/16 [==============================] - 0s 1ms/step - loss: 1.0584\n16/16 [==============================] - 0s 1ms/step - loss: 1.2057\n16/16 [==============================] - 0s 1ms/step - loss: 1.2512\n16/16 [==============================] - 0s 653us/step - loss: 1.2690\n16/16 [==============================] - 0s 640us/step - loss: 1.2744\n16/16 [==============================] - 0s 846us/step - loss: 1.2761\n16/16 [==============================] - 0s 782us/step - loss: 1.2766\n16/16 [==============================] - 0s 1ms/step - loss: 1.2766\nEpoch 22 of 60\n\nTesting for epoch 22 index 1:\n16/16 [==============================] - 0s 665us/step - loss: 0.5016\n16/16 [==============================] - 0s 1ms/step - loss: 0.8168\n16/16 [==============================] - 0s 729us/step - loss: 1.0701\n16/16 [==============================] - 0s 619us/step - loss: 1.2239\n16/16 [==============================] - 0s 952us/step - loss: 1.2703\n16/16 [==============================] - 0s 680us/step - loss: 1.2884\n16/16 [==============================] - 0s 1ms/step - loss: 1.2938\n16/16 [==============================] - 0s 1ms/step - loss: 1.2955\n16/16 [==============================] - 0s 674us/step - loss: 1.2959\n16/16 [==============================] - 0s 680us/step - loss: 1.2959\n\nTesting for epoch 22 index 2:\n16/16 [==============================] - 0s 638us/step - loss: 0.4978\n16/16 [==============================] - 0s 1ms/step - loss: 0.8174\n16/16 [==============================] - 0s 988us/step - loss: 1.0777\n16/16 [==============================] - 0s 683us/step - loss: 1.2388\n16/16 [==============================] - 0s 1ms/step - loss: 1.2871\n16/16 [==============================] - 0s 1ms/step - loss: 1.3063\n16/16 [==============================] - 0s 899us/step - loss: 1.3121\n16/16 [==============================] - 0s 701us/step - loss: 1.3140\n16/16 [==============================] - 0s 674us/step - loss: 1.3144\n16/16 [==============================] - 0s 894us/step - loss: 1.3144\nEpoch 23 of 60\n\nTesting for epoch 23 index 1:\n16/16 [==============================] - 0s 629us/step - loss: 0.4856\n16/16 [==============================] - 0s 911us/step - loss: 0.8190\n16/16 [==============================] - 0s 1ms/step - loss: 1.0913\n16/16 [==============================] - 0s 1ms/step - loss: 1.2605\n16/16 [==============================] - 0s 1ms/step - loss: 1.3112\n16/16 [==============================] - 0s 1ms/step - loss: 1.3310\n16/16 [==============================] - 0s 2ms/step - loss: 1.3370\n16/16 [==============================] - 0s 1ms/step - loss: 1.3389\n16/16 [==============================] - 0s 745us/step - loss: 1.3393\n16/16 [==============================] - 0s 964us/step - loss: 1.3393\n\nTesting for epoch 23 index 2:\n16/16 [==============================] - 0s 640us/step - loss: 0.4780\n16/16 [==============================] - 0s 851us/step - loss: 0.8265\n16/16 [==============================] - 0s 1ms/step - loss: 1.1094\n16/16 [==============================] - 0s 1ms/step - loss: 1.2901\n16/16 [==============================] - 0s 702us/step - loss: 1.3448\n16/16 [==============================] - 0s 939us/step - loss: 1.3665\n16/16 [==============================] - 0s 854us/step - loss: 1.3731\n16/16 [==============================] - 0s 872us/step - loss: 1.3753\n16/16 [==============================] - 0s 633us/step - loss: 1.3759\n16/16 [==============================] - 0s 1ms/step - loss: 1.3759\nEpoch 24 of 60\n\nTesting for epoch 24 index 1:\n16/16 [==============================] - 0s 709us/step - loss: 0.4680\n16/16 [==============================] - 0s 964us/step - loss: 0.8342\n16/16 [==============================] - 0s 717us/step - loss: 1.1328\n16/16 [==============================] - 0s 631us/step - loss: 1.3245\n16/16 [==============================] - 0s 1ms/step - loss: 1.3825\n16/16 [==============================] - 0s 1ms/step - loss: 1.4056\n16/16 [==============================] - 0s 1ms/step - loss: 1.4125\n16/16 [==============================] - 0s 675us/step - loss: 1.4148\n16/16 [==============================] - 0s 1ms/step - loss: 1.4154\n16/16 [==============================] - 0s 1ms/step - loss: 1.4154\n\nTesting for epoch 24 index 2:\n16/16 [==============================] - 0s 683us/step - loss: 0.4700\n16/16 [==============================] - 0s 1ms/step - loss: 0.8212\n16/16 [==============================] - 0s 608us/step - loss: 1.1067\n16/16 [==============================] - 0s 1ms/step - loss: 1.2919\n16/16 [==============================] - 0s 645us/step - loss: 1.3484\n16/16 [==============================] - 0s 655us/step - loss: 1.3713\n16/16 [==============================] - 0s 1ms/step - loss: 1.3780\n16/16 [==============================] - 0s 707us/step - loss: 1.3802\n16/16 [==============================] - 0s 1ms/step - loss: 1.3807\n16/16 [==============================] - 0s 1ms/step - loss: 1.3807\nEpoch 25 of 60\n\nTesting for epoch 25 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4568\n16/16 [==============================] - 0s 1ms/step - loss: 0.8264\n16/16 [==============================] - 0s 866us/step - loss: 1.1304\n16/16 [==============================] - 0s 737us/step - loss: 1.3292\n16/16 [==============================] - 0s 1ms/step - loss: 1.3891\n16/16 [==============================] - 0s 859us/step - loss: 1.4139\n16/16 [==============================] - 0s 664us/step - loss: 1.4209\n16/16 [==============================] - 0s 1ms/step - loss: 1.4233\n16/16 [==============================] - 0s 632us/step - loss: 1.4239\n16/16 [==============================] - 0s 2ms/step - loss: 1.4239\n\nTesting for epoch 25 index 2:\n16/16 [==============================] - 0s 660us/step - loss: 0.4472\n16/16 [==============================] - 0s 613us/step - loss: 0.8308\n16/16 [==============================] - 0s 633us/step - loss: 1.1472\n16/16 [==============================] - 0s 640us/step - loss: 1.3580\n16/16 [==============================] - 0s 1ms/step - loss: 1.4212\n16/16 [==============================] - 0s 644us/step - loss: 1.4477\n16/16 [==============================] - 0s 621us/step - loss: 1.4553\n16/16 [==============================] - 0s 601us/step - loss: 1.4579\n16/16 [==============================] - 0s 799us/step - loss: 1.4585\n16/16 [==============================] - 0s 663us/step - loss: 1.4585\nEpoch 26 of 60\n\nTesting for epoch 26 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4458\n16/16 [==============================] - 0s 639us/step - loss: 0.8332\n16/16 [==============================] - 0s 622us/step - loss: 1.1594\n16/16 [==============================] - 0s 620us/step - loss: 1.3754\n16/16 [==============================] - 0s 987us/step - loss: 1.4394\n16/16 [==============================] - 0s 652us/step - loss: 1.4660\n16/16 [==============================] - 0s 641us/step - loss: 1.4735\n16/16 [==============================] - 0s 628us/step - loss: 1.4761\n16/16 [==============================] - 0s 1ms/step - loss: 1.4766\n16/16 [==============================] - 0s 597us/step - loss: 1.4766\n\nTesting for epoch 26 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4323\n16/16 [==============================] - 0s 1ms/step - loss: 0.8289\n16/16 [==============================] - 0s 1ms/step - loss: 1.1745\n16/16 [==============================] - 0s 2ms/step - loss: 1.4047\n16/16 [==============================] - 0s 1ms/step - loss: 1.4730\n16/16 [==============================] - 0s 835us/step - loss: 1.5017\n16/16 [==============================] - 0s 684us/step - loss: 1.5100\n16/16 [==============================] - 0s 643us/step - loss: 1.5128\n16/16 [==============================] - 0s 1ms/step - loss: 1.5135\n16/16 [==============================] - 0s 1ms/step - loss: 1.5135\nEpoch 27 of 60\n\nTesting for epoch 27 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4311\n16/16 [==============================] - 0s 693us/step - loss: 0.8327\n16/16 [==============================] - 0s 639us/step - loss: 1.1867\n16/16 [==============================] - 0s 606us/step - loss: 1.4217\n16/16 [==============================] - 0s 816us/step - loss: 1.4904\n16/16 [==============================] - 0s 828us/step - loss: 1.5189\n16/16 [==============================] - 0s 1ms/step - loss: 1.5270\n16/16 [==============================] - 0s 1ms/step - loss: 1.5298\n16/16 [==============================] - 0s 1ms/step - loss: 1.5303\n16/16 [==============================] - 0s 779us/step - loss: 1.5303\n\nTesting for epoch 27 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4224\n16/16 [==============================] - 0s 1ms/step - loss: 0.8208\n16/16 [==============================] - 0s 998us/step - loss: 1.1776\n16/16 [==============================] - 0s 1ms/step - loss: 1.4157\n16/16 [==============================] - 0s 635us/step - loss: 1.4850\n16/16 [==============================] - 0s 1ms/step - loss: 1.5136\n16/16 [==============================] - 0s 1ms/step - loss: 1.5217\n16/16 [==============================] - 0s 640us/step - loss: 1.5245\n16/16 [==============================] - 0s 590us/step - loss: 1.5251\n16/16 [==============================] - 0s 1ms/step - loss: 1.5251\nEpoch 28 of 60\n\nTesting for epoch 28 index 1:\n16/16 [==============================] - 0s 707us/step - loss: 0.4220\n16/16 [==============================] - 0s 790us/step - loss: 0.8241\n16/16 [==============================] - 0s 1ms/step - loss: 1.1871\n16/16 [==============================] - 0s 829us/step - loss: 1.4277\n16/16 [==============================] - 0s 796us/step - loss: 1.4965\n16/16 [==============================] - 0s 1ms/step - loss: 1.5243\n16/16 [==============================] - 0s 1ms/step - loss: 1.5321\n16/16 [==============================] - 0s 1ms/step - loss: 1.5347\n16/16 [==============================] - 0s 611us/step - loss: 1.5352\n16/16 [==============================] - 0s 607us/step - loss: 1.5351\n\nTesting for epoch 28 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4162\n16/16 [==============================] - 0s 765us/step - loss: 0.8240\n16/16 [==============================] - 0s 1ms/step - loss: 1.1967\n16/16 [==============================] - 0s 1ms/step - loss: 1.4447\n16/16 [==============================] - 0s 1ms/step - loss: 1.5154\n16/16 [==============================] - 0s 718us/step - loss: 1.5437\n16/16 [==============================] - 0s 1ms/step - loss: 1.5517\n16/16 [==============================] - 0s 1ms/step - loss: 1.5543\n16/16 [==============================] - 0s 659us/step - loss: 1.5548\n16/16 [==============================] - 0s 2ms/step - loss: 1.5547\nEpoch 29 of 60\n\nTesting for epoch 29 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4156\n16/16 [==============================] - 0s 843us/step - loss: 0.8177\n16/16 [==============================] - 0s 623us/step - loss: 1.1876\n16/16 [==============================] - 0s 1ms/step - loss: 1.4313\n16/16 [==============================] - 0s 1ms/step - loss: 1.4993\n16/16 [==============================] - 0s 1ms/step - loss: 1.5259\n16/16 [==============================] - 0s 723us/step - loss: 1.5332\n16/16 [==============================] - 0s 640us/step - loss: 1.5355\n16/16 [==============================] - 0s 625us/step - loss: 1.5358\n16/16 [==============================] - 0s 634us/step - loss: 1.5357\n\nTesting for epoch 29 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4090\n16/16 [==============================] - 0s 1ms/step - loss: 0.8233\n16/16 [==============================] - 0s 1ms/step - loss: 1.2093\n16/16 [==============================] - 0s 643us/step - loss: 1.4641\n16/16 [==============================] - 0s 627us/step - loss: 1.5348\n16/16 [==============================] - 0s 668us/step - loss: 1.5623\n16/16 [==============================] - 0s 885us/step - loss: 1.5697\n16/16 [==============================] - 0s 887us/step - loss: 1.5721\n16/16 [==============================] - 0s 640us/step - loss: 1.5724\n16/16 [==============================] - 0s 1ms/step - loss: 1.5724\nEpoch 30 of 60\n\nTesting for epoch 30 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4023\n16/16 [==============================] - 0s 847us/step - loss: 0.8249\n16/16 [==============================] - 0s 1ms/step - loss: 1.2211\n16/16 [==============================] - 0s 669us/step - loss: 1.4795\n16/16 [==============================] - 0s 837us/step - loss: 1.5497\n16/16 [==============================] - 0s 1ms/step - loss: 1.5763\n16/16 [==============================] - 0s 792us/step - loss: 1.5833\n16/16 [==============================] - 0s 1ms/step - loss: 1.5854\n16/16 [==============================] - 0s 821us/step - loss: 1.5856\n16/16 [==============================] - 0s 654us/step - loss: 1.5855\n\nTesting for epoch 30 index 2:\n16/16 [==============================] - 0s 815us/step - loss: 0.4039\n16/16 [==============================] - 0s 621us/step - loss: 0.8273\n16/16 [==============================] - 0s 636us/step - loss: 1.2286\n16/16 [==============================] - 0s 1ms/step - loss: 1.4900\n16/16 [==============================] - 0s 1ms/step - loss: 1.5605\n16/16 [==============================] - 0s 1ms/step - loss: 1.5869\n16/16 [==============================] - 0s 1ms/step - loss: 1.5938\n16/16 [==============================] - 0s 2ms/step - loss: 1.5958\n16/16 [==============================] - 0s 1ms/step - loss: 1.5960\n16/16 [==============================] - 0s 721us/step - loss: 1.5958\nEpoch 31 of 60\n\nTesting for epoch 31 index 1:\n16/16 [==============================] - 0s 822us/step - loss: 0.3922\n16/16 [==============================] - 0s 618us/step - loss: 0.8303\n16/16 [==============================] - 0s 979us/step - loss: 1.2484\n16/16 [==============================] - 0s 594us/step - loss: 1.5177\n16/16 [==============================] - 0s 584us/step - loss: 1.5887\n16/16 [==============================] - 0s 886us/step - loss: 1.6148\n16/16 [==============================] - 0s 616us/step - loss: 1.6214\n16/16 [==============================] - 0s 986us/step - loss: 1.6232\n16/16 [==============================] - 0s 634us/step - loss: 1.6234\n16/16 [==============================] - 0s 647us/step - loss: 1.6232\n\nTesting for epoch 31 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3917\n16/16 [==============================] - 0s 638us/step - loss: 0.8412\n16/16 [==============================] - 0s 1ms/step - loss: 1.2745\n16/16 [==============================] - 0s 1ms/step - loss: 1.5525\n16/16 [==============================] - 0s 597us/step - loss: 1.6251\n16/16 [==============================] - 0s 1ms/step - loss: 1.6514\n16/16 [==============================] - 0s 1ms/step - loss: 1.6580\n16/16 [==============================] - 0s 1ms/step - loss: 1.6598\n16/16 [==============================] - 0s 1ms/step - loss: 1.6598\n16/16 [==============================] - 0s 875us/step - loss: 1.6597\nEpoch 32 of 60\n\nTesting for epoch 32 index 1:\n16/16 [==============================] - 0s 643us/step - loss: 0.3861\n16/16 [==============================] - 0s 1ms/step - loss: 0.8410\n16/16 [==============================] - 0s 1ms/step - loss: 1.2819\n16/16 [==============================] - 0s 747us/step - loss: 1.5604\n16/16 [==============================] - 0s 1ms/step - loss: 1.6314\n16/16 [==============================] - 0s 2ms/step - loss: 1.6566\n16/16 [==============================] - 0s 1ms/step - loss: 1.6625\n16/16 [==============================] - 0s 1ms/step - loss: 1.6641\n16/16 [==============================] - 0s 682us/step - loss: 1.6641\n16/16 [==============================] - 0s 868us/step - loss: 1.6639\n\nTesting for epoch 32 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3785\n16/16 [==============================] - 0s 640us/step - loss: 0.8366\n16/16 [==============================] - 0s 620us/step - loss: 1.2831\n16/16 [==============================] - 0s 630us/step - loss: 1.5628\n16/16 [==============================] - 0s 569us/step - loss: 1.6327\n16/16 [==============================] - 0s 1ms/step - loss: 1.6570\n16/16 [==============================] - 0s 1ms/step - loss: 1.6626\n16/16 [==============================] - 0s 671us/step - loss: 1.6639\n16/16 [==============================] - 0s 1ms/step - loss: 1.6638\n16/16 [==============================] - 0s 1ms/step - loss: 1.6636\nEpoch 33 of 60\n\nTesting for epoch 33 index 1:\n16/16 [==============================] - 0s 686us/step - loss: 0.3794\n16/16 [==============================] - 0s 1ms/step - loss: 0.8368\n16/16 [==============================] - 0s 590us/step - loss: 1.2836\n16/16 [==============================] - 0s 1ms/step - loss: 1.5578\n16/16 [==============================] - 0s 1ms/step - loss: 1.6242\n16/16 [==============================] - 0s 1ms/step - loss: 1.6467\n16/16 [==============================] - 0s 1ms/step - loss: 1.6516\n16/16 [==============================] - 0s 880us/step - loss: 1.6526\n16/16 [==============================] - 0s 1ms/step - loss: 1.6524\n16/16 [==============================] - 0s 744us/step - loss: 1.6521\n\nTesting for epoch 33 index 2:\n16/16 [==============================] - 0s 639us/step - loss: 0.3767\n16/16 [==============================] - 0s 675us/step - loss: 0.8386\n16/16 [==============================] - 0s 636us/step - loss: 1.2925\n16/16 [==============================] - 0s 667us/step - loss: 1.5686\n16/16 [==============================] - 0s 570us/step - loss: 1.6342\n16/16 [==============================] - 0s 650us/step - loss: 1.6560\n16/16 [==============================] - 0s 1ms/step - loss: 1.6605\n16/16 [==============================] - 0s 1ms/step - loss: 1.6614\n16/16 [==============================] - 0s 828us/step - loss: 1.6611\n16/16 [==============================] - 0s 754us/step - loss: 1.6608\nEpoch 34 of 60\n\nTesting for epoch 34 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3785\n16/16 [==============================] - 0s 606us/step - loss: 0.8568\n16/16 [==============================] - 0s 1ms/step - loss: 1.3267\n16/16 [==============================] - 0s 1ms/step - loss: 1.6075\n16/16 [==============================] - 0s 632us/step - loss: 1.6723\n16/16 [==============================] - 0s 700us/step - loss: 1.6932\n16/16 [==============================] - 0s 814us/step - loss: 1.6974\n16/16 [==============================] - 0s 1ms/step - loss: 1.6980\n16/16 [==============================] - 0s 2ms/step - loss: 1.6977\n16/16 [==============================] - 0s 1ms/step - loss: 1.6974\n\nTesting for epoch 34 index 2:\n16/16 [==============================] - 0s 611us/step - loss: 0.3640\n16/16 [==============================] - 0s 586us/step - loss: 0.8516\n16/16 [==============================] - 0s 613us/step - loss: 1.3334\n16/16 [==============================] - 0s 631us/step - loss: 1.6188\n16/16 [==============================] - 0s 635us/step - loss: 1.6834\n16/16 [==============================] - 0s 1ms/step - loss: 1.7039\n16/16 [==============================] - 0s 600us/step - loss: 1.7078\n16/16 [==============================] - 0s 784us/step - loss: 1.7083\n16/16 [==============================] - 0s 1ms/step - loss: 1.7080\n16/16 [==============================] - 0s 654us/step - loss: 1.7076\nEpoch 35 of 60\n\nTesting for epoch 35 index 1:\n16/16 [==============================] - 0s 642us/step - loss: 0.3623\n16/16 [==============================] - 0s 870us/step - loss: 0.8627\n16/16 [==============================] - 0s 1ms/step - loss: 1.3550\n16/16 [==============================] - 0s 935us/step - loss: 1.6411\n16/16 [==============================] - 0s 631us/step - loss: 1.7039\n16/16 [==============================] - 0s 1ms/step - loss: 1.7231\n16/16 [==============================] - 0s 659us/step - loss: 1.7264\n16/16 [==============================] - 0s 1ms/step - loss: 1.7267\n16/16 [==============================] - 0s 1ms/step - loss: 1.7262\n16/16 [==============================] - 0s 706us/step - loss: 1.7259\n\nTesting for epoch 35 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3604\n16/16 [==============================] - 0s 635us/step - loss: 0.8668\n16/16 [==============================] - 0s 889us/step - loss: 1.3676\n16/16 [==============================] - 0s 1ms/step - loss: 1.6572\n16/16 [==============================] - 0s 947us/step - loss: 1.7200\n16/16 [==============================] - 0s 2ms/step - loss: 1.7389\n16/16 [==============================] - 0s 772us/step - loss: 1.7421\n16/16 [==============================] - 0s 1ms/step - loss: 1.7424\n16/16 [==============================] - 0s 1ms/step - loss: 1.7419\n16/16 [==============================] - 0s 744us/step - loss: 1.7415\nEpoch 36 of 60\n\nTesting for epoch 36 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3571\n16/16 [==============================] - 0s 680us/step - loss: 0.8755\n16/16 [==============================] - 0s 751us/step - loss: 1.3899\n16/16 [==============================] - 0s 1ms/step - loss: 1.6814\n16/16 [==============================] - 0s 1ms/step - loss: 1.7429\n16/16 [==============================] - 0s 1ms/step - loss: 1.7609\n16/16 [==============================] - 0s 681us/step - loss: 1.7637\n16/16 [==============================] - 0s 677us/step - loss: 1.7638\n16/16 [==============================] - 0s 646us/step - loss: 1.7633\n16/16 [==============================] - 0s 1ms/step - loss: 1.7629\n\nTesting for epoch 36 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3680\n16/16 [==============================] - 0s 623us/step - loss: 0.8560\n16/16 [==============================] - 0s 652us/step - loss: 1.3413\n16/16 [==============================] - 0s 605us/step - loss: 1.6123\n16/16 [==============================] - 0s 622us/step - loss: 1.6680\n16/16 [==============================] - 0s 808us/step - loss: 1.6837\n16/16 [==============================] - 0s 1ms/step - loss: 1.6859\n16/16 [==============================] - 0s 889us/step - loss: 1.6858\n16/16 [==============================] - 0s 633us/step - loss: 1.6852\n16/16 [==============================] - 0s 626us/step - loss: 1.6848\nEpoch 37 of 60\n\nTesting for epoch 37 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3582\n16/16 [==============================] - 0s 660us/step - loss: 0.8667\n16/16 [==============================] - 0s 649us/step - loss: 1.3768\n16/16 [==============================] - 0s 1000us/step - loss: 1.6562\n16/16 [==============================] - 0s 1ms/step - loss: 1.7123\n16/16 [==============================] - 0s 634us/step - loss: 1.7277\n16/16 [==============================] - 0s 685us/step - loss: 1.7297\n16/16 [==============================] - 0s 1ms/step - loss: 1.7295\n16/16 [==============================] - 0s 1ms/step - loss: 1.7288\n16/16 [==============================] - 0s 628us/step - loss: 1.7284\n\nTesting for epoch 37 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3583\n16/16 [==============================] - 0s 1ms/step - loss: 0.8671\n16/16 [==============================] - 0s 1ms/step - loss: 1.3803\n16/16 [==============================] - 0s 1ms/step - loss: 1.6596\n16/16 [==============================] - 0s 1ms/step - loss: 1.7150\n16/16 [==============================] - 0s 692us/step - loss: 1.7298\n16/16 [==============================] - 0s 979us/step - loss: 1.7317\n16/16 [==============================] - 0s 1ms/step - loss: 1.7314\n16/16 [==============================] - 0s 1ms/step - loss: 1.7308\n16/16 [==============================] - 0s 1ms/step - loss: 1.7304\nEpoch 38 of 60\n\nTesting for epoch 38 index 1:\n16/16 [==============================] - 0s 863us/step - loss: 0.3471\n16/16 [==============================] - 0s 661us/step - loss: 0.8780\n16/16 [==============================] - 0s 659us/step - loss: 1.4219\n16/16 [==============================] - 0s 1ms/step - loss: 1.7117\n16/16 [==============================] - 0s 1ms/step - loss: 1.7680\n16/16 [==============================] - 0s 1ms/step - loss: 1.7827\n16/16 [==============================] - 0s 824us/step - loss: 1.7845\n16/16 [==============================] - 0s 1ms/step - loss: 1.7841\n16/16 [==============================] - 0s 1ms/step - loss: 1.7834\n16/16 [==============================] - 0s 2ms/step - loss: 1.7830\n\nTesting for epoch 38 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3466\n16/16 [==============================] - 0s 680us/step - loss: 0.8801\n16/16 [==============================] - 0s 1ms/step - loss: 1.4285\n16/16 [==============================] - 0s 615us/step - loss: 1.7186\n16/16 [==============================] - 0s 1ms/step - loss: 1.7739\n16/16 [==============================] - 0s 1ms/step - loss: 1.7880\n16/16 [==============================] - 0s 1ms/step - loss: 1.7895\n16/16 [==============================] - 0s 619us/step - loss: 1.7890\n16/16 [==============================] - 0s 1ms/step - loss: 1.7882\n16/16 [==============================] - 0s 587us/step - loss: 1.7878\nEpoch 39 of 60\n\nTesting for epoch 39 index 1:\n16/16 [==============================] - 0s 861us/step - loss: 0.3492\n16/16 [==============================] - 0s 879us/step - loss: 0.8719\n16/16 [==============================] - 0s 664us/step - loss: 1.4147\n16/16 [==============================] - 0s 643us/step - loss: 1.6946\n16/16 [==============================] - 0s 1ms/step - loss: 1.7465\n16/16 [==============================] - 0s 621us/step - loss: 1.7591\n16/16 [==============================] - 0s 594us/step - loss: 1.7602\n16/16 [==============================] - 0s 612us/step - loss: 1.7596\n16/16 [==============================] - 0s 594us/step - loss: 1.7588\n16/16 [==============================] - 0s 660us/step - loss: 1.7584\n\nTesting for epoch 39 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3448\n16/16 [==============================] - 0s 891us/step - loss: 0.8840\n16/16 [==============================] - 0s 1ms/step - loss: 1.4475\n16/16 [==============================] - 0s 634us/step - loss: 1.7374\n16/16 [==============================] - 0s 1ms/step - loss: 1.7907\n16/16 [==============================] - 0s 1ms/step - loss: 1.8035\n16/16 [==============================] - 0s 698us/step - loss: 1.8046\n16/16 [==============================] - 0s 660us/step - loss: 1.8040\n16/16 [==============================] - 0s 828us/step - loss: 1.8032\n16/16 [==============================] - 0s 1ms/step - loss: 1.8028\nEpoch 40 of 60\n\nTesting for epoch 40 index 1:\n16/16 [==============================] - 0s 604us/step - loss: 0.3427\n16/16 [==============================] - 0s 629us/step - loss: 0.8792\n16/16 [==============================] - 0s 613us/step - loss: 1.4449\n16/16 [==============================] - 0s 605us/step - loss: 1.7294\n16/16 [==============================] - 0s 1ms/step - loss: 1.7803\n16/16 [==============================] - 0s 1ms/step - loss: 1.7920\n16/16 [==============================] - 0s 670us/step - loss: 1.7928\n16/16 [==============================] - 0s 1ms/step - loss: 1.7921\n16/16 [==============================] - 0s 702us/step - loss: 1.7912\n16/16 [==============================] - 0s 978us/step - loss: 1.7908\n\nTesting for epoch 40 index 2:\n16/16 [==============================] - 0s 884us/step - loss: 0.3356\n16/16 [==============================] - 0s 1ms/step - loss: 0.8885\n16/16 [==============================] - 0s 850us/step - loss: 1.4743\n16/16 [==============================] - 0s 730us/step - loss: 1.7694\n16/16 [==============================] - 0s 1ms/step - loss: 1.8221\n16/16 [==============================] - 0s 944us/step - loss: 1.8343\n16/16 [==============================] - 0s 932us/step - loss: 1.8352\n16/16 [==============================] - 0s 696us/step - loss: 1.8345\n16/16 [==============================] - 0s 1ms/step - loss: 1.8337\n16/16 [==============================] - 0s 1ms/step - loss: 1.8333\nEpoch 41 of 60\n\nTesting for epoch 41 index 1:\n16/16 [==============================] - 0s 612us/step - loss: 0.3393\n16/16 [==============================] - 0s 595us/step - loss: 0.8775\n16/16 [==============================] - 0s 1ms/step - loss: 1.4502\n16/16 [==============================] - 0s 1ms/step - loss: 1.7321\n16/16 [==============================] - 0s 1ms/step - loss: 1.7808\n16/16 [==============================] - 0s 1ms/step - loss: 1.7913\n16/16 [==============================] - 0s 1ms/step - loss: 1.7917\n16/16 [==============================] - 0s 663us/step - loss: 1.7909\n16/16 [==============================] - 0s 688us/step - loss: 1.7900\n16/16 [==============================] - 0s 659us/step - loss: 1.7895\n\nTesting for epoch 41 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3349\n16/16 [==============================] - 0s 913us/step - loss: 0.8782\n16/16 [==============================] - 0s 683us/step - loss: 1.4573\n16/16 [==============================] - 0s 2ms/step - loss: 1.7431\n16/16 [==============================] - 0s 1ms/step - loss: 1.7923\n16/16 [==============================] - 0s 1ms/step - loss: 1.8028\n16/16 [==============================] - 0s 647us/step - loss: 1.8033\n16/16 [==============================] - 0s 633us/step - loss: 1.8024\n16/16 [==============================] - 0s 573us/step - loss: 1.8015\n16/16 [==============================] - 0s 2ms/step - loss: 1.8011\nEpoch 42 of 60\n\nTesting for epoch 42 index 1:\n16/16 [==============================] - 0s 712us/step - loss: 0.3296\n16/16 [==============================] - 0s 757us/step - loss: 0.8910\n16/16 [==============================] - 0s 1ms/step - loss: 1.4933\n16/16 [==============================] - 0s 676us/step - loss: 1.7869\n16/16 [==============================] - 0s 639us/step - loss: 1.8366\n16/16 [==============================] - 0s 622us/step - loss: 1.8470\n16/16 [==============================] - 0s 605us/step - loss: 1.8474\n16/16 [==============================] - 0s 1ms/step - loss: 1.8464\n16/16 [==============================] - 0s 1ms/step - loss: 1.8456\n16/16 [==============================] - 0s 1ms/step - loss: 1.8451\n\nTesting for epoch 42 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3304\n16/16 [==============================] - 0s 1ms/step - loss: 0.8862\n16/16 [==============================] - 0s 704us/step - loss: 1.4818\n16/16 [==============================] - 0s 1ms/step - loss: 1.7733\n16/16 [==============================] - 0s 679us/step - loss: 1.8222\n16/16 [==============================] - 0s 644us/step - loss: 1.8324\n16/16 [==============================] - 0s 1ms/step - loss: 1.8327\n16/16 [==============================] - 0s 1ms/step - loss: 1.8318\n16/16 [==============================] - 0s 653us/step - loss: 1.8309\n16/16 [==============================] - 0s 1ms/step - loss: 1.8304\nEpoch 43 of 60\n\nTesting for epoch 43 index 1:\n16/16 [==============================] - 0s 769us/step - loss: 0.3289\n16/16 [==============================] - 0s 1ms/step - loss: 0.8923\n16/16 [==============================] - 0s 889us/step - loss: 1.4981\n16/16 [==============================] - 0s 676us/step - loss: 1.7912\n16/16 [==============================] - 0s 576us/step - loss: 1.8395\n16/16 [==============================] - 0s 594us/step - loss: 1.8492\n16/16 [==============================] - 0s 614us/step - loss: 1.8493\n16/16 [==============================] - 0s 754us/step - loss: 1.8483\n16/16 [==============================] - 0s 1ms/step - loss: 1.8474\n16/16 [==============================] - 0s 620us/step - loss: 1.8469\n\nTesting for epoch 43 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3236\n16/16 [==============================] - 0s 669us/step - loss: 0.8897\n16/16 [==============================] - 0s 1ms/step - loss: 1.4962\n16/16 [==============================] - 0s 622us/step - loss: 1.7918\n16/16 [==============================] - 0s 636us/step - loss: 1.8402\n16/16 [==============================] - 0s 609us/step - loss: 1.8499\n16/16 [==============================] - 0s 589us/step - loss: 1.8499\n16/16 [==============================] - 0s 1ms/step - loss: 1.8488\n16/16 [==============================] - 0s 1ms/step - loss: 1.8479\n16/16 [==============================] - 0s 1ms/step - loss: 1.8474\nEpoch 44 of 60\n\nTesting for epoch 44 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3240\n16/16 [==============================] - 0s 1ms/step - loss: 0.9065\n16/16 [==============================] - 0s 1ms/step - loss: 1.5340\n16/16 [==============================] - 0s 699us/step - loss: 1.8380\n16/16 [==============================] - 0s 644us/step - loss: 1.8875\n16/16 [==============================] - 0s 603us/step - loss: 1.8973\n16/16 [==============================] - 0s 1ms/step - loss: 1.8974\n16/16 [==============================] - 0s 1ms/step - loss: 1.8964\n16/16 [==============================] - 0s 1ms/step - loss: 1.8954\n16/16 [==============================] - 0s 773us/step - loss: 1.8950\n\nTesting for epoch 44 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3120\n16/16 [==============================] - 0s 980us/step - loss: 0.9140\n16/16 [==============================] - 0s 1ms/step - loss: 1.5655\n16/16 [==============================] - 0s 1ms/step - loss: 1.8840\n16/16 [==============================] - 0s 1ms/step - loss: 1.9360\n16/16 [==============================] - 0s 677us/step - loss: 1.9465\n16/16 [==============================] - 0s 1ms/step - loss: 1.9468\n16/16 [==============================] - 0s 736us/step - loss: 1.9458\n16/16 [==============================] - 0s 645us/step - loss: 1.9449\n16/16 [==============================] - 0s 648us/step - loss: 1.9444\nEpoch 45 of 60\n\nTesting for epoch 45 index 1:\n16/16 [==============================] - 0s 673us/step - loss: 0.3280\n16/16 [==============================] - 0s 651us/step - loss: 0.8989\n16/16 [==============================] - 0s 1ms/step - loss: 1.5171\n16/16 [==============================] - 0s 626us/step - loss: 1.8154\n16/16 [==============================] - 0s 1ms/step - loss: 1.8629\n16/16 [==============================] - 0s 629us/step - loss: 1.8720\n16/16 [==============================] - 0s 1ms/step - loss: 1.8720\n16/16 [==============================] - 0s 1ms/step - loss: 1.8709\n16/16 [==============================] - 0s 1ms/step - loss: 1.8700\n16/16 [==============================] - 0s 632us/step - loss: 1.8696\n\nTesting for epoch 45 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3253\n16/16 [==============================] - 0s 816us/step - loss: 0.8820\n16/16 [==============================] - 0s 782us/step - loss: 1.4859\n16/16 [==============================] - 0s 1ms/step - loss: 1.7760\n16/16 [==============================] - 0s 1ms/step - loss: 1.8211\n16/16 [==============================] - 0s 672us/step - loss: 1.8292\n16/16 [==============================] - 0s 2ms/step - loss: 1.8287\n16/16 [==============================] - 0s 1ms/step - loss: 1.8275\n16/16 [==============================] - 0s 2ms/step - loss: 1.8265\n16/16 [==============================] - 0s 705us/step - loss: 1.8259\nEpoch 46 of 60\n\nTesting for epoch 46 index 1:\n16/16 [==============================] - 0s 629us/step - loss: 0.3018\n16/16 [==============================] - 0s 609us/step - loss: 0.9113\n16/16 [==============================] - 0s 1ms/step - loss: 1.5796\n16/16 [==============================] - 0s 1ms/step - loss: 1.8997\n16/16 [==============================] - 0s 1ms/step - loss: 1.9496\n16/16 [==============================] - 0s 657us/step - loss: 1.9589\n16/16 [==============================] - 0s 591us/step - loss: 1.9587\n16/16 [==============================] - 0s 1ms/step - loss: 1.9575\n16/16 [==============================] - 0s 2ms/step - loss: 1.9565\n16/16 [==============================] - 0s 893us/step - loss: 1.9560\n\nTesting for epoch 46 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3160\n16/16 [==============================] - 0s 649us/step - loss: 0.8929\n16/16 [==============================] - 0s 663us/step - loss: 1.5280\n16/16 [==============================] - 0s 668us/step - loss: 1.8325\n16/16 [==============================] - 0s 1ms/step - loss: 1.8797\n16/16 [==============================] - 0s 698us/step - loss: 1.8884\n16/16 [==============================] - 0s 700us/step - loss: 1.8881\n16/16 [==============================] - 0s 631us/step - loss: 1.8869\n16/16 [==============================] - 0s 1ms/step - loss: 1.8859\n16/16 [==============================] - 0s 649us/step - loss: 1.8854\nEpoch 47 of 60\n\nTesting for epoch 47 index 1:\n16/16 [==============================] - 0s 684us/step - loss: 0.3158\n16/16 [==============================] - 0s 627us/step - loss: 0.9064\n16/16 [==============================] - 0s 1ms/step - loss: 1.5588\n16/16 [==============================] - 0s 632us/step - loss: 1.8678\n16/16 [==============================] - 0s 618us/step - loss: 1.9149\n16/16 [==============================] - 0s 585us/step - loss: 1.9232\n16/16 [==============================] - 0s 1ms/step - loss: 1.9228\n16/16 [==============================] - 0s 1ms/step - loss: 1.9215\n16/16 [==============================] - 0s 1ms/step - loss: 1.9205\n16/16 [==============================] - 0s 694us/step - loss: 1.9200\n\nTesting for epoch 47 index 2:\n16/16 [==============================] - 0s 695us/step - loss: 0.3114\n16/16 [==============================] - 0s 805us/step - loss: 0.8972\n16/16 [==============================] - 0s 899us/step - loss: 1.5487\n16/16 [==============================] - 0s 1ms/step - loss: 1.8577\n16/16 [==============================] - 0s 705us/step - loss: 1.9042\n16/16 [==============================] - 0s 597us/step - loss: 1.9123\n16/16 [==============================] - 0s 630us/step - loss: 1.9118\n16/16 [==============================] - 0s 706us/step - loss: 1.9105\n16/16 [==============================] - 0s 1ms/step - loss: 1.9094\n16/16 [==============================] - 0s 790us/step - loss: 1.9089\nEpoch 48 of 60\n\nTesting for epoch 48 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3079\n16/16 [==============================] - 0s 1ms/step - loss: 0.9138\n16/16 [==============================] - 0s 610us/step - loss: 1.5910\n16/16 [==============================] - 0s 1ms/step - loss: 1.9092\n16/16 [==============================] - 0s 680us/step - loss: 1.9566\n16/16 [==============================] - 0s 609us/step - loss: 1.9647\n16/16 [==============================] - 0s 617us/step - loss: 1.9641\n16/16 [==============================] - 0s 620us/step - loss: 1.9628\n16/16 [==============================] - 0s 621us/step - loss: 1.9617\n16/16 [==============================] - 0s 639us/step - loss: 1.9612\n\nTesting for epoch 48 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2996\n16/16 [==============================] - 0s 668us/step - loss: 0.9044\n16/16 [==============================] - 0s 1ms/step - loss: 1.5879\n16/16 [==============================] - 0s 1ms/step - loss: 1.9109\n16/16 [==============================] - 0s 1ms/step - loss: 1.9594\n16/16 [==============================] - 0s 1ms/step - loss: 1.9680\n16/16 [==============================] - 0s 677us/step - loss: 1.9676\n16/16 [==============================] - 0s 640us/step - loss: 1.9664\n16/16 [==============================] - 0s 630us/step - loss: 1.9654\n16/16 [==============================] - 0s 612us/step - loss: 1.9649\nEpoch 49 of 60\n\nTesting for epoch 49 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.3126\n16/16 [==============================] - 0s 1ms/step - loss: 0.8881\n16/16 [==============================] - 0s 985us/step - loss: 1.5372\n16/16 [==============================] - 0s 2ms/step - loss: 1.8393\n16/16 [==============================] - 0s 621us/step - loss: 1.8833\n16/16 [==============================] - 0s 753us/step - loss: 1.8904\n16/16 [==============================] - 0s 1ms/step - loss: 1.8896\n16/16 [==============================] - 0s 602us/step - loss: 1.8883\n16/16 [==============================] - 0s 689us/step - loss: 1.8872\n16/16 [==============================] - 0s 1ms/step - loss: 1.8867\n\nTesting for epoch 49 index 2:\n16/16 [==============================] - 0s 658us/step - loss: 0.3055\n16/16 [==============================] - 0s 641us/step - loss: 0.8975\n16/16 [==============================] - 0s 577us/step - loss: 1.5715\n16/16 [==============================] - 0s 612us/step - loss: 1.8861\n16/16 [==============================] - 0s 605us/step - loss: 1.9320\n16/16 [==============================] - 0s 997us/step - loss: 1.9395\n16/16 [==============================] - 0s 1ms/step - loss: 1.9388\n16/16 [==============================] - 0s 644us/step - loss: 1.9374\n16/16 [==============================] - 0s 952us/step - loss: 1.9364\n16/16 [==============================] - 0s 628us/step - loss: 1.9358\nEpoch 50 of 60\n\nTesting for epoch 50 index 1:\n16/16 [==============================] - 0s 796us/step - loss: 0.2940\n16/16 [==============================] - 0s 649us/step - loss: 0.9002\n16/16 [==============================] - 0s 1ms/step - loss: 1.5930\n16/16 [==============================] - 0s 964us/step - loss: 1.9135\n16/16 [==============================] - 0s 1ms/step - loss: 1.9596\n16/16 [==============================] - 0s 674us/step - loss: 1.9670\n16/16 [==============================] - 0s 652us/step - loss: 1.9662\n16/16 [==============================] - 0s 642us/step - loss: 1.9648\n16/16 [==============================] - 0s 1ms/step - loss: 1.9638\n16/16 [==============================] - 0s 1ms/step - loss: 1.9633\n\nTesting for epoch 50 index 2:\n16/16 [==============================] - 0s 665us/step - loss: 0.3044\n16/16 [==============================] - 0s 665us/step - loss: 0.9019\n16/16 [==============================] - 0s 1ms/step - loss: 1.5888\n16/16 [==============================] - 0s 665us/step - loss: 1.9064\n16/16 [==============================] - 0s 1ms/step - loss: 1.9517\n16/16 [==============================] - 0s 660us/step - loss: 1.9588\n16/16 [==============================] - 0s 691us/step - loss: 1.9579\n16/16 [==============================] - 0s 1ms/step - loss: 1.9565\n16/16 [==============================] - 0s 1ms/step - loss: 1.9554\n16/16 [==============================] - 0s 693us/step - loss: 1.9549\nEpoch 51 of 60\n\nTesting for epoch 51 index 1:\n16/16 [==============================] - 0s 939us/step - loss: 0.3020\n16/16 [==============================] - 0s 739us/step - loss: 0.8987\n16/16 [==============================] - 0s 698us/step - loss: 1.5860\n16/16 [==============================] - 0s 1ms/step - loss: 1.9003\n16/16 [==============================] - 0s 652us/step - loss: 1.9445\n16/16 [==============================] - 0s 807us/step - loss: 1.9511\n16/16 [==============================] - 0s 1ms/step - loss: 1.9502\n16/16 [==============================] - 0s 1ms/step - loss: 1.9488\n16/16 [==============================] - 0s 669us/step - loss: 1.9477\n16/16 [==============================] - 0s 642us/step - loss: 1.9472\n\nTesting for epoch 51 index 2:\n16/16 [==============================] - 0s 919us/step - loss: 0.2888\n16/16 [==============================] - 0s 799us/step - loss: 0.9191\n16/16 [==============================] - 0s 639us/step - loss: 1.6532\n16/16 [==============================] - 0s 1ms/step - loss: 1.9907\n16/16 [==============================] - 0s 617us/step - loss: 2.0383\n16/16 [==============================] - 0s 1000us/step - loss: 2.0458\n16/16 [==============================] - 0s 1ms/step - loss: 2.0449\n16/16 [==============================] - 0s 617us/step - loss: 2.0435\n16/16 [==============================] - 0s 1ms/step - loss: 2.0424\n16/16 [==============================] - 0s 617us/step - loss: 2.0419\nEpoch 52 of 60\n\nTesting for epoch 52 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2896\n16/16 [==============================] - 0s 692us/step - loss: 0.9198\n16/16 [==============================] - 0s 662us/step - loss: 1.6543\n16/16 [==============================] - 0s 644us/step - loss: 1.9879\n16/16 [==============================] - 0s 1ms/step - loss: 2.0342\n16/16 [==============================] - 0s 642us/step - loss: 2.0410\n16/16 [==============================] - 0s 1ms/step - loss: 2.0401\n16/16 [==============================] - 0s 1ms/step - loss: 2.0387\n16/16 [==============================] - 0s 1ms/step - loss: 2.0376\n16/16 [==============================] - 0s 1ms/step - loss: 2.0371\n\nTesting for epoch 52 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2887\n16/16 [==============================] - 0s 655us/step - loss: 0.9164\n16/16 [==============================] - 0s 625us/step - loss: 1.6524\n16/16 [==============================] - 0s 664us/step - loss: 1.9863\n16/16 [==============================] - 0s 1ms/step - loss: 2.0320\n16/16 [==============================] - 0s 1ms/step - loss: 2.0386\n16/16 [==============================] - 0s 695us/step - loss: 2.0375\n16/16 [==============================] - 0s 671us/step - loss: 2.0360\n16/16 [==============================] - 0s 593us/step - loss: 2.0348\n16/16 [==============================] - 0s 592us/step - loss: 2.0343\nEpoch 53 of 60\n\nTesting for epoch 53 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2903\n16/16 [==============================] - 0s 652us/step - loss: 0.9111\n16/16 [==============================] - 0s 634us/step - loss: 1.6388\n16/16 [==============================] - 0s 820us/step - loss: 1.9646\n16/16 [==============================] - 0s 1ms/step - loss: 2.0082\n16/16 [==============================] - 0s 730us/step - loss: 2.0139\n16/16 [==============================] - 0s 592us/step - loss: 2.0126\n16/16 [==============================] - 0s 590us/step - loss: 2.0110\n16/16 [==============================] - 0s 854us/step - loss: 2.0098\n16/16 [==============================] - 0s 639us/step - loss: 2.0093\n\nTesting for epoch 53 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2907\n16/16 [==============================] - 0s 1ms/step - loss: 0.9147\n16/16 [==============================] - 0s 719us/step - loss: 1.6530\n16/16 [==============================] - 0s 658us/step - loss: 1.9850\n16/16 [==============================] - 0s 634us/step - loss: 2.0298\n16/16 [==============================] - 0s 653us/step - loss: 2.0361\n16/16 [==============================] - 0s 2ms/step - loss: 2.0350\n16/16 [==============================] - 0s 1ms/step - loss: 2.0335\n16/16 [==============================] - 0s 625us/step - loss: 2.0324\n16/16 [==============================] - 0s 2ms/step - loss: 2.0319\nEpoch 54 of 60\n\nTesting for epoch 54 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2956\n16/16 [==============================] - 0s 1ms/step - loss: 0.9136\n16/16 [==============================] - 0s 1ms/step - loss: 1.6438\n16/16 [==============================] - 0s 1ms/step - loss: 1.9674\n16/16 [==============================] - 0s 1ms/step - loss: 2.0101\n16/16 [==============================] - 0s 1ms/step - loss: 2.0156\n16/16 [==============================] - 0s 1ms/step - loss: 2.0143\n16/16 [==============================] - 0s 1ms/step - loss: 2.0127\n16/16 [==============================] - 0s 1ms/step - loss: 2.0116\n16/16 [==============================] - 0s 1ms/step - loss: 2.0111\n\nTesting for epoch 54 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2939\n16/16 [==============================] - 0s 1ms/step - loss: 0.9110\n16/16 [==============================] - 0s 1ms/step - loss: 1.6463\n16/16 [==============================] - 0s 1ms/step - loss: 1.9729\n16/16 [==============================] - 0s 2ms/step - loss: 2.0161\n16/16 [==============================] - 0s 2ms/step - loss: 2.0218\n16/16 [==============================] - 0s 1ms/step - loss: 2.0206\n16/16 [==============================] - 0s 1ms/step - loss: 2.0191\n16/16 [==============================] - 0s 1ms/step - loss: 2.0180\n16/16 [==============================] - 0s 679us/step - loss: 2.0175\nEpoch 55 of 60\n\nTesting for epoch 55 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2919\n16/16 [==============================] - 0s 2ms/step - loss: 0.9017\n16/16 [==============================] - 0s 1ms/step - loss: 1.6263\n16/16 [==============================] - 0s 1ms/step - loss: 1.9428\n16/16 [==============================] - 0s 1ms/step - loss: 1.9832\n16/16 [==============================] - 0s 1ms/step - loss: 1.9878\n16/16 [==============================] - 0s 694us/step - loss: 1.9863\n16/16 [==============================] - 0s 642us/step - loss: 1.9846\n16/16 [==============================] - 0s 593us/step - loss: 1.9834\n16/16 [==============================] - 0s 607us/step - loss: 1.9829\n\nTesting for epoch 55 index 2:\n16/16 [==============================] - 0s 627us/step - loss: 0.2932\n16/16 [==============================] - 0s 619us/step - loss: 0.8949\n16/16 [==============================] - 0s 621us/step - loss: 1.6136\n16/16 [==============================] - 0s 623us/step - loss: 1.9271\n16/16 [==============================] - 0s 610us/step - loss: 1.9666\n16/16 [==============================] - 0s 1ms/step - loss: 1.9708\n16/16 [==============================] - 0s 1ms/step - loss: 1.9692\n16/16 [==============================] - 0s 629us/step - loss: 1.9675\n16/16 [==============================] - 0s 1ms/step - loss: 1.9663\n16/16 [==============================] - 0s 580us/step - loss: 1.9658\nEpoch 56 of 60\n\nTesting for epoch 56 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2877\n16/16 [==============================] - 0s 1ms/step - loss: 0.9139\n16/16 [==============================] - 0s 1ms/step - loss: 1.6633\n16/16 [==============================] - 0s 1ms/step - loss: 1.9868\n16/16 [==============================] - 0s 1ms/step - loss: 2.0271\n16/16 [==============================] - 0s 654us/step - loss: 2.0314\n16/16 [==============================] - 0s 683us/step - loss: 2.0298\n16/16 [==============================] - 0s 615us/step - loss: 2.0281\n16/16 [==============================] - 0s 1ms/step - loss: 2.0269\n16/16 [==============================] - 0s 663us/step - loss: 2.0263\n\nTesting for epoch 56 index 2:\n16/16 [==============================] - 0s 742us/step - loss: 0.2883\n16/16 [==============================] - 0s 641us/step - loss: 0.8940\n16/16 [==============================] - 0s 1ms/step - loss: 1.6224\n16/16 [==============================] - 0s 634us/step - loss: 1.9361\n16/16 [==============================] - 0s 617us/step - loss: 1.9749\n16/16 [==============================] - 0s 1ms/step - loss: 1.9788\n16/16 [==============================] - 0s 628us/step - loss: 1.9771\n16/16 [==============================] - 0s 628us/step - loss: 1.9755\n16/16 [==============================] - 0s 1ms/step - loss: 1.9743\n16/16 [==============================] - 0s 802us/step - loss: 1.9737\nEpoch 57 of 60\n\nTesting for epoch 57 index 1:\n16/16 [==============================] - 0s 624us/step - loss: 0.2843\n16/16 [==============================] - 0s 1ms/step - loss: 0.9090\n16/16 [==============================] - 0s 1ms/step - loss: 1.6600\n16/16 [==============================] - 0s 712us/step - loss: 1.9791\n16/16 [==============================] - 0s 648us/step - loss: 2.0175\n16/16 [==============================] - 0s 647us/step - loss: 2.0210\n16/16 [==============================] - 0s 613us/step - loss: 2.0192\n16/16 [==============================] - 0s 629us/step - loss: 2.0174\n16/16 [==============================] - 0s 1ms/step - loss: 2.0162\n16/16 [==============================] - 0s 859us/step - loss: 2.0156\n\nTesting for epoch 57 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2836\n16/16 [==============================] - 0s 1ms/step - loss: 0.9146\n16/16 [==============================] - 0s 2ms/step - loss: 1.6782\n16/16 [==============================] - 0s 1ms/step - loss: 2.0028\n16/16 [==============================] - 0s 1ms/step - loss: 2.0419\n16/16 [==============================] - 0s 1ms/step - loss: 2.0457\n16/16 [==============================] - 0s 1ms/step - loss: 2.0439\n16/16 [==============================] - 0s 1ms/step - loss: 2.0422\n16/16 [==============================] - 0s 676us/step - loss: 2.0410\n16/16 [==============================] - 0s 2ms/step - loss: 2.0405\nEpoch 58 of 60\n\nTesting for epoch 58 index 1:\n16/16 [==============================] - 0s 684us/step - loss: 0.2791\n16/16 [==============================] - 0s 1ms/step - loss: 0.9413\n16/16 [==============================] - 0s 1ms/step - loss: 1.7429\n16/16 [==============================] - 0s 1ms/step - loss: 2.0793\n16/16 [==============================] - 0s 748us/step - loss: 2.1192\n16/16 [==============================] - 0s 1ms/step - loss: 2.1229\n16/16 [==============================] - 0s 657us/step - loss: 2.1211\n16/16 [==============================] - 0s 826us/step - loss: 2.1193\n16/16 [==============================] - 0s 654us/step - loss: 2.1181\n16/16 [==============================] - 0s 662us/step - loss: 2.1176\n\nTesting for epoch 58 index 2:\n16/16 [==============================] - 0s 623us/step - loss: 0.2745\n16/16 [==============================] - 0s 636us/step - loss: 0.9399\n16/16 [==============================] - 0s 604us/step - loss: 1.7502\n16/16 [==============================] - 0s 1ms/step - loss: 2.0900\n16/16 [==============================] - 0s 1ms/step - loss: 2.1304\n16/16 [==============================] - 0s 1ms/step - loss: 2.1343\n16/16 [==============================] - 0s 2ms/step - loss: 2.1327\n16/16 [==============================] - 0s 1ms/step - loss: 2.1310\n16/16 [==============================] - 0s 1ms/step - loss: 2.1299\n16/16 [==============================] - 0s 1ms/step - loss: 2.1293\nEpoch 59 of 60\n\nTesting for epoch 59 index 1:\n16/16 [==============================] - 0s 850us/step - loss: 0.2705\n16/16 [==============================] - 0s 624us/step - loss: 0.9431\n16/16 [==============================] - 0s 626us/step - loss: 1.7595\n16/16 [==============================] - 0s 946us/step - loss: 2.0960\n16/16 [==============================] - 0s 631us/step - loss: 2.1341\n16/16 [==============================] - 0s 653us/step - loss: 2.1372\n16/16 [==============================] - 0s 1ms/step - loss: 2.1353\n16/16 [==============================] - 0s 745us/step - loss: 2.1335\n16/16 [==============================] - 0s 1ms/step - loss: 2.1322\n16/16 [==============================] - 0s 1ms/step - loss: 2.1317\n\nTesting for epoch 59 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 0.2706\n16/16 [==============================] - 0s 1ms/step - loss: 0.9454\n16/16 [==============================] - 0s 1ms/step - loss: 1.7683\n16/16 [==============================] - 0s 1ms/step - loss: 2.1066\n16/16 [==============================] - 0s 1ms/step - loss: 2.1448\n16/16 [==============================] - 0s 1ms/step - loss: 2.1478\n16/16 [==============================] - 0s 1ms/step - loss: 2.1459\n16/16 [==============================] - 0s 1ms/step - loss: 2.1441\n16/16 [==============================] - 0s 676us/step - loss: 2.1429\n16/16 [==============================] - 0s 1ms/step - loss: 2.1423\nEpoch 60 of 60\n\nTesting for epoch 60 index 1:\n16/16 [==============================] - 0s 648us/step - loss: 0.2763\n16/16 [==============================] - 0s 1ms/step - loss: 0.9427\n16/16 [==============================] - 0s 690us/step - loss: 1.7522\n16/16 [==============================] - 0s 1ms/step - loss: 2.0797\n16/16 [==============================] - 0s 717us/step - loss: 2.1151\n16/16 [==============================] - 0s 650us/step - loss: 2.1173\n16/16 [==============================] - 0s 1ms/step - loss: 2.1151\n16/16 [==============================] - 0s 1ms/step - loss: 2.1133\n16/16 [==============================] - 0s 674us/step - loss: 2.1120\n16/16 [==============================] - 0s 1ms/step - loss: 2.1114\n\nTesting for epoch 60 index 2:\n16/16 [==============================] - 0s 660us/step - loss: 0.2810\n16/16 [==============================] - 0s 668us/step - loss: 0.9270\n16/16 [==============================] - 0s 615us/step - loss: 1.7161\n16/16 [==============================] - 0s 637us/step - loss: 2.0351\n16/16 [==============================] - 0s 1ms/step - loss: 2.0696\n16/16 [==============================] - 0s 694us/step - loss: 2.0717\n16/16 [==============================] - 0s 1ms/step - loss: 2.0696\n16/16 [==============================] - 0s 641us/step - loss: 2.0678\n16/16 [==============================] - 0s 599us/step - loss: 2.0666\n16/16 [==============================] - 0s 624us/step - loss: 2.0660\n\n\n\noutlier_MO_GAAL_one = list(clf.labels_)\n\n\noutlier_MO_GAAL_one = list(map(lambda x: 1 if x==0  else -1,outlier_MO_GAAL_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_MO_GAAL_one,tab_linear)\n\n\n_conf.conf(\"MO-GAAL (Liu et al., 2019)\")\n\n\n\n\nAccuracy: 0.879\nPrecision: 0.955\nRecall: 0.916\nF1 Score: 0.935\n\n\n\nthirteen = twelve.append(_conf.tab)\n\n\n\nLSCP\n\ndetectors = [KNN(), LOF(), OCSVM()]\nclf = LSCP(detectors)\nclf.fit(_df[['x', 'y']])\n_df['LSCP_clf'] = clf.labels_\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/pyod/models/lscp.py:382: UserWarning: The number of histogram bins is greater than the number of classifiers, reducing n_bins to n_clf.\n  warnings.warn(\n\n\n\noutlier_LSCP_one = list(clf.labels_)\n\n\noutlier_LSCP_one = list(map(lambda x: 1 if x==0  else -1,outlier_LSCP_one))\n\n\n_conf = Conf_matrx(outlier_true_one_1,outlier_LSCP_one,tab_linear)\n\n\n_conf.conf(\"LSCP (Zhao et al., 2019)\")\n\n\n\n\nAccuracy: 0.908\nPrecision: 0.977\nRecall: 0.925\nF1 Score: 0.950\n\n\n\nfourteen_linear = thirteen.append(_conf.tab)"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#linear-result",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#linear-result",
    "title": "Class code for Comparison Study",
    "section": "Linear Result",
    "text": "Linear Result\n\nround(fourteen_linear,3)\n\n\n\n\n\n  \n    \n      \n      Accuracy\n      Precision\n      Recall\n      F1\n    \n  \n  \n    \n      GODE\n      0.959\n      0.960\n      0.999\n      0.979\n    \n    \n      LOF (Breunig et al., 2000)\n      0.890\n      0.973\n      0.909\n      0.940\n    \n    \n      kNN (Ramaswamy et al., 2000)\n      0.912\n      0.979\n      0.927\n      0.952\n    \n    \n      CBLOF (He et al., 2003)\n      0.920\n      0.958\n      0.958\n      0.958\n    \n    \n      OCSVM (Sch Ìˆolkopf et al., 2001)\n      0.909\n      0.978\n      0.925\n      0.951\n    \n    \n      MCD (Hardin and Rocke, 2004)\n      0.918\n      0.982\n      0.931\n      0.956\n    \n    \n      Feature Bagging (Lazarevic and Kumar, 2005)\n      0.918\n      0.982\n      0.931\n      0.956\n    \n    \n      ABOD (Kriegel et al., 2008)\n      0.946\n      0.972\n      0.972\n      0.972\n    \n    \n      Isolation Forest (Liu et al., 2008)\n      0.800\n      0.984\n      0.802\n      0.884\n    \n    \n      HBOS (Goldstein and Dengel, 2012)\n      0.889\n      0.960\n      0.921\n      0.940\n    \n    \n      SOS (Janssens et al., 2012)\n      0.889\n      0.960\n      0.921\n      0.940\n    \n    \n      SO-GAAL (Liu et al., 2019)\n      0.868\n      0.954\n      0.904\n      0.929\n    \n    \n      MO-GAAL (Liu et al., 2019)\n      0.879\n      0.955\n      0.916\n      0.935\n    \n    \n      LSCP (Zhao et al., 2019)\n      0.908\n      0.977\n      0.925\n      0.950"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#orbit-ebayesthresh",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#orbit-ebayesthresh",
    "title": "Class code for Comparison Study",
    "section": "Orbit EbayesThresh",
    "text": "Orbit EbayesThresh\n\n%load_ext rpy2.ipython\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython\n\n\n\n%%R\nlibrary(EbayesThresh)\nset.seed(1)\nepsilon = rnorm(1000)\nsignal = sample(c(runif(25,-7,-5), runif(25,5,7), rep(0,950)))\nindex_of_trueoutlier = which(signal!=0)\nindex_of_trueoutlier\nx=signal+epsilon\nplot(1:1000,x)\npoints(index_of_trueoutlier,x[index_of_trueoutlier],col=2,cex=4)\n\n#plot(x,type='l')\n#mu <- EbayesThresh::ebayesthresh(x,sdev=2)\n#lines(mu,col=2,lty=2,lwd=2)\n\n\n\n\n\n%R -o x\n%R -o index_of_trueoutlier\n%R -o signal\n\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\n\n\nxhat = np.array(ebayesthresh(FloatVector(x)))\n\n\n# plt.plot(x)\n# plt.plot(xhat)\n\n\noutlier_true_index = index_of_trueoutlier\n\n\noutlier_true_value = x[index_of_trueoutlier]\n\npackageì™€ ë¹„êµë¥¼ ìœ„í•´ outlierëŠ” -1, inlierëŠ” 1ë¡œ í‘œì‹œ\n\noutlier_true_one = signal.copy()\n\n\noutlier_true_one = list(map(lambda x: -1 if x!=0 else 1,outlier_true_one))"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#orbit",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#orbit",
    "title": "Class code for Comparison Study",
    "section": "Orbit",
    "text": "Orbit\n\nnp.random.seed(777)\npi=np.pi\nn=1000\nang=np.linspace(-pi,pi-2*pi/n,n)\nr=5+np.cos(np.linspace(0,12*pi,n))\nvx=r*np.cos(ang)\nvy=r*np.sin(ang)\nf1=10*np.sin(np.linspace(0,6*pi,n))\nf = f1 + x\n\n\n_df = pd.DataFrame({'x' : vx, 'y' : vy, 'f' : f})\n\n\nX = np.array(_df)\n\n\nGODE\n\n_Orbit = Orbit(_df)\n\n\n_Orbit.get_distance()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:02<00:00, 497.54it/s]\n\n\n\n_Orbit.get_weightmatrix(theta=(_Orbit.D[_Orbit.D>0].mean()),kappa=2500) \n\n\n_Orbit.fit(sd=15,ref=20)\n\n\noutlier_simul_one = (_Orbit.df['Residual']**2).tolist()\n\n\noutlier_simul_one = list(map(lambda x: -1 if x > 20 else 1,outlier_simul_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_simul_one,tab_orbit)\n\n\n_conf.conf(\"GODE\")\n\n\n\n\nAccuracy: 0.997\nPrecision: 0.997\nRecall: 1.000\nF1 Score: 0.998\n\n\n\none = _conf.tab\n\n\n\nLOF\n\nclf = LocalOutlierFactor(n_neighbors=2)\n\n\n_conf = Conf_matrx(outlier_true_one,clf.fit_predict(X),tab_orbit)\n\n\n_conf.conf(\"LOF (Breunig et al., 2000)\")\n\n\n\n\nAccuracy: 0.886\nPrecision: 0.987\nRecall: 0.892\nF1 Score: 0.937\n\n\n\ntwo = one.append(_conf.tab)\n\n\n\nKNN\n\nclf = KNN()\nclf.fit(_df[['x', 'y','f']])\n_df['knn_clf'] = clf.labels_\n\n\noutlier_KNN_one = list(clf.labels_)\n\n\noutlier_KNN_one = list(map(lambda x: 1 if x==0  else -1,outlier_KNN_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_KNN_one,tab_orbit)\n\n\n_conf.conf(\"kNN (Ramaswamy et al., 2000)\")\n\n\n\n\nAccuracy: 0.948\nPrecision: 0.999\nRecall: 0.946\nF1 Score: 0.972\n\n\n\nthree = two.append(_conf.tab)\n\n\n\nCBLOF\n\nclf = CBLOF(contamination=0.05,check_estimator=False, random_state=77)\nclf.fit(_df[['x', 'y','f']])\n_df['CBLOF_Clf'] = clf.labels_\n\n\noutlier_CBLOF_one = list(clf.labels_)\n\n\noutlier_CBLOF_one = list(map(lambda x: 1 if x==0  else -1,outlier_CBLOF_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_CBLOF_one,tab_orbit)\n\n\n_conf.conf(\"CBLOF (He et al., 2003)\")\n\n\n\n\nAccuracy: 0.918\nPrecision: 0.957\nRecall: 0.957\nF1 Score: 0.957\n\n\n\nfour = three.append(_conf.tab)\n\n\n\nOCSVM\n\nclf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n\n\nclf.fit(X)\n\nOneClassSVM(gamma=0.1, nu=0.1)\n\n\n\noutlier_OSVM_one = list(clf.predict(X))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_OSVM_one,tab_orbit)\n\n\n_conf.conf(\"OCSVM (Sch Ìˆolkopf et al., 2001)\")\n\n\n\n\nAccuracy: 0.923\nPrecision: 0.988\nRecall: 0.931\nF1 Score: 0.958\n\n\n\nfive = four.append(_conf.tab)\n\n\n\nMCD\n\nclf = MCD()\nclf.fit(_df[['x', 'y','f']])\n_df['MCD_clf'] = clf.labels_\n\n\noutlier_MCD_one = list(clf.labels_)\n\n\noutlier_MCD_one = list(map(lambda x: 1 if x==0  else -1,outlier_MCD_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_MCD_one,tab_orbit)\n\n\n_conf.conf(\"MCD (Hardin and Rocke, 2004)\")\n\n\n\n\nAccuracy: 0.866\nPrecision: 0.953\nRecall: 0.903\nF1 Score: 0.928\n\n\n\nsix = five.append(_conf.tab)\n\n\n\nFeature Bagging\n\nclf = FeatureBagging()\nclf.fit(_df[['x', 'y','f']])\n_df['FeatureBagging_clf'] = clf.labels_\n\n\noutlier_FeatureBagging_one = list(clf.labels_)\n\n\noutlier_FeatureBagging_one = list(map(lambda x: 1 if x==0  else -1,outlier_FeatureBagging_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_FeatureBagging_one,tab_orbit)\n\n\n_conf.conf(\"Feature Bagging (Lazarevic and Kumar, 2005)\")\n\n\n\n\nAccuracy: 0.912\nPrecision: 0.979\nRecall: 0.927\nF1 Score: 0.952\n\n\n\nseven = six.append(_conf.tab)\n\n\n\nABOD\n\nclf = ABOD(contamination=0.05)\nclf.fit(_df[['x', 'y','f']])\n_df['ABOD_Clf'] = clf.labels_\n\n\noutlier_ABOD_one = list(clf.labels_)\n\n\noutlier_ABOD_one = list(map(lambda x: 1 if x==0  else -1,outlier_ABOD_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_ABOD_one,tab_orbit)\n\n\n_conf.conf(\"ABOD (Kriegel et al., 2008)\")\n\n\n\n\nAccuracy: 0.988\nPrecision: 0.994\nRecall: 0.994\nF1 Score: 0.994\n\n\n\neight = seven.append(_conf.tab)\n\n\n\nIForest\n\nod = IForest(\n    threshold=0.,\n    n_estimators=100\n)\n\n\nod.fit(_df[['x', 'y','f']])\n\n\npreds = od.predict(\n    _df[['x', 'y','f']],\n    return_instance_score=True\n)\n\n\n_df['IF_alibi'] = preds['data']['is_outlier']\n\n\noutlier_alibi_one = _df['IF_alibi']\n\n\noutlier_alibi_one = list(map(lambda x: 1 if x==0  else -1,outlier_alibi_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_alibi_one,tab_orbit)\n\n\n_conf.conf(\"Isolation Forest (Liu et al., 2008)\")\n\n\n\n\nAccuracy: 0.378\nPrecision: 0.997\nRecall: 0.346\nF1 Score: 0.514\n\n\n\nnine = eight.append(_conf.tab)\n\n\n\nHBOS\n\nclf = HBOS()\nclf.fit(_df[['x', 'y','f']])\n_df['HBOS_clf'] = clf.labels_\n\n\noutlier_HBOS_one = list(clf.labels_)\n\n\noutlier_HBOS_one = list(map(lambda x: 1 if x==0  else -1,outlier_HBOS_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_HBOS_one,tab_orbit)\n\n\n_conf.conf(\"HBOS (Goldstein and Dengel, 2012)\")\n\n\n\n\nAccuracy: 0.881\nPrecision: 0.961\nRecall: 0.912\nF1 Score: 0.936\n\n\n\nten = nine.append(_conf.tab)\n\n\n\nSOS\n\noutlier_SOS_one = list(clf.labels_)\n\n\noutlier_SOS_one = list(map(lambda x: 1 if x==0  else -1,outlier_SOS_one))\n\n\nclf = SOS()\nclf.fit(_df[['x', 'y','f']])\n_df['SOS_clf'] = clf.labels_\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_SOS_one,tab_orbit)\n\n\n_conf.conf(\"SOS (Janssens et al., 2012)\")\n\n\n\n\nAccuracy: 0.881\nPrecision: 0.961\nRecall: 0.912\nF1 Score: 0.936\n\n\n\neleven = ten.append(_conf.tab)\n\n\n\nSO_GAAL\n\nclf = SO_GAAL()\nclf.fit(_df[['x', 'y','f']])\n_df['SO_GAAL_clf'] = clf.labels_\n\nEpoch 1 of 60\n\nTesting for epoch 1 index 1:\n\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n\n\n\nTesting for epoch 1 index 2:\nEpoch 2 of 60\n\nTesting for epoch 2 index 1:\n\nTesting for epoch 2 index 2:\nEpoch 3 of 60\n\nTesting for epoch 3 index 1:\n\nTesting for epoch 3 index 2:\nEpoch 4 of 60\n\nTesting for epoch 4 index 1:\n\nTesting for epoch 4 index 2:\nEpoch 5 of 60\n\nTesting for epoch 5 index 1:\n\nTesting for epoch 5 index 2:\nEpoch 6 of 60\n\nTesting for epoch 6 index 1:\n\nTesting for epoch 6 index 2:\nEpoch 7 of 60\n\nTesting for epoch 7 index 1:\n\nTesting for epoch 7 index 2:\nEpoch 8 of 60\n\nTesting for epoch 8 index 1:\n\nTesting for epoch 8 index 2:\nEpoch 9 of 60\n\nTesting for epoch 9 index 1:\n\nTesting for epoch 9 index 2:\nEpoch 10 of 60\n\nTesting for epoch 10 index 1:\n\nTesting for epoch 10 index 2:\nEpoch 11 of 60\n\nTesting for epoch 11 index 1:\n\nTesting for epoch 11 index 2:\nEpoch 12 of 60\n\nTesting for epoch 12 index 1:\n\nTesting for epoch 12 index 2:\nEpoch 13 of 60\n\nTesting for epoch 13 index 1:\n\nTesting for epoch 13 index 2:\nEpoch 14 of 60\n\nTesting for epoch 14 index 1:\n\nTesting for epoch 14 index 2:\nEpoch 15 of 60\n\nTesting for epoch 15 index 1:\n\nTesting for epoch 15 index 2:\nEpoch 16 of 60\n\nTesting for epoch 16 index 1:\n\nTesting for epoch 16 index 2:\nEpoch 17 of 60\n\nTesting for epoch 17 index 1:\n\nTesting for epoch 17 index 2:\nEpoch 18 of 60\n\nTesting for epoch 18 index 1:\n\nTesting for epoch 18 index 2:\nEpoch 19 of 60\n\nTesting for epoch 19 index 1:\n\nTesting for epoch 19 index 2:\nEpoch 20 of 60\n\nTesting for epoch 20 index 1:\n\nTesting for epoch 20 index 2:\nEpoch 21 of 60\n\nTesting for epoch 21 index 1:\n\nTesting for epoch 21 index 2:\nEpoch 22 of 60\n\nTesting for epoch 22 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2135\n\nTesting for epoch 22 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2178\nEpoch 23 of 60\n\nTesting for epoch 23 index 1:\n16/16 [==============================] - 0s 891us/step - loss: 1.2227\n\nTesting for epoch 23 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2138\nEpoch 24 of 60\n\nTesting for epoch 24 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2244\n\nTesting for epoch 24 index 2:\n16/16 [==============================] - 0s 917us/step - loss: 1.2068\nEpoch 25 of 60\n\nTesting for epoch 25 index 1:\n16/16 [==============================] - 0s 624us/step - loss: 1.2319\n\nTesting for epoch 25 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2260\nEpoch 26 of 60\n\nTesting for epoch 26 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2357\n\nTesting for epoch 26 index 2:\n16/16 [==============================] - 0s 985us/step - loss: 1.2294\nEpoch 27 of 60\n\nTesting for epoch 27 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2426\n\nTesting for epoch 27 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2583\nEpoch 28 of 60\n\nTesting for epoch 28 index 1:\n16/16 [==============================] - 0s 815us/step - loss: 1.2599\n\nTesting for epoch 28 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2752\nEpoch 29 of 60\n\nTesting for epoch 29 index 1:\n16/16 [==============================] - 0s 639us/step - loss: 1.3019\n\nTesting for epoch 29 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.2905\nEpoch 30 of 60\n\nTesting for epoch 30 index 1:\n16/16 [==============================] - 0s 635us/step - loss: 1.3191\n\nTesting for epoch 30 index 2:\n16/16 [==============================] - 0s 781us/step - loss: 1.3229\nEpoch 31 of 60\n\nTesting for epoch 31 index 1:\n16/16 [==============================] - 0s 604us/step - loss: 1.3371\n\nTesting for epoch 31 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.3418\nEpoch 32 of 60\n\nTesting for epoch 32 index 1:\n16/16 [==============================] - 0s 803us/step - loss: 1.3589\n\nTesting for epoch 32 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.3819\nEpoch 33 of 60\n\nTesting for epoch 33 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.3966\n\nTesting for epoch 33 index 2:\n16/16 [==============================] - 0s 956us/step - loss: 1.3947\nEpoch 34 of 60\n\nTesting for epoch 34 index 1:\n16/16 [==============================] - 0s 609us/step - loss: 1.4201\n\nTesting for epoch 34 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4322\nEpoch 35 of 60\n\nTesting for epoch 35 index 1:\n16/16 [==============================] - 0s 617us/step - loss: 1.4333\n\nTesting for epoch 35 index 2:\n16/16 [==============================] - 0s 626us/step - loss: 1.4465\nEpoch 36 of 60\n\nTesting for epoch 36 index 1:\n16/16 [==============================] - 0s 640us/step - loss: 1.4560\n\nTesting for epoch 36 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.4823\nEpoch 37 of 60\n\nTesting for epoch 37 index 1:\n16/16 [==============================] - 0s 932us/step - loss: 1.4888\n\nTesting for epoch 37 index 2:\n16/16 [==============================] - 0s 782us/step - loss: 1.5030\nEpoch 38 of 60\n\nTesting for epoch 38 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5161\n\nTesting for epoch 38 index 2:\n16/16 [==============================] - 0s 599us/step - loss: 1.5196\nEpoch 39 of 60\n\nTesting for epoch 39 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5412\n\nTesting for epoch 39 index 2:\n16/16 [==============================] - 0s 877us/step - loss: 1.5368\nEpoch 40 of 60\n\nTesting for epoch 40 index 1:\n16/16 [==============================] - 0s 579us/step - loss: 1.5523\n\nTesting for epoch 40 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5574\nEpoch 41 of 60\n\nTesting for epoch 41 index 1:\n16/16 [==============================] - 0s 981us/step - loss: 1.5684\n\nTesting for epoch 41 index 2:\n16/16 [==============================] - 0s 643us/step - loss: 1.5748\nEpoch 42 of 60\n\nTesting for epoch 42 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5725\n\nTesting for epoch 42 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.5772\nEpoch 43 of 60\n\nTesting for epoch 43 index 1:\n16/16 [==============================] - 0s 648us/step - loss: 1.5934\n\nTesting for epoch 43 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6053\nEpoch 44 of 60\n\nTesting for epoch 44 index 1:\n16/16 [==============================] - 0s 907us/step - loss: 1.6078\n\nTesting for epoch 44 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6025\nEpoch 45 of 60\n\nTesting for epoch 45 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6277\n\nTesting for epoch 45 index 2:\n16/16 [==============================] - 0s 615us/step - loss: 1.6348\nEpoch 46 of 60\n\nTesting for epoch 46 index 1:\n16/16 [==============================] - 0s 815us/step - loss: 1.6427\n\nTesting for epoch 46 index 2:\n16/16 [==============================] - 0s 606us/step - loss: 1.6405\nEpoch 47 of 60\n\nTesting for epoch 47 index 1:\n16/16 [==============================] - 0s 619us/step - loss: 1.6498\n\nTesting for epoch 47 index 2:\n16/16 [==============================] - 0s 812us/step - loss: 1.6603\nEpoch 48 of 60\n\nTesting for epoch 48 index 1:\n16/16 [==============================] - 0s 614us/step - loss: 1.6775\n\nTesting for epoch 48 index 2:\n16/16 [==============================] - 0s 650us/step - loss: 1.6890\nEpoch 49 of 60\n\nTesting for epoch 49 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6979\n\nTesting for epoch 49 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.6971\nEpoch 50 of 60\n\nTesting for epoch 50 index 1:\n16/16 [==============================] - 0s 624us/step - loss: 1.7076\n\nTesting for epoch 50 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.7120\nEpoch 51 of 60\n\nTesting for epoch 51 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.7271\n\nTesting for epoch 51 index 2:\n16/16 [==============================] - 0s 863us/step - loss: 1.7406\nEpoch 52 of 60\n\nTesting for epoch 52 index 1:\n16/16 [==============================] - 0s 623us/step - loss: 1.7534\n\nTesting for epoch 52 index 2:\n16/16 [==============================] - 0s 677us/step - loss: 1.7597\nEpoch 53 of 60\n\nTesting for epoch 53 index 1:\n16/16 [==============================] - 0s 741us/step - loss: 1.7555\n\nTesting for epoch 53 index 2:\n16/16 [==============================] - 0s 678us/step - loss: 1.7716\nEpoch 54 of 60\n\nTesting for epoch 54 index 1:\n16/16 [==============================] - 0s 827us/step - loss: 1.7776\n\nTesting for epoch 54 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.7776\nEpoch 55 of 60\n\nTesting for epoch 55 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8009\n\nTesting for epoch 55 index 2:\n16/16 [==============================] - 0s 626us/step - loss: 1.8053\nEpoch 56 of 60\n\nTesting for epoch 56 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8205\n\nTesting for epoch 56 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8218\nEpoch 57 of 60\n\nTesting for epoch 57 index 1:\n16/16 [==============================] - 0s 666us/step - loss: 1.8259\n\nTesting for epoch 57 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8307\nEpoch 58 of 60\n\nTesting for epoch 58 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8576\n\nTesting for epoch 58 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8445\nEpoch 59 of 60\n\nTesting for epoch 59 index 1:\n16/16 [==============================] - 0s 742us/step - loss: 1.8687\n\nTesting for epoch 59 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8710\nEpoch 60 of 60\n\nTesting for epoch 60 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8824\n\nTesting for epoch 60 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8924\n\n\n\noutlier_SO_GAAL_one = list(clf.labels_)\n\n\noutlier_SO_GAAL_one = list(map(lambda x: 1 if x==0  else -1,outlier_SO_GAAL_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_SO_GAAL_one,tab_orbit)\n\n\n_conf.conf(\"SO-GAAL (Liu et al., 2019)\")\n\n\n\n\nAccuracy: 0.876\nPrecision: 0.959\nRecall: 0.908\nF1 Score: 0.933\n\n\n\ntwelve = eleven.append(_conf.tab)\n\n\n\nMO_GAAL\n\nclf = MO_GAAL()\nclf.fit(_df[['x', 'y','f']])\n_df['MO_GAAL_clf'] = clf.labels_\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n\n\nEpoch 1 of 60\n\nTesting for epoch 1 index 1:\n\nTesting for epoch 1 index 2:\nEpoch 2 of 60\n\nTesting for epoch 2 index 1:\n\nTesting for epoch 2 index 2:\nEpoch 3 of 60\n\nTesting for epoch 3 index 1:\n\nTesting for epoch 3 index 2:\nEpoch 4 of 60\n\nTesting for epoch 4 index 1:\n\nTesting for epoch 4 index 2:\nEpoch 5 of 60\n\nTesting for epoch 5 index 1:\n\nTesting for epoch 5 index 2:\nEpoch 6 of 60\n\nTesting for epoch 6 index 1:\n\nTesting for epoch 6 index 2:\nEpoch 7 of 60\n\nTesting for epoch 7 index 1:\n\nTesting for epoch 7 index 2:\nEpoch 8 of 60\n\nTesting for epoch 8 index 1:\n\nTesting for epoch 8 index 2:\nEpoch 9 of 60\n\nTesting for epoch 9 index 1:\n\nTesting for epoch 9 index 2:\nEpoch 10 of 60\n\nTesting for epoch 10 index 1:\n\nTesting for epoch 10 index 2:\nEpoch 11 of 60\n\nTesting for epoch 11 index 1:\n\nTesting for epoch 11 index 2:\nEpoch 12 of 60\n\nTesting for epoch 12 index 1:\n\nTesting for epoch 12 index 2:\nEpoch 13 of 60\n\nTesting for epoch 13 index 1:\n\nTesting for epoch 13 index 2:\nEpoch 14 of 60\n\nTesting for epoch 14 index 1:\n\nTesting for epoch 14 index 2:\nEpoch 15 of 60\n\nTesting for epoch 15 index 1:\n\nTesting for epoch 15 index 2:\nEpoch 16 of 60\n\nTesting for epoch 16 index 1:\n\nTesting for epoch 16 index 2:\nEpoch 17 of 60\n\nTesting for epoch 17 index 1:\n\nTesting for epoch 17 index 2:\nEpoch 18 of 60\n\nTesting for epoch 18 index 1:\n\nTesting for epoch 18 index 2:\nEpoch 19 of 60\n\nTesting for epoch 19 index 1:\n\nTesting for epoch 19 index 2:\nEpoch 20 of 60\n\nTesting for epoch 20 index 1:\n\nTesting for epoch 20 index 2:\nEpoch 21 of 60\n\nTesting for epoch 21 index 1:\n\nTesting for epoch 21 index 2:\n16/16 [==============================] - 0s 638us/step - loss: 0.5986\n16/16 [==============================] - 0s 1ms/step - loss: 1.2168\n16/16 [==============================] - 0s 643us/step - loss: 1.2657\n16/16 [==============================] - 0s 637us/step - loss: 1.2688\n16/16 [==============================] - 0s 1ms/step - loss: 1.2695\n16/16 [==============================] - 0s 1ms/step - loss: 1.2696\n16/16 [==============================] - 0s 653us/step - loss: 1.2696\n16/16 [==============================] - 0s 649us/step - loss: 1.2696\n16/16 [==============================] - 0s 711us/step - loss: 1.2696\n16/16 [==============================] - 0s 1ms/step - loss: 1.2696\nEpoch 22 of 60\n\nTesting for epoch 22 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.6392\n16/16 [==============================] - 0s 664us/step - loss: 1.2086\n16/16 [==============================] - 0s 711us/step - loss: 1.2461\n16/16 [==============================] - 0s 2ms/step - loss: 1.2488\n16/16 [==============================] - 0s 707us/step - loss: 1.2493\n16/16 [==============================] - 0s 628us/step - loss: 1.2494\n16/16 [==============================] - 0s 642us/step - loss: 1.2494\n16/16 [==============================] - 0s 698us/step - loss: 1.2494\n16/16 [==============================] - 0s 674us/step - loss: 1.2494\n16/16 [==============================] - 0s 788us/step - loss: 1.2494\n\nTesting for epoch 22 index 2:\n16/16 [==============================] - 0s 663us/step - loss: 0.6763\n16/16 [==============================] - 0s 1ms/step - loss: 1.2250\n16/16 [==============================] - 0s 1ms/step - loss: 1.2559\n16/16 [==============================] - 0s 642us/step - loss: 1.2583\n16/16 [==============================] - 0s 1ms/step - loss: 1.2588\n16/16 [==============================] - 0s 672us/step - loss: 1.2589\n16/16 [==============================] - 0s 629us/step - loss: 1.2589\n16/16 [==============================] - 0s 1ms/step - loss: 1.2589\n16/16 [==============================] - 0s 1ms/step - loss: 1.2589\n16/16 [==============================] - 0s 1ms/step - loss: 1.2589\nEpoch 23 of 60\n\nTesting for epoch 23 index 1:\n16/16 [==============================] - 0s 658us/step - loss: 0.7044\n16/16 [==============================] - 0s 682us/step - loss: 1.2426\n16/16 [==============================] - 0s 661us/step - loss: 1.2710\n16/16 [==============================] - 0s 1ms/step - loss: 1.2733\n16/16 [==============================] - 0s 757us/step - loss: 1.2738\n16/16 [==============================] - 0s 725us/step - loss: 1.2739\n16/16 [==============================] - 0s 1ms/step - loss: 1.2739\n16/16 [==============================] - 0s 1ms/step - loss: 1.2739\n16/16 [==============================] - 0s 638us/step - loss: 1.2739\n16/16 [==============================] - 0s 648us/step - loss: 1.2739\n\nTesting for epoch 23 index 2:\n16/16 [==============================] - 0s 660us/step - loss: 0.7203\n16/16 [==============================] - 0s 878us/step - loss: 1.2467\n16/16 [==============================] - 0s 647us/step - loss: 1.2715\n16/16 [==============================] - 0s 1ms/step - loss: 1.2737\n16/16 [==============================] - 0s 1ms/step - loss: 1.2741\n16/16 [==============================] - 0s 637us/step - loss: 1.2742\n16/16 [==============================] - 0s 656us/step - loss: 1.2742\n16/16 [==============================] - 0s 1ms/step - loss: 1.2742\n16/16 [==============================] - 0s 645us/step - loss: 1.2742\n16/16 [==============================] - 0s 689us/step - loss: 1.2742\nEpoch 24 of 60\n\nTesting for epoch 24 index 1:\n16/16 [==============================] - 0s 656us/step - loss: 0.7260\n16/16 [==============================] - 0s 1ms/step - loss: 1.2580\n16/16 [==============================] - 0s 655us/step - loss: 1.2817\n16/16 [==============================] - 0s 1ms/step - loss: 1.2839\n16/16 [==============================] - 0s 846us/step - loss: 1.2844\n16/16 [==============================] - 0s 696us/step - loss: 1.2844\n16/16 [==============================] - 0s 926us/step - loss: 1.2845\n16/16 [==============================] - 0s 661us/step - loss: 1.2845\n16/16 [==============================] - 0s 1ms/step - loss: 1.2845\n16/16 [==============================] - 0s 1ms/step - loss: 1.2845\n\nTesting for epoch 24 index 2:\n16/16 [==============================] - 0s 641us/step - loss: 0.7292\n16/16 [==============================] - 0s 632us/step - loss: 1.2735\n16/16 [==============================] - 0s 780us/step - loss: 1.2970\n16/16 [==============================] - 0s 1ms/step - loss: 1.2995\n16/16 [==============================] - 0s 637us/step - loss: 1.2999\n16/16 [==============================] - 0s 1ms/step - loss: 1.3000\n16/16 [==============================] - 0s 1ms/step - loss: 1.3000\n16/16 [==============================] - 0s 981us/step - loss: 1.3000\n16/16 [==============================] - 0s 1ms/step - loss: 1.3000\n16/16 [==============================] - 0s 1ms/step - loss: 1.3000\nEpoch 25 of 60\n\nTesting for epoch 25 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.7228\n16/16 [==============================] - 0s 1ms/step - loss: 1.2928\n16/16 [==============================] - 0s 1ms/step - loss: 1.3171\n16/16 [==============================] - 0s 821us/step - loss: 1.3198\n16/16 [==============================] - 0s 611us/step - loss: 1.3203\n16/16 [==============================] - 0s 690us/step - loss: 1.3204\n16/16 [==============================] - 0s 647us/step - loss: 1.3204\n16/16 [==============================] - 0s 1ms/step - loss: 1.3204\n16/16 [==============================] - 0s 974us/step - loss: 1.3204\n16/16 [==============================] - 0s 589us/step - loss: 1.3204\n\nTesting for epoch 25 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.7136\n16/16 [==============================] - 0s 1ms/step - loss: 1.3031\n16/16 [==============================] - 0s 643us/step - loss: 1.3286\n16/16 [==============================] - 0s 1ms/step - loss: 1.3313\n16/16 [==============================] - 0s 948us/step - loss: 1.3319\n16/16 [==============================] - 0s 1ms/step - loss: 1.3320\n16/16 [==============================] - 0s 802us/step - loss: 1.3320\n16/16 [==============================] - 0s 1ms/step - loss: 1.3320\n16/16 [==============================] - 0s 1ms/step - loss: 1.3320\n16/16 [==============================] - 0s 837us/step - loss: 1.3320\nEpoch 26 of 60\n\nTesting for epoch 26 index 1:\n16/16 [==============================] - 0s 631us/step - loss: 0.6966\n16/16 [==============================] - 0s 1ms/step - loss: 1.3288\n16/16 [==============================] - 0s 820us/step - loss: 1.3566\n16/16 [==============================] - 0s 934us/step - loss: 1.3598\n16/16 [==============================] - 0s 1ms/step - loss: 1.3604\n16/16 [==============================] - 0s 1ms/step - loss: 1.3605\n16/16 [==============================] - 0s 1ms/step - loss: 1.3605\n16/16 [==============================] - 0s 635us/step - loss: 1.3605\n16/16 [==============================] - 0s 865us/step - loss: 1.3605\n16/16 [==============================] - 0s 630us/step - loss: 1.3605\n\nTesting for epoch 26 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.6781\n16/16 [==============================] - 0s 1ms/step - loss: 1.3420\n16/16 [==============================] - 0s 862us/step - loss: 1.3719\n16/16 [==============================] - 0s 635us/step - loss: 1.3756\n16/16 [==============================] - 0s 611us/step - loss: 1.3763\n16/16 [==============================] - 0s 626us/step - loss: 1.3764\n16/16 [==============================] - 0s 796us/step - loss: 1.3764\n16/16 [==============================] - 0s 1ms/step - loss: 1.3764\n16/16 [==============================] - 0s 920us/step - loss: 1.3764\n16/16 [==============================] - 0s 596us/step - loss: 1.3764\nEpoch 27 of 60\n\nTesting for epoch 27 index 1:\n16/16 [==============================] - 0s 629us/step - loss: 0.6549\n16/16 [==============================] - 0s 1ms/step - loss: 1.3709\n16/16 [==============================] - 0s 712us/step - loss: 1.4048\n16/16 [==============================] - 0s 724us/step - loss: 1.4090\n16/16 [==============================] - 0s 749us/step - loss: 1.4098\n16/16 [==============================] - 0s 653us/step - loss: 1.4099\n16/16 [==============================] - 0s 629us/step - loss: 1.4099\n16/16 [==============================] - 0s 1ms/step - loss: 1.4099\n16/16 [==============================] - 0s 723us/step - loss: 1.4099\n16/16 [==============================] - 0s 634us/step - loss: 1.4099\n\nTesting for epoch 27 index 2:\n16/16 [==============================] - 0s 625us/step - loss: 0.6334\n16/16 [==============================] - 0s 618us/step - loss: 1.3962\n16/16 [==============================] - 0s 603us/step - loss: 1.4358\n16/16 [==============================] - 0s 1ms/step - loss: 1.4403\n16/16 [==============================] - 0s 1ms/step - loss: 1.4413\n16/16 [==============================] - 0s 1ms/step - loss: 1.4414\n16/16 [==============================] - 0s 893us/step - loss: 1.4415\n16/16 [==============================] - 0s 598us/step - loss: 1.4415\n16/16 [==============================] - 0s 1ms/step - loss: 1.4414\n16/16 [==============================] - 0s 1ms/step - loss: 1.4414\nEpoch 28 of 60\n\nTesting for epoch 28 index 1:\n16/16 [==============================] - 0s 662us/step - loss: 0.6050\n16/16 [==============================] - 0s 1ms/step - loss: 1.4078\n16/16 [==============================] - 0s 1ms/step - loss: 1.4521\n16/16 [==============================] - 0s 818us/step - loss: 1.4572\n16/16 [==============================] - 0s 993us/step - loss: 1.4584\n16/16 [==============================] - 0s 1ms/step - loss: 1.4585\n16/16 [==============================] - 0s 1ms/step - loss: 1.4586\n16/16 [==============================] - 0s 1ms/step - loss: 1.4586\n16/16 [==============================] - 0s 765us/step - loss: 1.4585\n16/16 [==============================] - 0s 1ms/step - loss: 1.4585\n\nTesting for epoch 28 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.5843\n16/16 [==============================] - 0s 739us/step - loss: 1.4360\n16/16 [==============================] - 0s 1ms/step - loss: 1.4867\n16/16 [==============================] - 0s 582us/step - loss: 1.4928\n16/16 [==============================] - 0s 1ms/step - loss: 1.4941\n16/16 [==============================] - 0s 684us/step - loss: 1.4943\n16/16 [==============================] - 0s 884us/step - loss: 1.4943\n16/16 [==============================] - 0s 746us/step - loss: 1.4943\n16/16 [==============================] - 0s 997us/step - loss: 1.4943\n16/16 [==============================] - 0s 1ms/step - loss: 1.4942\nEpoch 29 of 60\n\nTesting for epoch 29 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.5581\n16/16 [==============================] - 0s 1ms/step - loss: 1.4546\n16/16 [==============================] - 0s 1ms/step - loss: 1.5115\n16/16 [==============================] - 0s 1ms/step - loss: 1.5182\n16/16 [==============================] - 0s 1ms/step - loss: 1.5197\n16/16 [==============================] - 0s 1ms/step - loss: 1.5199\n16/16 [==============================] - 0s 1ms/step - loss: 1.5199\n16/16 [==============================] - 0s 612us/step - loss: 1.5199\n16/16 [==============================] - 0s 650us/step - loss: 1.5199\n16/16 [==============================] - 0s 642us/step - loss: 1.5199\n\nTesting for epoch 29 index 2:\n16/16 [==============================] - 0s 601us/step - loss: 0.5402\n16/16 [==============================] - 0s 900us/step - loss: 1.4761\n16/16 [==============================] - 0s 1ms/step - loss: 1.5388\n16/16 [==============================] - 0s 1ms/step - loss: 1.5458\n16/16 [==============================] - 0s 1ms/step - loss: 1.5476\n16/16 [==============================] - 0s 1ms/step - loss: 1.5478\n16/16 [==============================] - 0s 1ms/step - loss: 1.5478\n16/16 [==============================] - 0s 593us/step - loss: 1.5478\n16/16 [==============================] - 0s 1ms/step - loss: 1.5478\n16/16 [==============================] - 0s 1ms/step - loss: 1.5478\nEpoch 30 of 60\n\nTesting for epoch 30 index 1:\n16/16 [==============================] - 0s 769us/step - loss: 0.5184\n16/16 [==============================] - 0s 606us/step - loss: 1.5000\n16/16 [==============================] - 0s 908us/step - loss: 1.5668\n16/16 [==============================] - 0s 1ms/step - loss: 1.5748\n16/16 [==============================] - 0s 1ms/step - loss: 1.5767\n16/16 [==============================] - 0s 1ms/step - loss: 1.5769\n16/16 [==============================] - 0s 1ms/step - loss: 1.5769\n16/16 [==============================] - 0s 705us/step - loss: 1.5769\n16/16 [==============================] - 0s 613us/step - loss: 1.5769\n16/16 [==============================] - 0s 671us/step - loss: 1.5768\n\nTesting for epoch 30 index 2:\n16/16 [==============================] - 0s 603us/step - loss: 0.5062\n16/16 [==============================] - 0s 629us/step - loss: 1.5280\n16/16 [==============================] - 0s 751us/step - loss: 1.5999\n16/16 [==============================] - 0s 615us/step - loss: 1.6088\n16/16 [==============================] - 0s 929us/step - loss: 1.6109\n16/16 [==============================] - 0s 1ms/step - loss: 1.6112\n16/16 [==============================] - 0s 1ms/step - loss: 1.6112\n16/16 [==============================] - 0s 613us/step - loss: 1.6112\n16/16 [==============================] - 0s 1ms/step - loss: 1.6112\n16/16 [==============================] - 0s 876us/step - loss: 1.6112\nEpoch 31 of 60\n\nTesting for epoch 31 index 1:\n16/16 [==============================] - 0s 663us/step - loss: 0.4910\n16/16 [==============================] - 0s 1ms/step - loss: 1.5266\n16/16 [==============================] - 0s 1ms/step - loss: 1.6021\n16/16 [==============================] - 0s 1ms/step - loss: 1.6113\n16/16 [==============================] - 0s 763us/step - loss: 1.6135\n16/16 [==============================] - 0s 956us/step - loss: 1.6138\n16/16 [==============================] - 0s 611us/step - loss: 1.6139\n16/16 [==============================] - 0s 613us/step - loss: 1.6139\n16/16 [==============================] - 0s 1ms/step - loss: 1.6138\n16/16 [==============================] - 0s 718us/step - loss: 1.6138\n\nTesting for epoch 31 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4859\n16/16 [==============================] - 0s 622us/step - loss: 1.5514\n16/16 [==============================] - 0s 608us/step - loss: 1.6294\n16/16 [==============================] - 0s 1ms/step - loss: 1.6390\n16/16 [==============================] - 0s 634us/step - loss: 1.6414\n16/16 [==============================] - 0s 593us/step - loss: 1.6417\n16/16 [==============================] - 0s 656us/step - loss: 1.6417\n16/16 [==============================] - 0s 606us/step - loss: 1.6417\n16/16 [==============================] - 0s 627us/step - loss: 1.6417\n16/16 [==============================] - 0s 951us/step - loss: 1.6416\nEpoch 32 of 60\n\nTesting for epoch 32 index 1:\n16/16 [==============================] - 0s 886us/step - loss: 0.4775\n16/16 [==============================] - 0s 631us/step - loss: 1.5698\n16/16 [==============================] - 0s 1ms/step - loss: 1.6491\n16/16 [==============================] - 0s 1ms/step - loss: 1.6591\n16/16 [==============================] - 0s 642us/step - loss: 1.6617\n16/16 [==============================] - 0s 1ms/step - loss: 1.6620\n16/16 [==============================] - 0s 606us/step - loss: 1.6621\n16/16 [==============================] - 0s 649us/step - loss: 1.6620\n16/16 [==============================] - 0s 622us/step - loss: 1.6620\n16/16 [==============================] - 0s 621us/step - loss: 1.6619\n\nTesting for epoch 32 index 2:\n16/16 [==============================] - 0s 629us/step - loss: 0.4795\n16/16 [==============================] - 0s 638us/step - loss: 1.5898\n16/16 [==============================] - 0s 634us/step - loss: 1.6681\n16/16 [==============================] - 0s 1ms/step - loss: 1.6781\n16/16 [==============================] - 0s 677us/step - loss: 1.6809\n16/16 [==============================] - 0s 977us/step - loss: 1.6812\n16/16 [==============================] - 0s 1ms/step - loss: 1.6812\n16/16 [==============================] - 0s 1ms/step - loss: 1.6812\n16/16 [==============================] - 0s 1ms/step - loss: 1.6812\n16/16 [==============================] - 0s 1ms/step - loss: 1.6812\nEpoch 33 of 60\n\nTesting for epoch 33 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4773\n16/16 [==============================] - 0s 634us/step - loss: 1.5951\n16/16 [==============================] - 0s 1ms/step - loss: 1.6703\n16/16 [==============================] - 0s 599us/step - loss: 1.6803\n16/16 [==============================] - 0s 685us/step - loss: 1.6830\n16/16 [==============================] - 0s 617us/step - loss: 1.6833\n16/16 [==============================] - 0s 945us/step - loss: 1.6833\n16/16 [==============================] - 0s 1ms/step - loss: 1.6833\n16/16 [==============================] - 0s 1ms/step - loss: 1.6833\n16/16 [==============================] - 0s 602us/step - loss: 1.6832\n\nTesting for epoch 33 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4855\n16/16 [==============================] - 0s 1ms/step - loss: 1.6287\n16/16 [==============================] - 0s 1ms/step - loss: 1.7034\n16/16 [==============================] - 0s 1ms/step - loss: 1.7137\n16/16 [==============================] - 0s 639us/step - loss: 1.7163\n16/16 [==============================] - 0s 681us/step - loss: 1.7166\n16/16 [==============================] - 0s 620us/step - loss: 1.7167\n16/16 [==============================] - 0s 583us/step - loss: 1.7167\n16/16 [==============================] - 0s 1ms/step - loss: 1.7167\n16/16 [==============================] - 0s 1ms/step - loss: 1.7166\nEpoch 34 of 60\n\nTesting for epoch 34 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.4900\n16/16 [==============================] - 0s 773us/step - loss: 1.6487\n16/16 [==============================] - 0s 624us/step - loss: 1.7219\n16/16 [==============================] - 0s 1ms/step - loss: 1.7322\n16/16 [==============================] - 0s 1ms/step - loss: 1.7348\n16/16 [==============================] - 0s 1ms/step - loss: 1.7352\n16/16 [==============================] - 0s 1ms/step - loss: 1.7352\n16/16 [==============================] - 0s 1ms/step - loss: 1.7352\n16/16 [==============================] - 0s 1ms/step - loss: 1.7352\n16/16 [==============================] - 0s 1ms/step - loss: 1.7351\n\nTesting for epoch 34 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.5048\n16/16 [==============================] - 0s 1ms/step - loss: 1.6447\n16/16 [==============================] - 0s 1ms/step - loss: 1.7129\n16/16 [==============================] - 0s 1ms/step - loss: 1.7226\n16/16 [==============================] - 0s 1ms/step - loss: 1.7250\n16/16 [==============================] - 0s 1ms/step - loss: 1.7253\n16/16 [==============================] - 0s 717us/step - loss: 1.7253\n16/16 [==============================] - 0s 830us/step - loss: 1.7253\n16/16 [==============================] - 0s 624us/step - loss: 1.7253\n16/16 [==============================] - 0s 682us/step - loss: 1.7252\nEpoch 35 of 60\n\nTesting for epoch 35 index 1:\n16/16 [==============================] - 0s 648us/step - loss: 0.5151\n16/16 [==============================] - 0s 1ms/step - loss: 1.6720\n16/16 [==============================] - 0s 838us/step - loss: 1.7389\n16/16 [==============================] - 0s 1ms/step - loss: 1.7487\n16/16 [==============================] - 0s 1ms/step - loss: 1.7510\n16/16 [==============================] - 0s 832us/step - loss: 1.7513\n16/16 [==============================] - 0s 1ms/step - loss: 1.7514\n16/16 [==============================] - 0s 595us/step - loss: 1.7514\n16/16 [==============================] - 0s 1ms/step - loss: 1.7514\n16/16 [==============================] - 0s 639us/step - loss: 1.7513\n\nTesting for epoch 35 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.5348\n16/16 [==============================] - 0s 870us/step - loss: 1.6661\n16/16 [==============================] - 0s 1ms/step - loss: 1.7277\n16/16 [==============================] - 0s 640us/step - loss: 1.7367\n16/16 [==============================] - 0s 897us/step - loss: 1.7389\n16/16 [==============================] - 0s 1ms/step - loss: 1.7391\n16/16 [==============================] - 0s 1ms/step - loss: 1.7392\n16/16 [==============================] - 0s 1ms/step - loss: 1.7392\n16/16 [==============================] - 0s 745us/step - loss: 1.7392\n16/16 [==============================] - 0s 1ms/step - loss: 1.7391\nEpoch 36 of 60\n\nTesting for epoch 36 index 1:\n16/16 [==============================] - 0s 688us/step - loss: 0.5491\n16/16 [==============================] - 0s 635us/step - loss: 1.6837\n16/16 [==============================] - 0s 601us/step - loss: 1.7423\n16/16 [==============================] - 0s 601us/step - loss: 1.7511\n16/16 [==============================] - 0s 1ms/step - loss: 1.7531\n16/16 [==============================] - 0s 656us/step - loss: 1.7534\n16/16 [==============================] - 0s 1ms/step - loss: 1.7534\n16/16 [==============================] - 0s 1ms/step - loss: 1.7534\n16/16 [==============================] - 0s 1ms/step - loss: 1.7534\n16/16 [==============================] - 0s 1ms/step - loss: 1.7534\n\nTesting for epoch 36 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.5738\n16/16 [==============================] - 0s 1ms/step - loss: 1.6942\n16/16 [==============================] - 0s 1ms/step - loss: 1.7482\n16/16 [==============================] - 0s 1ms/step - loss: 1.7566\n16/16 [==============================] - 0s 623us/step - loss: 1.7585\n16/16 [==============================] - 0s 741us/step - loss: 1.7588\n16/16 [==============================] - 0s 774us/step - loss: 1.7588\n16/16 [==============================] - 0s 1ms/step - loss: 1.7588\n16/16 [==============================] - 0s 1ms/step - loss: 1.7588\n16/16 [==============================] - 0s 1ms/step - loss: 1.7587\nEpoch 37 of 60\n\nTesting for epoch 37 index 1:\n16/16 [==============================] - 0s 604us/step - loss: 0.5910\n16/16 [==============================] - 0s 1ms/step - loss: 1.7056\n16/16 [==============================] - 0s 1ms/step - loss: 1.7570\n16/16 [==============================] - 0s 1ms/step - loss: 1.7652\n16/16 [==============================] - 0s 596us/step - loss: 1.7670\n16/16 [==============================] - 0s 944us/step - loss: 1.7673\n16/16 [==============================] - 0s 1ms/step - loss: 1.7673\n16/16 [==============================] - 0s 619us/step - loss: 1.7673\n16/16 [==============================] - 0s 617us/step - loss: 1.7673\n16/16 [==============================] - 0s 1ms/step - loss: 1.7673\n\nTesting for epoch 37 index 2:\n16/16 [==============================] - 0s 648us/step - loss: 0.6175\n16/16 [==============================] - 0s 628us/step - loss: 1.7136\n16/16 [==============================] - 0s 666us/step - loss: 1.7615\n16/16 [==============================] - 0s 612us/step - loss: 1.7696\n16/16 [==============================] - 0s 1ms/step - loss: 1.7713\n16/16 [==============================] - 0s 742us/step - loss: 1.7715\n16/16 [==============================] - 0s 626us/step - loss: 1.7716\n16/16 [==============================] - 0s 614us/step - loss: 1.7716\n16/16 [==============================] - 0s 623us/step - loss: 1.7715\n16/16 [==============================] - 0s 932us/step - loss: 1.7715\nEpoch 38 of 60\n\nTesting for epoch 38 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.6365\n16/16 [==============================] - 0s 1ms/step - loss: 1.7285\n16/16 [==============================] - 0s 1ms/step - loss: 1.7737\n16/16 [==============================] - 0s 1ms/step - loss: 1.7815\n16/16 [==============================] - 0s 1ms/step - loss: 1.7831\n16/16 [==============================] - 0s 617us/step - loss: 1.7834\n16/16 [==============================] - 0s 1ms/step - loss: 1.7834\n16/16 [==============================] - 0s 1ms/step - loss: 1.7834\n16/16 [==============================] - 0s 601us/step - loss: 1.7834\n16/16 [==============================] - 0s 929us/step - loss: 1.7834\n\nTesting for epoch 38 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.6645\n16/16 [==============================] - 0s 1ms/step - loss: 1.7425\n16/16 [==============================] - 0s 1ms/step - loss: 1.7848\n16/16 [==============================] - 0s 641us/step - loss: 1.7924\n16/16 [==============================] - 0s 1ms/step - loss: 1.7938\n16/16 [==============================] - 0s 1ms/step - loss: 1.7941\n16/16 [==============================] - 0s 821us/step - loss: 1.7941\n16/16 [==============================] - 0s 1ms/step - loss: 1.7941\n16/16 [==============================] - 0s 626us/step - loss: 1.7941\n16/16 [==============================] - 0s 636us/step - loss: 1.7941\nEpoch 39 of 60\n\nTesting for epoch 39 index 1:\n16/16 [==============================] - 0s 634us/step - loss: 0.6840\n16/16 [==============================] - 0s 759us/step - loss: 1.7590\n16/16 [==============================] - 0s 1ms/step - loss: 1.7997\n16/16 [==============================] - 0s 1ms/step - loss: 1.8070\n16/16 [==============================] - 0s 1ms/step - loss: 1.8084\n16/16 [==============================] - 0s 1ms/step - loss: 1.8086\n16/16 [==============================] - 0s 639us/step - loss: 1.8086\n16/16 [==============================] - 0s 621us/step - loss: 1.8086\n16/16 [==============================] - 0s 670us/step - loss: 1.8086\n16/16 [==============================] - 0s 1ms/step - loss: 1.8086\n\nTesting for epoch 39 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.7124\n16/16 [==============================] - 0s 999us/step - loss: 1.7783\n16/16 [==============================] - 0s 1ms/step - loss: 1.8175\n16/16 [==============================] - 0s 688us/step - loss: 1.8245\n16/16 [==============================] - 0s 1ms/step - loss: 1.8258\n16/16 [==============================] - 0s 1ms/step - loss: 1.8261\n16/16 [==============================] - 0s 716us/step - loss: 1.8261\n16/16 [==============================] - 0s 658us/step - loss: 1.8261\n16/16 [==============================] - 0s 632us/step - loss: 1.8261\n16/16 [==============================] - 0s 635us/step - loss: 1.8261\nEpoch 40 of 60\n\nTesting for epoch 40 index 1:\n16/16 [==============================] - 0s 921us/step - loss: 0.7273\n16/16 [==============================] - 0s 863us/step - loss: 1.7767\n16/16 [==============================] - 0s 995us/step - loss: 1.8143\n16/16 [==============================] - 0s 593us/step - loss: 1.8210\n16/16 [==============================] - 0s 1ms/step - loss: 1.8223\n16/16 [==============================] - 0s 877us/step - loss: 1.8225\n16/16 [==============================] - 0s 1ms/step - loss: 1.8226\n16/16 [==============================] - 0s 820us/step - loss: 1.8226\n16/16 [==============================] - 0s 1ms/step - loss: 1.8225\n16/16 [==============================] - 0s 1ms/step - loss: 1.8225\n\nTesting for epoch 40 index 2:\n16/16 [==============================] - 0s 688us/step - loss: 0.7514\n16/16 [==============================] - 0s 1ms/step - loss: 1.7804\n16/16 [==============================] - 0s 1ms/step - loss: 1.8152\n16/16 [==============================] - 0s 585us/step - loss: 1.8214\n16/16 [==============================] - 0s 1ms/step - loss: 1.8225\n16/16 [==============================] - 0s 619us/step - loss: 1.8227\n16/16 [==============================] - 0s 1ms/step - loss: 1.8228\n16/16 [==============================] - 0s 1ms/step - loss: 1.8228\n16/16 [==============================] - 0s 1ms/step - loss: 1.8227\n16/16 [==============================] - 0s 852us/step - loss: 1.8227\nEpoch 41 of 60\n\nTesting for epoch 41 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.7649\n16/16 [==============================] - 0s 1ms/step - loss: 1.7944\n16/16 [==============================] - 0s 1ms/step - loss: 1.8299\n16/16 [==============================] - 0s 804us/step - loss: 1.8361\n16/16 [==============================] - 0s 1ms/step - loss: 1.8372\n16/16 [==============================] - 0s 612us/step - loss: 1.8375\n16/16 [==============================] - 0s 1ms/step - loss: 1.8375\n16/16 [==============================] - 0s 1ms/step - loss: 1.8375\n16/16 [==============================] - 0s 1ms/step - loss: 1.8375\n16/16 [==============================] - 0s 641us/step - loss: 1.8375\n\nTesting for epoch 41 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.7882\n16/16 [==============================] - 0s 853us/step - loss: 1.8148\n16/16 [==============================] - 0s 659us/step - loss: 1.8491\n16/16 [==============================] - 0s 615us/step - loss: 1.8553\n16/16 [==============================] - 0s 931us/step - loss: 1.8563\n16/16 [==============================] - 0s 1ms/step - loss: 1.8566\n16/16 [==============================] - 0s 634us/step - loss: 1.8566\n16/16 [==============================] - 0s 861us/step - loss: 1.8566\n16/16 [==============================] - 0s 960us/step - loss: 1.8566\n16/16 [==============================] - 0s 1ms/step - loss: 1.8566\nEpoch 42 of 60\n\nTesting for epoch 42 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.7975\n16/16 [==============================] - 0s 646us/step - loss: 1.8225\n16/16 [==============================] - 0s 1ms/step - loss: 1.8555\n16/16 [==============================] - 0s 588us/step - loss: 1.8616\n16/16 [==============================] - 0s 898us/step - loss: 1.8626\n16/16 [==============================] - 0s 1ms/step - loss: 1.8628\n16/16 [==============================] - 0s 1ms/step - loss: 1.8628\n16/16 [==============================] - 0s 1ms/step - loss: 1.8628\n16/16 [==============================] - 0s 661us/step - loss: 1.8628\n16/16 [==============================] - 0s 636us/step - loss: 1.8628\n\nTesting for epoch 42 index 2:\n16/16 [==============================] - 0s 590us/step - loss: 0.8169\n16/16 [==============================] - 0s 1ms/step - loss: 1.8391\n16/16 [==============================] - 0s 633us/step - loss: 1.8715\n16/16 [==============================] - 0s 585us/step - loss: 1.8774\n16/16 [==============================] - 0s 615us/step - loss: 1.8784\n16/16 [==============================] - 0s 596us/step - loss: 1.8786\n16/16 [==============================] - 0s 1ms/step - loss: 1.8787\n16/16 [==============================] - 0s 1ms/step - loss: 1.8787\n16/16 [==============================] - 0s 631us/step - loss: 1.8787\n16/16 [==============================] - 0s 671us/step - loss: 1.8787\nEpoch 43 of 60\n\nTesting for epoch 43 index 1:\n16/16 [==============================] - 0s 865us/step - loss: 0.8231\n16/16 [==============================] - 0s 1ms/step - loss: 1.8496\n16/16 [==============================] - 0s 620us/step - loss: 1.8823\n16/16 [==============================] - 0s 664us/step - loss: 1.8883\n16/16 [==============================] - 0s 600us/step - loss: 1.8893\n16/16 [==============================] - 0s 1ms/step - loss: 1.8895\n16/16 [==============================] - 0s 1ms/step - loss: 1.8896\n16/16 [==============================] - 0s 1ms/step - loss: 1.8896\n16/16 [==============================] - 0s 628us/step - loss: 1.8896\n16/16 [==============================] - 0s 1ms/step - loss: 1.8895\n\nTesting for epoch 43 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.8391\n16/16 [==============================] - 0s 609us/step - loss: 1.8681\n16/16 [==============================] - 0s 871us/step - loss: 1.9009\n16/16 [==============================] - 0s 565us/step - loss: 1.9070\n16/16 [==============================] - 0s 1ms/step - loss: 1.9079\n16/16 [==============================] - 0s 610us/step - loss: 1.9081\n16/16 [==============================] - 0s 637us/step - loss: 1.9082\n16/16 [==============================] - 0s 1ms/step - loss: 1.9082\n16/16 [==============================] - 0s 1ms/step - loss: 1.9082\n16/16 [==============================] - 0s 880us/step - loss: 1.9082\nEpoch 44 of 60\n\nTesting for epoch 44 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.8437\n16/16 [==============================] - 0s 1ms/step - loss: 1.8798\n16/16 [==============================] - 0s 923us/step - loss: 1.9120\n16/16 [==============================] - 0s 1ms/step - loss: 1.9179\n16/16 [==============================] - 0s 612us/step - loss: 1.9189\n16/16 [==============================] - 0s 1ms/step - loss: 1.9191\n16/16 [==============================] - 0s 681us/step - loss: 1.9191\n16/16 [==============================] - 0s 1ms/step - loss: 1.9191\n16/16 [==============================] - 0s 1ms/step - loss: 1.9191\n16/16 [==============================] - 0s 1ms/step - loss: 1.9191\n\nTesting for epoch 44 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.8544\n16/16 [==============================] - 0s 645us/step - loss: 1.8871\n16/16 [==============================] - 0s 632us/step - loss: 1.9189\n16/16 [==============================] - 0s 1ms/step - loss: 1.9248\n16/16 [==============================] - 0s 1ms/step - loss: 1.9257\n16/16 [==============================] - 0s 1ms/step - loss: 1.9259\n16/16 [==============================] - 0s 836us/step - loss: 1.9259\n16/16 [==============================] - 0s 695us/step - loss: 1.9259\n16/16 [==============================] - 0s 1ms/step - loss: 1.9259\n16/16 [==============================] - 0s 808us/step - loss: 1.9259\nEpoch 45 of 60\n\nTesting for epoch 45 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.8583\n16/16 [==============================] - 0s 1ms/step - loss: 1.9099\n16/16 [==============================] - 0s 785us/step - loss: 1.9431\n16/16 [==============================] - 0s 782us/step - loss: 1.9491\n16/16 [==============================] - 0s 1ms/step - loss: 1.9501\n16/16 [==============================] - 0s 711us/step - loss: 1.9503\n16/16 [==============================] - 0s 617us/step - loss: 1.9503\n16/16 [==============================] - 0s 601us/step - loss: 1.9503\n16/16 [==============================] - 0s 671us/step - loss: 1.9503\n16/16 [==============================] - 0s 602us/step - loss: 1.9504\n\nTesting for epoch 45 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.8705\n16/16 [==============================] - 0s 1ms/step - loss: 1.9270\n16/16 [==============================] - 0s 928us/step - loss: 1.9599\n16/16 [==============================] - 0s 758us/step - loss: 1.9658\n16/16 [==============================] - 0s 2ms/step - loss: 1.9668\n16/16 [==============================] - 0s 1ms/step - loss: 1.9670\n16/16 [==============================] - 0s 594us/step - loss: 1.9670\n16/16 [==============================] - 0s 600us/step - loss: 1.9670\n16/16 [==============================] - 0s 599us/step - loss: 1.9670\n16/16 [==============================] - 0s 924us/step - loss: 1.9670\nEpoch 46 of 60\n\nTesting for epoch 46 index 1:\n16/16 [==============================] - 0s 643us/step - loss: 0.8683\n16/16 [==============================] - 0s 1ms/step - loss: 1.9287\n16/16 [==============================] - 0s 665us/step - loss: 1.9614\n16/16 [==============================] - 0s 1ms/step - loss: 1.9673\n16/16 [==============================] - 0s 1ms/step - loss: 1.9683\n16/16 [==============================] - 0s 1ms/step - loss: 1.9685\n16/16 [==============================] - 0s 654us/step - loss: 1.9685\n16/16 [==============================] - 0s 762us/step - loss: 1.9685\n16/16 [==============================] - 0s 628us/step - loss: 1.9685\n16/16 [==============================] - 0s 654us/step - loss: 1.9685\n\nTesting for epoch 46 index 2:\n16/16 [==============================] - 0s 659us/step - loss: 0.8815\n16/16 [==============================] - 0s 1ms/step - loss: 1.9514\n16/16 [==============================] - 0s 1ms/step - loss: 1.9846\n16/16 [==============================] - 0s 1ms/step - loss: 1.9905\n16/16 [==============================] - 0s 1ms/step - loss: 1.9915\n16/16 [==============================] - 0s 786us/step - loss: 1.9916\n16/16 [==============================] - 0s 655us/step - loss: 1.9917\n16/16 [==============================] - 0s 1ms/step - loss: 1.9917\n16/16 [==============================] - 0s 973us/step - loss: 1.9917\n16/16 [==============================] - 0s 1ms/step - loss: 1.9917\nEpoch 47 of 60\n\nTesting for epoch 47 index 1:\n16/16 [==============================] - 0s 741us/step - loss: 0.8773\n16/16 [==============================] - 0s 672us/step - loss: 1.9488\n16/16 [==============================] - 0s 660us/step - loss: 1.9816\n16/16 [==============================] - 0s 1ms/step - loss: 1.9875\n16/16 [==============================] - 0s 899us/step - loss: 1.9885\n16/16 [==============================] - 0s 1ms/step - loss: 1.9887\n16/16 [==============================] - 0s 885us/step - loss: 1.9887\n16/16 [==============================] - 0s 661us/step - loss: 1.9887\n16/16 [==============================] - 0s 1ms/step - loss: 1.9887\n16/16 [==============================] - 0s 972us/step - loss: 1.9887\n\nTesting for epoch 47 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 0.8923\n16/16 [==============================] - 0s 802us/step - loss: 1.9764\n16/16 [==============================] - 0s 733us/step - loss: 2.0094\n16/16 [==============================] - 0s 878us/step - loss: 2.0152\n16/16 [==============================] - 0s 667us/step - loss: 2.0162\n16/16 [==============================] - 0s 1ms/step - loss: 2.0164\n16/16 [==============================] - 0s 831us/step - loss: 2.0164\n16/16 [==============================] - 0s 1ms/step - loss: 2.0164\n16/16 [==============================] - 0s 1ms/step - loss: 2.0164\n16/16 [==============================] - 0s 1ms/step - loss: 2.0165\nEpoch 48 of 60\n\nTesting for epoch 48 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.8980\n16/16 [==============================] - 0s 829us/step - loss: 2.0059\n16/16 [==============================] - 0s 1ms/step - loss: 2.0403\n16/16 [==============================] - 0s 1ms/step - loss: 2.0464\n16/16 [==============================] - 0s 991us/step - loss: 2.0474\n16/16 [==============================] - 0s 657us/step - loss: 2.0476\n16/16 [==============================] - 0s 1ms/step - loss: 2.0476\n16/16 [==============================] - 0s 1ms/step - loss: 2.0476\n16/16 [==============================] - 0s 625us/step - loss: 2.0476\n16/16 [==============================] - 0s 1ms/step - loss: 2.0476\n\nTesting for epoch 48 index 2:\n16/16 [==============================] - 0s 640us/step - loss: 0.9077\n16/16 [==============================] - 0s 1ms/step - loss: 2.0130\n16/16 [==============================] - 0s 1ms/step - loss: 2.0470\n16/16 [==============================] - 0s 953us/step - loss: 2.0530\n16/16 [==============================] - 0s 1ms/step - loss: 2.0539\n16/16 [==============================] - 0s 1ms/step - loss: 2.0541\n16/16 [==============================] - 0s 1ms/step - loss: 2.0542\n16/16 [==============================] - 0s 1ms/step - loss: 2.0542\n16/16 [==============================] - 0s 1ms/step - loss: 2.0542\n16/16 [==============================] - 0s 1ms/step - loss: 2.0542\nEpoch 49 of 60\n\nTesting for epoch 49 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.9081\n16/16 [==============================] - 0s 1ms/step - loss: 2.0204\n16/16 [==============================] - 0s 1ms/step - loss: 2.0543\n16/16 [==============================] - 0s 694us/step - loss: 2.0602\n16/16 [==============================] - 0s 1ms/step - loss: 2.0611\n16/16 [==============================] - 0s 1ms/step - loss: 2.0613\n16/16 [==============================] - 0s 815us/step - loss: 2.0614\n16/16 [==============================] - 0s 1ms/step - loss: 2.0614\n16/16 [==============================] - 0s 804us/step - loss: 2.0614\n16/16 [==============================] - 0s 1ms/step - loss: 2.0614\n\nTesting for epoch 49 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.9192\n16/16 [==============================] - 0s 2ms/step - loss: 2.0292\n16/16 [==============================] - 0s 1ms/step - loss: 2.0625\n16/16 [==============================] - 0s 1ms/step - loss: 2.0683\n16/16 [==============================] - 0s 1ms/step - loss: 2.0692\n16/16 [==============================] - 0s 1ms/step - loss: 2.0693\n16/16 [==============================] - 0s 1ms/step - loss: 2.0694\n16/16 [==============================] - 0s 1ms/step - loss: 2.0694\n16/16 [==============================] - 0s 1ms/step - loss: 2.0694\n16/16 [==============================] - 0s 1ms/step - loss: 2.0694\nEpoch 50 of 60\n\nTesting for epoch 50 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.9220\n16/16 [==============================] - 0s 1ms/step - loss: 2.0413\n16/16 [==============================] - 0s 1ms/step - loss: 2.0749\n16/16 [==============================] - 0s 1ms/step - loss: 2.0807\n16/16 [==============================] - 0s 1ms/step - loss: 2.0816\n16/16 [==============================] - 0s 1ms/step - loss: 2.0818\n16/16 [==============================] - 0s 1ms/step - loss: 2.0819\n16/16 [==============================] - 0s 1ms/step - loss: 2.0819\n16/16 [==============================] - 0s 1ms/step - loss: 2.0819\n16/16 [==============================] - 0s 1ms/step - loss: 2.0819\n\nTesting for epoch 50 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 0.9344\n16/16 [==============================] - 0s 2ms/step - loss: 2.0501\n16/16 [==============================] - 0s 2ms/step - loss: 2.0831\n16/16 [==============================] - 0s 2ms/step - loss: 2.0889\n16/16 [==============================] - 0s 2ms/step - loss: 2.0898\n16/16 [==============================] - 0s 2ms/step - loss: 2.0900\n16/16 [==============================] - 0s 2ms/step - loss: 2.0900\n16/16 [==============================] - 0s 2ms/step - loss: 2.0900\n16/16 [==============================] - 0s 2ms/step - loss: 2.0900\n16/16 [==============================] - 0s 2ms/step - loss: 2.0900\nEpoch 51 of 60\n\nTesting for epoch 51 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 0.9430\n16/16 [==============================] - 0s 2ms/step - loss: 2.0784\n16/16 [==============================] - 0s 1ms/step - loss: 2.1121\n16/16 [==============================] - 0s 1ms/step - loss: 2.1180\n16/16 [==============================] - 0s 1ms/step - loss: 2.1189\n16/16 [==============================] - 0s 670us/step - loss: 2.1191\n16/16 [==============================] - 0s 707us/step - loss: 2.1191\n16/16 [==============================] - 0s 720us/step - loss: 2.1191\n16/16 [==============================] - 0s 685us/step - loss: 2.1191\n16/16 [==============================] - 0s 600us/step - loss: 2.1191\n\nTesting for epoch 51 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.9590\n16/16 [==============================] - 0s 1ms/step - loss: 2.0944\n16/16 [==============================] - 0s 2ms/step - loss: 2.1283\n16/16 [==============================] - 0s 1ms/step - loss: 2.1342\n16/16 [==============================] - 0s 2ms/step - loss: 2.1351\n16/16 [==============================] - 0s 2ms/step - loss: 2.1353\n16/16 [==============================] - 0s 2ms/step - loss: 2.1353\n16/16 [==============================] - 0s 1ms/step - loss: 2.1353\n16/16 [==============================] - 0s 1ms/step - loss: 2.1353\n16/16 [==============================] - 0s 1ms/step - loss: 2.1354\nEpoch 52 of 60\n\nTesting for epoch 52 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.9632\n16/16 [==============================] - 0s 1ms/step - loss: 2.1018\n16/16 [==============================] - 0s 1ms/step - loss: 2.1355\n16/16 [==============================] - 0s 1ms/step - loss: 2.1415\n16/16 [==============================] - 0s 1ms/step - loss: 2.1424\n16/16 [==============================] - 0s 1ms/step - loss: 2.1426\n16/16 [==============================] - 0s 1ms/step - loss: 2.1426\n16/16 [==============================] - 0s 2ms/step - loss: 2.1426\n16/16 [==============================] - 0s 1ms/step - loss: 2.1426\n16/16 [==============================] - 0s 1ms/step - loss: 2.1427\n\nTesting for epoch 52 index 2:\n16/16 [==============================] - 0s 593us/step - loss: 0.9811\n16/16 [==============================] - 0s 1ms/step - loss: 2.1158\n16/16 [==============================] - 0s 601us/step - loss: 2.1494\n16/16 [==============================] - 0s 604us/step - loss: 2.1553\n16/16 [==============================] - 0s 1ms/step - loss: 2.1562\n16/16 [==============================] - 0s 580us/step - loss: 2.1564\n16/16 [==============================] - 0s 606us/step - loss: 2.1564\n16/16 [==============================] - 0s 669us/step - loss: 2.1565\n16/16 [==============================] - 0s 933us/step - loss: 2.1565\n16/16 [==============================] - 0s 604us/step - loss: 2.1565\nEpoch 53 of 60\n\nTesting for epoch 53 index 1:\n16/16 [==============================] - 0s 617us/step - loss: 0.9849\n16/16 [==============================] - 0s 874us/step - loss: 2.1127\n16/16 [==============================] - 0s 601us/step - loss: 2.1443\n16/16 [==============================] - 0s 721us/step - loss: 2.1500\n16/16 [==============================] - 0s 709us/step - loss: 2.1508\n16/16 [==============================] - 0s 636us/step - loss: 2.1510\n16/16 [==============================] - 0s 641us/step - loss: 2.1510\n16/16 [==============================] - 0s 3ms/step - loss: 2.1510\n16/16 [==============================] - 0s 832us/step - loss: 2.1510\n16/16 [==============================] - 0s 943us/step - loss: 2.1511\n\nTesting for epoch 53 index 2:\n16/16 [==============================] - 0s 960us/step - loss: 1.0029\n16/16 [==============================] - 0s 847us/step - loss: 2.1223\n16/16 [==============================] - 0s 1ms/step - loss: 2.1535\n16/16 [==============================] - 0s 663us/step - loss: 2.1592\n16/16 [==============================] - 0s 1ms/step - loss: 2.1600\n16/16 [==============================] - 0s 1ms/step - loss: 2.1601\n16/16 [==============================] - 0s 1ms/step - loss: 2.1602\n16/16 [==============================] - 0s 1ms/step - loss: 2.1602\n16/16 [==============================] - 0s 1ms/step - loss: 2.1602\n16/16 [==============================] - 0s 1ms/step - loss: 2.1602\nEpoch 54 of 60\n\nTesting for epoch 54 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.0201\n16/16 [==============================] - 0s 2ms/step - loss: 2.1555\n16/16 [==============================] - 0s 698us/step - loss: 2.1869\n16/16 [==============================] - 0s 738us/step - loss: 2.1925\n16/16 [==============================] - 0s 974us/step - loss: 2.1933\n16/16 [==============================] - 0s 1ms/step - loss: 2.1935\n16/16 [==============================] - 0s 811us/step - loss: 2.1935\n16/16 [==============================] - 0s 1ms/step - loss: 2.1935\n16/16 [==============================] - 0s 1ms/step - loss: 2.1935\n16/16 [==============================] - 0s 685us/step - loss: 2.1935\n\nTesting for epoch 54 index 2:\n16/16 [==============================] - 0s 644us/step - loss: 1.0391\n16/16 [==============================] - 0s 1ms/step - loss: 2.1625\n16/16 [==============================] - 0s 1ms/step - loss: 2.1931\n16/16 [==============================] - 0s 894us/step - loss: 2.1987\n16/16 [==============================] - 0s 2ms/step - loss: 2.1995\n16/16 [==============================] - 0s 700us/step - loss: 2.1996\n16/16 [==============================] - 0s 1ms/step - loss: 2.1997\n16/16 [==============================] - 0s 851us/step - loss: 2.1997\n16/16 [==============================] - 0s 925us/step - loss: 2.1997\n16/16 [==============================] - 0s 868us/step - loss: 2.1997\nEpoch 55 of 60\n\nTesting for epoch 55 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 1.0520\n16/16 [==============================] - 0s 627us/step - loss: 2.1801\n16/16 [==============================] - 0s 1ms/step - loss: 2.2107\n16/16 [==============================] - 0s 633us/step - loss: 2.2163\n16/16 [==============================] - 0s 899us/step - loss: 2.2171\n16/16 [==============================] - 0s 711us/step - loss: 2.2172\n16/16 [==============================] - 0s 1ms/step - loss: 2.2173\n16/16 [==============================] - 0s 664us/step - loss: 2.2173\n16/16 [==============================] - 0s 1ms/step - loss: 2.2173\n16/16 [==============================] - 0s 1ms/step - loss: 2.2173\n\nTesting for epoch 55 index 2:\n16/16 [==============================] - 0s 589us/step - loss: 1.0727\n16/16 [==============================] - 0s 603us/step - loss: 2.1879\n16/16 [==============================] - 0s 581us/step - loss: 2.2176\n16/16 [==============================] - 0s 580us/step - loss: 2.2229\n16/16 [==============================] - 0s 582us/step - loss: 2.2236\n16/16 [==============================] - 0s 571us/step - loss: 2.2238\n16/16 [==============================] - 0s 574us/step - loss: 2.2238\n16/16 [==============================] - 0s 561us/step - loss: 2.2238\n16/16 [==============================] - 0s 506us/step - loss: 2.2238\n16/16 [==============================] - 0s 526us/step - loss: 2.2239\nEpoch 56 of 60\n\nTesting for epoch 56 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 1.0790\n16/16 [==============================] - 0s 1ms/step - loss: 2.1883\n16/16 [==============================] - 0s 2ms/step - loss: 2.2177\n16/16 [==============================] - 0s 2ms/step - loss: 2.2230\n16/16 [==============================] - 0s 2ms/step - loss: 2.2237\n16/16 [==============================] - 0s 2ms/step - loss: 2.2239\n16/16 [==============================] - 0s 2ms/step - loss: 2.2239\n16/16 [==============================] - 0s 2ms/step - loss: 2.2239\n16/16 [==============================] - 0s 2ms/step - loss: 2.2240\n16/16 [==============================] - 0s 2ms/step - loss: 2.2240\n\nTesting for epoch 56 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 1.1019\n16/16 [==============================] - 0s 2ms/step - loss: 2.2025\n16/16 [==============================] - 0s 2ms/step - loss: 2.2310\n16/16 [==============================] - 0s 2ms/step - loss: 2.2362\n16/16 [==============================] - 0s 2ms/step - loss: 2.2369\n16/16 [==============================] - 0s 2ms/step - loss: 2.2371\n16/16 [==============================] - 0s 2ms/step - loss: 2.2371\n16/16 [==============================] - 0s 2ms/step - loss: 2.2371\n16/16 [==============================] - 0s 2ms/step - loss: 2.2371\n16/16 [==============================] - 0s 2ms/step - loss: 2.2372\nEpoch 57 of 60\n\nTesting for epoch 57 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 1.1130\n16/16 [==============================] - 0s 2ms/step - loss: 2.2126\n16/16 [==============================] - 0s 2ms/step - loss: 2.2408\n16/16 [==============================] - 0s 2ms/step - loss: 2.2460\n16/16 [==============================] - 0s 2ms/step - loss: 2.2466\n16/16 [==============================] - 0s 2ms/step - loss: 2.2468\n16/16 [==============================] - 0s 2ms/step - loss: 2.2468\n16/16 [==============================] - 0s 2ms/step - loss: 2.2469\n16/16 [==============================] - 0s 2ms/step - loss: 2.2469\n16/16 [==============================] - 0s 2ms/step - loss: 2.2469\n\nTesting for epoch 57 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 1.1360\n16/16 [==============================] - 0s 2ms/step - loss: 2.2253\n16/16 [==============================] - 0s 2ms/step - loss: 2.2519\n16/16 [==============================] - 0s 2ms/step - loss: 2.2569\n16/16 [==============================] - 0s 2ms/step - loss: 2.2576\n16/16 [==============================] - 0s 2ms/step - loss: 2.2577\n16/16 [==============================] - 0s 2ms/step - loss: 2.2577\n16/16 [==============================] - 0s 2ms/step - loss: 2.2577\n16/16 [==============================] - 0s 2ms/step - loss: 2.2578\n16/16 [==============================] - 0s 1ms/step - loss: 2.2578\nEpoch 58 of 60\n\nTesting for epoch 58 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.1524\n16/16 [==============================] - 0s 2ms/step - loss: 2.2520\n16/16 [==============================] - 0s 1ms/step - loss: 2.2798\n16/16 [==============================] - 0s 2ms/step - loss: 2.2850\n16/16 [==============================] - 0s 2ms/step - loss: 2.2857\n16/16 [==============================] - 0s 1ms/step - loss: 2.2858\n16/16 [==============================] - 0s 2ms/step - loss: 2.2859\n16/16 [==============================] - 0s 966us/step - loss: 2.2859\n16/16 [==============================] - 0s 2ms/step - loss: 2.2859\n16/16 [==============================] - 0s 2ms/step - loss: 2.2859\n\nTesting for epoch 58 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 1.1690\n16/16 [==============================] - 0s 984us/step - loss: 2.2519\n16/16 [==============================] - 0s 2ms/step - loss: 2.2784\n16/16 [==============================] - 0s 2ms/step - loss: 2.2834\n16/16 [==============================] - 0s 925us/step - loss: 2.2840\n16/16 [==============================] - 0s 2ms/step - loss: 2.2842\n16/16 [==============================] - 0s 2ms/step - loss: 2.2842\n16/16 [==============================] - 0s 2ms/step - loss: 2.2842\n16/16 [==============================] - 0s 767us/step - loss: 2.2842\n16/16 [==============================] - 0s 1ms/step - loss: 2.2843\nEpoch 59 of 60\n\nTesting for epoch 59 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 1.1811\n16/16 [==============================] - 0s 2ms/step - loss: 2.2676\n16/16 [==============================] - 0s 2ms/step - loss: 2.2948\n16/16 [==============================] - 0s 2ms/step - loss: 2.2999\n16/16 [==============================] - 0s 2ms/step - loss: 2.3005\n16/16 [==============================] - 0s 2ms/step - loss: 2.3007\n16/16 [==============================] - 0s 2ms/step - loss: 2.3008\n16/16 [==============================] - 0s 1ms/step - loss: 2.3008\n16/16 [==============================] - 0s 657us/step - loss: 2.3008\n16/16 [==============================] - 0s 654us/step - loss: 2.3008\n\nTesting for epoch 59 index 2:\n16/16 [==============================] - 0s 3ms/step - loss: 1.1986\n16/16 [==============================] - 0s 600us/step - loss: 2.2691\n16/16 [==============================] - 0s 579us/step - loss: 2.2946\n16/16 [==============================] - 0s 2ms/step - loss: 2.2995\n16/16 [==============================] - 0s 2ms/step - loss: 2.3001\n16/16 [==============================] - 0s 2ms/step - loss: 2.3003\n16/16 [==============================] - 0s 2ms/step - loss: 2.3003\n16/16 [==============================] - 0s 2ms/step - loss: 2.3003\n16/16 [==============================] - 0s 2ms/step - loss: 2.3004\n16/16 [==============================] - 0s 2ms/step - loss: 2.3004\nEpoch 60 of 60\n\nTesting for epoch 60 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 1.2123\n16/16 [==============================] - 0s 2ms/step - loss: 2.2903\n16/16 [==============================] - 0s 2ms/step - loss: 2.3163\n16/16 [==============================] - 0s 2ms/step - loss: 2.3213\n16/16 [==============================] - 0s 2ms/step - loss: 2.3219\n16/16 [==============================] - 0s 2ms/step - loss: 2.3221\n16/16 [==============================] - 0s 2ms/step - loss: 2.3221\n16/16 [==============================] - 0s 2ms/step - loss: 2.3221\n16/16 [==============================] - 0s 2ms/step - loss: 2.3221\n16/16 [==============================] - 0s 2ms/step - loss: 2.3222\n\nTesting for epoch 60 index 2:\n16/16 [==============================] - 0s 795us/step - loss: 1.2301\n16/16 [==============================] - 0s 1ms/step - loss: 2.2941\n16/16 [==============================] - 0s 597us/step - loss: 2.3188\n16/16 [==============================] - 0s 524us/step - loss: 2.3236\n16/16 [==============================] - 0s 889us/step - loss: 2.3242\n16/16 [==============================] - 0s 675us/step - loss: 2.3243\n16/16 [==============================] - 0s 523us/step - loss: 2.3243\n16/16 [==============================] - 0s 577us/step - loss: 2.3243\n16/16 [==============================] - 0s 531us/step - loss: 2.3243\n16/16 [==============================] - 0s 646us/step - loss: 2.3244\n\n\n\noutlier_MO_GAAL_one = list(clf.labels_)\n\n\noutlier_MO_GAAL_one = list(map(lambda x: 1 if x==0  else -1,outlier_MO_GAAL_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_MO_GAAL_one,tab_orbit)\n\n\n_conf.conf(\"MO-GAAL (Liu et al., 2019)\")\n\n\n\n\nAccuracy: 0.950\nPrecision: 0.950\nRecall: 1.000\nF1 Score: 0.974\n\n\n\nthirteen = twelve.append(_conf.tab)\n\n\n\nLSCP\n\ndetectors = [KNN(), LOF(), OCSVM()]\nclf = LSCP(detectors)\nclf.fit(_df[['x', 'y','f']])\n_df['LSCP_clf'] = clf.labels_\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/pyod/models/lscp.py:382: UserWarning: The number of histogram bins is greater than the number of classifiers, reducing n_bins to n_clf.\n  warnings.warn(\n\n\n\noutlier_LSCP_one = list(clf.labels_)\n\n\noutlier_LSCP_one = list(map(lambda x: 1 if x==0  else -1,outlier_LSCP_one))\n\n\n_conf = Conf_matrx(outlier_true_one,outlier_LSCP_one,tab_orbit)\n\n\n_conf.conf(\"LSCP (Zhao et al., 2019)\")\n\n\n\n\nAccuracy: 0.948\nPrecision: 0.999\nRecall: 0.946\nF1 Score: 0.972\n\n\n\nfourteen_orbit = thirteen.append(_conf.tab)"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#orbit-result",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#orbit-result",
    "title": "Class code for Comparison Study",
    "section": "Orbit Result",
    "text": "Orbit Result\n\nround(fourteen_orbit,4)\n\n\n\n\n\n  \n    \n      \n      Accuracy\n      Precision\n      Recall\n      F1\n    \n  \n  \n    \n      GODE\n      0.997\n      0.9969\n      1.0000\n      0.9984\n    \n    \n      LOF (Breunig et al., 2000)\n      0.886\n      0.9872\n      0.8916\n      0.9369\n    \n    \n      kNN (Ramaswamy et al., 2000)\n      0.948\n      0.9989\n      0.9463\n      0.9719\n    \n    \n      CBLOF (He et al., 2003)\n      0.918\n      0.9568\n      0.9568\n      0.9568\n    \n    \n      OCSVM (Sch Ìˆolkopf et al., 2001)\n      0.923\n      0.9877\n      0.9305\n      0.9583\n    \n    \n      MCD (Hardin and Rocke, 2004)\n      0.866\n      0.9533\n      0.9032\n      0.9276\n    \n    \n      Feature Bagging (Lazarevic and Kumar, 2005)\n      0.912\n      0.9789\n      0.9274\n      0.9524\n    \n    \n      ABOD (Kriegel et al., 2008)\n      0.988\n      0.9937\n      0.9937\n      0.9937\n    \n    \n      Isolation Forest (Liu et al., 2008)\n      0.378\n      0.9970\n      0.3463\n      0.5141\n    \n    \n      HBOS (Goldstein and Dengel, 2012)\n      0.881\n      0.9612\n      0.9116\n      0.9357\n    \n    \n      SOS (Janssens et al., 2012)\n      0.881\n      0.9612\n      0.9116\n      0.9357\n    \n    \n      SO-GAAL (Liu et al., 2019)\n      0.876\n      0.9589\n      0.9084\n      0.9330\n    \n    \n      MO-GAAL (Liu et al., 2019)\n      0.950\n      0.9500\n      1.0000\n      0.9744\n    \n    \n      LSCP (Zhao et al., 2019)\n      0.948\n      0.9989\n      0.9463\n      0.9719"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#bunny",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#bunny",
    "title": "Class code for Comparison Study",
    "section": "Bunny",
    "text": "Bunny\n\nG = graphs.Bunny()\nn = G.N\n\n\ng = filters.Heat(G, tau=75) \n\n\nnormal = np.random.randn(n)\nunif = np.concatenate([np.random.uniform(low=3,high=7,size=60), np.random.uniform(low=-7,high=-3,size=60),np.zeros(n-120)]); np.random.shuffle(unif)\nnoise = normal + unif\nindex_of_trueoutlier2 = np.where(unif!=0)\n\n\nf = np.zeros(n)\nf[1000] = -3234\nf = g.filter(f, method='chebyshev') \n\n2022-11-26 07:54:05,353:[WARNING](pygsp.graphs.graph.lmax): The largest eigenvalue G.lmax is not available, we need to estimate it. Explicitly call G.estimate_lmax() or G.compute_fourier_basis() once beforehand to suppress the warning.\n\n\n\nG.coords.shape\n\n(2503, 3)\n\n\n\n_W = G.W.toarray()\n_x = G.coords[:,0]\n_y = G.coords[:,1]\n_z = -G.coords[:,2]\n\n\n_df = pd.DataFrame({'x' : _x, 'y' : _y, 'z' : _z, 'fnoise':f+noise,'f' : f, 'noise': noise})\n\n\noutlier_true_one_2 = unif.copy()\n\n\noutlier_true_one_2 = list(map(lambda x: -1 if x !=0  else 1,outlier_true_one_2))\n\n\nX = np.array(_df)[:,:4]\n\n\nGODE\n\n_BUNNY = BUNNY(_df)\n\n\n_BUNNY.fit(sd=20,ref=10)\n\n\noutlier_simul_one = (_BUNNY.df['Residual']**2).tolist()\n\n\noutlier_simul_one = list(map(lambda x: -1 if x > 10 else 1,outlier_simul_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_simul_one,tab_bunny)\n\n\n_conf.conf(\"GODE\")\n\n\n\n\nAccuracy: 0.995\nPrecision: 0.995\nRecall: 0.999\nF1 Score: 0.997\n\n\n\none = _conf.tab\n\n\n\nLOF\n\nclf = LocalOutlierFactor(n_neighbors=2)\n\n\n_conf = Conf_matrx(outlier_true_one_2,clf.fit_predict(X),tab_bunny)\n\n\n_conf.conf(\"LOF (Breunig et al., 2000)\")\n\n\n\n\nAccuracy: 0.928\nPrecision: 0.957\nRecall: 0.969\nF1 Score: 0.963\n\n\n\ntwo = one.append(_conf.tab)\n\n\n\nKNN\n\nclf = KNN()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['knn_Clf'] = clf.labels_\n\n\noutlier_KNN_one = list(clf.labels_)\n\n\noutlier_KNN_one = list(map(lambda x: 1 if x==0  else -1,outlier_KNN_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_KNN_one,tab_bunny)\n\n\n_conf.conf(\"kNN (Ramaswamy et al., 2000)\")\n\n\n\n\nAccuracy: 0.940\nPrecision: 0.996\nRecall: 0.941\nF1 Score: 0.968\n\n\n\nthree = two.append(_conf.tab)\n\n\n\nCBLOF\n\nclf = CBLOF(contamination=0.05,check_estimator=False, random_state=77)\nclf.fit(_df[['x', 'y','fnoise']])\n_df['CBLOF_Clf'] = clf.labels_\n\n\noutlier_CBLOF_one = list(clf.labels_)\n\n\noutlier_CBLOF_one = list(map(lambda x: 1 if x==0  else -1,outlier_CBLOF_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_CBLOF_one,tab_bunny)\n\n\n_conf.conf(\"CBLOF (He et al., 2003)\")\n\n\n\n\nAccuracy: 0.978\nPrecision: 0.989\nRecall: 0.987\nF1 Score: 0.988\n\n\n\nfour = three.append(_conf.tab)\n\n\n\nOCSVM\n\nclf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n\n\nclf.fit(X)\n\nOneClassSVM(gamma=0.1, nu=0.1)\n\n\n\noutlier_OSVM_one = list(clf.predict(X))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_OSVM_one,tab_bunny)\n\n\n_conf.conf(\"OCSVM (Sch Ìˆolkopf et al., 2001)\")\n\n\n\n\nAccuracy: 0.932\nPrecision: 0.991\nRecall: 0.937\nF1 Score: 0.963\n\n\n\nfive = four.append(_conf.tab)\n\n\n\nMCD\n\nclf = MCD()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['MCD_clf'] = clf.labels_\n\n\noutlier_MCD_one = list(clf.labels_)\n\n\noutlier_MCD_one = list(map(lambda x: 1 if x==0  else -1,outlier_MCD_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_MCD_one,tab_bunny)\n\n\n_conf.conf(\"MCD (Hardin and Rocke, 2004)\")\n\n\n\n\nAccuracy: 0.935\nPrecision: 0.993\nRecall: 0.938\nF1 Score: 0.965\n\n\n\nsix = five.append(_conf.tab)\n\n\n\nFeature Bagging\n\nclf = FeatureBagging()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['FeatureBagging_clf'] = clf.labels_\n\n\noutlier_FeatureBagging_one = list(clf.labels_)\n\n\noutlier_FeatureBagging_one = list(map(lambda x: 1 if x==0  else -1,outlier_FeatureBagging_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_FeatureBagging_one,tab_bunny)\n\n\n_conf.conf(\"Feature Bagging (Lazarevic and Kumar, 2005)\")\n\n\n\n\nAccuracy: 0.915\nPrecision: 0.982\nRecall: 0.928\nF1 Score: 0.954\n\n\n\nseven = six.append(_conf.tab)\n\n\n\nABOD\n\nclf = ABOD(contamination=0.05)\nclf.fit(_df[['x', 'y','fnoise']])\n_df['ABOD_Clf'] = clf.labels_\n\n\noutlier_ABOD_one = list(clf.labels_)\n\n\noutlier_ABOD_one = list(map(lambda x: 1 if x==0  else -1,outlier_ABOD_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_ABOD_one,tab_bunny)\n\n\n_conf.conf(\"ABOD (Kriegel et al., 2008)\")\n\n\n\n\nAccuracy: 0.977\nPrecision: 0.989\nRecall: 0.987\nF1 Score: 0.988\n\n\n\neight = seven.append(_conf.tab)\n\n\n\nIForest\n\nod = IForest(\n    threshold=0.,\n    n_estimators=100\n)\n\n\nod.fit(_df[['x', 'y','fnoise']])\n\n\npreds = od.predict(\n    _df[['x', 'y','fnoise']],\n    return_instance_score=True\n)\n\n\n_df['IF_alibi'] = preds['data']['is_outlier']\n\n\noutlier_alibi_one = _df['IF_alibi']\n\n\noutlier_alibi_one = list(map(lambda x: 1 if x==0  else -1,outlier_alibi_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_alibi_one,tab_bunny)\n\n\n_conf.conf(\"Isolation Forest (Liu et al., 2008)\")\n\n\n\n\nAccuracy: 0.794\nPrecision: 0.995\nRecall: 0.788\nF1 Score: 0.879\n\n\n\nnine = eight.append(_conf.tab)\n\n\n\nHBOS\n\nclf = HBOS()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['HBOS_clf'] = clf.labels_\n\n\noutlier_HBOS_one = list(clf.labels_)\n\n\noutlier_HBOS_one = list(map(lambda x: 1 if x==0  else -1,outlier_HBOS_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_HBOS_one,tab_bunny)\n\n\n_conf.conf(\"HBOS (Goldstein and Dengel, 2012)\")\n\n\n\n\nAccuracy: 0.895\nPrecision: 0.969\nRecall: 0.919\nF1 Score: 0.944\n\n\n\nten = nine.append(_conf.tab)\n\n\n\nSOS\n\noutlier_SOS_one = list(clf.labels_)\n\n\noutlier_SOS_one = list(map(lambda x: 1 if x==0  else -1,outlier_SOS_one))\n\n\nclf = SOS()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['SOS_clf'] = clf.labels_\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_SOS_one,tab_bunny)\n\n\n_conf.conf(\"SOS (Janssens et al., 2012)\")\n\n\n\n\nAccuracy: 0.895\nPrecision: 0.969\nRecall: 0.919\nF1 Score: 0.944\n\n\n\neleven = ten.append(_conf.tab)\n\n\n\nSO_GAAL\n\nclf = SO_GAAL()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['SO_GAAL_clf'] = clf.labels_\n\nEpoch 1 of 60\n\nTesting for epoch 1 index 1:\n\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n\n\n\nTesting for epoch 1 index 2:\n\nTesting for epoch 1 index 3:\n\nTesting for epoch 1 index 4:\n\nTesting for epoch 1 index 5:\nEpoch 2 of 60\n\nTesting for epoch 2 index 1:\n\nTesting for epoch 2 index 2:\n\nTesting for epoch 2 index 3:\n\nTesting for epoch 2 index 4:\n\nTesting for epoch 2 index 5:\nEpoch 3 of 60\n\nTesting for epoch 3 index 1:\n\nTesting for epoch 3 index 2:\n\nTesting for epoch 3 index 3:\n\nTesting for epoch 3 index 4:\n\nTesting for epoch 3 index 5:\nEpoch 4 of 60\n\nTesting for epoch 4 index 1:\n\nTesting for epoch 4 index 2:\n\nTesting for epoch 4 index 3:\n\nTesting for epoch 4 index 4:\n\nTesting for epoch 4 index 5:\nEpoch 5 of 60\n\nTesting for epoch 5 index 1:\n\nTesting for epoch 5 index 2:\n\nTesting for epoch 5 index 3:\n\nTesting for epoch 5 index 4:\n\nTesting for epoch 5 index 5:\nEpoch 6 of 60\n\nTesting for epoch 6 index 1:\n\nTesting for epoch 6 index 2:\n\nTesting for epoch 6 index 3:\n\nTesting for epoch 6 index 4:\n\nTesting for epoch 6 index 5:\nEpoch 7 of 60\n\nTesting for epoch 7 index 1:\n\nTesting for epoch 7 index 2:\n\nTesting for epoch 7 index 3:\n\nTesting for epoch 7 index 4:\n\nTesting for epoch 7 index 5:\nEpoch 8 of 60\n\nTesting for epoch 8 index 1:\n\nTesting for epoch 8 index 2:\n\nTesting for epoch 8 index 3:\n\nTesting for epoch 8 index 4:\n\nTesting for epoch 8 index 5:\nEpoch 9 of 60\n\nTesting for epoch 9 index 1:\n\nTesting for epoch 9 index 2:\n\nTesting for epoch 9 index 3:\n\nTesting for epoch 9 index 4:\n\nTesting for epoch 9 index 5:\nEpoch 10 of 60\n\nTesting for epoch 10 index 1:\n\nTesting for epoch 10 index 2:\n\nTesting for epoch 10 index 3:\n\nTesting for epoch 10 index 4:\n\nTesting for epoch 10 index 5:\nEpoch 11 of 60\n\nTesting for epoch 11 index 1:\n\nTesting for epoch 11 index 2:\n\nTesting for epoch 11 index 3:\n\nTesting for epoch 11 index 4:\n\nTesting for epoch 11 index 5:\nEpoch 12 of 60\n\nTesting for epoch 12 index 1:\n\nTesting for epoch 12 index 2:\n\nTesting for epoch 12 index 3:\n\nTesting for epoch 12 index 4:\n\nTesting for epoch 12 index 5:\nEpoch 13 of 60\n\nTesting for epoch 13 index 1:\n\nTesting for epoch 13 index 2:\n\nTesting for epoch 13 index 3:\n\nTesting for epoch 13 index 4:\n\nTesting for epoch 13 index 5:\nEpoch 14 of 60\n\nTesting for epoch 14 index 1:\n\nTesting for epoch 14 index 2:\n\nTesting for epoch 14 index 3:\n\nTesting for epoch 14 index 4:\n\nTesting for epoch 14 index 5:\nEpoch 15 of 60\n\nTesting for epoch 15 index 1:\n\nTesting for epoch 15 index 2:\n\nTesting for epoch 15 index 3:\n\nTesting for epoch 15 index 4:\n\nTesting for epoch 15 index 5:\nEpoch 16 of 60\n\nTesting for epoch 16 index 1:\n\nTesting for epoch 16 index 2:\n\nTesting for epoch 16 index 3:\n\nTesting for epoch 16 index 4:\n\nTesting for epoch 16 index 5:\nEpoch 17 of 60\n\nTesting for epoch 17 index 1:\n\nTesting for epoch 17 index 2:\n\nTesting for epoch 17 index 3:\n\nTesting for epoch 17 index 4:\n\nTesting for epoch 17 index 5:\nEpoch 18 of 60\n\nTesting for epoch 18 index 1:\n\nTesting for epoch 18 index 2:\n\nTesting for epoch 18 index 3:\n\nTesting for epoch 18 index 4:\n\nTesting for epoch 18 index 5:\nEpoch 19 of 60\n\nTesting for epoch 19 index 1:\n\nTesting for epoch 19 index 2:\n\nTesting for epoch 19 index 3:\n\nTesting for epoch 19 index 4:\n\nTesting for epoch 19 index 5:\nEpoch 20 of 60\n\nTesting for epoch 20 index 1:\n\nTesting for epoch 20 index 2:\n\nTesting for epoch 20 index 3:\n\nTesting for epoch 20 index 4:\n\nTesting for epoch 20 index 5:\nEpoch 21 of 60\n\nTesting for epoch 21 index 1:\n\nTesting for epoch 21 index 2:\n\nTesting for epoch 21 index 3:\n\nTesting for epoch 21 index 4:\n\nTesting for epoch 21 index 5:\nEpoch 22 of 60\n\nTesting for epoch 22 index 1:\n16/16 [==============================] - 0s 894us/step - loss: 1.8529\n\nTesting for epoch 22 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8921\n\nTesting for epoch 22 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9309\n\nTesting for epoch 22 index 4:\n16/16 [==============================] - 0s 690us/step - loss: 1.8584\n\nTesting for epoch 22 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 1.8820\nEpoch 23 of 60\n\nTesting for epoch 23 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9128\n\nTesting for epoch 23 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9055\n\nTesting for epoch 23 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9463\n\nTesting for epoch 23 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9150\n\nTesting for epoch 23 index 5:\n16/16 [==============================] - 0s 755us/step - loss: 1.9138\nEpoch 24 of 60\n\nTesting for epoch 24 index 1:\n16/16 [==============================] - 0s 636us/step - loss: 2.0252\n\nTesting for epoch 24 index 2:\n16/16 [==============================] - 0s 640us/step - loss: 1.9456\n\nTesting for epoch 24 index 3:\n16/16 [==============================] - 0s 701us/step - loss: 1.9662\n\nTesting for epoch 24 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9841\n\nTesting for epoch 24 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0037\nEpoch 25 of 60\n\nTesting for epoch 25 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 1.9889\n\nTesting for epoch 25 index 2:\n16/16 [==============================] - 0s 871us/step - loss: 1.9856\n\nTesting for epoch 25 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0014\n\nTesting for epoch 25 index 4:\n16/16 [==============================] - 0s 778us/step - loss: 2.0162\n\nTesting for epoch 25 index 5:\n16/16 [==============================] - 0s 664us/step - loss: 2.0739\nEpoch 26 of 60\n\nTesting for epoch 26 index 1:\n16/16 [==============================] - 0s 637us/step - loss: 2.0179\n\nTesting for epoch 26 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0133\n\nTesting for epoch 26 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0655\n\nTesting for epoch 26 index 4:\n16/16 [==============================] - 0s 637us/step - loss: 2.0657\n\nTesting for epoch 26 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0669\nEpoch 27 of 60\n\nTesting for epoch 27 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0880\n\nTesting for epoch 27 index 2:\n16/16 [==============================] - 0s 800us/step - loss: 2.0889\n\nTesting for epoch 27 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1112\n\nTesting for epoch 27 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0641\n\nTesting for epoch 27 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0520\nEpoch 28 of 60\n\nTesting for epoch 28 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0533\n\nTesting for epoch 28 index 2:\n16/16 [==============================] - 0s 601us/step - loss: 2.1067\n\nTesting for epoch 28 index 3:\n16/16 [==============================] - 0s 645us/step - loss: 2.1065\n\nTesting for epoch 28 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0956\n\nTesting for epoch 28 index 5:\n16/16 [==============================] - 0s 634us/step - loss: 2.0811\nEpoch 29 of 60\n\nTesting for epoch 29 index 1:\n16/16 [==============================] - 0s 633us/step - loss: 2.0727\n\nTesting for epoch 29 index 2:\n16/16 [==============================] - 0s 687us/step - loss: 2.1834\n\nTesting for epoch 29 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.0984\n\nTesting for epoch 29 index 4:\n16/16 [==============================] - 0s 599us/step - loss: 2.1578\n\nTesting for epoch 29 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1489\nEpoch 30 of 60\n\nTesting for epoch 30 index 1:\n16/16 [==============================] - 0s 671us/step - loss: 2.1636\n\nTesting for epoch 30 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1516\n\nTesting for epoch 30 index 3:\n16/16 [==============================] - 0s 636us/step - loss: 2.1534\n\nTesting for epoch 30 index 4:\n16/16 [==============================] - 0s 776us/step - loss: 2.1465\n\nTesting for epoch 30 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1006\nEpoch 31 of 60\n\nTesting for epoch 31 index 1:\n16/16 [==============================] - 0s 768us/step - loss: 2.1580\n\nTesting for epoch 31 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1679\n\nTesting for epoch 31 index 3:\n16/16 [==============================] - 0s 932us/step - loss: 2.1854\n\nTesting for epoch 31 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1869\n\nTesting for epoch 31 index 5:\n16/16 [==============================] - 0s 600us/step - loss: 2.1570\nEpoch 32 of 60\n\nTesting for epoch 32 index 1:\n16/16 [==============================] - 0s 701us/step - loss: 2.2004\n\nTesting for epoch 32 index 2:\n16/16 [==============================] - 0s 664us/step - loss: 2.2094\n\nTesting for epoch 32 index 3:\n16/16 [==============================] - 0s 665us/step - loss: 2.2316\n\nTesting for epoch 32 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.1808\n\nTesting for epoch 32 index 5:\n16/16 [==============================] - 0s 606us/step - loss: 2.2633\nEpoch 33 of 60\n\nTesting for epoch 33 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2481\n\nTesting for epoch 33 index 2:\n16/16 [==============================] - 0s 629us/step - loss: 2.2154\n\nTesting for epoch 33 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2065\n\nTesting for epoch 33 index 4:\n16/16 [==============================] - 0s 632us/step - loss: 2.2313\n\nTesting for epoch 33 index 5:\n16/16 [==============================] - 0s 728us/step - loss: 2.2298\nEpoch 34 of 60\n\nTesting for epoch 34 index 1:\n16/16 [==============================] - 0s 651us/step - loss: 2.2541\n\nTesting for epoch 34 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2413\n\nTesting for epoch 34 index 3:\n16/16 [==============================] - 0s 665us/step - loss: 2.1930\n\nTesting for epoch 34 index 4:\n16/16 [==============================] - 0s 607us/step - loss: 2.2856\n\nTesting for epoch 34 index 5:\n16/16 [==============================] - 0s 650us/step - loss: 2.2537\nEpoch 35 of 60\n\nTesting for epoch 35 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2461\n\nTesting for epoch 35 index 2:\n16/16 [==============================] - 0s 654us/step - loss: 2.3097\n\nTesting for epoch 35 index 3:\n16/16 [==============================] - 0s 831us/step - loss: 2.3159\n\nTesting for epoch 35 index 4:\n16/16 [==============================] - 0s 934us/step - loss: 2.2306\n\nTesting for epoch 35 index 5:\n16/16 [==============================] - 0s 654us/step - loss: 2.2956\nEpoch 36 of 60\n\nTesting for epoch 36 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2296\n\nTesting for epoch 36 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2378\n\nTesting for epoch 36 index 3:\n16/16 [==============================] - 0s 926us/step - loss: 2.2114\n\nTesting for epoch 36 index 4:\n16/16 [==============================] - 0s 716us/step - loss: 2.2166\n\nTesting for epoch 36 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2483\nEpoch 37 of 60\n\nTesting for epoch 37 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2669\n\nTesting for epoch 37 index 2:\n16/16 [==============================] - 0s 718us/step - loss: 2.2966\n\nTesting for epoch 37 index 3:\n16/16 [==============================] - 0s 776us/step - loss: 2.2346\n\nTesting for epoch 37 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3040\n\nTesting for epoch 37 index 5:\n16/16 [==============================] - 0s 780us/step - loss: 2.3003\nEpoch 38 of 60\n\nTesting for epoch 38 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2809\n\nTesting for epoch 38 index 2:\n16/16 [==============================] - 0s 789us/step - loss: 2.2804\n\nTesting for epoch 38 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2915\n\nTesting for epoch 38 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.2829\n\nTesting for epoch 38 index 5:\n16/16 [==============================] - 0s 923us/step - loss: 2.3199\nEpoch 39 of 60\n\nTesting for epoch 39 index 1:\n16/16 [==============================] - 0s 980us/step - loss: 2.2642\n\nTesting for epoch 39 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3208\n\nTesting for epoch 39 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3127\n\nTesting for epoch 39 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3514\n\nTesting for epoch 39 index 5:\n16/16 [==============================] - 0s 829us/step - loss: 2.3363\nEpoch 40 of 60\n\nTesting for epoch 40 index 1:\n16/16 [==============================] - 0s 629us/step - loss: 2.3203\n\nTesting for epoch 40 index 2:\n16/16 [==============================] - 0s 658us/step - loss: 2.3100\n\nTesting for epoch 40 index 3:\n16/16 [==============================] - 0s 625us/step - loss: 2.2837\n\nTesting for epoch 40 index 4:\n16/16 [==============================] - 0s 640us/step - loss: 2.2877\n\nTesting for epoch 40 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3374\nEpoch 41 of 60\n\nTesting for epoch 41 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3149\n\nTesting for epoch 41 index 2:\n16/16 [==============================] - 0s 658us/step - loss: 2.3535\n\nTesting for epoch 41 index 3:\n16/16 [==============================] - 0s 652us/step - loss: 2.3861\n\nTesting for epoch 41 index 4:\n16/16 [==============================] - 0s 723us/step - loss: 2.3328\n\nTesting for epoch 41 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3450\nEpoch 42 of 60\n\nTesting for epoch 42 index 1:\n16/16 [==============================] - 0s 641us/step - loss: 2.3578\n\nTesting for epoch 42 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3235\n\nTesting for epoch 42 index 3:\n16/16 [==============================] - 0s 958us/step - loss: 2.3421\n\nTesting for epoch 42 index 4:\n16/16 [==============================] - 0s 593us/step - loss: 2.3656\n\nTesting for epoch 42 index 5:\n16/16 [==============================] - 0s 623us/step - loss: 2.3044\nEpoch 43 of 60\n\nTesting for epoch 43 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3273\n\nTesting for epoch 43 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3797\n\nTesting for epoch 43 index 3:\n16/16 [==============================] - 0s 654us/step - loss: 2.3372\n\nTesting for epoch 43 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3387\n\nTesting for epoch 43 index 5:\n16/16 [==============================] - 0s 608us/step - loss: 2.4377\nEpoch 44 of 60\n\nTesting for epoch 44 index 1:\n16/16 [==============================] - 0s 965us/step - loss: 2.4568\n\nTesting for epoch 44 index 2:\n16/16 [==============================] - 0s 640us/step - loss: 2.4050\n\nTesting for epoch 44 index 3:\n16/16 [==============================] - 0s 943us/step - loss: 2.3936\n\nTesting for epoch 44 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3910\n\nTesting for epoch 44 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4026\nEpoch 45 of 60\n\nTesting for epoch 45 index 1:\n16/16 [==============================] - 0s 647us/step - loss: 2.4177\n\nTesting for epoch 45 index 2:\n16/16 [==============================] - 0s 941us/step - loss: 2.4015\n\nTesting for epoch 45 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3971\n\nTesting for epoch 45 index 4:\n16/16 [==============================] - 0s 678us/step - loss: 2.3933\n\nTesting for epoch 45 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4488\nEpoch 46 of 60\n\nTesting for epoch 46 index 1:\n16/16 [==============================] - 0s 594us/step - loss: 2.3598\n\nTesting for epoch 46 index 2:\n16/16 [==============================] - 0s 655us/step - loss: 2.4883\n\nTesting for epoch 46 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4234\n\nTesting for epoch 46 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3641\n\nTesting for epoch 46 index 5:\n16/16 [==============================] - 0s 649us/step - loss: 2.4212\nEpoch 47 of 60\n\nTesting for epoch 47 index 1:\n16/16 [==============================] - 0s 828us/step - loss: 2.5119\n\nTesting for epoch 47 index 2:\n16/16 [==============================] - 0s 625us/step - loss: 2.4255\n\nTesting for epoch 47 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4828\n\nTesting for epoch 47 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4336\n\nTesting for epoch 47 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3916\nEpoch 48 of 60\n\nTesting for epoch 48 index 1:\n16/16 [==============================] - 0s 630us/step - loss: 2.4157\n\nTesting for epoch 48 index 2:\n16/16 [==============================] - 0s 621us/step - loss: 2.4543\n\nTesting for epoch 48 index 3:\n16/16 [==============================] - 0s 672us/step - loss: 2.3956\n\nTesting for epoch 48 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4783\n\nTesting for epoch 48 index 5:\n16/16 [==============================] - 0s 630us/step - loss: 2.4045\nEpoch 49 of 60\n\nTesting for epoch 49 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4787\n\nTesting for epoch 49 index 2:\n16/16 [==============================] - 0s 880us/step - loss: 2.4557\n\nTesting for epoch 49 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4497\n\nTesting for epoch 49 index 4:\n16/16 [==============================] - 0s 635us/step - loss: 2.4115\n\nTesting for epoch 49 index 5:\n16/16 [==============================] - 0s 613us/step - loss: 2.4469\nEpoch 50 of 60\n\nTesting for epoch 50 index 1:\n16/16 [==============================] - 0s 764us/step - loss: 2.4250\n\nTesting for epoch 50 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4706\n\nTesting for epoch 50 index 3:\n16/16 [==============================] - 0s 620us/step - loss: 2.3919\n\nTesting for epoch 50 index 4:\n16/16 [==============================] - 0s 698us/step - loss: 2.4463\n\nTesting for epoch 50 index 5:\n16/16 [==============================] - 0s 958us/step - loss: 2.4810\nEpoch 51 of 60\n\nTesting for epoch 51 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4359\n\nTesting for epoch 51 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4080\n\nTesting for epoch 51 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4634\n\nTesting for epoch 51 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5226\n\nTesting for epoch 51 index 5:\n16/16 [==============================] - 0s 894us/step - loss: 2.4385\nEpoch 52 of 60\n\nTesting for epoch 52 index 1:\n16/16 [==============================] - 0s 856us/step - loss: 2.5063\n\nTesting for epoch 52 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4672\n\nTesting for epoch 52 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5011\n\nTesting for epoch 52 index 4:\n16/16 [==============================] - 0s 618us/step - loss: 2.5610\n\nTesting for epoch 52 index 5:\n16/16 [==============================] - 0s 679us/step - loss: 2.5239\nEpoch 53 of 60\n\nTesting for epoch 53 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5248\n\nTesting for epoch 53 index 2:\n16/16 [==============================] - 0s 990us/step - loss: 2.5142\n\nTesting for epoch 53 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5164\n\nTesting for epoch 53 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.3996\n\nTesting for epoch 53 index 5:\n16/16 [==============================] - 0s 894us/step - loss: 2.4939\nEpoch 54 of 60\n\nTesting for epoch 54 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4897\n\nTesting for epoch 54 index 2:\n16/16 [==============================] - 0s 617us/step - loss: 2.5320\n\nTesting for epoch 54 index 3:\n16/16 [==============================] - 0s 619us/step - loss: 2.5544\n\nTesting for epoch 54 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4986\n\nTesting for epoch 54 index 5:\n16/16 [==============================] - 0s 648us/step - loss: 2.5618\nEpoch 55 of 60\n\nTesting for epoch 55 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5605\n\nTesting for epoch 55 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4780\n\nTesting for epoch 55 index 3:\n16/16 [==============================] - 0s 665us/step - loss: 2.4659\n\nTesting for epoch 55 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4696\n\nTesting for epoch 55 index 5:\n16/16 [==============================] - 0s 643us/step - loss: 2.5610\nEpoch 56 of 60\n\nTesting for epoch 56 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4586\n\nTesting for epoch 56 index 2:\n16/16 [==============================] - 0s 665us/step - loss: 2.4735\n\nTesting for epoch 56 index 3:\n16/16 [==============================] - 0s 964us/step - loss: 2.5013\n\nTesting for epoch 56 index 4:\n16/16 [==============================] - 0s 840us/step - loss: 2.4765\n\nTesting for epoch 56 index 5:\n16/16 [==============================] - 0s 908us/step - loss: 2.5925\nEpoch 57 of 60\n\nTesting for epoch 57 index 1:\n16/16 [==============================] - 0s 644us/step - loss: 2.5213\n\nTesting for epoch 57 index 2:\n16/16 [==============================] - 0s 624us/step - loss: 2.5540\n\nTesting for epoch 57 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5273\n\nTesting for epoch 57 index 4:\n16/16 [==============================] - 0s 665us/step - loss: 2.5155\n\nTesting for epoch 57 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5001\nEpoch 58 of 60\n\nTesting for epoch 58 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5154\n\nTesting for epoch 58 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5593\n\nTesting for epoch 58 index 3:\n16/16 [==============================] - 0s 653us/step - loss: 2.4897\n\nTesting for epoch 58 index 4:\n16/16 [==============================] - 0s 621us/step - loss: 2.5391\n\nTesting for epoch 58 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5966\nEpoch 59 of 60\n\nTesting for epoch 59 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5325\n\nTesting for epoch 59 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5563\n\nTesting for epoch 59 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 2.4993\n\nTesting for epoch 59 index 4:\n16/16 [==============================] - 0s 625us/step - loss: 2.5589\n\nTesting for epoch 59 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 2.5403\nEpoch 60 of 60\n\nTesting for epoch 60 index 1:\n16/16 [==============================] - 0s 833us/step - loss: 2.5143\n\nTesting for epoch 60 index 2:\n16/16 [==============================] - 0s 808us/step - loss: 2.5618\n\nTesting for epoch 60 index 3:\n16/16 [==============================] - 0s 796us/step - loss: 2.5960\n\nTesting for epoch 60 index 4:\n16/16 [==============================] - 0s 599us/step - loss: 2.5405\n\nTesting for epoch 60 index 5:\n16/16 [==============================] - 0s 650us/step - loss: 2.5440\n\n\n\noutlier_SO_GAAL_one = list(clf.labels_)\n\n\noutlier_SO_GAAL_one = list(map(lambda x: 1 if x==0  else -1,outlier_SO_GAAL_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_SO_GAAL_one,tab_bunny)\n\n\n_conf.conf(\"SO-GAAL (Liu et al., 2019)\")\n\n\n\n\nAccuracy: 0.952\nPrecision: 0.952\nRecall: 1.000\nF1 Score: 0.975\n\n\n\ntwelve = eleven.append(_conf.tab)\n\n\n\nMO_GAAL\n\nclf = MO_GAAL()\nclf.fit(_df[['x', 'y','fnoise']])\n_df['MO_GAAL_clf'] = clf.labels_\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(SGD, self).__init__(name, **kwargs)\n\n\nEpoch 1 of 60\n\nTesting for epoch 1 index 1:\n\nTesting for epoch 1 index 2:\n\nTesting for epoch 1 index 3:\n\nTesting for epoch 1 index 4:\n\nTesting for epoch 1 index 5:\nEpoch 2 of 60\n\nTesting for epoch 2 index 1:\n\nTesting for epoch 2 index 2:\n\nTesting for epoch 2 index 3:\n\nTesting for epoch 2 index 4:\n\nTesting for epoch 2 index 5:\nEpoch 3 of 60\n\nTesting for epoch 3 index 1:\n\nTesting for epoch 3 index 2:\n\nTesting for epoch 3 index 3:\n\nTesting for epoch 3 index 4:\n\nTesting for epoch 3 index 5:\nEpoch 4 of 60\n\nTesting for epoch 4 index 1:\n\nTesting for epoch 4 index 2:\n\nTesting for epoch 4 index 3:\n\nTesting for epoch 4 index 4:\n\nTesting for epoch 4 index 5:\nEpoch 5 of 60\n\nTesting for epoch 5 index 1:\n\nTesting for epoch 5 index 2:\n\nTesting for epoch 5 index 3:\n\nTesting for epoch 5 index 4:\n\nTesting for epoch 5 index 5:\nEpoch 6 of 60\n\nTesting for epoch 6 index 1:\n\nTesting for epoch 6 index 2:\n\nTesting for epoch 6 index 3:\n\nTesting for epoch 6 index 4:\n\nTesting for epoch 6 index 5:\nEpoch 7 of 60\n\nTesting for epoch 7 index 1:\n\nTesting for epoch 7 index 2:\n\nTesting for epoch 7 index 3:\n\nTesting for epoch 7 index 4:\n\nTesting for epoch 7 index 5:\nEpoch 8 of 60\n\nTesting for epoch 8 index 1:\n\nTesting for epoch 8 index 2:\n\nTesting for epoch 8 index 3:\n\nTesting for epoch 8 index 4:\n\nTesting for epoch 8 index 5:\nEpoch 9 of 60\n\nTesting for epoch 9 index 1:\n\nTesting for epoch 9 index 2:\n\nTesting for epoch 9 index 3:\n\nTesting for epoch 9 index 4:\n\nTesting for epoch 9 index 5:\nEpoch 10 of 60\n\nTesting for epoch 10 index 1:\n\nTesting for epoch 10 index 2:\n\nTesting for epoch 10 index 3:\n\nTesting for epoch 10 index 4:\n\nTesting for epoch 10 index 5:\nEpoch 11 of 60\n\nTesting for epoch 11 index 1:\n\nTesting for epoch 11 index 2:\n\nTesting for epoch 11 index 3:\n\nTesting for epoch 11 index 4:\n\nTesting for epoch 11 index 5:\nEpoch 12 of 60\n\nTesting for epoch 12 index 1:\n\nTesting for epoch 12 index 2:\n\nTesting for epoch 12 index 3:\n\nTesting for epoch 12 index 4:\n\nTesting for epoch 12 index 5:\nEpoch 13 of 60\n\nTesting for epoch 13 index 1:\n\nTesting for epoch 13 index 2:\n\nTesting for epoch 13 index 3:\n\nTesting for epoch 13 index 4:\n\nTesting for epoch 13 index 5:\nEpoch 14 of 60\n\nTesting for epoch 14 index 1:\n\nTesting for epoch 14 index 2:\n\nTesting for epoch 14 index 3:\n\nTesting for epoch 14 index 4:\n\nTesting for epoch 14 index 5:\nEpoch 15 of 60\n\nTesting for epoch 15 index 1:\n\nTesting for epoch 15 index 2:\n\nTesting for epoch 15 index 3:\n\nTesting for epoch 15 index 4:\n\nTesting for epoch 15 index 5:\nEpoch 16 of 60\n\nTesting for epoch 16 index 1:\n\nTesting for epoch 16 index 2:\n\nTesting for epoch 16 index 3:\n\nTesting for epoch 16 index 4:\n\nTesting for epoch 16 index 5:\nEpoch 17 of 60\n\nTesting for epoch 17 index 1:\n\nTesting for epoch 17 index 2:\n\nTesting for epoch 17 index 3:\n\nTesting for epoch 17 index 4:\n\nTesting for epoch 17 index 5:\nEpoch 18 of 60\n\nTesting for epoch 18 index 1:\n\nTesting for epoch 18 index 2:\n\nTesting for epoch 18 index 3:\n\nTesting for epoch 18 index 4:\n\nTesting for epoch 18 index 5:\nEpoch 19 of 60\n\nTesting for epoch 19 index 1:\n\nTesting for epoch 19 index 2:\n\nTesting for epoch 19 index 3:\n\nTesting for epoch 19 index 4:\n\nTesting for epoch 19 index 5:\nEpoch 20 of 60\n\nTesting for epoch 20 index 1:\n\nTesting for epoch 20 index 2:\n\nTesting for epoch 20 index 3:\n\nTesting for epoch 20 index 4:\n\nTesting for epoch 20 index 5:\nEpoch 21 of 60\n\nTesting for epoch 21 index 1:\n\nTesting for epoch 21 index 2:\n16/16 [==============================] - 0s 839us/step - loss: 0.2862\n16/16 [==============================] - 0s 1ms/step - loss: 1.3562\n16/16 [==============================] - 0s 879us/step - loss: 1.6391\n16/16 [==============================] - 0s 676us/step - loss: 1.7457\n16/16 [==============================] - 0s 668us/step - loss: 1.7800\n16/16 [==============================] - 0s 797us/step - loss: 1.7893\n16/16 [==============================] - 0s 1ms/step - loss: 1.7882\n16/16 [==============================] - 0s 750us/step - loss: 1.7810\n16/16 [==============================] - 0s 661us/step - loss: 1.7768\n16/16 [==============================] - 0s 1ms/step - loss: 1.7746\n\nTesting for epoch 21 index 3:\n16/16 [==============================] - 0s 709us/step - loss: 0.2829\n16/16 [==============================] - 0s 1ms/step - loss: 1.3627\n16/16 [==============================] - 0s 1ms/step - loss: 1.6520\n16/16 [==============================] - 0s 1ms/step - loss: 1.7617\n16/16 [==============================] - 0s 648us/step - loss: 1.7969\n16/16 [==============================] - 0s 655us/step - loss: 1.8064\n16/16 [==============================] - 0s 1ms/step - loss: 1.8050\n16/16 [==============================] - 0s 1ms/step - loss: 1.7975\n16/16 [==============================] - 0s 1ms/step - loss: 1.7932\n16/16 [==============================] - 0s 1ms/step - loss: 1.7909\n\nTesting for epoch 21 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2819\n16/16 [==============================] - 0s 662us/step - loss: 1.3750\n16/16 [==============================] - 0s 649us/step - loss: 1.6692\n16/16 [==============================] - 0s 664us/step - loss: 1.7821\n16/16 [==============================] - 0s 644us/step - loss: 1.8194\n16/16 [==============================] - 0s 671us/step - loss: 1.8316\n16/16 [==============================] - 0s 651us/step - loss: 1.8318\n16/16 [==============================] - 0s 661us/step - loss: 1.8249\n16/16 [==============================] - 0s 990us/step - loss: 1.8208\n16/16 [==============================] - 0s 1ms/step - loss: 1.8185\n\nTesting for epoch 21 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2784\n16/16 [==============================] - 0s 1ms/step - loss: 1.3590\n16/16 [==============================] - 0s 645us/step - loss: 1.6490\n16/16 [==============================] - 0s 1ms/step - loss: 1.7586\n16/16 [==============================] - 0s 655us/step - loss: 1.7914\n16/16 [==============================] - 0s 1ms/step - loss: 1.7998\n16/16 [==============================] - 0s 1ms/step - loss: 1.7975\n16/16 [==============================] - 0s 653us/step - loss: 1.7896\n16/16 [==============================] - 0s 675us/step - loss: 1.7852\n16/16 [==============================] - 0s 1ms/step - loss: 1.7829\nEpoch 22 of 60\n\nTesting for epoch 22 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2787\n16/16 [==============================] - 0s 1ms/step - loss: 1.3475\n16/16 [==============================] - 0s 646us/step - loss: 1.6341\n16/16 [==============================] - 0s 690us/step - loss: 1.7422\n16/16 [==============================] - 0s 1ms/step - loss: 1.7757\n16/16 [==============================] - 0s 1ms/step - loss: 1.7855\n16/16 [==============================] - 0s 1ms/step - loss: 1.7843\n16/16 [==============================] - 0s 1ms/step - loss: 1.7771\n16/16 [==============================] - 0s 1ms/step - loss: 1.7729\n16/16 [==============================] - 0s 1ms/step - loss: 1.7708\n\nTesting for epoch 22 index 2:\n16/16 [==============================] - 0s 680us/step - loss: 0.2756\n16/16 [==============================] - 0s 1ms/step - loss: 1.3616\n16/16 [==============================] - 0s 678us/step - loss: 1.6485\n16/16 [==============================] - 0s 946us/step - loss: 1.7539\n16/16 [==============================] - 0s 673us/step - loss: 1.7847\n16/16 [==============================] - 0s 656us/step - loss: 1.7921\n16/16 [==============================] - 0s 1ms/step - loss: 1.7895\n16/16 [==============================] - 0s 660us/step - loss: 1.7812\n16/16 [==============================] - 0s 1ms/step - loss: 1.7768\n16/16 [==============================] - 0s 730us/step - loss: 1.7745\n\nTesting for epoch 22 index 3:\n16/16 [==============================] - 0s 660us/step - loss: 0.2723\n16/16 [==============================] - 0s 1ms/step - loss: 1.3959\n16/16 [==============================] - 0s 642us/step - loss: 1.7002\n16/16 [==============================] - 0s 874us/step - loss: 1.8105\n16/16 [==============================] - 0s 1ms/step - loss: 1.8423\n16/16 [==============================] - 0s 1ms/step - loss: 1.8494\n16/16 [==============================] - 0s 657us/step - loss: 1.8460\n16/16 [==============================] - 0s 940us/step - loss: 1.8371\n16/16 [==============================] - 0s 634us/step - loss: 1.8324\n16/16 [==============================] - 0s 905us/step - loss: 1.8299\n\nTesting for epoch 22 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2702\n16/16 [==============================] - 0s 649us/step - loss: 1.3792\n16/16 [==============================] - 0s 867us/step - loss: 1.6818\n16/16 [==============================] - 0s 619us/step - loss: 1.7923\n16/16 [==============================] - 0s 859us/step - loss: 1.8248\n16/16 [==============================] - 0s 609us/step - loss: 1.8327\n16/16 [==============================] - 0s 1ms/step - loss: 1.8298\n16/16 [==============================] - 0s 584us/step - loss: 1.8209\n16/16 [==============================] - 0s 590us/step - loss: 1.8163\n16/16 [==============================] - 0s 602us/step - loss: 1.8139\n\nTesting for epoch 22 index 5:\n16/16 [==============================] - 0s 683us/step - loss: 0.2694\n16/16 [==============================] - 0s 794us/step - loss: 1.3853\n16/16 [==============================] - 0s 1ms/step - loss: 1.6907\n16/16 [==============================] - 0s 1ms/step - loss: 1.8014\n16/16 [==============================] - 0s 634us/step - loss: 1.8329\n16/16 [==============================] - 0s 1ms/step - loss: 1.8400\n16/16 [==============================] - 0s 694us/step - loss: 1.8367\n16/16 [==============================] - 0s 1ms/step - loss: 1.8275\n16/16 [==============================] - 0s 589us/step - loss: 1.8228\n16/16 [==============================] - 0s 1ms/step - loss: 1.8204\nEpoch 23 of 60\n\nTesting for epoch 23 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2679\n16/16 [==============================] - 0s 1ms/step - loss: 1.4280\n16/16 [==============================] - 0s 1ms/step - loss: 1.7530\n16/16 [==============================] - 0s 1ms/step - loss: 1.8709\n16/16 [==============================] - 0s 1ms/step - loss: 1.9036\n16/16 [==============================] - 0s 643us/step - loss: 1.9107\n16/16 [==============================] - 0s 1ms/step - loss: 1.9074\n16/16 [==============================] - 0s 1ms/step - loss: 1.8981\n16/16 [==============================] - 0s 1ms/step - loss: 1.8933\n16/16 [==============================] - 0s 1ms/step - loss: 1.8908\n\nTesting for epoch 23 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2673\n16/16 [==============================] - 0s 974us/step - loss: 1.4127\n16/16 [==============================] - 0s 820us/step - loss: 1.7343\n16/16 [==============================] - 0s 630us/step - loss: 1.8524\n16/16 [==============================] - 0s 606us/step - loss: 1.8844\n16/16 [==============================] - 0s 719us/step - loss: 1.8917\n16/16 [==============================] - 0s 1ms/step - loss: 1.8882\n16/16 [==============================] - 0s 611us/step - loss: 1.8784\n16/16 [==============================] - 0s 666us/step - loss: 1.8735\n16/16 [==============================] - 0s 1ms/step - loss: 1.8709\n\nTesting for epoch 23 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2645\n16/16 [==============================] - 0s 1ms/step - loss: 1.4021\n16/16 [==============================] - 0s 594us/step - loss: 1.7169\n16/16 [==============================] - 0s 587us/step - loss: 1.8300\n16/16 [==============================] - 0s 613us/step - loss: 1.8582\n16/16 [==============================] - 0s 1ms/step - loss: 1.8634\n16/16 [==============================] - 0s 1ms/step - loss: 1.8590\n16/16 [==============================] - 0s 644us/step - loss: 1.8494\n16/16 [==============================] - 0s 622us/step - loss: 1.8445\n16/16 [==============================] - 0s 617us/step - loss: 1.8420\n\nTesting for epoch 23 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2641\n16/16 [==============================] - 0s 1ms/step - loss: 1.4346\n16/16 [==============================] - 0s 1ms/step - loss: 1.7641\n16/16 [==============================] - 0s 860us/step - loss: 1.8848\n16/16 [==============================] - 0s 1ms/step - loss: 1.9154\n16/16 [==============================] - 0s 962us/step - loss: 1.9222\n16/16 [==============================] - 0s 634us/step - loss: 1.9176\n16/16 [==============================] - 0s 1ms/step - loss: 1.9075\n16/16 [==============================] - 0s 640us/step - loss: 1.9024\n16/16 [==============================] - 0s 1ms/step - loss: 1.8998\n\nTesting for epoch 23 index 5:\n16/16 [==============================] - 0s 935us/step - loss: 0.2571\n16/16 [==============================] - 0s 600us/step - loss: 1.4423\n16/16 [==============================] - 0s 1ms/step - loss: 1.7744\n16/16 [==============================] - 0s 1ms/step - loss: 1.8942\n16/16 [==============================] - 0s 1ms/step - loss: 1.9222\n16/16 [==============================] - 0s 1ms/step - loss: 1.9268\n16/16 [==============================] - 0s 702us/step - loss: 1.9205\n16/16 [==============================] - 0s 637us/step - loss: 1.9092\n16/16 [==============================] - 0s 1ms/step - loss: 1.9038\n16/16 [==============================] - 0s 638us/step - loss: 1.9011\nEpoch 24 of 60\n\nTesting for epoch 24 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2576\n16/16 [==============================] - 0s 1ms/step - loss: 1.4150\n16/16 [==============================] - 0s 586us/step - loss: 1.7381\n16/16 [==============================] - 0s 825us/step - loss: 1.8529\n16/16 [==============================] - 0s 848us/step - loss: 1.8794\n16/16 [==============================] - 0s 716us/step - loss: 1.8834\n16/16 [==============================] - 0s 1ms/step - loss: 1.8775\n16/16 [==============================] - 0s 598us/step - loss: 1.8670\n16/16 [==============================] - 0s 614us/step - loss: 1.8618\n16/16 [==============================] - 0s 1ms/step - loss: 1.8593\n\nTesting for epoch 24 index 2:\n16/16 [==============================] - 0s 604us/step - loss: 0.2602\n16/16 [==============================] - 0s 1ms/step - loss: 1.4321\n16/16 [==============================] - 0s 1ms/step - loss: 1.7581\n16/16 [==============================] - 0s 1ms/step - loss: 1.8731\n16/16 [==============================] - 0s 636us/step - loss: 1.8998\n16/16 [==============================] - 0s 1ms/step - loss: 1.9035\n16/16 [==============================] - 0s 607us/step - loss: 1.8975\n16/16 [==============================] - 0s 1ms/step - loss: 1.8867\n16/16 [==============================] - 0s 646us/step - loss: 1.8815\n16/16 [==============================] - 0s 1ms/step - loss: 1.8790\n\nTesting for epoch 24 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2559\n16/16 [==============================] - 0s 1ms/step - loss: 1.4384\n16/16 [==============================] - 0s 610us/step - loss: 1.7673\n16/16 [==============================] - 0s 1ms/step - loss: 1.8808\n16/16 [==============================] - 0s 663us/step - loss: 1.9062\n16/16 [==============================] - 0s 844us/step - loss: 1.9082\n16/16 [==============================] - 0s 780us/step - loss: 1.9007\n16/16 [==============================] - 0s 602us/step - loss: 1.8887\n16/16 [==============================] - 0s 735us/step - loss: 1.8831\n16/16 [==============================] - 0s 1ms/step - loss: 1.8805\n\nTesting for epoch 24 index 4:\n16/16 [==============================] - 0s 828us/step - loss: 0.2595\n16/16 [==============================] - 0s 1ms/step - loss: 1.4660\n16/16 [==============================] - 0s 1ms/step - loss: 1.8046\n16/16 [==============================] - 0s 1ms/step - loss: 1.9238\n16/16 [==============================] - 0s 613us/step - loss: 1.9510\n16/16 [==============================] - 0s 613us/step - loss: 1.9550\n16/16 [==============================] - 0s 607us/step - loss: 1.9486\n16/16 [==============================] - 0s 600us/step - loss: 1.9375\n16/16 [==============================] - 0s 1ms/step - loss: 1.9321\n16/16 [==============================] - 0s 1ms/step - loss: 1.9295\n\nTesting for epoch 24 index 5:\n16/16 [==============================] - 0s 602us/step - loss: 0.2490\n16/16 [==============================] - 0s 783us/step - loss: 1.4405\n16/16 [==============================] - 0s 962us/step - loss: 1.7687\n16/16 [==============================] - 0s 1ms/step - loss: 1.8795\n16/16 [==============================] - 0s 1ms/step - loss: 1.9005\n16/16 [==============================] - 0s 1ms/step - loss: 1.8995\n16/16 [==============================] - 0s 1ms/step - loss: 1.8901\n16/16 [==============================] - 0s 1ms/step - loss: 1.8774\n16/16 [==============================] - 0s 1ms/step - loss: 1.8717\n16/16 [==============================] - 0s 1ms/step - loss: 1.8690\nEpoch 25 of 60\n\nTesting for epoch 25 index 1:\n16/16 [==============================] - 0s 951us/step - loss: 0.2496\n16/16 [==============================] - 0s 610us/step - loss: 1.4539\n16/16 [==============================] - 0s 1ms/step - loss: 1.7891\n16/16 [==============================] - 0s 1ms/step - loss: 1.9046\n16/16 [==============================] - 0s 628us/step - loss: 1.9285\n16/16 [==============================] - 0s 645us/step - loss: 1.9295\n16/16 [==============================] - 0s 1ms/step - loss: 1.9219\n16/16 [==============================] - 0s 602us/step - loss: 1.9101\n16/16 [==============================] - 0s 1ms/step - loss: 1.9046\n16/16 [==============================] - 0s 888us/step - loss: 1.9020\n\nTesting for epoch 25 index 2:\n16/16 [==============================] - 0s 647us/step - loss: 0.2496\n16/16 [==============================] - 0s 600us/step - loss: 1.4771\n16/16 [==============================] - 0s 677us/step - loss: 1.8228\n16/16 [==============================] - 0s 1ms/step - loss: 1.9419\n16/16 [==============================] - 0s 663us/step - loss: 1.9656\n16/16 [==============================] - 0s 1ms/step - loss: 1.9663\n16/16 [==============================] - 0s 1ms/step - loss: 1.9579\n16/16 [==============================] - 0s 629us/step - loss: 1.9450\n16/16 [==============================] - 0s 638us/step - loss: 1.9393\n16/16 [==============================] - 0s 1ms/step - loss: 1.9365\n\nTesting for epoch 25 index 3:\n16/16 [==============================] - 0s 652us/step - loss: 0.2531\n16/16 [==============================] - 0s 591us/step - loss: 1.4810\n16/16 [==============================] - 0s 613us/step - loss: 1.8268\n16/16 [==============================] - 0s 1ms/step - loss: 1.9468\n16/16 [==============================] - 0s 992us/step - loss: 1.9711\n16/16 [==============================] - 0s 1ms/step - loss: 1.9715\n16/16 [==============================] - 0s 595us/step - loss: 1.9633\n16/16 [==============================] - 0s 1ms/step - loss: 1.9507\n16/16 [==============================] - 0s 638us/step - loss: 1.9451\n16/16 [==============================] - 0s 1ms/step - loss: 1.9424\n\nTesting for epoch 25 index 4:\n16/16 [==============================] - 0s 642us/step - loss: 0.2477\n16/16 [==============================] - 0s 608us/step - loss: 1.4965\n16/16 [==============================] - 0s 1ms/step - loss: 1.8427\n16/16 [==============================] - 0s 1ms/step - loss: 1.9619\n16/16 [==============================] - 0s 707us/step - loss: 1.9847\n16/16 [==============================] - 0s 1ms/step - loss: 1.9844\n16/16 [==============================] - 0s 1ms/step - loss: 1.9761\n16/16 [==============================] - 0s 627us/step - loss: 1.9633\n16/16 [==============================] - 0s 604us/step - loss: 1.9575\n16/16 [==============================] - 0s 1ms/step - loss: 1.9548\n\nTesting for epoch 25 index 5:\n16/16 [==============================] - 0s 666us/step - loss: 0.2442\n16/16 [==============================] - 0s 1ms/step - loss: 1.5123\n16/16 [==============================] - 0s 663us/step - loss: 1.8671\n16/16 [==============================] - 0s 868us/step - loss: 1.9886\n16/16 [==============================] - 0s 1ms/step - loss: 2.0106\n16/16 [==============================] - 0s 1ms/step - loss: 2.0090\n16/16 [==============================] - 0s 1ms/step - loss: 1.9998\n16/16 [==============================] - 0s 1ms/step - loss: 1.9861\n16/16 [==============================] - 0s 1ms/step - loss: 1.9801\n16/16 [==============================] - 0s 897us/step - loss: 1.9772\nEpoch 26 of 60\n\nTesting for epoch 26 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2455\n16/16 [==============================] - 0s 641us/step - loss: 1.4842\n16/16 [==============================] - 0s 1ms/step - loss: 1.8252\n16/16 [==============================] - 0s 604us/step - loss: 1.9429\n16/16 [==============================] - 0s 853us/step - loss: 1.9640\n16/16 [==============================] - 0s 1ms/step - loss: 1.9625\n16/16 [==============================] - 0s 616us/step - loss: 1.9538\n16/16 [==============================] - 0s 861us/step - loss: 1.9412\n16/16 [==============================] - 0s 1ms/step - loss: 1.9356\n16/16 [==============================] - 0s 1ms/step - loss: 1.9330\n\nTesting for epoch 26 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2433\n16/16 [==============================] - 0s 1ms/step - loss: 1.4858\n16/16 [==============================] - 0s 837us/step - loss: 1.8253\n16/16 [==============================] - 0s 1ms/step - loss: 1.9440\n16/16 [==============================] - 0s 1ms/step - loss: 1.9633\n16/16 [==============================] - 0s 1ms/step - loss: 1.9608\n16/16 [==============================] - 0s 1ms/step - loss: 1.9512\n16/16 [==============================] - 0s 595us/step - loss: 1.9375\n16/16 [==============================] - 0s 623us/step - loss: 1.9316\n16/16 [==============================] - 0s 632us/step - loss: 1.9289\n\nTesting for epoch 26 index 3:\n16/16 [==============================] - 0s 947us/step - loss: 0.2431\n16/16 [==============================] - 0s 727us/step - loss: 1.5060\n16/16 [==============================] - 0s 614us/step - loss: 1.8472\n16/16 [==============================] - 0s 619us/step - loss: 1.9665\n16/16 [==============================] - 0s 1ms/step - loss: 1.9845\n16/16 [==============================] - 0s 1ms/step - loss: 1.9803\n16/16 [==============================] - 0s 1ms/step - loss: 1.9697\n16/16 [==============================] - 0s 640us/step - loss: 1.9554\n16/16 [==============================] - 0s 1ms/step - loss: 1.9494\n16/16 [==============================] - 0s 1ms/step - loss: 1.9467\n\nTesting for epoch 26 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2399\n16/16 [==============================] - 0s 1ms/step - loss: 1.4785\n16/16 [==============================] - 0s 1ms/step - loss: 1.8107\n16/16 [==============================] - 0s 571us/step - loss: 1.9298\n16/16 [==============================] - 0s 1ms/step - loss: 1.9481\n16/16 [==============================] - 0s 1ms/step - loss: 1.9445\n16/16 [==============================] - 0s 625us/step - loss: 1.9342\n16/16 [==============================] - 0s 631us/step - loss: 1.9201\n16/16 [==============================] - 0s 646us/step - loss: 1.9141\n16/16 [==============================] - 0s 626us/step - loss: 1.9114\n\nTesting for epoch 26 index 5:\n16/16 [==============================] - 0s 880us/step - loss: 0.2394\n16/16 [==============================] - 0s 1ms/step - loss: 1.5075\n16/16 [==============================] - 0s 632us/step - loss: 1.8517\n16/16 [==============================] - 0s 687us/step - loss: 1.9741\n16/16 [==============================] - 0s 644us/step - loss: 1.9911\n16/16 [==============================] - 0s 671us/step - loss: 1.9874\n16/16 [==============================] - 0s 621us/step - loss: 1.9765\n16/16 [==============================] - 0s 604us/step - loss: 1.9621\n16/16 [==============================] - 0s 610us/step - loss: 1.9561\n16/16 [==============================] - 0s 1ms/step - loss: 1.9533\nEpoch 27 of 60\n\nTesting for epoch 27 index 1:\n16/16 [==============================] - 0s 626us/step - loss: 0.2399\n16/16 [==============================] - 0s 1ms/step - loss: 1.5196\n16/16 [==============================] - 0s 1ms/step - loss: 1.8658\n16/16 [==============================] - 0s 1ms/step - loss: 1.9878\n16/16 [==============================] - 0s 937us/step - loss: 2.0043\n16/16 [==============================] - 0s 1ms/step - loss: 2.0001\n16/16 [==============================] - 0s 755us/step - loss: 1.9881\n16/16 [==============================] - 0s 607us/step - loss: 1.9734\n16/16 [==============================] - 0s 1ms/step - loss: 1.9673\n16/16 [==============================] - 0s 675us/step - loss: 1.9644\n\nTesting for epoch 27 index 2:\n16/16 [==============================] - 0s 584us/step - loss: 0.2383\n16/16 [==============================] - 0s 1ms/step - loss: 1.5491\n16/16 [==============================] - 0s 1ms/step - loss: 1.9024\n16/16 [==============================] - 0s 587us/step - loss: 2.0252\n16/16 [==============================] - 0s 793us/step - loss: 2.0399\n16/16 [==============================] - 0s 1ms/step - loss: 2.0364\n16/16 [==============================] - 0s 589us/step - loss: 2.0246\n16/16 [==============================] - 0s 625us/step - loss: 2.0102\n16/16 [==============================] - 0s 604us/step - loss: 2.0042\n16/16 [==============================] - 0s 600us/step - loss: 2.0014\n\nTesting for epoch 27 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2373\n16/16 [==============================] - 0s 1ms/step - loss: 1.5548\n16/16 [==============================] - 0s 1ms/step - loss: 1.9083\n16/16 [==============================] - 0s 1ms/step - loss: 2.0331\n16/16 [==============================] - 0s 594us/step - loss: 2.0485\n16/16 [==============================] - 0s 609us/step - loss: 2.0460\n16/16 [==============================] - 0s 917us/step - loss: 2.0331\n16/16 [==============================] - 0s 1ms/step - loss: 2.0181\n16/16 [==============================] - 0s 1ms/step - loss: 2.0119\n16/16 [==============================] - 0s 1ms/step - loss: 2.0090\n\nTesting for epoch 27 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2391\n16/16 [==============================] - 0s 713us/step - loss: 1.5529\n16/16 [==============================] - 0s 1ms/step - loss: 1.9056\n16/16 [==============================] - 0s 666us/step - loss: 2.0294\n16/16 [==============================] - 0s 1ms/step - loss: 2.0429\n16/16 [==============================] - 0s 1ms/step - loss: 2.0388\n16/16 [==============================] - 0s 1ms/step - loss: 2.0248\n16/16 [==============================] - 0s 1ms/step - loss: 2.0092\n16/16 [==============================] - 0s 1ms/step - loss: 2.0028\n16/16 [==============================] - 0s 1ms/step - loss: 1.9998\n\nTesting for epoch 27 index 5:\n16/16 [==============================] - 0s 643us/step - loss: 0.2391\n16/16 [==============================] - 0s 1ms/step - loss: 1.5363\n16/16 [==============================] - 0s 896us/step - loss: 1.8850\n16/16 [==============================] - 0s 1ms/step - loss: 2.0052\n16/16 [==============================] - 0s 610us/step - loss: 2.0195\n16/16 [==============================] - 0s 1ms/step - loss: 2.0152\n16/16 [==============================] - 0s 1ms/step - loss: 2.0012\n16/16 [==============================] - 0s 1ms/step - loss: 1.9857\n16/16 [==============================] - 0s 619us/step - loss: 1.9794\n16/16 [==============================] - 0s 1ms/step - loss: 1.9765\nEpoch 28 of 60\n\nTesting for epoch 28 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2338\n16/16 [==============================] - 0s 1ms/step - loss: 1.5579\n16/16 [==============================] - 0s 624us/step - loss: 1.9117\n16/16 [==============================] - 0s 1ms/step - loss: 2.0320\n16/16 [==============================] - 0s 612us/step - loss: 2.0450\n16/16 [==============================] - 0s 645us/step - loss: 2.0389\n16/16 [==============================] - 0s 721us/step - loss: 2.0244\n16/16 [==============================] - 0s 696us/step - loss: 2.0088\n16/16 [==============================] - 0s 608us/step - loss: 2.0024\n16/16 [==============================] - 0s 826us/step - loss: 1.9995\n\nTesting for epoch 28 index 2:\n16/16 [==============================] - 0s 748us/step - loss: 0.2338\n16/16 [==============================] - 0s 1ms/step - loss: 1.5385\n16/16 [==============================] - 0s 1ms/step - loss: 1.8839\n16/16 [==============================] - 0s 670us/step - loss: 2.0016\n16/16 [==============================] - 0s 626us/step - loss: 2.0138\n16/16 [==============================] - 0s 625us/step - loss: 2.0076\n16/16 [==============================] - 0s 1ms/step - loss: 1.9939\n16/16 [==============================] - 0s 579us/step - loss: 1.9788\n16/16 [==============================] - 0s 898us/step - loss: 1.9727\n16/16 [==============================] - 0s 1ms/step - loss: 1.9699\n\nTesting for epoch 28 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2316\n16/16 [==============================] - 0s 590us/step - loss: 1.5536\n16/16 [==============================] - 0s 627us/step - loss: 1.9020\n16/16 [==============================] - 0s 1ms/step - loss: 2.0198\n16/16 [==============================] - 0s 1ms/step - loss: 2.0311\n16/16 [==============================] - 0s 1ms/step - loss: 2.0229\n16/16 [==============================] - 0s 646us/step - loss: 2.0074\n16/16 [==============================] - 0s 606us/step - loss: 1.9907\n16/16 [==============================] - 0s 591us/step - loss: 1.9841\n16/16 [==============================] - 0s 1ms/step - loss: 1.9812\n\nTesting for epoch 28 index 4:\n16/16 [==============================] - 0s 618us/step - loss: 0.2325\n16/16 [==============================] - 0s 1ms/step - loss: 1.5661\n16/16 [==============================] - 0s 654us/step - loss: 1.9179\n16/16 [==============================] - 0s 1ms/step - loss: 2.0361\n16/16 [==============================] - 0s 1ms/step - loss: 2.0486\n16/16 [==============================] - 0s 589us/step - loss: 2.0418\n16/16 [==============================] - 0s 676us/step - loss: 2.0272\n16/16 [==============================] - 0s 686us/step - loss: 2.0113\n16/16 [==============================] - 0s 664us/step - loss: 2.0049\n16/16 [==============================] - 0s 1ms/step - loss: 2.0020\n\nTesting for epoch 28 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2280\n16/16 [==============================] - 0s 1ms/step - loss: 1.5553\n16/16 [==============================] - 0s 646us/step - loss: 1.9013\n16/16 [==============================] - 0s 1ms/step - loss: 2.0131\n16/16 [==============================] - 0s 929us/step - loss: 2.0218\n16/16 [==============================] - 0s 838us/step - loss: 2.0122\n16/16 [==============================] - 0s 882us/step - loss: 1.9958\n16/16 [==============================] - 0s 602us/step - loss: 1.9791\n16/16 [==============================] - 0s 608us/step - loss: 1.9725\n16/16 [==============================] - 0s 657us/step - loss: 1.9696\nEpoch 29 of 60\n\nTesting for epoch 29 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2275\n16/16 [==============================] - 0s 767us/step - loss: 1.6042\n16/16 [==============================] - 0s 629us/step - loss: 1.9618\n16/16 [==============================] - 0s 1ms/step - loss: 2.0777\n16/16 [==============================] - 0s 1ms/step - loss: 2.0859\n16/16 [==============================] - 0s 645us/step - loss: 2.0750\n16/16 [==============================] - 0s 646us/step - loss: 2.0571\n16/16 [==============================] - 0s 900us/step - loss: 2.0390\n16/16 [==============================] - 0s 1ms/step - loss: 2.0319\n16/16 [==============================] - 0s 1ms/step - loss: 2.0288\n\nTesting for epoch 29 index 2:\n16/16 [==============================] - 0s 600us/step - loss: 0.2278\n16/16 [==============================] - 0s 625us/step - loss: 1.6003\n16/16 [==============================] - 0s 627us/step - loss: 1.9580\n16/16 [==============================] - 0s 599us/step - loss: 2.0748\n16/16 [==============================] - 0s 608us/step - loss: 2.0844\n16/16 [==============================] - 0s 1ms/step - loss: 2.0744\n16/16 [==============================] - 0s 911us/step - loss: 2.0568\n16/16 [==============================] - 0s 1ms/step - loss: 2.0391\n16/16 [==============================] - 0s 1ms/step - loss: 2.0323\n16/16 [==============================] - 0s 632us/step - loss: 2.0293\n\nTesting for epoch 29 index 3:\n16/16 [==============================] - 0s 606us/step - loss: 0.2275\n16/16 [==============================] - 0s 640us/step - loss: 1.5908\n16/16 [==============================] - 0s 787us/step - loss: 1.9416\n16/16 [==============================] - 0s 614us/step - loss: 2.0550\n16/16 [==============================] - 0s 655us/step - loss: 2.0633\n16/16 [==============================] - 0s 1ms/step - loss: 2.0544\n16/16 [==============================] - 0s 624us/step - loss: 2.0373\n16/16 [==============================] - 0s 590us/step - loss: 2.0202\n16/16 [==============================] - 0s 665us/step - loss: 2.0135\n16/16 [==============================] - 0s 1ms/step - loss: 2.0105\n\nTesting for epoch 29 index 4:\n16/16 [==============================] - 0s 637us/step - loss: 0.2262\n16/16 [==============================] - 0s 1ms/step - loss: 1.6456\n16/16 [==============================] - 0s 861us/step - loss: 2.0122\n16/16 [==============================] - 0s 1ms/step - loss: 2.1306\n16/16 [==============================] - 0s 1ms/step - loss: 2.1391\n16/16 [==============================] - 0s 1ms/step - loss: 2.1297\n16/16 [==============================] - 0s 621us/step - loss: 2.1118\n16/16 [==============================] - 0s 608us/step - loss: 2.0941\n16/16 [==============================] - 0s 1ms/step - loss: 2.0872\n16/16 [==============================] - 0s 652us/step - loss: 2.0842\n\nTesting for epoch 29 index 5:\n16/16 [==============================] - 0s 876us/step - loss: 0.2207\n16/16 [==============================] - 0s 656us/step - loss: 1.5952\n16/16 [==============================] - 0s 647us/step - loss: 1.9409\n16/16 [==============================] - 0s 611us/step - loss: 2.0496\n16/16 [==============================] - 0s 616us/step - loss: 2.0545\n16/16 [==============================] - 0s 1ms/step - loss: 2.0428\n16/16 [==============================] - 0s 599us/step - loss: 2.0236\n16/16 [==============================] - 0s 625us/step - loss: 2.0050\n16/16 [==============================] - 0s 897us/step - loss: 1.9980\n16/16 [==============================] - 0s 1ms/step - loss: 1.9949\nEpoch 30 of 60\n\nTesting for epoch 30 index 1:\n16/16 [==============================] - 0s 595us/step - loss: 0.2217\n16/16 [==============================] - 0s 1ms/step - loss: 1.6089\n16/16 [==============================] - 0s 1ms/step - loss: 1.9546\n16/16 [==============================] - 0s 1ms/step - loss: 2.0641\n16/16 [==============================] - 0s 638us/step - loss: 2.0694\n16/16 [==============================] - 0s 636us/step - loss: 2.0580\n16/16 [==============================] - 0s 1ms/step - loss: 2.0394\n16/16 [==============================] - 0s 1ms/step - loss: 2.0217\n16/16 [==============================] - 0s 673us/step - loss: 2.0150\n16/16 [==============================] - 0s 1ms/step - loss: 2.0120\n\nTesting for epoch 30 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2210\n16/16 [==============================] - 0s 828us/step - loss: 1.6202\n16/16 [==============================] - 0s 1ms/step - loss: 1.9660\n16/16 [==============================] - 0s 1ms/step - loss: 2.0750\n16/16 [==============================] - 0s 1ms/step - loss: 2.0805\n16/16 [==============================] - 0s 655us/step - loss: 2.0695\n16/16 [==============================] - 0s 1ms/step - loss: 2.0510\n16/16 [==============================] - 0s 1ms/step - loss: 2.0329\n16/16 [==============================] - 0s 681us/step - loss: 2.0260\n16/16 [==============================] - 0s 660us/step - loss: 2.0231\n\nTesting for epoch 30 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2210\n16/16 [==============================] - 0s 1ms/step - loss: 1.6647\n16/16 [==============================] - 0s 1ms/step - loss: 2.0279\n16/16 [==============================] - 0s 1ms/step - loss: 2.1422\n16/16 [==============================] - 0s 638us/step - loss: 2.1469\n16/16 [==============================] - 0s 1ms/step - loss: 2.1338\n16/16 [==============================] - 0s 787us/step - loss: 2.1132\n16/16 [==============================] - 0s 1ms/step - loss: 2.0938\n16/16 [==============================] - 0s 1ms/step - loss: 2.0865\n16/16 [==============================] - 0s 1ms/step - loss: 2.0833\n\nTesting for epoch 30 index 4:\n16/16 [==============================] - 0s 636us/step - loss: 0.2203\n16/16 [==============================] - 0s 640us/step - loss: 1.6660\n16/16 [==============================] - 0s 629us/step - loss: 2.0216\n16/16 [==============================] - 0s 1ms/step - loss: 2.1328\n16/16 [==============================] - 0s 607us/step - loss: 2.1365\n16/16 [==============================] - 0s 1ms/step - loss: 2.1239\n16/16 [==============================] - 0s 1ms/step - loss: 2.1042\n16/16 [==============================] - 0s 1ms/step - loss: 2.0851\n16/16 [==============================] - 0s 637us/step - loss: 2.0778\n16/16 [==============================] - 0s 1ms/step - loss: 2.0747\n\nTesting for epoch 30 index 5:\n16/16 [==============================] - 0s 975us/step - loss: 0.2161\n16/16 [==============================] - 0s 1ms/step - loss: 1.6263\n16/16 [==============================] - 0s 1ms/step - loss: 1.9708\n16/16 [==============================] - 0s 601us/step - loss: 2.0786\n16/16 [==============================] - 0s 806us/step - loss: 2.0799\n16/16 [==============================] - 0s 787us/step - loss: 2.0658\n16/16 [==============================] - 0s 870us/step - loss: 2.0449\n16/16 [==============================] - 0s 1ms/step - loss: 2.0250\n16/16 [==============================] - 0s 1ms/step - loss: 2.0177\n16/16 [==============================] - 0s 826us/step - loss: 2.0146\nEpoch 31 of 60\n\nTesting for epoch 31 index 1:\n16/16 [==============================] - 0s 611us/step - loss: 0.2170\n16/16 [==============================] - 0s 1ms/step - loss: 1.6675\n16/16 [==============================] - 0s 814us/step - loss: 2.0171\n16/16 [==============================] - 0s 610us/step - loss: 2.1244\n16/16 [==============================] - 0s 619us/step - loss: 2.1233\n16/16 [==============================] - 0s 1ms/step - loss: 2.1079\n16/16 [==============================] - 0s 856us/step - loss: 2.0871\n16/16 [==============================] - 0s 1ms/step - loss: 2.0677\n16/16 [==============================] - 0s 632us/step - loss: 2.0605\n16/16 [==============================] - 0s 602us/step - loss: 2.0574\n\nTesting for epoch 31 index 2:\n16/16 [==============================] - 0s 636us/step - loss: 0.2144\n16/16 [==============================] - 0s 709us/step - loss: 1.6430\n16/16 [==============================] - 0s 667us/step - loss: 1.9817\n16/16 [==============================] - 0s 682us/step - loss: 2.0881\n16/16 [==============================] - 0s 1ms/step - loss: 2.0878\n16/16 [==============================] - 0s 1ms/step - loss: 2.0724\n16/16 [==============================] - 0s 629us/step - loss: 2.0513\n16/16 [==============================] - 0s 1ms/step - loss: 2.0314\n16/16 [==============================] - 0s 884us/step - loss: 2.0241\n16/16 [==============================] - 0s 585us/step - loss: 2.0210\n\nTesting for epoch 31 index 3:\n16/16 [==============================] - 0s 726us/step - loss: 0.2157\n16/16 [==============================] - 0s 1ms/step - loss: 1.6566\n16/16 [==============================] - 0s 1ms/step - loss: 2.0048\n16/16 [==============================] - 0s 708us/step - loss: 2.1166\n16/16 [==============================] - 0s 1ms/step - loss: 2.1189\n16/16 [==============================] - 0s 1ms/step - loss: 2.1054\n16/16 [==============================] - 0s 684us/step - loss: 2.0858\n16/16 [==============================] - 0s 922us/step - loss: 2.0670\n16/16 [==============================] - 0s 1ms/step - loss: 2.0600\n16/16 [==============================] - 0s 602us/step - loss: 2.0569\n\nTesting for epoch 31 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2162\n16/16 [==============================] - 0s 1ms/step - loss: 1.6785\n16/16 [==============================] - 0s 1ms/step - loss: 2.0249\n16/16 [==============================] - 0s 1ms/step - loss: 2.1348\n16/16 [==============================] - 0s 630us/step - loss: 2.1339\n16/16 [==============================] - 0s 1ms/step - loss: 2.1192\n16/16 [==============================] - 0s 604us/step - loss: 2.0985\n16/16 [==============================] - 0s 660us/step - loss: 2.0787\n16/16 [==============================] - 0s 741us/step - loss: 2.0714\n16/16 [==============================] - 0s 1ms/step - loss: 2.0683\n\nTesting for epoch 31 index 5:\n16/16 [==============================] - 0s 636us/step - loss: 0.2121\n16/16 [==============================] - 0s 1ms/step - loss: 1.7297\n16/16 [==============================] - 0s 1ms/step - loss: 2.0914\n16/16 [==============================] - 0s 1ms/step - loss: 2.2077\n16/16 [==============================] - 0s 615us/step - loss: 2.2066\n16/16 [==============================] - 0s 615us/step - loss: 2.1895\n16/16 [==============================] - 0s 1ms/step - loss: 2.1665\n16/16 [==============================] - 0s 659us/step - loss: 2.1450\n16/16 [==============================] - 0s 1ms/step - loss: 2.1372\n16/16 [==============================] - 0s 911us/step - loss: 2.1339\nEpoch 32 of 60\n\nTesting for epoch 32 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2120\n16/16 [==============================] - 0s 787us/step - loss: 1.7164\n16/16 [==============================] - 0s 644us/step - loss: 2.0674\n16/16 [==============================] - 0s 629us/step - loss: 2.1800\n16/16 [==============================] - 0s 1ms/step - loss: 2.1770\n16/16 [==============================] - 0s 652us/step - loss: 2.1600\n16/16 [==============================] - 0s 620us/step - loss: 2.1373\n16/16 [==============================] - 0s 784us/step - loss: 2.1164\n16/16 [==============================] - 0s 644us/step - loss: 2.1088\n16/16 [==============================] - 0s 606us/step - loss: 2.1055\n\nTesting for epoch 32 index 2:\n16/16 [==============================] - 0s 685us/step - loss: 0.2108\n16/16 [==============================] - 0s 789us/step - loss: 1.7226\n16/16 [==============================] - 0s 653us/step - loss: 2.0726\n16/16 [==============================] - 0s 1ms/step - loss: 2.1850\n16/16 [==============================] - 0s 737us/step - loss: 2.1834\n16/16 [==============================] - 0s 659us/step - loss: 2.1657\n16/16 [==============================] - 0s 633us/step - loss: 2.1428\n16/16 [==============================] - 0s 851us/step - loss: 2.1217\n16/16 [==============================] - 0s 636us/step - loss: 2.1141\n16/16 [==============================] - 0s 599us/step - loss: 2.1108\n\nTesting for epoch 32 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2095\n16/16 [==============================] - 0s 894us/step - loss: 1.7267\n16/16 [==============================] - 0s 693us/step - loss: 2.0755\n16/16 [==============================] - 0s 1ms/step - loss: 2.1840\n16/16 [==============================] - 0s 1ms/step - loss: 2.1795\n16/16 [==============================] - 0s 932us/step - loss: 2.1586\n16/16 [==============================] - 0s 617us/step - loss: 2.1342\n16/16 [==============================] - 0s 1ms/step - loss: 2.1125\n16/16 [==============================] - 0s 672us/step - loss: 2.1046\n16/16 [==============================] - 0s 1ms/step - loss: 2.1013\n\nTesting for epoch 32 index 4:\n16/16 [==============================] - 0s 629us/step - loss: 0.2097\n16/16 [==============================] - 0s 656us/step - loss: 1.7201\n16/16 [==============================] - 0s 725us/step - loss: 2.0713\n16/16 [==============================] - 0s 610us/step - loss: 2.1837\n16/16 [==============================] - 0s 857us/step - loss: 2.1826\n16/16 [==============================] - 0s 1ms/step - loss: 2.1660\n16/16 [==============================] - 0s 619us/step - loss: 2.1441\n16/16 [==============================] - 0s 1ms/step - loss: 2.1235\n16/16 [==============================] - 0s 759us/step - loss: 2.1159\n16/16 [==============================] - 0s 1ms/step - loss: 2.1127\n\nTesting for epoch 32 index 5:\n16/16 [==============================] - 0s 617us/step - loss: 0.2078\n16/16 [==============================] - 0s 1ms/step - loss: 1.7352\n16/16 [==============================] - 0s 633us/step - loss: 2.0834\n16/16 [==============================] - 0s 617us/step - loss: 2.1909\n16/16 [==============================] - 0s 661us/step - loss: 2.1858\n16/16 [==============================] - 0s 635us/step - loss: 2.1659\n16/16 [==============================] - 0s 675us/step - loss: 2.1419\n16/16 [==============================] - 0s 1ms/step - loss: 2.1201\n16/16 [==============================] - 0s 638us/step - loss: 2.1122\n16/16 [==============================] - 0s 924us/step - loss: 2.1089\nEpoch 33 of 60\n\nTesting for epoch 33 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2075\n16/16 [==============================] - 0s 1ms/step - loss: 1.7201\n16/16 [==============================] - 0s 808us/step - loss: 2.0682\n16/16 [==============================] - 0s 1ms/step - loss: 2.1789\n16/16 [==============================] - 0s 1ms/step - loss: 2.1772\n16/16 [==============================] - 0s 832us/step - loss: 2.1596\n16/16 [==============================] - 0s 1ms/step - loss: 2.1366\n16/16 [==============================] - 0s 631us/step - loss: 2.1155\n16/16 [==============================] - 0s 1ms/step - loss: 2.1079\n16/16 [==============================] - 0s 717us/step - loss: 2.1048\n\nTesting for epoch 33 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2033\n16/16 [==============================] - 0s 880us/step - loss: 1.7117\n16/16 [==============================] - 0s 689us/step - loss: 2.0529\n16/16 [==============================] - 0s 646us/step - loss: 2.1588\n16/16 [==============================] - 0s 629us/step - loss: 2.1535\n16/16 [==============================] - 0s 977us/step - loss: 2.1332\n16/16 [==============================] - 0s 564us/step - loss: 2.1092\n16/16 [==============================] - 0s 647us/step - loss: 2.0876\n16/16 [==============================] - 0s 1ms/step - loss: 2.0798\n16/16 [==============================] - 0s 591us/step - loss: 2.0766\n\nTesting for epoch 33 index 3:\n16/16 [==============================] - 0s 990us/step - loss: 0.2022\n16/16 [==============================] - 0s 1ms/step - loss: 1.7204\n16/16 [==============================] - 0s 612us/step - loss: 2.0604\n16/16 [==============================] - 0s 631us/step - loss: 2.1673\n16/16 [==============================] - 0s 1ms/step - loss: 2.1620\n16/16 [==============================] - 0s 629us/step - loss: 2.1416\n16/16 [==============================] - 0s 929us/step - loss: 2.1173\n16/16 [==============================] - 0s 1ms/step - loss: 2.0956\n16/16 [==============================] - 0s 1ms/step - loss: 2.0879\n16/16 [==============================] - 0s 617us/step - loss: 2.0846\n\nTesting for epoch 33 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.2030\n16/16 [==============================] - 0s 1ms/step - loss: 1.7209\n16/16 [==============================] - 0s 605us/step - loss: 2.0625\n16/16 [==============================] - 0s 1ms/step - loss: 2.1710\n16/16 [==============================] - 0s 700us/step - loss: 2.1667\n16/16 [==============================] - 0s 1ms/step - loss: 2.1474\n16/16 [==============================] - 0s 902us/step - loss: 2.1242\n16/16 [==============================] - 0s 667us/step - loss: 2.1033\n16/16 [==============================] - 0s 1ms/step - loss: 2.0957\n16/16 [==============================] - 0s 1ms/step - loss: 2.0925\n\nTesting for epoch 33 index 5:\n16/16 [==============================] - 0s 621us/step - loss: 0.2029\n16/16 [==============================] - 0s 610us/step - loss: 1.7547\n16/16 [==============================] - 0s 611us/step - loss: 2.1118\n16/16 [==============================] - 0s 597us/step - loss: 2.2254\n16/16 [==============================] - 0s 576us/step - loss: 2.2233\n16/16 [==============================] - 0s 580us/step - loss: 2.2050\n16/16 [==============================] - 0s 612us/step - loss: 2.1821\n16/16 [==============================] - 0s 605us/step - loss: 2.1610\n16/16 [==============================] - 0s 807us/step - loss: 2.1534\n16/16 [==============================] - 0s 1ms/step - loss: 2.1502\nEpoch 34 of 60\n\nTesting for epoch 34 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1969\n16/16 [==============================] - 0s 955us/step - loss: 1.7401\n16/16 [==============================] - 0s 642us/step - loss: 2.0891\n16/16 [==============================] - 0s 691us/step - loss: 2.1926\n16/16 [==============================] - 0s 1ms/step - loss: 2.1827\n16/16 [==============================] - 0s 706us/step - loss: 2.1592\n16/16 [==============================] - 0s 1ms/step - loss: 2.1331\n16/16 [==============================] - 0s 835us/step - loss: 2.1100\n16/16 [==============================] - 0s 1ms/step - loss: 2.1018\n16/16 [==============================] - 0s 1ms/step - loss: 2.0983\n\nTesting for epoch 34 index 2:\n16/16 [==============================] - 0s 659us/step - loss: 0.1983\n16/16 [==============================] - 0s 664us/step - loss: 1.7511\n16/16 [==============================] - 0s 1ms/step - loss: 2.1021\n16/16 [==============================] - 0s 632us/step - loss: 2.2065\n16/16 [==============================] - 0s 1ms/step - loss: 2.1983\n16/16 [==============================] - 0s 1ms/step - loss: 2.1764\n16/16 [==============================] - 0s 1ms/step - loss: 2.1510\n16/16 [==============================] - 0s 639us/step - loss: 2.1285\n16/16 [==============================] - 0s 641us/step - loss: 2.1205\n16/16 [==============================] - 0s 1ms/step - loss: 2.1171\n\nTesting for epoch 34 index 3:\n16/16 [==============================] - 0s 774us/step - loss: 0.1992\n16/16 [==============================] - 0s 643us/step - loss: 1.7271\n16/16 [==============================] - 0s 1ms/step - loss: 2.0661\n16/16 [==============================] - 0s 1ms/step - loss: 2.1656\n16/16 [==============================] - 0s 1ms/step - loss: 2.1575\n16/16 [==============================] - 0s 1ms/step - loss: 2.1343\n16/16 [==============================] - 0s 620us/step - loss: 2.1083\n16/16 [==============================] - 0s 675us/step - loss: 2.0856\n16/16 [==============================] - 0s 1ms/step - loss: 2.0776\n16/16 [==============================] - 0s 641us/step - loss: 2.0743\n\nTesting for epoch 34 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1973\n16/16 [==============================] - 0s 634us/step - loss: 1.7673\n16/16 [==============================] - 0s 641us/step - loss: 2.1133\n16/16 [==============================] - 0s 807us/step - loss: 2.2143\n16/16 [==============================] - 0s 1ms/step - loss: 2.2060\n16/16 [==============================] - 0s 680us/step - loss: 2.1823\n16/16 [==============================] - 0s 915us/step - loss: 2.1568\n16/16 [==============================] - 0s 1ms/step - loss: 2.1343\n16/16 [==============================] - 0s 637us/step - loss: 2.1262\n16/16 [==============================] - 0s 1ms/step - loss: 2.1229\n\nTesting for epoch 34 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1957\n16/16 [==============================] - 0s 1ms/step - loss: 1.7630\n16/16 [==============================] - 0s 1ms/step - loss: 2.1149\n16/16 [==============================] - 0s 1ms/step - loss: 2.2172\n16/16 [==============================] - 0s 1ms/step - loss: 2.2101\n16/16 [==============================] - 0s 785us/step - loss: 2.1867\n16/16 [==============================] - 0s 767us/step - loss: 2.1611\n16/16 [==============================] - 0s 1ms/step - loss: 2.1384\n16/16 [==============================] - 0s 1ms/step - loss: 2.1303\n16/16 [==============================] - 0s 835us/step - loss: 2.1270\nEpoch 35 of 60\n\nTesting for epoch 35 index 1:\n16/16 [==============================] - 0s 659us/step - loss: 0.1958\n16/16 [==============================] - 0s 636us/step - loss: 1.7773\n16/16 [==============================] - 0s 580us/step - loss: 2.1316\n16/16 [==============================] - 0s 632us/step - loss: 2.2359\n16/16 [==============================] - 0s 699us/step - loss: 2.2285\n16/16 [==============================] - 0s 642us/step - loss: 2.2029\n16/16 [==============================] - 0s 953us/step - loss: 2.1759\n16/16 [==============================] - 0s 1ms/step - loss: 2.1522\n16/16 [==============================] - 0s 1ms/step - loss: 2.1437\n16/16 [==============================] - 0s 843us/step - loss: 2.1402\n\nTesting for epoch 35 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1914\n16/16 [==============================] - 0s 908us/step - loss: 1.7573\n16/16 [==============================] - 0s 718us/step - loss: 2.1000\n16/16 [==============================] - 0s 1ms/step - loss: 2.1991\n16/16 [==============================] - 0s 626us/step - loss: 2.1890\n16/16 [==============================] - 0s 746us/step - loss: 2.1621\n16/16 [==============================] - 0s 649us/step - loss: 2.1343\n16/16 [==============================] - 0s 653us/step - loss: 2.1108\n16/16 [==============================] - 0s 1ms/step - loss: 2.1026\n16/16 [==============================] - 0s 1ms/step - loss: 2.0992\n\nTesting for epoch 35 index 3:\n16/16 [==============================] - 0s 615us/step - loss: 0.1948\n16/16 [==============================] - 0s 637us/step - loss: 1.8010\n16/16 [==============================] - 0s 1ms/step - loss: 2.1603\n16/16 [==============================] - 0s 628us/step - loss: 2.2660\n16/16 [==============================] - 0s 1ms/step - loss: 2.2587\n16/16 [==============================] - 0s 706us/step - loss: 2.2337\n16/16 [==============================] - 0s 637us/step - loss: 2.2068\n16/16 [==============================] - 0s 631us/step - loss: 2.1834\n16/16 [==============================] - 0s 1ms/step - loss: 2.1750\n16/16 [==============================] - 0s 1ms/step - loss: 2.1716\n\nTesting for epoch 35 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1911\n16/16 [==============================] - 0s 641us/step - loss: 1.7696\n16/16 [==============================] - 0s 841us/step - loss: 2.1178\n16/16 [==============================] - 0s 778us/step - loss: 2.2186\n16/16 [==============================] - 0s 1ms/step - loss: 2.2089\n16/16 [==============================] - 0s 1ms/step - loss: 2.1826\n16/16 [==============================] - 0s 1ms/step - loss: 2.1555\n16/16 [==============================] - 0s 1ms/step - loss: 2.1326\n16/16 [==============================] - 0s 1ms/step - loss: 2.1245\n16/16 [==============================] - 0s 1ms/step - loss: 2.1212\n\nTesting for epoch 35 index 5:\n16/16 [==============================] - 0s 728us/step - loss: 0.1921\n16/16 [==============================] - 0s 635us/step - loss: 1.8098\n16/16 [==============================] - 0s 1ms/step - loss: 2.1669\n16/16 [==============================] - 0s 661us/step - loss: 2.2703\n16/16 [==============================] - 0s 1ms/step - loss: 2.2623\n16/16 [==============================] - 0s 664us/step - loss: 2.2363\n16/16 [==============================] - 0s 650us/step - loss: 2.2087\n16/16 [==============================] - 0s 648us/step - loss: 2.1849\n16/16 [==============================] - 0s 642us/step - loss: 2.1766\n16/16 [==============================] - 0s 662us/step - loss: 2.1732\nEpoch 36 of 60\n\nTesting for epoch 36 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1943\n16/16 [==============================] - 0s 873us/step - loss: 1.7990\n16/16 [==============================] - 0s 1ms/step - loss: 2.1590\n16/16 [==============================] - 0s 1ms/step - loss: 2.2620\n16/16 [==============================] - 0s 1ms/step - loss: 2.2528\n16/16 [==============================] - 0s 600us/step - loss: 2.2259\n16/16 [==============================] - 0s 584us/step - loss: 2.1981\n16/16 [==============================] - 0s 1ms/step - loss: 2.1742\n16/16 [==============================] - 0s 1ms/step - loss: 2.1658\n16/16 [==============================] - 0s 1ms/step - loss: 2.1623\n\nTesting for epoch 36 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1929\n16/16 [==============================] - 0s 1ms/step - loss: 1.7656\n16/16 [==============================] - 0s 784us/step - loss: 2.1149\n16/16 [==============================] - 0s 619us/step - loss: 2.2138\n16/16 [==============================] - 0s 587us/step - loss: 2.2056\n16/16 [==============================] - 0s 590us/step - loss: 2.1795\n16/16 [==============================] - 0s 567us/step - loss: 2.1526\n16/16 [==============================] - 0s 616us/step - loss: 2.1299\n16/16 [==============================] - 0s 1ms/step - loss: 2.1219\n16/16 [==============================] - 0s 594us/step - loss: 2.1186\n\nTesting for epoch 36 index 3:\n16/16 [==============================] - 0s 761us/step - loss: 0.1915\n16/16 [==============================] - 0s 1ms/step - loss: 1.7777\n16/16 [==============================] - 0s 1ms/step - loss: 2.1276\n16/16 [==============================] - 0s 1ms/step - loss: 2.2233\n16/16 [==============================] - 0s 1ms/step - loss: 2.2119\n16/16 [==============================] - 0s 1ms/step - loss: 2.1831\n16/16 [==============================] - 0s 1ms/step - loss: 2.1549\n16/16 [==============================] - 0s 1ms/step - loss: 2.1315\n16/16 [==============================] - 0s 1ms/step - loss: 2.1235\n16/16 [==============================] - 0s 1ms/step - loss: 2.1202\n\nTesting for epoch 36 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1890\n16/16 [==============================] - 0s 947us/step - loss: 1.7926\n16/16 [==============================] - 0s 1ms/step - loss: 2.1436\n16/16 [==============================] - 0s 1ms/step - loss: 2.2395\n16/16 [==============================] - 0s 625us/step - loss: 2.2279\n16/16 [==============================] - 0s 1ms/step - loss: 2.1978\n16/16 [==============================] - 0s 1ms/step - loss: 2.1692\n16/16 [==============================] - 0s 1ms/step - loss: 2.1451\n16/16 [==============================] - 0s 1ms/step - loss: 2.1368\n16/16 [==============================] - 0s 1ms/step - loss: 2.1334\n\nTesting for epoch 36 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1867\n16/16 [==============================] - 0s 1ms/step - loss: 1.8739\n16/16 [==============================] - 0s 1ms/step - loss: 2.2491\n16/16 [==============================] - 0s 992us/step - loss: 2.3538\n16/16 [==============================] - 0s 1ms/step - loss: 2.3422\n16/16 [==============================] - 0s 870us/step - loss: 2.3119\n16/16 [==============================] - 0s 1ms/step - loss: 2.2828\n16/16 [==============================] - 0s 627us/step - loss: 2.2576\n16/16 [==============================] - 0s 615us/step - loss: 2.2488\n16/16 [==============================] - 0s 748us/step - loss: 2.2452\nEpoch 37 of 60\n\nTesting for epoch 37 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1852\n16/16 [==============================] - 0s 725us/step - loss: 1.8347\n16/16 [==============================] - 0s 646us/step - loss: 2.1942\n16/16 [==============================] - 0s 856us/step - loss: 2.2953\n16/16 [==============================] - 0s 661us/step - loss: 2.2842\n16/16 [==============================] - 0s 604us/step - loss: 2.2557\n16/16 [==============================] - 0s 602us/step - loss: 2.2280\n16/16 [==============================] - 0s 908us/step - loss: 2.2039\n16/16 [==============================] - 0s 1ms/step - loss: 2.1955\n16/16 [==============================] - 0s 1ms/step - loss: 2.1921\n\nTesting for epoch 37 index 2:\n16/16 [==============================] - 0s 896us/step - loss: 0.1867\n16/16 [==============================] - 0s 668us/step - loss: 1.7861\n16/16 [==============================] - 0s 1ms/step - loss: 2.1329\n16/16 [==============================] - 0s 777us/step - loss: 2.2272\n16/16 [==============================] - 0s 1ms/step - loss: 2.2136\n16/16 [==============================] - 0s 736us/step - loss: 2.1838\n16/16 [==============================] - 0s 606us/step - loss: 2.1556\n16/16 [==============================] - 0s 1ms/step - loss: 2.1315\n16/16 [==============================] - 0s 1ms/step - loss: 2.1232\n16/16 [==============================] - 0s 626us/step - loss: 2.1198\n\nTesting for epoch 37 index 3:\n16/16 [==============================] - 0s 579us/step - loss: 0.1835\n16/16 [==============================] - 0s 1ms/step - loss: 1.8104\n16/16 [==============================] - 0s 1ms/step - loss: 2.1585\n16/16 [==============================] - 0s 634us/step - loss: 2.2523\n16/16 [==============================] - 0s 702us/step - loss: 2.2377\n16/16 [==============================] - 0s 635us/step - loss: 2.2062\n16/16 [==============================] - 0s 604us/step - loss: 2.1763\n16/16 [==============================] - 0s 1ms/step - loss: 2.1511\n16/16 [==============================] - 0s 1ms/step - loss: 2.1427\n16/16 [==============================] - 0s 625us/step - loss: 2.1392\n\nTesting for epoch 37 index 4:\n16/16 [==============================] - 0s 613us/step - loss: 0.1853\n16/16 [==============================] - 0s 700us/step - loss: 1.8215\n16/16 [==============================] - 0s 1ms/step - loss: 2.1727\n16/16 [==============================] - 0s 723us/step - loss: 2.2672\n16/16 [==============================] - 0s 714us/step - loss: 2.2522\n16/16 [==============================] - 0s 621us/step - loss: 2.2214\n16/16 [==============================] - 0s 669us/step - loss: 2.1932\n16/16 [==============================] - 0s 607us/step - loss: 2.1690\n16/16 [==============================] - 0s 700us/step - loss: 2.1607\n16/16 [==============================] - 0s 651us/step - loss: 2.1573\n\nTesting for epoch 37 index 5:\n16/16 [==============================] - 0s 628us/step - loss: 0.1840\n16/16 [==============================] - 0s 600us/step - loss: 1.8633\n16/16 [==============================] - 0s 622us/step - loss: 2.2307\n16/16 [==============================] - 0s 620us/step - loss: 2.3265\n16/16 [==============================] - 0s 622us/step - loss: 2.3089\n16/16 [==============================] - 0s 652us/step - loss: 2.2739\n16/16 [==============================] - 0s 1ms/step - loss: 2.2421\n16/16 [==============================] - 0s 968us/step - loss: 2.2152\n16/16 [==============================] - 0s 1ms/step - loss: 2.2061\n16/16 [==============================] - 0s 851us/step - loss: 2.2024\nEpoch 38 of 60\n\nTesting for epoch 38 index 1:\n16/16 [==============================] - 0s 639us/step - loss: 0.1830\n16/16 [==============================] - 0s 1ms/step - loss: 1.8733\n16/16 [==============================] - 0s 621us/step - loss: 2.2434\n16/16 [==============================] - 0s 966us/step - loss: 2.3381\n16/16 [==============================] - 0s 1ms/step - loss: 2.3205\n16/16 [==============================] - 0s 1ms/step - loss: 2.2861\n16/16 [==============================] - 0s 1ms/step - loss: 2.2544\n16/16 [==============================] - 0s 1ms/step - loss: 2.2276\n16/16 [==============================] - 0s 936us/step - loss: 2.2186\n16/16 [==============================] - 0s 617us/step - loss: 2.2149\n\nTesting for epoch 38 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1816\n16/16 [==============================] - 0s 618us/step - loss: 1.8904\n16/16 [==============================] - 0s 1ms/step - loss: 2.2667\n16/16 [==============================] - 0s 707us/step - loss: 2.3649\n16/16 [==============================] - 0s 1ms/step - loss: 2.3499\n16/16 [==============================] - 0s 1ms/step - loss: 2.3184\n16/16 [==============================] - 0s 1ms/step - loss: 2.2894\n16/16 [==============================] - 0s 1ms/step - loss: 2.2641\n16/16 [==============================] - 0s 1ms/step - loss: 2.2555\n16/16 [==============================] - 0s 632us/step - loss: 2.2520\n\nTesting for epoch 38 index 3:\n16/16 [==============================] - 0s 607us/step - loss: 0.1833\n16/16 [==============================] - 0s 641us/step - loss: 1.8758\n16/16 [==============================] - 0s 1ms/step - loss: 2.2495\n16/16 [==============================] - 0s 617us/step - loss: 2.3460\n16/16 [==============================] - 0s 639us/step - loss: 2.3310\n16/16 [==============================] - 0s 930us/step - loss: 2.2983\n16/16 [==============================] - 0s 622us/step - loss: 2.2679\n16/16 [==============================] - 0s 1ms/step - loss: 2.2418\n16/16 [==============================] - 0s 624us/step - loss: 2.2328\n16/16 [==============================] - 0s 1ms/step - loss: 2.2291\n\nTesting for epoch 38 index 4:\n16/16 [==============================] - 0s 969us/step - loss: 0.1794\n16/16 [==============================] - 0s 1ms/step - loss: 1.9049\n16/16 [==============================] - 0s 1ms/step - loss: 2.2851\n16/16 [==============================] - 0s 615us/step - loss: 2.3832\n16/16 [==============================] - 0s 712us/step - loss: 2.3669\n16/16 [==============================] - 0s 616us/step - loss: 2.3326\n16/16 [==============================] - 0s 1ms/step - loss: 2.3017\n16/16 [==============================] - 0s 630us/step - loss: 2.2754\n16/16 [==============================] - 0s 837us/step - loss: 2.2665\n16/16 [==============================] - 0s 1ms/step - loss: 2.2628\n\nTesting for epoch 38 index 5:\n16/16 [==============================] - 0s 632us/step - loss: 0.1800\n16/16 [==============================] - 0s 626us/step - loss: 1.8637\n16/16 [==============================] - 0s 904us/step - loss: 2.2352\n16/16 [==============================] - 0s 1ms/step - loss: 2.3308\n16/16 [==============================] - 0s 1ms/step - loss: 2.3141\n16/16 [==============================] - 0s 1ms/step - loss: 2.2801\n16/16 [==============================] - 0s 1ms/step - loss: 2.2488\n16/16 [==============================] - 0s 1ms/step - loss: 2.2227\n16/16 [==============================] - 0s 919us/step - loss: 2.2139\n16/16 [==============================] - 0s 966us/step - loss: 2.2103\nEpoch 39 of 60\n\nTesting for epoch 39 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1775\n16/16 [==============================] - 0s 644us/step - loss: 1.9059\n16/16 [==============================] - 0s 669us/step - loss: 2.2860\n16/16 [==============================] - 0s 1ms/step - loss: 2.3835\n16/16 [==============================] - 0s 664us/step - loss: 2.3656\n16/16 [==============================] - 0s 1ms/step - loss: 2.3309\n16/16 [==============================] - 0s 689us/step - loss: 2.2992\n16/16 [==============================] - 0s 1ms/step - loss: 2.2725\n16/16 [==============================] - 0s 1ms/step - loss: 2.2635\n16/16 [==============================] - 0s 1ms/step - loss: 2.2598\n\nTesting for epoch 39 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1770\n16/16 [==============================] - 0s 882us/step - loss: 1.8485\n16/16 [==============================] - 0s 1ms/step - loss: 2.2087\n16/16 [==============================] - 0s 843us/step - loss: 2.3001\n16/16 [==============================] - 0s 1ms/step - loss: 2.2820\n16/16 [==============================] - 0s 1ms/step - loss: 2.2470\n16/16 [==============================] - 0s 808us/step - loss: 2.2158\n16/16 [==============================] - 0s 674us/step - loss: 2.1898\n16/16 [==============================] - 0s 665us/step - loss: 2.1810\n16/16 [==============================] - 0s 666us/step - loss: 2.1774\n\nTesting for epoch 39 index 3:\n16/16 [==============================] - 0s 702us/step - loss: 0.1778\n16/16 [==============================] - 0s 1ms/step - loss: 1.8604\n16/16 [==============================] - 0s 1ms/step - loss: 2.2199\n16/16 [==============================] - 0s 1ms/step - loss: 2.3081\n16/16 [==============================] - 0s 1ms/step - loss: 2.2886\n16/16 [==============================] - 0s 1ms/step - loss: 2.2537\n16/16 [==============================] - 0s 899us/step - loss: 2.2230\n16/16 [==============================] - 0s 847us/step - loss: 2.1975\n16/16 [==============================] - 0s 674us/step - loss: 2.1890\n16/16 [==============================] - 0s 648us/step - loss: 2.1856\n\nTesting for epoch 39 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1772\n16/16 [==============================] - 0s 1ms/step - loss: 1.9029\n16/16 [==============================] - 0s 1ms/step - loss: 2.2795\n16/16 [==============================] - 0s 929us/step - loss: 2.3721\n16/16 [==============================] - 0s 656us/step - loss: 2.3526\n16/16 [==============================] - 0s 685us/step - loss: 2.3171\n16/16 [==============================] - 0s 697us/step - loss: 2.2856\n16/16 [==============================] - 0s 686us/step - loss: 2.2594\n16/16 [==============================] - 0s 1ms/step - loss: 2.2505\n16/16 [==============================] - 0s 938us/step - loss: 2.2469\n\nTesting for epoch 39 index 5:\n16/16 [==============================] - 0s 736us/step - loss: 0.1773\n16/16 [==============================] - 0s 1ms/step - loss: 1.8690\n16/16 [==============================] - 0s 1ms/step - loss: 2.2337\n16/16 [==============================] - 0s 997us/step - loss: 2.3199\n16/16 [==============================] - 0s 1ms/step - loss: 2.2990\n16/16 [==============================] - 0s 965us/step - loss: 2.2626\n16/16 [==============================] - 0s 1ms/step - loss: 2.2305\n16/16 [==============================] - 0s 681us/step - loss: 2.2046\n16/16 [==============================] - 0s 701us/step - loss: 2.1959\n16/16 [==============================] - 0s 684us/step - loss: 2.1924\nEpoch 40 of 60\n\nTesting for epoch 40 index 1:\n16/16 [==============================] - 0s 857us/step - loss: 0.1749\n16/16 [==============================] - 0s 647us/step - loss: 1.8422\n16/16 [==============================] - 0s 695us/step - loss: 2.2055\n16/16 [==============================] - 0s 1ms/step - loss: 2.2933\n16/16 [==============================] - 0s 858us/step - loss: 2.2752\n16/16 [==============================] - 0s 1ms/step - loss: 2.2414\n16/16 [==============================] - 0s 1ms/step - loss: 2.2106\n16/16 [==============================] - 0s 702us/step - loss: 2.1853\n16/16 [==============================] - 0s 1ms/step - loss: 2.1768\n16/16 [==============================] - 0s 653us/step - loss: 2.1733\n\nTesting for epoch 40 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 0.1735\n16/16 [==============================] - 0s 2ms/step - loss: 1.8843\n16/16 [==============================] - 0s 2ms/step - loss: 2.2440\n16/16 [==============================] - 0s 2ms/step - loss: 2.3291\n16/16 [==============================] - 0s 2ms/step - loss: 2.3069\n16/16 [==============================] - 0s 2ms/step - loss: 2.2710\n16/16 [==============================] - 0s 2ms/step - loss: 2.2398\n16/16 [==============================] - 0s 2ms/step - loss: 2.2144\n16/16 [==============================] - 0s 2ms/step - loss: 2.2058\n16/16 [==============================] - 0s 1ms/step - loss: 2.2023\n\nTesting for epoch 40 index 3:\n16/16 [==============================] - 0s 776us/step - loss: 0.1737\n16/16 [==============================] - 0s 2ms/step - loss: 1.8765\n16/16 [==============================] - 0s 2ms/step - loss: 2.2332\n16/16 [==============================] - 0s 2ms/step - loss: 2.3172\n16/16 [==============================] - 0s 2ms/step - loss: 2.2945\n16/16 [==============================] - 0s 2ms/step - loss: 2.2569\n16/16 [==============================] - 0s 3ms/step - loss: 2.2234\n16/16 [==============================] - 0s 2ms/step - loss: 2.1967\n16/16 [==============================] - 0s 2ms/step - loss: 2.1880\n16/16 [==============================] - 0s 2ms/step - loss: 2.1844\n\nTesting for epoch 40 index 4:\n16/16 [==============================] - 0s 2ms/step - loss: 0.1761\n16/16 [==============================] - 0s 716us/step - loss: 1.8934\n16/16 [==============================] - 0s 703us/step - loss: 2.2560\n16/16 [==============================] - 0s 675us/step - loss: 2.3469\n16/16 [==============================] - 0s 683us/step - loss: 2.3263\n16/16 [==============================] - 0s 735us/step - loss: 2.2914\n16/16 [==============================] - 0s 2ms/step - loss: 2.2595\n16/16 [==============================] - 0s 2ms/step - loss: 2.2333\n16/16 [==============================] - 0s 2ms/step - loss: 2.2245\n16/16 [==============================] - 0s 1ms/step - loss: 2.2209\n\nTesting for epoch 40 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1722\n16/16 [==============================] - 0s 2ms/step - loss: 1.9280\n16/16 [==============================] - 0s 1ms/step - loss: 2.2976\n16/16 [==============================] - 0s 1ms/step - loss: 2.3871\n16/16 [==============================] - 0s 1ms/step - loss: 2.3629\n16/16 [==============================] - 0s 1ms/step - loss: 2.3248\n16/16 [==============================] - 0s 2ms/step - loss: 2.2908\n16/16 [==============================] - 0s 1ms/step - loss: 2.2634\n16/16 [==============================] - 0s 2ms/step - loss: 2.2542\n16/16 [==============================] - 0s 2ms/step - loss: 2.2505\nEpoch 41 of 60\n\nTesting for epoch 41 index 1:\n16/16 [==============================] - 0s 665us/step - loss: 0.1699\n16/16 [==============================] - 0s 649us/step - loss: 1.9410\n16/16 [==============================] - 0s 642us/step - loss: 2.3137\n16/16 [==============================] - 0s 647us/step - loss: 2.4031\n16/16 [==============================] - 0s 634us/step - loss: 2.3771\n16/16 [==============================] - 0s 655us/step - loss: 2.3376\n16/16 [==============================] - 0s 2ms/step - loss: 2.3020\n16/16 [==============================] - 0s 1ms/step - loss: 2.2737\n16/16 [==============================] - 0s 670us/step - loss: 2.2643\n16/16 [==============================] - 0s 643us/step - loss: 2.2604\n\nTesting for epoch 41 index 2:\n16/16 [==============================] - 0s 629us/step - loss: 0.1723\n16/16 [==============================] - 0s 2ms/step - loss: 1.8916\n16/16 [==============================] - 0s 807us/step - loss: 2.2597\n16/16 [==============================] - 0s 933us/step - loss: 2.3476\n16/16 [==============================] - 0s 933us/step - loss: 2.3238\n16/16 [==============================] - 0s 923us/step - loss: 2.2886\n16/16 [==============================] - 0s 952us/step - loss: 2.2566\n16/16 [==============================] - 0s 1ms/step - loss: 2.2308\n16/16 [==============================] - 0s 995us/step - loss: 2.2222\n16/16 [==============================] - 0s 911us/step - loss: 2.2188\n\nTesting for epoch 41 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1698\n16/16 [==============================] - 0s 2ms/step - loss: 1.9182\n16/16 [==============================] - 0s 1ms/step - loss: 2.2868\n16/16 [==============================] - 0s 755us/step - loss: 2.3684\n16/16 [==============================] - 0s 652us/step - loss: 2.3398\n16/16 [==============================] - 0s 650us/step - loss: 2.3013\n16/16 [==============================] - 0s 648us/step - loss: 2.2666\n16/16 [==============================] - 0s 660us/step - loss: 2.2391\n16/16 [==============================] - 0s 1ms/step - loss: 2.2300\n16/16 [==============================] - 0s 2ms/step - loss: 2.2264\n\nTesting for epoch 41 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1686\n16/16 [==============================] - 0s 648us/step - loss: 1.9577\n16/16 [==============================] - 0s 887us/step - loss: 2.3428\n16/16 [==============================] - 0s 1ms/step - loss: 2.4313\n16/16 [==============================] - 0s 645us/step - loss: 2.4052\n16/16 [==============================] - 0s 1ms/step - loss: 2.3674\n16/16 [==============================] - 0s 2ms/step - loss: 2.3327\n16/16 [==============================] - 0s 653us/step - loss: 2.3051\n16/16 [==============================] - 0s 2ms/step - loss: 2.2958\n16/16 [==============================] - 0s 2ms/step - loss: 2.2921\n\nTesting for epoch 41 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1707\n16/16 [==============================] - 0s 1ms/step - loss: 1.9381\n16/16 [==============================] - 0s 626us/step - loss: 2.3141\n16/16 [==============================] - 0s 2ms/step - loss: 2.3976\n16/16 [==============================] - 0s 1ms/step - loss: 2.3711\n16/16 [==============================] - 0s 2ms/step - loss: 2.3333\n16/16 [==============================] - 0s 2ms/step - loss: 2.2988\n16/16 [==============================] - 0s 2ms/step - loss: 2.2715\n16/16 [==============================] - 0s 1ms/step - loss: 2.2624\n16/16 [==============================] - 0s 2ms/step - loss: 2.2588\nEpoch 42 of 60\n\nTesting for epoch 42 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 0.1687\n16/16 [==============================] - 0s 2ms/step - loss: 1.9747\n16/16 [==============================] - 0s 2ms/step - loss: 2.3635\n16/16 [==============================] - 0s 2ms/step - loss: 2.4468\n16/16 [==============================] - 0s 2ms/step - loss: 2.4175\n16/16 [==============================] - 0s 2ms/step - loss: 2.3761\n16/16 [==============================] - 0s 1ms/step - loss: 2.3383\n16/16 [==============================] - 0s 2ms/step - loss: 2.3088\n16/16 [==============================] - 0s 1ms/step - loss: 2.2990\n16/16 [==============================] - 0s 2ms/step - loss: 2.2950\n\nTesting for epoch 42 index 2:\n16/16 [==============================] - 0s 2ms/step - loss: 0.1671\n16/16 [==============================] - 0s 1ms/step - loss: 1.9391\n16/16 [==============================] - 0s 2ms/step - loss: 2.3156\n16/16 [==============================] - 0s 1ms/step - loss: 2.3979\n16/16 [==============================] - 0s 2ms/step - loss: 2.3719\n16/16 [==============================] - 0s 2ms/step - loss: 2.3335\n16/16 [==============================] - 0s 2ms/step - loss: 2.2984\n16/16 [==============================] - 0s 2ms/step - loss: 2.2707\n16/16 [==============================] - 0s 2ms/step - loss: 2.2614\n16/16 [==============================] - 0s 2ms/step - loss: 2.2577\n\nTesting for epoch 42 index 3:\n16/16 [==============================] - 0s 2ms/step - loss: 0.1662\n16/16 [==============================] - 0s 1ms/step - loss: 1.9564\n16/16 [==============================] - 0s 1ms/step - loss: 2.3366\n16/16 [==============================] - 0s 2ms/step - loss: 2.4170\n16/16 [==============================] - 0s 2ms/step - loss: 2.3906\n16/16 [==============================] - 0s 2ms/step - loss: 2.3517\n16/16 [==============================] - 0s 1ms/step - loss: 2.3168\n16/16 [==============================] - 0s 1ms/step - loss: 2.2891\n16/16 [==============================] - 0s 2ms/step - loss: 2.2799\n16/16 [==============================] - 0s 3ms/step - loss: 2.2762\n\nTesting for epoch 42 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1664\n16/16 [==============================] - 0s 836us/step - loss: 1.9228\n16/16 [==============================] - 0s 857us/step - loss: 2.2967\n16/16 [==============================] - 0s 827us/step - loss: 2.3769\n16/16 [==============================] - 0s 2ms/step - loss: 2.3522\n16/16 [==============================] - 0s 2ms/step - loss: 2.3146\n16/16 [==============================] - 0s 2ms/step - loss: 2.2804\n16/16 [==============================] - 0s 2ms/step - loss: 2.2535\n16/16 [==============================] - 0s 2ms/step - loss: 2.2446\n16/16 [==============================] - 0s 2ms/step - loss: 2.2410\n\nTesting for epoch 42 index 5:\n16/16 [==============================] - 0s 968us/step - loss: 0.1647\n16/16 [==============================] - 0s 836us/step - loss: 1.9825\n16/16 [==============================] - 0s 1ms/step - loss: 2.3655\n16/16 [==============================] - 0s 991us/step - loss: 2.4400\n16/16 [==============================] - 0s 708us/step - loss: 2.4065\n16/16 [==============================] - 0s 988us/step - loss: 2.3618\n16/16 [==============================] - 0s 1ms/step - loss: 2.3236\n16/16 [==============================] - 0s 769us/step - loss: 2.2942\n16/16 [==============================] - 0s 630us/step - loss: 2.2846\n16/16 [==============================] - 0s 832us/step - loss: 2.2808\nEpoch 43 of 60\n\nTesting for epoch 43 index 1:\n16/16 [==============================] - 0s 2ms/step - loss: 0.1616\n16/16 [==============================] - 0s 2ms/step - loss: 1.9799\n16/16 [==============================] - 0s 2ms/step - loss: 2.3682\n16/16 [==============================] - 0s 2ms/step - loss: 2.4457\n16/16 [==============================] - 0s 2ms/step - loss: 2.4156\n16/16 [==============================] - 0s 2ms/step - loss: 2.3727\n16/16 [==============================] - 0s 2ms/step - loss: 2.3349\n16/16 [==============================] - 0s 2ms/step - loss: 2.3056\n16/16 [==============================] - 0s 2ms/step - loss: 2.2959\n16/16 [==============================] - 0s 2ms/step - loss: 2.2921\n\nTesting for epoch 43 index 2:\n16/16 [==============================] - 0s 594us/step - loss: 0.1670\n16/16 [==============================] - 0s 542us/step - loss: 1.9252\n16/16 [==============================] - 0s 603us/step - loss: 2.3035\n16/16 [==============================] - 0s 638us/step - loss: 2.3810\n16/16 [==============================] - 0s 646us/step - loss: 2.3532\n16/16 [==============================] - 0s 490us/step - loss: 2.3132\n16/16 [==============================] - 0s 517us/step - loss: 2.2779\n16/16 [==============================] - 0s 1ms/step - loss: 2.2503\n16/16 [==============================] - 0s 895us/step - loss: 2.2412\n16/16 [==============================] - 0s 570us/step - loss: 2.2376\n\nTesting for epoch 43 index 3:\n16/16 [==============================] - 0s 626us/step - loss: 0.1649\n16/16 [==============================] - 0s 921us/step - loss: 1.9655\n16/16 [==============================] - 0s 1ms/step - loss: 2.3495\n16/16 [==============================] - 0s 708us/step - loss: 2.4250\n16/16 [==============================] - 0s 1ms/step - loss: 2.3950\n16/16 [==============================] - 0s 1ms/step - loss: 2.3532\n16/16 [==============================] - 0s 681us/step - loss: 2.3167\n16/16 [==============================] - 0s 632us/step - loss: 2.2886\n16/16 [==============================] - 0s 1ms/step - loss: 2.2794\n16/16 [==============================] - 0s 644us/step - loss: 2.2756\n\nTesting for epoch 43 index 4:\n16/16 [==============================] - 0s 716us/step - loss: 0.1628\n16/16 [==============================] - 0s 631us/step - loss: 1.9172\n16/16 [==============================] - 0s 634us/step - loss: 2.2852\n16/16 [==============================] - 0s 601us/step - loss: 2.3543\n16/16 [==============================] - 0s 1ms/step - loss: 2.3241\n16/16 [==============================] - 0s 1ms/step - loss: 2.2817\n16/16 [==============================] - 0s 1ms/step - loss: 2.2446\n16/16 [==============================] - 0s 637us/step - loss: 2.2162\n16/16 [==============================] - 0s 1ms/step - loss: 2.2068\n16/16 [==============================] - 0s 622us/step - loss: 2.2031\n\nTesting for epoch 43 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1632\n16/16 [==============================] - 0s 614us/step - loss: 1.9494\n16/16 [==============================] - 0s 585us/step - loss: 2.3248\n16/16 [==============================] - 0s 1ms/step - loss: 2.3958\n16/16 [==============================] - 0s 605us/step - loss: 2.3639\n16/16 [==============================] - 0s 1ms/step - loss: 2.3212\n16/16 [==============================] - 0s 659us/step - loss: 2.2846\n16/16 [==============================] - 0s 920us/step - loss: 2.2564\n16/16 [==============================] - 0s 642us/step - loss: 2.2472\n16/16 [==============================] - 0s 1ms/step - loss: 2.2436\nEpoch 44 of 60\n\nTesting for epoch 44 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1623\n16/16 [==============================] - 0s 632us/step - loss: 1.9453\n16/16 [==============================] - 0s 701us/step - loss: 2.3227\n16/16 [==============================] - 0s 1ms/step - loss: 2.3937\n16/16 [==============================] - 0s 598us/step - loss: 2.3616\n16/16 [==============================] - 0s 623us/step - loss: 2.3172\n16/16 [==============================] - 0s 1ms/step - loss: 2.2785\n16/16 [==============================] - 0s 802us/step - loss: 2.2493\n16/16 [==============================] - 0s 1ms/step - loss: 2.2398\n16/16 [==============================] - 0s 900us/step - loss: 2.2360\n\nTesting for epoch 44 index 2:\n16/16 [==============================] - 0s 639us/step - loss: 0.1612\n16/16 [==============================] - 0s 709us/step - loss: 1.9442\n16/16 [==============================] - 0s 617us/step - loss: 2.3257\n16/16 [==============================] - 0s 598us/step - loss: 2.4020\n16/16 [==============================] - 0s 1ms/step - loss: 2.3753\n16/16 [==============================] - 0s 625us/step - loss: 2.3369\n16/16 [==============================] - 0s 611us/step - loss: 2.3024\n16/16 [==============================] - 0s 601us/step - loss: 2.2753\n16/16 [==============================] - 0s 764us/step - loss: 2.2663\n16/16 [==============================] - 0s 1ms/step - loss: 2.2626\n\nTesting for epoch 44 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1585\n16/16 [==============================] - 0s 1ms/step - loss: 2.0528\n16/16 [==============================] - 0s 609us/step - loss: 2.4585\n16/16 [==============================] - 0s 789us/step - loss: 2.5358\n16/16 [==============================] - 0s 587us/step - loss: 2.5023\n16/16 [==============================] - 0s 672us/step - loss: 2.4563\n16/16 [==============================] - 0s 805us/step - loss: 2.4162\n16/16 [==============================] - 0s 1ms/step - loss: 2.3857\n16/16 [==============================] - 0s 1ms/step - loss: 2.3757\n16/16 [==============================] - 0s 1ms/step - loss: 2.3718\n\nTesting for epoch 44 index 4:\n16/16 [==============================] - 0s 735us/step - loss: 0.1614\n16/16 [==============================] - 0s 1ms/step - loss: 2.0477\n16/16 [==============================] - 0s 646us/step - loss: 2.4531\n16/16 [==============================] - 0s 1ms/step - loss: 2.5273\n16/16 [==============================] - 0s 560us/step - loss: 2.4941\n16/16 [==============================] - 0s 604us/step - loss: 2.4496\n16/16 [==============================] - 0s 1ms/step - loss: 2.4104\n16/16 [==============================] - 0s 599us/step - loss: 2.3801\n16/16 [==============================] - 0s 1ms/step - loss: 2.3702\n16/16 [==============================] - 0s 1ms/step - loss: 2.3661\n\nTesting for epoch 44 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1584\n16/16 [==============================] - 0s 1ms/step - loss: 2.0297\n16/16 [==============================] - 0s 1ms/step - loss: 2.4309\n16/16 [==============================] - 0s 1ms/step - loss: 2.5028\n16/16 [==============================] - 0s 844us/step - loss: 2.4704\n16/16 [==============================] - 0s 721us/step - loss: 2.4258\n16/16 [==============================] - 0s 1ms/step - loss: 2.3870\n16/16 [==============================] - 0s 724us/step - loss: 2.3575\n16/16 [==============================] - 0s 1ms/step - loss: 2.3479\n16/16 [==============================] - 0s 1ms/step - loss: 2.3440\nEpoch 45 of 60\n\nTesting for epoch 45 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1607\n16/16 [==============================] - 0s 1ms/step - loss: 1.9936\n16/16 [==============================] - 0s 1ms/step - loss: 2.3817\n16/16 [==============================] - 0s 618us/step - loss: 2.4495\n16/16 [==============================] - 0s 1ms/step - loss: 2.4171\n16/16 [==============================] - 0s 1ms/step - loss: 2.3735\n16/16 [==============================] - 0s 929us/step - loss: 2.3355\n16/16 [==============================] - 0s 1ms/step - loss: 2.3067\n16/16 [==============================] - 0s 1ms/step - loss: 2.2974\n16/16 [==============================] - 0s 967us/step - loss: 2.2937\n\nTesting for epoch 45 index 2:\n16/16 [==============================] - 0s 625us/step - loss: 0.1592\n16/16 [==============================] - 0s 748us/step - loss: 2.0080\n16/16 [==============================] - 0s 948us/step - loss: 2.3934\n16/16 [==============================] - 0s 626us/step - loss: 2.4562\n16/16 [==============================] - 0s 1ms/step - loss: 2.4202\n16/16 [==============================] - 0s 1ms/step - loss: 2.3733\n16/16 [==============================] - 0s 600us/step - loss: 2.3338\n16/16 [==============================] - 0s 1ms/step - loss: 2.3041\n16/16 [==============================] - 0s 1ms/step - loss: 2.2945\n16/16 [==============================] - 0s 579us/step - loss: 2.2907\n\nTesting for epoch 45 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1563\n16/16 [==============================] - 0s 1ms/step - loss: 1.9989\n16/16 [==============================] - 0s 791us/step - loss: 2.3823\n16/16 [==============================] - 0s 592us/step - loss: 2.4454\n16/16 [==============================] - 0s 657us/step - loss: 2.4112\n16/16 [==============================] - 0s 731us/step - loss: 2.3670\n16/16 [==============================] - 0s 1ms/step - loss: 2.3288\n16/16 [==============================] - 0s 745us/step - loss: 2.2995\n16/16 [==============================] - 0s 595us/step - loss: 2.2900\n16/16 [==============================] - 0s 629us/step - loss: 2.2862\n\nTesting for epoch 45 index 4:\n16/16 [==============================] - 0s 948us/step - loss: 0.1570\n16/16 [==============================] - 0s 860us/step - loss: 1.9843\n16/16 [==============================] - 0s 836us/step - loss: 2.3656\n16/16 [==============================] - 0s 1ms/step - loss: 2.4254\n16/16 [==============================] - 0s 1ms/step - loss: 2.3894\n16/16 [==============================] - 0s 1ms/step - loss: 2.3424\n16/16 [==============================] - 0s 644us/step - loss: 2.3025\n16/16 [==============================] - 0s 606us/step - loss: 2.2727\n16/16 [==============================] - 0s 1ms/step - loss: 2.2631\n16/16 [==============================] - 0s 810us/step - loss: 2.2593\n\nTesting for epoch 45 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1540\n16/16 [==============================] - 0s 621us/step - loss: 2.0578\n16/16 [==============================] - 0s 754us/step - loss: 2.4644\n16/16 [==============================] - 0s 1ms/step - loss: 2.5294\n16/16 [==============================] - 0s 1ms/step - loss: 2.4946\n16/16 [==============================] - 0s 1ms/step - loss: 2.4481\n16/16 [==============================] - 0s 1ms/step - loss: 2.4079\n16/16 [==============================] - 0s 759us/step - loss: 2.3773\n16/16 [==============================] - 0s 1ms/step - loss: 2.3674\n16/16 [==============================] - 0s 634us/step - loss: 2.3635\nEpoch 46 of 60\n\nTesting for epoch 46 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1569\n16/16 [==============================] - 0s 624us/step - loss: 2.0009\n16/16 [==============================] - 0s 644us/step - loss: 2.3983\n16/16 [==============================] - 0s 620us/step - loss: 2.4650\n16/16 [==============================] - 0s 1ms/step - loss: 2.4337\n16/16 [==============================] - 0s 657us/step - loss: 2.3895\n16/16 [==============================] - 0s 662us/step - loss: 2.3506\n16/16 [==============================] - 0s 1ms/step - loss: 2.3207\n16/16 [==============================] - 0s 826us/step - loss: 2.3109\n16/16 [==============================] - 0s 1ms/step - loss: 2.3070\n\nTesting for epoch 46 index 2:\n16/16 [==============================] - 0s 648us/step - loss: 0.1564\n16/16 [==============================] - 0s 906us/step - loss: 1.9920\n16/16 [==============================] - 0s 1ms/step - loss: 2.3850\n16/16 [==============================] - 0s 632us/step - loss: 2.4473\n16/16 [==============================] - 0s 914us/step - loss: 2.4129\n16/16 [==============================] - 0s 650us/step - loss: 2.3665\n16/16 [==============================] - 0s 1ms/step - loss: 2.3262\n16/16 [==============================] - 0s 1ms/step - loss: 2.2963\n16/16 [==============================] - 0s 631us/step - loss: 2.2867\n16/16 [==============================] - 0s 1ms/step - loss: 2.2828\n\nTesting for epoch 46 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1523\n16/16 [==============================] - 0s 610us/step - loss: 1.9810\n16/16 [==============================] - 0s 1ms/step - loss: 2.3695\n16/16 [==============================] - 0s 599us/step - loss: 2.4291\n16/16 [==============================] - 0s 1ms/step - loss: 2.3939\n16/16 [==============================] - 0s 596us/step - loss: 2.3485\n16/16 [==============================] - 0s 651us/step - loss: 2.3100\n16/16 [==============================] - 0s 1ms/step - loss: 2.2810\n16/16 [==============================] - 0s 1ms/step - loss: 2.2716\n16/16 [==============================] - 0s 789us/step - loss: 2.2678\n\nTesting for epoch 46 index 4:\n16/16 [==============================] - 0s 605us/step - loss: 0.1539\n16/16 [==============================] - 0s 946us/step - loss: 2.0070\n16/16 [==============================] - 0s 1ms/step - loss: 2.3983\n16/16 [==============================] - 0s 562us/step - loss: 2.4547\n16/16 [==============================] - 0s 1ms/step - loss: 2.4185\n16/16 [==============================] - 0s 646us/step - loss: 2.3714\n16/16 [==============================] - 0s 1ms/step - loss: 2.3316\n16/16 [==============================] - 0s 1ms/step - loss: 2.3019\n16/16 [==============================] - 0s 1ms/step - loss: 2.2923\n16/16 [==============================] - 0s 629us/step - loss: 2.2885\n\nTesting for epoch 46 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1531\n16/16 [==============================] - 0s 1ms/step - loss: 2.0956\n16/16 [==============================] - 0s 1ms/step - loss: 2.5234\n16/16 [==============================] - 0s 649us/step - loss: 2.5872\n16/16 [==============================] - 0s 1ms/step - loss: 2.5491\n16/16 [==============================] - 0s 1ms/step - loss: 2.4983\n16/16 [==============================] - 0s 1ms/step - loss: 2.4550\n16/16 [==============================] - 0s 1ms/step - loss: 2.4226\n16/16 [==============================] - 0s 1ms/step - loss: 2.4122\n16/16 [==============================] - 0s 1ms/step - loss: 2.4080\nEpoch 47 of 60\n\nTesting for epoch 47 index 1:\n16/16 [==============================] - 0s 631us/step - loss: 0.1539\n16/16 [==============================] - 0s 877us/step - loss: 2.0259\n16/16 [==============================] - 0s 1ms/step - loss: 2.4331\n16/16 [==============================] - 0s 724us/step - loss: 2.4907\n16/16 [==============================] - 0s 631us/step - loss: 2.4546\n16/16 [==============================] - 0s 631us/step - loss: 2.4070\n16/16 [==============================] - 0s 1ms/step - loss: 2.3665\n16/16 [==============================] - 0s 1ms/step - loss: 2.3359\n16/16 [==============================] - 0s 995us/step - loss: 2.3261\n16/16 [==============================] - 0s 1ms/step - loss: 2.3222\n\nTesting for epoch 47 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1519\n16/16 [==============================] - 0s 639us/step - loss: 2.0542\n16/16 [==============================] - 0s 934us/step - loss: 2.4779\n16/16 [==============================] - 0s 1ms/step - loss: 2.5430\n16/16 [==============================] - 0s 666us/step - loss: 2.5111\n16/16 [==============================] - 0s 645us/step - loss: 2.4667\n16/16 [==============================] - 0s 606us/step - loss: 2.4283\n16/16 [==============================] - 0s 842us/step - loss: 2.3987\n16/16 [==============================] - 0s 617us/step - loss: 2.3890\n16/16 [==============================] - 0s 1ms/step - loss: 2.3851\n\nTesting for epoch 47 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1528\n16/16 [==============================] - 0s 957us/step - loss: 2.0201\n16/16 [==============================] - 0s 1ms/step - loss: 2.4226\n16/16 [==============================] - 0s 610us/step - loss: 2.4747\n16/16 [==============================] - 0s 626us/step - loss: 2.4384\n16/16 [==============================] - 0s 1ms/step - loss: 2.3899\n16/16 [==============================] - 0s 1ms/step - loss: 2.3487\n16/16 [==============================] - 0s 1ms/step - loss: 2.3181\n16/16 [==============================] - 0s 1ms/step - loss: 2.3083\n16/16 [==============================] - 0s 635us/step - loss: 2.3044\n\nTesting for epoch 47 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1513\n16/16 [==============================] - 0s 1ms/step - loss: 2.0221\n16/16 [==============================] - 0s 953us/step - loss: 2.4286\n16/16 [==============================] - 0s 584us/step - loss: 2.4816\n16/16 [==============================] - 0s 794us/step - loss: 2.4463\n16/16 [==============================] - 0s 1ms/step - loss: 2.3995\n16/16 [==============================] - 0s 1ms/step - loss: 2.3596\n16/16 [==============================] - 0s 1ms/step - loss: 2.3298\n16/16 [==============================] - 0s 635us/step - loss: 2.3202\n16/16 [==============================] - 0s 1ms/step - loss: 2.3163\n\nTesting for epoch 47 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1505\n16/16 [==============================] - 0s 1ms/step - loss: 2.0238\n16/16 [==============================] - 0s 1ms/step - loss: 2.4319\n16/16 [==============================] - 0s 1ms/step - loss: 2.4849\n16/16 [==============================] - 0s 628us/step - loss: 2.4502\n16/16 [==============================] - 0s 1ms/step - loss: 2.4034\n16/16 [==============================] - 0s 660us/step - loss: 2.3643\n16/16 [==============================] - 0s 614us/step - loss: 2.3350\n16/16 [==============================] - 0s 628us/step - loss: 2.3256\n16/16 [==============================] - 0s 1ms/step - loss: 2.3218\nEpoch 48 of 60\n\nTesting for epoch 48 index 1:\n16/16 [==============================] - 0s 637us/step - loss: 0.1506\n16/16 [==============================] - 0s 929us/step - loss: 2.0292\n16/16 [==============================] - 0s 635us/step - loss: 2.4452\n16/16 [==============================] - 0s 625us/step - loss: 2.5006\n16/16 [==============================] - 0s 639us/step - loss: 2.4658\n16/16 [==============================] - 0s 1ms/step - loss: 2.4185\n16/16 [==============================] - 0s 640us/step - loss: 2.3780\n16/16 [==============================] - 0s 1ms/step - loss: 2.3475\n16/16 [==============================] - 0s 593us/step - loss: 2.3377\n16/16 [==============================] - 0s 947us/step - loss: 2.3338\n\nTesting for epoch 48 index 2:\n16/16 [==============================] - 0s 642us/step - loss: 0.1520\n16/16 [==============================] - 0s 927us/step - loss: 2.0468\n16/16 [==============================] - 0s 907us/step - loss: 2.4645\n16/16 [==============================] - 0s 1ms/step - loss: 2.5202\n16/16 [==============================] - 0s 721us/step - loss: 2.4857\n16/16 [==============================] - 0s 1ms/step - loss: 2.4381\n16/16 [==============================] - 0s 1ms/step - loss: 2.3972\n16/16 [==============================] - 0s 1ms/step - loss: 2.3664\n16/16 [==============================] - 0s 755us/step - loss: 2.3565\n16/16 [==============================] - 0s 1ms/step - loss: 2.3525\n\nTesting for epoch 48 index 3:\n16/16 [==============================] - 0s 836us/step - loss: 0.1459\n16/16 [==============================] - 0s 1ms/step - loss: 2.0895\n16/16 [==============================] - 0s 1ms/step - loss: 2.5139\n16/16 [==============================] - 0s 1ms/step - loss: 2.5640\n16/16 [==============================] - 0s 773us/step - loss: 2.5243\n16/16 [==============================] - 0s 754us/step - loss: 2.4719\n16/16 [==============================] - 0s 617us/step - loss: 2.4279\n16/16 [==============================] - 0s 1ms/step - loss: 2.3952\n16/16 [==============================] - 0s 935us/step - loss: 2.3847\n16/16 [==============================] - 0s 1ms/step - loss: 2.3804\n\nTesting for epoch 48 index 4:\n16/16 [==============================] - 0s 635us/step - loss: 0.1465\n16/16 [==============================] - 0s 640us/step - loss: 2.0292\n16/16 [==============================] - 0s 610us/step - loss: 2.4415\n16/16 [==============================] - 0s 635us/step - loss: 2.4915\n16/16 [==============================] - 0s 634us/step - loss: 2.4547\n16/16 [==============================] - 0s 621us/step - loss: 2.4048\n16/16 [==============================] - 0s 644us/step - loss: 2.3627\n16/16 [==============================] - 0s 750us/step - loss: 2.3313\n16/16 [==============================] - 0s 1ms/step - loss: 2.3213\n16/16 [==============================] - 0s 705us/step - loss: 2.3173\n\nTesting for epoch 48 index 5:\n16/16 [==============================] - 0s 624us/step - loss: 0.1449\n16/16 [==============================] - 0s 1ms/step - loss: 2.0603\n16/16 [==============================] - 0s 1ms/step - loss: 2.4760\n16/16 [==============================] - 0s 1ms/step - loss: 2.5243\n16/16 [==============================] - 0s 1ms/step - loss: 2.4877\n16/16 [==============================] - 0s 1ms/step - loss: 2.4388\n16/16 [==============================] - 0s 1ms/step - loss: 2.3979\n16/16 [==============================] - 0s 615us/step - loss: 2.3674\n16/16 [==============================] - 0s 1ms/step - loss: 2.3576\n16/16 [==============================] - 0s 1ms/step - loss: 2.3537\nEpoch 49 of 60\n\nTesting for epoch 49 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1456\n16/16 [==============================] - 0s 1ms/step - loss: 2.1177\n16/16 [==============================] - 0s 1ms/step - loss: 2.5539\n16/16 [==============================] - 0s 1ms/step - loss: 2.6038\n16/16 [==============================] - 0s 641us/step - loss: 2.5627\n16/16 [==============================] - 0s 1ms/step - loss: 2.5085\n16/16 [==============================] - 0s 1ms/step - loss: 2.4635\n16/16 [==============================] - 0s 1ms/step - loss: 2.4301\n16/16 [==============================] - 0s 1ms/step - loss: 2.4196\n16/16 [==============================] - 0s 1ms/step - loss: 2.4155\n\nTesting for epoch 49 index 2:\n16/16 [==============================] - 0s 633us/step - loss: 0.1446\n16/16 [==============================] - 0s 650us/step - loss: 2.0868\n16/16 [==============================] - 0s 634us/step - loss: 2.5135\n16/16 [==============================] - 0s 605us/step - loss: 2.5636\n16/16 [==============================] - 0s 1ms/step - loss: 2.5272\n16/16 [==============================] - 0s 640us/step - loss: 2.4769\n16/16 [==============================] - 0s 1ms/step - loss: 2.4353\n16/16 [==============================] - 0s 608us/step - loss: 2.4045\n16/16 [==============================] - 0s 1ms/step - loss: 2.3946\n16/16 [==============================] - 0s 1ms/step - loss: 2.3907\n\nTesting for epoch 49 index 3:\n16/16 [==============================] - 0s 649us/step - loss: 0.1457\n16/16 [==============================] - 0s 624us/step - loss: 2.0862\n16/16 [==============================] - 0s 875us/step - loss: 2.5107\n16/16 [==============================] - 0s 590us/step - loss: 2.5578\n16/16 [==============================] - 0s 573us/step - loss: 2.5165\n16/16 [==============================] - 0s 1ms/step - loss: 2.4632\n16/16 [==============================] - 0s 1ms/step - loss: 2.4191\n16/16 [==============================] - 0s 1ms/step - loss: 2.3864\n16/16 [==============================] - 0s 1ms/step - loss: 2.3760\n16/16 [==============================] - 0s 611us/step - loss: 2.3718\n\nTesting for epoch 49 index 4:\n16/16 [==============================] - 0s 664us/step - loss: 0.1438\n16/16 [==============================] - 0s 610us/step - loss: 2.0347\n16/16 [==============================] - 0s 601us/step - loss: 2.4431\n16/16 [==============================] - 0s 1ms/step - loss: 2.4871\n16/16 [==============================] - 0s 636us/step - loss: 2.4479\n16/16 [==============================] - 0s 606us/step - loss: 2.3982\n16/16 [==============================] - 0s 618us/step - loss: 2.3571\n16/16 [==============================] - 0s 639us/step - loss: 2.3267\n16/16 [==============================] - 0s 657us/step - loss: 2.3170\n16/16 [==============================] - 0s 629us/step - loss: 2.3131\n\nTesting for epoch 49 index 5:\n16/16 [==============================] - 0s 641us/step - loss: 0.1474\n16/16 [==============================] - 0s 988us/step - loss: 2.0366\n16/16 [==============================] - 0s 1ms/step - loss: 2.4480\n16/16 [==============================] - 0s 903us/step - loss: 2.4936\n16/16 [==============================] - 0s 1ms/step - loss: 2.4559\n16/16 [==============================] - 0s 1ms/step - loss: 2.4066\n16/16 [==============================] - 0s 635us/step - loss: 2.3658\n16/16 [==============================] - 0s 1ms/step - loss: 2.3355\n16/16 [==============================] - 0s 797us/step - loss: 2.3259\n16/16 [==============================] - 0s 735us/step - loss: 2.3222\nEpoch 50 of 60\n\nTesting for epoch 50 index 1:\n16/16 [==============================] - 0s 696us/step - loss: 0.1436\n16/16 [==============================] - 0s 905us/step - loss: 2.1021\n16/16 [==============================] - 0s 841us/step - loss: 2.5288\n16/16 [==============================] - 0s 1ms/step - loss: 2.5741\n16/16 [==============================] - 0s 623us/step - loss: 2.5332\n16/16 [==============================] - 0s 637us/step - loss: 2.4820\n16/16 [==============================] - 0s 644us/step - loss: 2.4395\n16/16 [==============================] - 0s 949us/step - loss: 2.4076\n16/16 [==============================] - 0s 640us/step - loss: 2.3974\n16/16 [==============================] - 0s 1ms/step - loss: 2.3933\n\nTesting for epoch 50 index 2:\n16/16 [==============================] - 0s 745us/step - loss: 0.1413\n16/16 [==============================] - 0s 1ms/step - loss: 2.0536\n16/16 [==============================] - 0s 1ms/step - loss: 2.4615\n16/16 [==============================] - 0s 1ms/step - loss: 2.4976\n16/16 [==============================] - 0s 1ms/step - loss: 2.4550\n16/16 [==============================] - 0s 1ms/step - loss: 2.4020\n16/16 [==============================] - 0s 1ms/step - loss: 2.3590\n16/16 [==============================] - 0s 1ms/step - loss: 2.3276\n16/16 [==============================] - 0s 1ms/step - loss: 2.3176\n16/16 [==============================] - 0s 1ms/step - loss: 2.3137\n\nTesting for epoch 50 index 3:\n16/16 [==============================] - 0s 624us/step - loss: 0.1436\n16/16 [==============================] - 0s 1ms/step - loss: 2.0689\n16/16 [==============================] - 0s 1ms/step - loss: 2.4842\n16/16 [==============================] - 0s 1ms/step - loss: 2.5212\n16/16 [==============================] - 0s 1ms/step - loss: 2.4800\n16/16 [==============================] - 0s 613us/step - loss: 2.4299\n16/16 [==============================] - 0s 614us/step - loss: 2.3885\n16/16 [==============================] - 0s 622us/step - loss: 2.3575\n16/16 [==============================] - 0s 1ms/step - loss: 2.3477\n16/16 [==============================] - 0s 1ms/step - loss: 2.3438\n\nTesting for epoch 50 index 4:\n16/16 [==============================] - 0s 636us/step - loss: 0.1412\n16/16 [==============================] - 0s 622us/step - loss: 2.0655\n16/16 [==============================] - 0s 584us/step - loss: 2.4782\n16/16 [==============================] - 0s 999us/step - loss: 2.5137\n16/16 [==============================] - 0s 648us/step - loss: 2.4722\n16/16 [==============================] - 0s 843us/step - loss: 2.4209\n16/16 [==============================] - 0s 889us/step - loss: 2.3791\n16/16 [==============================] - 0s 1ms/step - loss: 2.3482\n16/16 [==============================] - 0s 1ms/step - loss: 2.3383\n16/16 [==============================] - 0s 1ms/step - loss: 2.3344\n\nTesting for epoch 50 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1411\n16/16 [==============================] - 0s 608us/step - loss: 2.0600\n16/16 [==============================] - 0s 591us/step - loss: 2.4682\n16/16 [==============================] - 0s 1ms/step - loss: 2.5009\n16/16 [==============================] - 0s 885us/step - loss: 2.4578\n16/16 [==============================] - 0s 1ms/step - loss: 2.4045\n16/16 [==============================] - 0s 918us/step - loss: 2.3612\n16/16 [==============================] - 0s 1ms/step - loss: 2.3293\n16/16 [==============================] - 0s 1ms/step - loss: 2.3193\n16/16 [==============================] - 0s 648us/step - loss: 2.3153\nEpoch 51 of 60\n\nTesting for epoch 51 index 1:\n16/16 [==============================] - 0s 658us/step - loss: 0.1459\n16/16 [==============================] - 0s 1ms/step - loss: 2.0524\n16/16 [==============================] - 0s 593us/step - loss: 2.4624\n16/16 [==============================] - 0s 1ms/step - loss: 2.4994\n16/16 [==============================] - 0s 1ms/step - loss: 2.4614\n16/16 [==============================] - 0s 647us/step - loss: 2.4144\n16/16 [==============================] - 0s 956us/step - loss: 2.3751\n16/16 [==============================] - 0s 619us/step - loss: 2.3456\n16/16 [==============================] - 0s 1ms/step - loss: 2.3363\n16/16 [==============================] - 0s 1ms/step - loss: 2.3326\n\nTesting for epoch 51 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1414\n16/16 [==============================] - 0s 691us/step - loss: 2.1374\n16/16 [==============================] - 0s 586us/step - loss: 2.5654\n16/16 [==============================] - 0s 1ms/step - loss: 2.5987\n16/16 [==============================] - 0s 601us/step - loss: 2.5514\n16/16 [==============================] - 0s 615us/step - loss: 2.4948\n16/16 [==============================] - 0s 952us/step - loss: 2.4484\n16/16 [==============================] - 0s 996us/step - loss: 2.4143\n16/16 [==============================] - 0s 1ms/step - loss: 2.4035\n16/16 [==============================] - 0s 1ms/step - loss: 2.3993\n\nTesting for epoch 51 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1408\n16/16 [==============================] - 0s 1ms/step - loss: 2.1433\n16/16 [==============================] - 0s 1ms/step - loss: 2.5741\n16/16 [==============================] - 0s 1ms/step - loss: 2.6115\n16/16 [==============================] - 0s 684us/step - loss: 2.5677\n16/16 [==============================] - 0s 566us/step - loss: 2.5132\n16/16 [==============================] - 0s 617us/step - loss: 2.4685\n16/16 [==============================] - 0s 1ms/step - loss: 2.4356\n16/16 [==============================] - 0s 1ms/step - loss: 2.4250\n16/16 [==============================] - 0s 977us/step - loss: 2.4208\n\nTesting for epoch 51 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1386\n16/16 [==============================] - 0s 626us/step - loss: 2.2159\n16/16 [==============================] - 0s 607us/step - loss: 2.6627\n16/16 [==============================] - 0s 1ms/step - loss: 2.7016\n16/16 [==============================] - 0s 616us/step - loss: 2.6579\n16/16 [==============================] - 0s 624us/step - loss: 2.6043\n16/16 [==============================] - 0s 593us/step - loss: 2.5597\n16/16 [==============================] - 0s 1ms/step - loss: 2.5264\n16/16 [==============================] - 0s 596us/step - loss: 2.5158\n16/16 [==============================] - 0s 1ms/step - loss: 2.5116\n\nTesting for epoch 51 index 5:\n16/16 [==============================] - 0s 988us/step - loss: 0.1406\n16/16 [==============================] - 0s 819us/step - loss: 2.1877\n16/16 [==============================] - 0s 1ms/step - loss: 2.6298\n16/16 [==============================] - 0s 1ms/step - loss: 2.6666\n16/16 [==============================] - 0s 1ms/step - loss: 2.6212\n16/16 [==============================] - 0s 611us/step - loss: 2.5652\n16/16 [==============================] - 0s 1ms/step - loss: 2.5194\n16/16 [==============================] - 0s 1ms/step - loss: 2.4854\n16/16 [==============================] - 0s 787us/step - loss: 2.4747\n16/16 [==============================] - 0s 1ms/step - loss: 2.4704\nEpoch 52 of 60\n\nTesting for epoch 52 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1391\n16/16 [==============================] - 0s 1ms/step - loss: 2.1726\n16/16 [==============================] - 0s 1ms/step - loss: 2.6126\n16/16 [==============================] - 0s 1ms/step - loss: 2.6499\n16/16 [==============================] - 0s 622us/step - loss: 2.6046\n16/16 [==============================] - 0s 1ms/step - loss: 2.5491\n16/16 [==============================] - 0s 602us/step - loss: 2.5038\n16/16 [==============================] - 0s 1ms/step - loss: 2.4703\n16/16 [==============================] - 0s 1ms/step - loss: 2.4596\n16/16 [==============================] - 0s 625us/step - loss: 2.4553\n\nTesting for epoch 52 index 2:\n16/16 [==============================] - 0s 722us/step - loss: 0.1402\n16/16 [==============================] - 0s 1ms/step - loss: 2.1077\n16/16 [==============================] - 0s 1ms/step - loss: 2.5214\n16/16 [==============================] - 0s 1ms/step - loss: 2.5507\n16/16 [==============================] - 0s 1ms/step - loss: 2.5038\n16/16 [==============================] - 0s 638us/step - loss: 2.4486\n16/16 [==============================] - 0s 1ms/step - loss: 2.4039\n16/16 [==============================] - 0s 1ms/step - loss: 2.3711\n16/16 [==============================] - 0s 1ms/step - loss: 2.3608\n16/16 [==============================] - 0s 806us/step - loss: 2.3567\n\nTesting for epoch 52 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1413\n16/16 [==============================] - 0s 1ms/step - loss: 2.0789\n16/16 [==============================] - 0s 635us/step - loss: 2.4879\n16/16 [==============================] - 0s 827us/step - loss: 2.5226\n16/16 [==============================] - 0s 1ms/step - loss: 2.4818\n16/16 [==============================] - 0s 1ms/step - loss: 2.4316\n16/16 [==============================] - 0s 1ms/step - loss: 2.3905\n16/16 [==============================] - 0s 1ms/step - loss: 2.3598\n16/16 [==============================] - 0s 702us/step - loss: 2.3501\n16/16 [==============================] - 0s 672us/step - loss: 2.3462\n\nTesting for epoch 52 index 4:\n16/16 [==============================] - 0s 645us/step - loss: 0.1397\n16/16 [==============================] - 0s 615us/step - loss: 2.2173\n16/16 [==============================] - 0s 828us/step - loss: 2.6506\n16/16 [==============================] - 0s 1ms/step - loss: 2.6828\n16/16 [==============================] - 0s 1ms/step - loss: 2.6351\n16/16 [==============================] - 0s 1ms/step - loss: 2.5777\n16/16 [==============================] - 0s 603us/step - loss: 2.5312\n16/16 [==============================] - 0s 586us/step - loss: 2.4972\n16/16 [==============================] - 0s 874us/step - loss: 2.4864\n16/16 [==============================] - 0s 806us/step - loss: 2.4822\n\nTesting for epoch 52 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1400\n16/16 [==============================] - 0s 1ms/step - loss: 2.1024\n16/16 [==============================] - 0s 1ms/step - loss: 2.5108\n16/16 [==============================] - 0s 1ms/step - loss: 2.5405\n16/16 [==============================] - 0s 656us/step - loss: 2.4959\n16/16 [==============================] - 0s 1ms/step - loss: 2.4414\n16/16 [==============================] - 0s 1ms/step - loss: 2.3972\n16/16 [==============================] - 0s 928us/step - loss: 2.3648\n16/16 [==============================] - 0s 1ms/step - loss: 2.3547\n16/16 [==============================] - 0s 1ms/step - loss: 2.3508\nEpoch 53 of 60\n\nTesting for epoch 53 index 1:\n16/16 [==============================] - 0s 638us/step - loss: 0.1365\n16/16 [==============================] - 0s 618us/step - loss: 2.0904\n16/16 [==============================] - 0s 1ms/step - loss: 2.4896\n16/16 [==============================] - 0s 894us/step - loss: 2.5170\n16/16 [==============================] - 0s 1ms/step - loss: 2.4731\n16/16 [==============================] - 0s 995us/step - loss: 2.4215\n16/16 [==============================] - 0s 1ms/step - loss: 2.3790\n16/16 [==============================] - 0s 625us/step - loss: 2.3476\n16/16 [==============================] - 0s 634us/step - loss: 2.3377\n16/16 [==============================] - 0s 645us/step - loss: 2.3337\n\nTesting for epoch 53 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1351\n16/16 [==============================] - 0s 1ms/step - loss: 2.1565\n16/16 [==============================] - 0s 618us/step - loss: 2.5687\n16/16 [==============================] - 0s 762us/step - loss: 2.5966\n16/16 [==============================] - 0s 1ms/step - loss: 2.5494\n16/16 [==============================] - 0s 1ms/step - loss: 2.4930\n16/16 [==============================] - 0s 648us/step - loss: 2.4471\n16/16 [==============================] - 0s 608us/step - loss: 2.4134\n16/16 [==============================] - 0s 959us/step - loss: 2.4028\n16/16 [==============================] - 0s 1ms/step - loss: 2.3986\n\nTesting for epoch 53 index 3:\n16/16 [==============================] - 0s 809us/step - loss: 0.1389\n16/16 [==============================] - 0s 1ms/step - loss: 2.1419\n16/16 [==============================] - 0s 1ms/step - loss: 2.5512\n16/16 [==============================] - 0s 677us/step - loss: 2.5797\n16/16 [==============================] - 0s 668us/step - loss: 2.5344\n16/16 [==============================] - 0s 671us/step - loss: 2.4806\n16/16 [==============================] - 0s 657us/step - loss: 2.4371\n16/16 [==============================] - 0s 1ms/step - loss: 2.4050\n16/16 [==============================] - 0s 652us/step - loss: 2.3948\n16/16 [==============================] - 0s 1ms/step - loss: 2.3908\n\nTesting for epoch 53 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1349\n16/16 [==============================] - 0s 1ms/step - loss: 2.1901\n16/16 [==============================] - 0s 716us/step - loss: 2.6044\n16/16 [==============================] - 0s 1ms/step - loss: 2.6295\n16/16 [==============================] - 0s 1ms/step - loss: 2.5800\n16/16 [==============================] - 0s 632us/step - loss: 2.5219\n16/16 [==============================] - 0s 1ms/step - loss: 2.4752\n16/16 [==============================] - 0s 1ms/step - loss: 2.4411\n16/16 [==============================] - 0s 1ms/step - loss: 2.4303\n16/16 [==============================] - 0s 608us/step - loss: 2.4260\n\nTesting for epoch 53 index 5:\n16/16 [==============================] - 0s 626us/step - loss: 0.1361\n16/16 [==============================] - 0s 1ms/step - loss: 2.1862\n16/16 [==============================] - 0s 634us/step - loss: 2.6026\n16/16 [==============================] - 0s 1ms/step - loss: 2.6289\n16/16 [==============================] - 0s 602us/step - loss: 2.5794\n16/16 [==============================] - 0s 608us/step - loss: 2.5210\n16/16 [==============================] - 0s 945us/step - loss: 2.4747\n16/16 [==============================] - 0s 1ms/step - loss: 2.4412\n16/16 [==============================] - 0s 1ms/step - loss: 2.4307\n16/16 [==============================] - 0s 1ms/step - loss: 2.4265\nEpoch 54 of 60\n\nTesting for epoch 54 index 1:\n16/16 [==============================] - 0s 591us/step - loss: 0.1354\n16/16 [==============================] - 0s 716us/step - loss: 2.1140\n16/16 [==============================] - 0s 645us/step - loss: 2.5078\n16/16 [==============================] - 0s 1ms/step - loss: 2.5307\n16/16 [==============================] - 0s 576us/step - loss: 2.4835\n16/16 [==============================] - 0s 1ms/step - loss: 2.4278\n16/16 [==============================] - 0s 1ms/step - loss: 2.3835\n16/16 [==============================] - 0s 721us/step - loss: 2.3512\n16/16 [==============================] - 0s 683us/step - loss: 2.3411\n16/16 [==============================] - 0s 973us/step - loss: 2.3371\n\nTesting for epoch 54 index 2:\n16/16 [==============================] - 0s 746us/step - loss: 0.1375\n16/16 [==============================] - 0s 1ms/step - loss: 2.1227\n16/16 [==============================] - 0s 1ms/step - loss: 2.5193\n16/16 [==============================] - 0s 1ms/step - loss: 2.5434\n16/16 [==============================] - 0s 1ms/step - loss: 2.4969\n16/16 [==============================] - 0s 928us/step - loss: 2.4432\n16/16 [==============================] - 0s 851us/step - loss: 2.4006\n16/16 [==============================] - 0s 661us/step - loss: 2.3696\n16/16 [==============================] - 0s 980us/step - loss: 2.3598\n16/16 [==============================] - 0s 1ms/step - loss: 2.3560\n\nTesting for epoch 54 index 3:\n16/16 [==============================] - 0s 939us/step - loss: 0.1385\n16/16 [==============================] - 0s 1ms/step - loss: 2.1281\n16/16 [==============================] - 0s 629us/step - loss: 2.5285\n16/16 [==============================] - 0s 1ms/step - loss: 2.5562\n16/16 [==============================] - 0s 598us/step - loss: 2.5119\n16/16 [==============================] - 0s 633us/step - loss: 2.4583\n16/16 [==============================] - 0s 638us/step - loss: 2.4151\n16/16 [==============================] - 0s 639us/step - loss: 2.3836\n16/16 [==============================] - 0s 900us/step - loss: 2.3737\n16/16 [==============================] - 0s 1ms/step - loss: 2.3698\n\nTesting for epoch 54 index 4:\n16/16 [==============================] - 0s 650us/step - loss: 0.1350\n16/16 [==============================] - 0s 636us/step - loss: 2.1239\n16/16 [==============================] - 0s 1ms/step - loss: 2.5171\n16/16 [==============================] - 0s 853us/step - loss: 2.5400\n16/16 [==============================] - 0s 596us/step - loss: 2.4914\n16/16 [==============================] - 0s 619us/step - loss: 2.4353\n16/16 [==============================] - 0s 1ms/step - loss: 2.3913\n16/16 [==============================] - 0s 1ms/step - loss: 2.3594\n16/16 [==============================] - 0s 1ms/step - loss: 2.3494\n16/16 [==============================] - 0s 665us/step - loss: 2.3455\n\nTesting for epoch 54 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1328\n16/16 [==============================] - 0s 1ms/step - loss: 2.1880\n16/16 [==============================] - 0s 1ms/step - loss: 2.5967\n16/16 [==============================] - 0s 619us/step - loss: 2.6189\n16/16 [==============================] - 0s 603us/step - loss: 2.5676\n16/16 [==============================] - 0s 1ms/step - loss: 2.5074\n16/16 [==============================] - 0s 1ms/step - loss: 2.4597\n16/16 [==============================] - 0s 1ms/step - loss: 2.4251\n16/16 [==============================] - 0s 1ms/step - loss: 2.4143\n16/16 [==============================] - 0s 1ms/step - loss: 2.4100\nEpoch 55 of 60\n\nTesting for epoch 55 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1320\n16/16 [==============================] - 0s 624us/step - loss: 2.2799\n16/16 [==============================] - 0s 824us/step - loss: 2.7184\n16/16 [==============================] - 0s 644us/step - loss: 2.7484\n16/16 [==============================] - 0s 655us/step - loss: 2.6979\n16/16 [==============================] - 0s 690us/step - loss: 2.6378\n16/16 [==============================] - 0s 1ms/step - loss: 2.5895\n16/16 [==============================] - 0s 637us/step - loss: 2.5547\n16/16 [==============================] - 0s 806us/step - loss: 2.5438\n16/16 [==============================] - 0s 757us/step - loss: 2.5395\n\nTesting for epoch 55 index 2:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1319\n16/16 [==============================] - 0s 1ms/step - loss: 2.2685\n16/16 [==============================] - 0s 1ms/step - loss: 2.6951\n16/16 [==============================] - 0s 1ms/step - loss: 2.7210\n16/16 [==============================] - 0s 1ms/step - loss: 2.6693\n16/16 [==============================] - 0s 905us/step - loss: 2.6086\n16/16 [==============================] - 0s 985us/step - loss: 2.5598\n16/16 [==============================] - 0s 897us/step - loss: 2.5247\n16/16 [==============================] - 0s 1ms/step - loss: 2.5138\n16/16 [==============================] - 0s 726us/step - loss: 2.5094\n\nTesting for epoch 55 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1306\n16/16 [==============================] - 0s 628us/step - loss: 2.1294\n16/16 [==============================] - 0s 571us/step - loss: 2.5162\n16/16 [==============================] - 0s 651us/step - loss: 2.5364\n16/16 [==============================] - 0s 657us/step - loss: 2.4872\n16/16 [==============================] - 0s 620us/step - loss: 2.4317\n16/16 [==============================] - 0s 627us/step - loss: 2.3874\n16/16 [==============================] - 0s 631us/step - loss: 2.3554\n16/16 [==============================] - 0s 911us/step - loss: 2.3455\n16/16 [==============================] - 0s 1ms/step - loss: 2.3416\n\nTesting for epoch 55 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1298\n16/16 [==============================] - 0s 633us/step - loss: 2.2952\n16/16 [==============================] - 0s 1ms/step - loss: 2.7216\n16/16 [==============================] - 0s 1ms/step - loss: 2.7446\n16/16 [==============================] - 0s 1ms/step - loss: 2.6897\n16/16 [==============================] - 0s 611us/step - loss: 2.6254\n16/16 [==============================] - 0s 1ms/step - loss: 2.5745\n16/16 [==============================] - 0s 1ms/step - loss: 2.5375\n16/16 [==============================] - 0s 1ms/step - loss: 2.5260\n16/16 [==============================] - 0s 935us/step - loss: 2.5215\n\nTesting for epoch 55 index 5:\n16/16 [==============================] - 0s 604us/step - loss: 0.1304\n16/16 [==============================] - 0s 1ms/step - loss: 2.1990\n16/16 [==============================] - 0s 598us/step - loss: 2.6016\n16/16 [==============================] - 0s 1ms/step - loss: 2.6241\n16/16 [==============================] - 0s 1ms/step - loss: 2.5736\n16/16 [==============================] - 0s 753us/step - loss: 2.5147\n16/16 [==============================] - 0s 1ms/step - loss: 2.4677\n16/16 [==============================] - 0s 626us/step - loss: 2.4338\n16/16 [==============================] - 0s 718us/step - loss: 2.4233\n16/16 [==============================] - 0s 852us/step - loss: 2.4191\nEpoch 56 of 60\n\nTesting for epoch 56 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1279\n16/16 [==============================] - 0s 629us/step - loss: 2.1722\n16/16 [==============================] - 0s 1ms/step - loss: 2.5696\n16/16 [==============================] - 0s 1ms/step - loss: 2.5888\n16/16 [==============================] - 0s 687us/step - loss: 2.5353\n16/16 [==============================] - 0s 644us/step - loss: 2.4735\n16/16 [==============================] - 0s 1ms/step - loss: 2.4255\n16/16 [==============================] - 0s 1ms/step - loss: 2.3910\n16/16 [==============================] - 0s 1ms/step - loss: 2.3803\n16/16 [==============================] - 0s 1ms/step - loss: 2.3761\n\nTesting for epoch 56 index 2:\n16/16 [==============================] - 0s 725us/step - loss: 0.1313\n16/16 [==============================] - 0s 1ms/step - loss: 2.1862\n16/16 [==============================] - 0s 644us/step - loss: 2.5858\n16/16 [==============================] - 0s 1ms/step - loss: 2.6081\n16/16 [==============================] - 0s 1ms/step - loss: 2.5577\n16/16 [==============================] - 0s 1ms/step - loss: 2.4986\n16/16 [==============================] - 0s 1ms/step - loss: 2.4523\n16/16 [==============================] - 0s 1ms/step - loss: 2.4191\n16/16 [==============================] - 0s 708us/step - loss: 2.4087\n16/16 [==============================] - 0s 632us/step - loss: 2.4046\n\nTesting for epoch 56 index 3:\n16/16 [==============================] - 0s 620us/step - loss: 0.1281\n16/16 [==============================] - 0s 632us/step - loss: 2.2017\n16/16 [==============================] - 0s 607us/step - loss: 2.5988\n16/16 [==============================] - 0s 573us/step - loss: 2.6178\n16/16 [==============================] - 0s 653us/step - loss: 2.5652\n16/16 [==============================] - 0s 1ms/step - loss: 2.5044\n16/16 [==============================] - 0s 808us/step - loss: 2.4563\n16/16 [==============================] - 0s 648us/step - loss: 2.4216\n16/16 [==============================] - 0s 724us/step - loss: 2.4108\n16/16 [==============================] - 0s 727us/step - loss: 2.4066\n\nTesting for epoch 56 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1283\n16/16 [==============================] - 0s 643us/step - loss: 2.2432\n16/16 [==============================] - 0s 1ms/step - loss: 2.6497\n16/16 [==============================] - 0s 612us/step - loss: 2.6688\n16/16 [==============================] - 0s 1ms/step - loss: 2.6142\n16/16 [==============================] - 0s 837us/step - loss: 2.5514\n16/16 [==============================] - 0s 836us/step - loss: 2.5023\n16/16 [==============================] - 0s 1ms/step - loss: 2.4675\n16/16 [==============================] - 0s 641us/step - loss: 2.4567\n16/16 [==============================] - 0s 1ms/step - loss: 2.4524\n\nTesting for epoch 56 index 5:\n16/16 [==============================] - 0s 876us/step - loss: 0.1307\n16/16 [==============================] - 0s 1ms/step - loss: 2.2389\n16/16 [==============================] - 0s 1ms/step - loss: 2.6476\n16/16 [==============================] - 0s 1ms/step - loss: 2.6694\n16/16 [==============================] - 0s 646us/step - loss: 2.6174\n16/16 [==============================] - 0s 651us/step - loss: 2.5567\n16/16 [==============================] - 0s 630us/step - loss: 2.5086\n16/16 [==============================] - 0s 645us/step - loss: 2.4744\n16/16 [==============================] - 0s 1ms/step - loss: 2.4637\n16/16 [==============================] - 0s 634us/step - loss: 2.4595\nEpoch 57 of 60\n\nTesting for epoch 57 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1294\n16/16 [==============================] - 0s 1ms/step - loss: 2.2039\n16/16 [==============================] - 0s 1ms/step - loss: 2.6016\n16/16 [==============================] - 0s 1ms/step - loss: 2.6199\n16/16 [==============================] - 0s 618us/step - loss: 2.5666\n16/16 [==============================] - 0s 1ms/step - loss: 2.5066\n16/16 [==============================] - 0s 1ms/step - loss: 2.4594\n16/16 [==============================] - 0s 1ms/step - loss: 2.4255\n16/16 [==============================] - 0s 967us/step - loss: 2.4150\n16/16 [==============================] - 0s 594us/step - loss: 2.4109\n\nTesting for epoch 57 index 2:\n16/16 [==============================] - 0s 638us/step - loss: 0.1284\n16/16 [==============================] - 0s 633us/step - loss: 2.2483\n16/16 [==============================] - 0s 996us/step - loss: 2.6576\n16/16 [==============================] - 0s 1ms/step - loss: 2.6801\n16/16 [==============================] - 0s 610us/step - loss: 2.6274\n16/16 [==============================] - 0s 1ms/step - loss: 2.5661\n16/16 [==============================] - 0s 1ms/step - loss: 2.5171\n16/16 [==============================] - 0s 905us/step - loss: 2.4817\n16/16 [==============================] - 0s 1ms/step - loss: 2.4707\n16/16 [==============================] - 0s 1ms/step - loss: 2.4664\n\nTesting for epoch 57 index 3:\n16/16 [==============================] - 0s 629us/step - loss: 0.1308\n16/16 [==============================] - 0s 1ms/step - loss: 2.2241\n16/16 [==============================] - 0s 776us/step - loss: 2.6243\n16/16 [==============================] - 0s 1ms/step - loss: 2.6430\n16/16 [==============================] - 0s 1ms/step - loss: 2.5906\n16/16 [==============================] - 0s 647us/step - loss: 2.5309\n16/16 [==============================] - 0s 632us/step - loss: 2.4834\n16/16 [==============================] - 0s 617us/step - loss: 2.4493\n16/16 [==============================] - 0s 622us/step - loss: 2.4387\n16/16 [==============================] - 0s 619us/step - loss: 2.4346\n\nTesting for epoch 57 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1249\n16/16 [==============================] - 0s 1ms/step - loss: 2.2824\n16/16 [==============================] - 0s 886us/step - loss: 2.6944\n16/16 [==============================] - 0s 1ms/step - loss: 2.7119\n16/16 [==============================] - 0s 840us/step - loss: 2.6561\n16/16 [==============================] - 0s 1ms/step - loss: 2.5941\n16/16 [==============================] - 0s 749us/step - loss: 2.5456\n16/16 [==============================] - 0s 633us/step - loss: 2.5112\n16/16 [==============================] - 0s 631us/step - loss: 2.5004\n16/16 [==============================] - 0s 598us/step - loss: 2.4962\n\nTesting for epoch 57 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1268\n16/16 [==============================] - 0s 641us/step - loss: 2.2303\n16/16 [==============================] - 0s 987us/step - loss: 2.6315\n16/16 [==============================] - 0s 755us/step - loss: 2.6520\n16/16 [==============================] - 0s 1ms/step - loss: 2.6023\n16/16 [==============================] - 0s 1ms/step - loss: 2.5447\n16/16 [==============================] - 0s 1ms/step - loss: 2.4980\n16/16 [==============================] - 0s 1ms/step - loss: 2.4644\n16/16 [==============================] - 0s 1ms/step - loss: 2.4540\n16/16 [==============================] - 0s 1ms/step - loss: 2.4499\nEpoch 58 of 60\n\nTesting for epoch 58 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1253\n16/16 [==============================] - 0s 1ms/step - loss: 2.2804\n16/16 [==============================] - 0s 1ms/step - loss: 2.6909\n16/16 [==============================] - 0s 1ms/step - loss: 2.7047\n16/16 [==============================] - 0s 1ms/step - loss: 2.6461\n16/16 [==============================] - 0s 890us/step - loss: 2.5810\n16/16 [==============================] - 0s 1ms/step - loss: 2.5304\n16/16 [==============================] - 0s 1ms/step - loss: 2.4944\n16/16 [==============================] - 0s 1ms/step - loss: 2.4833\n16/16 [==============================] - 0s 1ms/step - loss: 2.4788\n\nTesting for epoch 58 index 2:\n16/16 [==============================] - 0s 749us/step - loss: 0.1268\n16/16 [==============================] - 0s 1ms/step - loss: 2.2590\n16/16 [==============================] - 0s 613us/step - loss: 2.6587\n16/16 [==============================] - 0s 617us/step - loss: 2.6734\n16/16 [==============================] - 0s 1ms/step - loss: 2.6186\n16/16 [==============================] - 0s 1ms/step - loss: 2.5570\n16/16 [==============================] - 0s 950us/step - loss: 2.5081\n16/16 [==============================] - 0s 1ms/step - loss: 2.4730\n16/16 [==============================] - 0s 661us/step - loss: 2.4621\n16/16 [==============================] - 0s 623us/step - loss: 2.4579\n\nTesting for epoch 58 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1254\n16/16 [==============================] - 0s 854us/step - loss: 2.2600\n16/16 [==============================] - 0s 1ms/step - loss: 2.6602\n16/16 [==============================] - 0s 655us/step - loss: 2.6753\n16/16 [==============================] - 0s 1ms/step - loss: 2.6206\n16/16 [==============================] - 0s 1ms/step - loss: 2.5580\n16/16 [==============================] - 0s 706us/step - loss: 2.5089\n16/16 [==============================] - 0s 1ms/step - loss: 2.4739\n16/16 [==============================] - 0s 1ms/step - loss: 2.4630\n16/16 [==============================] - 0s 1ms/step - loss: 2.4588\n\nTesting for epoch 58 index 4:\n16/16 [==============================] - 0s 681us/step - loss: 0.1267\n16/16 [==============================] - 0s 775us/step - loss: 2.2358\n16/16 [==============================] - 0s 608us/step - loss: 2.6324\n16/16 [==============================] - 0s 628us/step - loss: 2.6479\n16/16 [==============================] - 0s 610us/step - loss: 2.5940\n16/16 [==============================] - 0s 608us/step - loss: 2.5341\n16/16 [==============================] - 0s 637us/step - loss: 2.4871\n16/16 [==============================] - 0s 611us/step - loss: 2.4534\n16/16 [==============================] - 0s 641us/step - loss: 2.4430\n16/16 [==============================] - 0s 763us/step - loss: 2.4389\n\nTesting for epoch 58 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1232\n16/16 [==============================] - 0s 1ms/step - loss: 2.3007\n16/16 [==============================] - 0s 1ms/step - loss: 2.7090\n16/16 [==============================] - 0s 631us/step - loss: 2.7214\n16/16 [==============================] - 0s 973us/step - loss: 2.6639\n16/16 [==============================] - 0s 667us/step - loss: 2.5993\n16/16 [==============================] - 0s 1ms/step - loss: 2.5482\n16/16 [==============================] - 0s 1ms/step - loss: 2.5115\n16/16 [==============================] - 0s 1ms/step - loss: 2.5002\n16/16 [==============================] - 0s 1ms/step - loss: 2.4957\nEpoch 59 of 60\n\nTesting for epoch 59 index 1:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1249\n16/16 [==============================] - 0s 703us/step - loss: 2.2634\n16/16 [==============================] - 0s 1ms/step - loss: 2.6562\n16/16 [==============================] - 0s 1ms/step - loss: 2.6629\n16/16 [==============================] - 0s 1ms/step - loss: 2.6041\n16/16 [==============================] - 0s 872us/step - loss: 2.5390\n16/16 [==============================] - 0s 617us/step - loss: 2.4883\n16/16 [==============================] - 0s 687us/step - loss: 2.4527\n16/16 [==============================] - 0s 615us/step - loss: 2.4419\n16/16 [==============================] - 0s 783us/step - loss: 2.4376\n\nTesting for epoch 59 index 2:\n16/16 [==============================] - 0s 700us/step - loss: 0.1249\n16/16 [==============================] - 0s 601us/step - loss: 2.2865\n16/16 [==============================] - 0s 625us/step - loss: 2.6880\n16/16 [==============================] - 0s 581us/step - loss: 2.7004\n16/16 [==============================] - 0s 1ms/step - loss: 2.6446\n16/16 [==============================] - 0s 714us/step - loss: 2.5815\n16/16 [==============================] - 0s 1ms/step - loss: 2.5315\n16/16 [==============================] - 0s 1ms/step - loss: 2.4957\n16/16 [==============================] - 0s 740us/step - loss: 2.4847\n16/16 [==============================] - 0s 1ms/step - loss: 2.4803\n\nTesting for epoch 59 index 3:\n16/16 [==============================] - 0s 584us/step - loss: 0.1245\n16/16 [==============================] - 0s 787us/step - loss: 2.2729\n16/16 [==============================] - 0s 1ms/step - loss: 2.6637\n16/16 [==============================] - 0s 1ms/step - loss: 2.6712\n16/16 [==============================] - 0s 608us/step - loss: 2.6147\n16/16 [==============================] - 0s 654us/step - loss: 2.5526\n16/16 [==============================] - 0s 1ms/step - loss: 2.5033\n16/16 [==============================] - 0s 891us/step - loss: 2.4684\n16/16 [==============================] - 0s 697us/step - loss: 2.4576\n16/16 [==============================] - 0s 639us/step - loss: 2.4534\n\nTesting for epoch 59 index 4:\n16/16 [==============================] - 0s 637us/step - loss: 0.1250\n16/16 [==============================] - 0s 941us/step - loss: 2.2686\n16/16 [==============================] - 0s 599us/step - loss: 2.6612\n16/16 [==============================] - 0s 1ms/step - loss: 2.6698\n16/16 [==============================] - 0s 778us/step - loss: 2.6129\n16/16 [==============================] - 0s 691us/step - loss: 2.5509\n16/16 [==============================] - 0s 1ms/step - loss: 2.5026\n16/16 [==============================] - 0s 623us/step - loss: 2.4683\n16/16 [==============================] - 0s 885us/step - loss: 2.4578\n16/16 [==============================] - 0s 701us/step - loss: 2.4537\n\nTesting for epoch 59 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1234\n16/16 [==============================] - 0s 1ms/step - loss: 2.3238\n16/16 [==============================] - 0s 803us/step - loss: 2.7242\n16/16 [==============================] - 0s 601us/step - loss: 2.7327\n16/16 [==============================] - 0s 647us/step - loss: 2.6743\n16/16 [==============================] - 0s 970us/step - loss: 2.6098\n16/16 [==============================] - 0s 926us/step - loss: 2.5591\n16/16 [==============================] - 0s 1ms/step - loss: 2.5229\n16/16 [==============================] - 0s 1ms/step - loss: 2.5118\n16/16 [==============================] - 0s 678us/step - loss: 2.5074\nEpoch 60 of 60\n\nTesting for epoch 60 index 1:\n16/16 [==============================] - 0s 642us/step - loss: 0.1248\n16/16 [==============================] - 0s 597us/step - loss: 2.2835\n16/16 [==============================] - 0s 629us/step - loss: 2.6774\n16/16 [==============================] - 0s 1ms/step - loss: 2.6863\n16/16 [==============================] - 0s 644us/step - loss: 2.6273\n16/16 [==============================] - 0s 1ms/step - loss: 2.5622\n16/16 [==============================] - 0s 956us/step - loss: 2.5114\n16/16 [==============================] - 0s 664us/step - loss: 2.4755\n16/16 [==============================] - 0s 875us/step - loss: 2.4643\n16/16 [==============================] - 0s 593us/step - loss: 2.4600\n\nTesting for epoch 60 index 2:\n16/16 [==============================] - 0s 716us/step - loss: 0.1237\n16/16 [==============================] - 0s 626us/step - loss: 2.2727\n16/16 [==============================] - 0s 1ms/step - loss: 2.6608\n16/16 [==============================] - 0s 1ms/step - loss: 2.6688\n16/16 [==============================] - 0s 1ms/step - loss: 2.6110\n16/16 [==============================] - 0s 919us/step - loss: 2.5473\n16/16 [==============================] - 0s 1ms/step - loss: 2.4975\n16/16 [==============================] - 0s 780us/step - loss: 2.4622\n16/16 [==============================] - 0s 716us/step - loss: 2.4513\n16/16 [==============================] - 0s 638us/step - loss: 2.4470\n\nTesting for epoch 60 index 3:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1213\n16/16 [==============================] - 0s 616us/step - loss: 2.3241\n16/16 [==============================] - 0s 618us/step - loss: 2.7152\n16/16 [==============================] - 0s 1ms/step - loss: 2.7211\n16/16 [==============================] - 0s 1ms/step - loss: 2.6613\n16/16 [==============================] - 0s 631us/step - loss: 2.5967\n16/16 [==============================] - 0s 741us/step - loss: 2.5461\n16/16 [==============================] - 0s 633us/step - loss: 2.5105\n16/16 [==============================] - 0s 2ms/step - loss: 2.4996\n16/16 [==============================] - 0s 599us/step - loss: 2.4952\n\nTesting for epoch 60 index 4:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1217\n16/16 [==============================] - 0s 1ms/step - loss: 2.3310\n16/16 [==============================] - 0s 1ms/step - loss: 2.7333\n16/16 [==============================] - 0s 1ms/step - loss: 2.7466\n16/16 [==============================] - 0s 1ms/step - loss: 2.6903\n16/16 [==============================] - 0s 833us/step - loss: 2.6273\n16/16 [==============================] - 0s 608us/step - loss: 2.5776\n16/16 [==============================] - 0s 1ms/step - loss: 2.5424\n16/16 [==============================] - 0s 626us/step - loss: 2.5314\n16/16 [==============================] - 0s 709us/step - loss: 2.5270\n\nTesting for epoch 60 index 5:\n16/16 [==============================] - 0s 1ms/step - loss: 0.1211\n16/16 [==============================] - 0s 1ms/step - loss: 2.2989\n16/16 [==============================] - 0s 1ms/step - loss: 2.6855\n16/16 [==============================] - 0s 1ms/step - loss: 2.6923\n16/16 [==============================] - 0s 1ms/step - loss: 2.6322\n16/16 [==============================] - 0s 650us/step - loss: 2.5665\n16/16 [==============================] - 0s 1ms/step - loss: 2.5155\n16/16 [==============================] - 0s 1ms/step - loss: 2.4800\n16/16 [==============================] - 0s 844us/step - loss: 2.4690\n16/16 [==============================] - 0s 593us/step - loss: 2.4647\n\n\n\noutlier_MO_GAAL_one = list(clf.labels_)\n\n\noutlier_MO_GAAL_one = list(map(lambda x: 1 if x==0  else -1,outlier_MO_GAAL_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_MO_GAAL_one,tab_bunny)\n\n\n_conf.conf(\"MO-GAAL (Liu et al., 2019)\")\n\n\n\n\nAccuracy: 0.952\nPrecision: 0.952\nRecall: 1.000\nF1 Score: 0.975\n\n\n\nthirteen = twelve.append(_conf.tab)\n\n\n\nLSCP\n\ndetectors = [KNN(), LOF(), OCSVM()]\nclf = LSCP(detectors)\nclf.fit(_df[['x', 'y','fnoise']])\n_df['LSCP_clf'] = clf.labels_\n\n/home/csy/anaconda3/envs/csy/lib/python3.8/site-packages/pyod/models/lscp.py:382: UserWarning: The number of histogram bins is greater than the number of classifiers, reducing n_bins to n_clf.\n  warnings.warn(\n\n\n\noutlier_LSCP_one = list(clf.labels_)\n\n\noutlier_LSCP_one = list(map(lambda x: 1 if x==0  else -1,outlier_LSCP_one))\n\n\n_conf = Conf_matrx(outlier_true_one_2,outlier_LSCP_one,tab_bunny)\n\n\n_conf.conf(\"LSCP (Zhao et al., 2019)\")\n\n\n\n\nAccuracy: 0.940\nPrecision: 0.996\nRecall: 0.941\nF1 Score: 0.967\n\n\n\nfourteen_bunny = thirteen.append(_conf.tab)"
  },
  {
    "objectID": "posts/GODE/2022-11-19-class_code_for_paper.html#bunny-result",
    "href": "posts/GODE/2022-11-19-class_code_for_paper.html#bunny-result",
    "title": "Class code for Comparison Study",
    "section": "Bunny Result",
    "text": "Bunny Result\n\nround(fourteen_bunny,4)\n\n\n\n\n\n  \n    \n      \n      Accuracy\n      Precision\n      Recall\n      F1\n    \n  \n  \n    \n      GODE\n      0.9948\n      0.9954\n      0.9992\n      0.9973\n    \n    \n      LOF (Breunig et al., 2000)\n      0.9285\n      0.9569\n      0.9685\n      0.9627\n    \n    \n      kNN (Ramaswamy et al., 2000)\n      0.9405\n      0.9960\n      0.9413\n      0.9679\n    \n    \n      CBLOF (He et al., 2003)\n      0.9776\n      0.9895\n      0.9870\n      0.9882\n    \n    \n      OCSVM (Sch Ìˆolkopf et al., 2001)\n      0.9321\n      0.9911\n      0.9371\n      0.9633\n    \n    \n      MCD (Hardin and Rocke, 2004)\n      0.9349\n      0.9929\n      0.9383\n      0.9648\n    \n    \n      Feature Bagging (Lazarevic and Kumar, 2005)\n      0.9149\n      0.9818\n      0.9278\n      0.9540\n    \n    \n      ABOD (Kriegel et al., 2008)\n      0.9768\n      0.9891\n      0.9866\n      0.9878\n    \n    \n      Isolation Forest (Liu et al., 2008)\n      0.7942\n      0.9947\n      0.7881\n      0.8794\n    \n    \n      HBOS (Goldstein and Dengel, 2012)\n      0.8953\n      0.9695\n      0.9190\n      0.9436\n    \n    \n      SOS (Janssens et al., 2012)\n      0.8953\n      0.9695\n      0.9190\n      0.9436\n    \n    \n      SO-GAAL (Liu et al., 2019)\n      0.9521\n      0.9521\n      1.0000\n      0.9754\n    \n    \n      MO-GAAL (Liu et al., 2019)\n      0.9521\n      0.9521\n      1.0000\n      0.9754\n    \n    \n      LSCP (Zhao et al., 2019)\n      0.9397\n      0.9956\n      0.9408\n      0.9674"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html",
    "href": "posts/GODE/2022-09-02-paper_simulation.html",
    "title": "Simulation",
    "section": "",
    "text": "Simulation"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#imports",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#imports",
    "title": "Simulation",
    "section": "imports",
    "text": "imports\n\nimport rpy2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport plotly.graph_objects as go\n\n\nimport tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport plotly.express as px\nimport warnings\nwarnings.simplefilter(\"ignore\", np.ComplexWarning)\nfrom haversine import haversine\nfrom IPython.display import HTML\n\n\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr\n\n\nfrom plotly.subplots import make_subplots"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#ebayesthresh",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#ebayesthresh",
    "title": "Simulation",
    "section": "EbayesThresh",
    "text": "EbayesThresh\n\n%load_ext rpy2.ipython\n\n\n%%R\nlibrary(EbayesThresh)\nset.seed(1)\nx <- rnorm(1000) + sample(c( runif(25,-7,7), rep(0,975)))\n#plot(x,type='l')\n#mu <- EbayesThresh::ebayesthresh(x,sdev=2)\n#lines(mu,col=2,lty=2,lwd=2)\n\n\nR + python\n- Rí™˜ê²½ì— ìˆë˜ xë¥¼ ê°€ì§€ê³  ì˜¤ê¸°\n\n%R -o x \n\n- Rí™˜ê²½ì— ìˆëŠ” ebayesthresh í•¨ìˆ˜ë¥¼ ê°€ì§€ê³  ì˜¤ê¸°\n\nebayesthresh = importr('EbayesThresh').ebayesthresh\n\n\nxhat = np.array(ebayesthresh(FloatVector(x)))\n\n\n#plt.plot(x)\n#plt.plot(xhat)"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#ì‹œë„-1",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#ì‹œë„-1",
    "title": "Simulation",
    "section": "ì‹œë„ 1",
    "text": "ì‹œë„ 1\n\n_x = np.linspace(0,2,1000)\n_y1 = 5*_x\n_y = _y1 + x # x is epsilon\n\n\ndf1=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\nw=np.zeros((1000,1000))\n\n\nfor i in range(1000):\n    for j in range(1000):\n        if i==j :\n            w[i,j] = 0\n        elif np.abs(i-j) <= 1 : \n            w[i,j] = 1\n\n\nclass SIMUL:\n    def __init__(self,df):\n        self.df = df\n        self.y = df.y.to_numpy()\n        self.y1 = df.y1.to_numpy()\n        self.x = df.x.to_numpy()\n        self.n = len(self.y)\n        self.W = w\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)      \n    def fit(self,sd=5): # fit with ebayesthresh\n        self._eigen()\n        self.ybar = self.Psi.T @ self.y # fbar := graph fourier transform of f\n        self.power = self.ybar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.ybar**2),sd=sd))\n        self.ybar_threshed = np.where(self.power_threshed>0,self.ybar,0)\n        self.yhat = self.Psi@self.ybar_threshed\n        self.df = self.df.assign(yHat = self.yhat)\n        self.df = self.df.assign(Residual = self.df.y- self.df.yHat)\n        self.differ=(np.abs(self.y-self.yhat)-np.min(np.abs(self.y-self.yhat)))/(np.max(np.abs(self.y-self.yhat))-np.min(np.abs(self.y-self.yhat))) #color í‘œí˜„ì€ ìœ„í•¸ í‘œì¤€í™”\n        self.df = self.df.assign(differ = self.differ)\n        #with plt.style.context('seaborn-dark'):\n            #plt.figure(figsize=(16,10))\n            #plt.scatter(self.x,self.y,c=self.differ3,cmap='Purples',s=50)\n            #plt.plot(self.x,self.yhat, 'k--')\n    def vis(self,ref=60):\n        fig = go.Figure()\n        fig.add_scatter(x=self.x,y=self.y,mode=\"markers\",marker=dict(size=2, color=\"#9fc5e8\"),name='y',opacity=0.7)\n        fig.add_scatter(x=self.x,y=self.yhat,mode=\"markers\",marker=dict(size=2, color=\"#000000\"),name='yhat',opacity=0.7)\n        fig.add_scatter(x=self.df.query('Residual**2>@ref')['x'],y=self.df.query('Residual**2>@ref')['y'],mode=\"markers\",marker=dict(size=3, color=\"#f20505\"),name='R square',opacity=1)\n        fig.add_trace(go.Scatter(x=self.x,y=self.y1,mode='lines',line_color='#0000FF',name='underline'))\n        fig.update_layout(width=1000,height=1000,autosize=False,margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n    def sub(self):\n        fig, axs = plt.subplots(2,2,figsize=(16,10))\n\n        axs[0,0].plot(self.power)\n        axs[0,0].plot(self.power_threshed)\n        axs[0,0].set_title('power_threshed')\n\n        axs[0,1].plot(self.power[1:])\n        axs[0,1].plot(self.power_threshed[1:])\n        axs[0,1].set_title('power_threshed 1:')\n\n        axs[1,0].plot(self.power[2:])\n        axs[1,0].plot(self.power_threshed[2:])\n        axs[1,0].set_title('power_threshed 2:')\n\n        axs[1,1].plot((self.df.Residual)**2)\n        axs[1,1].set_title('Residual square')\n\n        plt.tight_layout()\n        plt.show()\n    def subvis(self,ref=60):\n        fig = make_subplots(rows=2, cols=2, subplot_titles=(\"y\", \"yhat\", \"Residual Square\", \"Graph\"))\n                            \n        fig.add_scatter(x=self.x,y=self.y, mode=\"markers\",marker=dict(size=3, color=\"#9fc5e8\"),name='y',opacity=0.7,row=1,col=1)\n        fig.add_trace(go.Scatter(x=self.x,y=self.y1,mode='lines',line_color='#000000',name='underline'),row=1,col=1)\n        \n        fig.add_scatter(x=self.x,y=self.yhat, mode=\"markers\",marker=dict(size=3, color=\"#999999\"),name='yhat',opacity=0.7,row=1,col=2)\n        fig.add_trace(go.Scatter(x=self.x,y=self.y1,mode='lines',line_color='#000000',name='underline'),row=1,col=2)\n        \n        fig.add_scatter(x=self.df.query('Residual**2>@ref')['x'],y=self.df.query('Residual**2>@ref')['y'], mode=\"markers\",marker=dict(size=3, color=\"#f20505\"),name='R square',opacity=1,row=2,col=1)\n        fig.add_trace(go.Scatter(x=self.x,y=self.y1,mode='lines',line_color='#000000',name='underline'),row=2,col=1)\n        \n        fig.add_scatter(x=self.x,y=self.y, mode=\"markers\",marker=dict(size=3, color=\"#9fc5e8\"),name='y',opacity=0.7,row=2,col=2)        \n        fig.add_scatter(x=self.x,y=self.yhat, mode=\"markers\",marker=dict(size=3, color=\"#999999\"),name='yhat',opacity=0.7,row=2,col=2)        \n        fig.add_scatter(x=self.df.query('Residual**2>@ref')['x'],y=self.df.query('Residual**2>@ref')['y'], mode=\"markers\",marker=dict(size=3, color=\"#f20505\"),name='R square',opacity=1,row=2,col=2)\n        fig.add_trace(go.Scatter(x=self.x,y=self.y1,mode='lines',line_color='#000000',name='underline'),row=2,col=2)\n        \n        fig.update_xaxes(range=[0, 2], row=1, col=1)\n        fig.update_yaxes(range=[-5, 15], row=1, col=1)\n        \n        fig.update_xaxes(range=[0, 2], row=1, col=2)\n        fig.update_yaxes(range=[-5, 15], row=1, col=2)\n        \n        fig.update_xaxes(range=[0, 2], row=2, col=1)\n        fig.update_yaxes(range=[-5, 15], row=2, col=1)\n        \n        fig.update_xaxes(range=[0, 2], row=2, col=2)\n        fig.update_yaxes(range=[-5, 15], row=2, col=2)\n        \n        fig.update_layout(width=1000,height=1000,autosize=False,showlegend=False,title_text=\"The result\")\n        \n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n\n\nclass SIMUL2(SIMUL):\n    def fit2(self,sd=5,ref=60,cuts=0,cutf=995):\n        self.fit()\n        with plt.style.context('seaborn-dark'):\n            plt.figure(figsize=(16,10))\n            plt.scatter(self.x,self.y,c=self.differ3,cmap='Purples',s=50)\n            plt.scatter(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],color='red',s=50)\n            plt.plot(self.x,self.y1,'b--')\n            plt.plot(self.x[cuts:cutf],self.yhat[cuts:cutf], 'k--')\n    def fit3(self,sd=5,ref=30,ymin=-5,ymax=20,cuts=0,cutf=995):\n        self.fit()\n        with plt.style.context('seaborn-dark'):\n            fig, axs = plt.subplots(2,2,figsize=(16,10))\n\n            axs[0,0].scatter(self.x,self.y,c=self.differ,cmap='Purples',s=50)\n            axs[0,0].set_title('y')\n            axs[0,0].set_ylim([ymin,ymax])\n            \n\n            axs[0,1].plot(self.x[cuts:cutf],self.yhat[cuts:cutf], 'k')\n            axs[0,1].plot(self.x[cuts:cutf],self.y1[cuts:cutf], 'b',alpha=0.5)\n            axs[0,1].set_title('yhat')\n            axs[0,1].set_ylim([ymin,ymax])\n\n            axs[1,0].scatter(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],color='red',s=50,marker='*')\n            axs[1,0].plot(self.x[cuts:cutf],self.y1[cuts:cutf], 'b',alpha=0.5)\n            axs[1,0].set_title('Residual square')\n            axs[1,0].set_ylim([ymin,ymax])\n\n            axs[1,1].scatter(self.x,self.y,c=self.differ,cmap='Purples',s=50)\n            axs[1,1].plot(self.x[cuts:cutf],self.yhat[cuts:cutf], 'k')\n            axs[1,1].scatter(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],color='red',s=50,marker='*')\n            axs[1,1].set_title('Graph')\n            axs[1,1].set_ylim([ymin,ymax])\n\n            plt.tight_layout()\n            plt.show()\n\n\n_simul = SIMUL2(df1)\n\n\n_simul.fit3(sd=5,ref=20,ymin=-10,ymax=15)\n\n\n\n\n\n#_simul.vis()\n\n\n_simul.sub()\n\n\n\n\n\n#_simul.subvis()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#ì‹œë„-2",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#ì‹œë„-2",
    "title": "Simulation",
    "section": "ì‹œë„ 2",
    "text": "ì‹œë„ 2\n\n_x = np.linspace(0,2,1000)\n_y1 = 5*_x**2\n_y = _y1 + x # x is epsilon\n\n\ndf2=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul = SIMUL2(df2)\n\n\n_simul.fit3(sd=6,ref=20,ymin=-10,ymax=25)\n\n\n\n\n\n#_simul.vis()\n\n\n_simul.sub()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#ì‹œë„-3",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#ì‹œë„-3",
    "title": "Simulation",
    "section": "ì‹œë„ 3",
    "text": "ì‹œë„ 3\n\n_x = np.linspace(0,2,1000)\n_y1 = 5*_x**3 \n_y = _y1 + x # x is epsilon\n\n\ndf3=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul = SIMUL2(df3)\n\n\n_simul.fit3(ymin=-10,ymax=45)\n\n\n\n\n\n#_simul.vis()\n\n\n_simul.sub()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#ì‹œë„-4",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#ì‹œë„-4",
    "title": "Simulation",
    "section": "ì‹œë„ 4",
    "text": "ì‹œë„ 4\n\n_x = np.linspace(0,2,1000)\n_y1 = -2+ 3*np.cos(_x) + 1*np.cos(2*_x) + 5*np.cos(5*_x)\n_y = _y1 + x\n\n\n# _x = np.linspace(0,2,1000)\n# _y1 = 5*np.sin(_x) \n# _y = _y1 + x # x is epsilon\n\n\ndf4=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul = SIMUL2(df4)\n\n\n_simul.fit3(ref=10,ymin=-15,ymax=10)\n\n\n\n\n\n#_simul.vis()\n\n\n_simul.sub()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#ì‹œë„-5",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#ì‹œë„-5",
    "title": "Simulation",
    "section": "ì‹œë„ 5",
    "text": "ì‹œë„ 5\n\n# _x = np.linspace(0,2,1000)\n# _y1 =  3*np.cos(_x) + 1*np.cos(_x**2) + 0.5*np.cos(5*_x) \n# _y = _y1 + x # x is epsilon\n\n\n_x = np.linspace(0,2,1000)\n_y1 =  3*np.sin(_x) + 1*np.sin(_x**2) + 5*np.sin(5*_x) \n_y = _y1 + x # x is epsilon\n\n\ndf5=pd.DataFrame({'x':_x, 'y':_y, 'y1':_y1})\n\n\n_simul = SIMUL2(df5)\n\n\n_simul.fit3(ref=15,ymin=-10,ymax=15,cuts=5)\n\n\n\n\n\n#_simul.vis()\n\n\n_simul.sub()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#d-ì‹œë„-1",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#d-ì‹œë„-1",
    "title": "Simulation",
    "section": "3D ì‹œë„ 1",
    "text": "3D ì‹œë„ 1\n\n### Example 2\nnp.random.seed(777)\npi=np.pi\nn=1000\nang=np.linspace(-pi,pi-2*pi/n,n)\nr=2+np.sin(np.linspace(0,6*pi,n))\nvx=r*np.cos(ang)\nvy=r*np.sin(ang)\nf1=10*np.sin(np.linspace(0,3*pi,n))\nf = f1 + x\n\n# 1. \np=plt.figure(figsize=(12,4), dpi=200)  # Make figure object \n\n# 2. \nax=p.add_subplot(1,1,1, projection='3d')\nax.grid(False)\nax.ticklabel_format(style='sci', axis='x',scilimits=(0,0))\nax.ticklabel_format(style='sci', axis='y',scilimits=(0,0))\nax.ticklabel_format(style='sci', axis='z',scilimits=(0,0))\ntop = f\nbottom = np.zeros_like(top)\nwidth=depth=0.05\n#ax.bar3d(vx, vy, bottom, width, depth, top, shade=False)\nax.scatter3D(vx,vy,f,zdir='z',s=10,marker='.')\nax.scatter3D(vx,vy,f1,zdir='z',s=10,marker='.')\nax.bar3d(vx, vy, bottom, width, depth, 0, color='Black',shade=False)\nax.set_xlim(-3,3)\nax.set_ylim(-3,3)\nax.set_zlim(-10,10)\n\ndf = pd.DataFrame({'x' : vx, 'y' : vy, 'f' : f, 'f1' : f1})\n\n\nclass SIMUL:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.f.to_numpy()\n        self.f1 = df.f1.to_numpy()\n        self.x = df.x.to_numpy()\n        self.y = df.y.to_numpy()\n        self.n = len(self.f)\n        self.theta= None\n    def get_distance(self):\n        self.D = np.zeros([self.n,self.n])\n        locations = np.stack([self.x, self.y],axis=1)\n        for i in tqdm.tqdm(range(self.n)):\n            for j in range(i,self.n):\n                self.D[i,j]=np.linalg.norm(locations[i]-locations[j])\n        self.D = self.D + self.D.T\n    def get_weightmatrix(self,theta=1,beta=0.5,kappa=4000):\n        self.theta = theta\n        dist = np.where(self.D < kappa,self.D,0)\n        self.W = np.exp(-(dist/self.theta)**2)\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)       \n    def fit(self,sd=5,ref=60): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.f # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2),sd=sd))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(fHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.f- self.df.fHat)\n        self.dif=(np.abs(self.f-self.fhat)-np.min(np.abs(self.f-self.fhat)))/(np.max(np.abs(self.f-self.fhat))-np.min(np.abs(self.f-self.fhat)))\n        self.df = self.df.assign(dif = self.dif)\n        self.bottom = np.zeros_like(self.f)\n        self.width=0.05\n        self.depth=0.05\n        \n        fig, axs = plt.subplots(2,2,figsize=(16,16),subplot_kw={\"projection\":\"3d\"})\n        axs[0,0].grid(False)\n        axs[0,0].scatter3D(self.x,self.y,self.f,c=self.dif,cmap='winter',zdir='z',s=50,marker='.',alpha=0.2)\n        axs[0,0].plot3D(self.x,self.y,[0]*1000,'black')\n        axs[0,0].set_xlim(-3,3)\n        axs[0,0].set_ylim(-3,3)\n        axs[0,0].set_zlim(-10,10)\n        axs[0,0].view_init(elev=20., azim=40)\n        \n        axs[0,1].grid(False)\n        axs[0,1].scatter3D(self.x,self.y,self.fhat,color='black',zdir='z',s=50,marker='.',alpha=0.2)\n        axs[0,1].plot3D(self.x,self.y,self.f1,'blue')\n        axs[0,1].plot3D(self.x,self.y,[0]*1000,'black')\n        axs[0,1].set_xlim(-3,3)\n        axs[0,1].set_ylim(-3,3)\n        axs[0,1].set_zlim(-10,10)\n        axs[0,1].view_init(elev=20., azim=40)\n        \n        axs[1,0].grid(False)\n        axs[1,0].scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['f'],color='red',zdir='z',s=100,marker='.',alpha=1)\n        axs[1,0].plot3D(self.x,self.y,self.f1,'blue')\n        axs[1,0].plot3D(self.x,self.y,[0]*1000,'black')\n        axs[1,0].set_xlim(-3,3)\n        axs[1,0].set_ylim(-3,3)\n        axs[1,0].set_zlim(-10,10)\n        axs[1,0].view_init(elev=20., azim=40)\n        \n        axs[1,1].grid(False)\n        axs[1,1].scatter3D(self.x,self.y,self.f,c=self.dif,cmap='winter',zdir='z',s=50,marker='.',alpha=0.2)\n        axs[1,1].scatter3D(self.x,self.y,self.fhat,color='black',zdir='z',s=50,marker='.',alpha=0.2)\n        axs[1,1].scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['f'],color='red',zdir='z',s=100,marker='.',alpha=1)\n        axs[1,1].plot3D(self.x,self.y,self.f1,'black')\n        axs[1,1].plot3D(self.x,self.y,[0]*1000,'black')\n        axs[1,1].set_xlim(-3,3)\n        axs[1,1].set_ylim(-3,3)\n        axs[1,1].set_zlim(-10,10)\n        axs[1,1].view_init(elev=20., azim=40)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        # p = plt.figure(figsize=(16,16))\n        # ax = p.add_subplot(1,1,1, projection='3d')\n        # ax.grid(False)\n        # ax.ticklabel_format(style='sci', axis='x',scilimits=(0,0))\n        # ax.ticklabel_format(style='sci', axis='y',scilimits=(0,0))\n        # ax.ticklabel_format(style='sci', axis='z',scilimits=(0,0))\n        # ax.scatter3D(self.x,self.y,self.f,c=self.dif,cmap='winter',zdir='z',s=50,marker='.',alpha=0.2)\n        # ax.scatter3D(self.x,self.y,self.fhat,color='black',zdir='z',s=50,marker='.',alpha=0.2)\n        # #ax.plot3D(self.x,self.y,self.fhat,'black')\n        # ax.scatter3D(self.df.query('Residual**2>@ref')['x'],self.df.query('Residual**2>@ref')['y'],self.df.query('Residual**2>@ref')['f'],color='red',zdir='z',s=100,marker='.',alpha=1)\n        # ax.plot3D(self.x,self.y,self.f1,'black')\n        # ax.plot3D(self.x,self.y,[0]*1000,'black')\n        # ax.set_xlim(-3,3)\n        # ax.set_ylim(-3,3)\n        # ax.set_zlim(-10,10)\n    def vis(self,ref=60):\n        fig = go.Figure()\n        fig.add_scatter3d(x=self.x,y=self.y,z=self.f, mode=\"markers\",marker=dict(size=3, color=\"#9fc5e8\"),name='f',opacity=0.2)\n        fig.add_scatter3d(x=self.x,y=self.y,z=self.fhat, mode=\"markers\",marker=dict(size=3, color=\"#999999\"),name='fhat',opacity=0.2)\n        #fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=self.fhat,mode='lines',line_color='#000000'))\n        fig.add_scatter3d(x=self.df.query('Residual**2>@ref')['x'],y=self.df.query('Residual**2>@ref')['y'],z=self.df.query('Residual**2>@ref')['f'], mode=\"markers\",marker=dict(size=3, color=\"#f20505\"),name='R square',opacity=1)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=self.f1,mode='lines',line_color='#000000',name='underline'))\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=[0]*1000,mode='lines',line_color='#000000',name='z=0'))\n        fig.update_layout(width=1000,height=1000,autosize=False,margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n    def sub(self):\n        fig, axs = plt.subplots(2,2,figsize=(16,10))\n\n        axs[0,0].plot(_simul.power)\n        axs[0,0].plot(_simul.power_threshed)\n        axs[0,0].set_title('power_threshed')\n\n        axs[0,1].plot(_simul.power[1:])\n        axs[0,1].plot(_simul.power_threshed[1:])\n        axs[0,1].set_title('power_threshed 1:')\n\n        axs[1,0].plot(_simul.power[2:])\n        axs[1,0].plot(_simul.power_threshed[2:])\n        axs[1,0].set_title('power_threshed 2:')\n\n        axs[1,1].plot((_simul.df.Residual)**2)\n        axs[1,1].set_title('Residual square')\n\n        plt.tight_layout()\n        plt.show()\n    def subvis(self,ref=60):\n        fig = make_subplots(2,2,specs=[[{'type': 'surface'}, {'type': 'surface'}],[{'type': 'surface'}, {'type': 'surface'}]],subplot_titles=(\"f\", \"fhat\", \"Residual Square\", \"Graph\"))\n        \n        fig.add_scatter3d(x=self.x,y=self.y,z=self.f, mode=\"markers\",marker=dict(size=3, color=\"#9fc5e8\"),name='f',opacity=0.2,row=1,col=1)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=self.f1,mode='lines',line_color='#000000',name='underline'),row=1,col=1)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=[0]*1000,mode='lines',line_color='#000000',name='z=0'),row=1,col=1)\n        \n        fig.add_scatter3d(x=self.x,y=self.y,z=self.fhat, mode=\"markers\",marker=dict(size=3, color=\"#999999\"),name='fhat',opacity=0.2,row=1,col=2)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=self.f1,mode='lines',line_color='#000000',name='underline'),row=1,col=2)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=[0]*1000,mode='lines',line_color='#000000',name='z=0'),row=1,col=2)\n        \n        fig.add_scatter3d(x=self.df.query('Residual**2>@ref')['x'],y=self.df.query('Residual**2>@ref')['y'],z=self.df.query('Residual**2>@ref')['f'], mode=\"markers\",marker=dict(size=3, color=\"#f20505\"),name='R square',opacity=1,row=2,col=1)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=self.f1,mode='lines',line_color='#000000',name='underline'),row=2,col=1)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=[0]*1000,mode='lines',line_color='#000000',name='z=0'),row=2,col=1)\n        \n        fig.add_scatter3d(x=self.x,y=self.y,z=self.f, mode=\"markers\",marker=dict(size=3, color=\"#9fc5e8\"),name='f',opacity=0.2,row=2,col=2)        \n        fig.add_scatter3d(x=self.x,y=self.y,z=self.fhat, mode=\"markers\",marker=dict(size=3, color=\"#999999\"),name='fhat',opacity=0.2,row=2,col=2)        \n        fig.add_scatter3d(x=self.df.query('Residual**2>@ref')['x'],y=self.df.query('Residual**2>@ref')['y'],z=self.df.query('Residual**2>@ref')['f'], mode=\"markers\",marker=dict(size=3, color=\"#f20505\"),name='R square',opacity=1,row=2,col=2)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=self.f1,mode='lines',line_color='#000000',name='underline'),row=2,col=2)\n        fig.add_trace(go.Scatter3d(x=self.x,y=self.y,z=[0]*1000,mode='lines',line_color='#000000',name='z=0'),row=2,col=2)\n        \n        fig.update_layout(scene = dict(xaxis = dict(range=[-3,3],),\n                                         yaxis = dict(range=[-3,3],),\n                                         zaxis = dict(range=[-10,10],),),\n                                      width=1000,height=1000,autosize=False)\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n\n\n_simul = SIMUL(df)\n\n\n_simul.get_distance()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:01<00:00, 532.20it/s]\n\n\n\n_simul.D[_simul.D>0].mean()\n\n2.6888234729389295\n\n\n\nplt.hist(_simul.D[_simul.D>0])\n\n(array([ 66308.,  64352.,  68358., 177302., 166964., 114648.,  94344.,\n        111136.,  75508.,  60080.]),\n array([0.00628415, 0.54637775, 1.08647135, 1.62656495, 2.16665855,\n        2.70675214, 3.24684574, 3.78693934, 4.32703294, 4.86712654,\n        5.40722013]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n\n_simul.get_weightmatrix(theta=(2.6888234729389295),kappa=2500) \n\n\n_simul.fit(sd=5,ref=20)\n\n\n\n\n\n#_simul.vis(ref=20)\n\n\n#_simul.subvis(ref=20)\n\n\n_simul.sub()\n\n\n\n\n\n#_simul.subvis()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#d-ì‹œë„-2",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#d-ì‹œë„-2",
    "title": "Simulation",
    "section": "3D ì‹œë„ 2",
    "text": "3D ì‹œë„ 2\n\n### Example 2\nnp.random.seed(777)\npi=np.pi\nn=1000\nang=np.linspace(-pi,pi-2*pi/n,n)\nr=2+np.sin(np.linspace(0,8*pi,n))\nvx=r*np.cos(ang)\nvy=r*np.sin(ang)\nf1=10*np.sin(np.linspace(0,3*pi,n))\nf = f1 + x\n\n\ndf1 = pd.DataFrame({'x' : vx, 'y' : vy, 'f' : f,'f1':f1})\n\n\n_simul = SIMUL(df1)\n\n\n_simul.get_distance()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:01<00:00, 521.69it/s]\n\n\n\n_simul.D[_simul.D>0].mean()\n\n2.6984753461932702\n\n\n\nplt.hist(_simul.D[_simul.D>0])\n\n(array([ 63450.,  64118., 146970., 169756., 138202., 126198., 162650.,\n         75642.,  28416.,  23598.]),\n array([0.0062838 , 0.60565122, 1.20501864, 1.80438605, 2.40375347,\n        3.00312089, 3.6024883 , 4.20185572, 4.80122314, 5.40059055,\n        5.99995797]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n\n_simul.get_weightmatrix(theta=(2.6984753461932702),kappa=2500) \n\n\n_simul.fit(sd=5,ref=30)\n\n\n\n\n\n#_simul.vis(ref=50)\n\n\n_simul.sub()\n\n\n\n\n\n#_simul.subvis()"
  },
  {
    "objectID": "posts/GODE/2022-09-02-paper_simulation.html#d-ì‹œë„-3",
    "href": "posts/GODE/2022-09-02-paper_simulation.html#d-ì‹œë„-3",
    "title": "Simulation",
    "section": "3D ì‹œë„ 3",
    "text": "3D ì‹œë„ 3\n\n### Example 2\nnp.random.seed(777)\npi=np.pi\nn=1000\nang=np.linspace(-pi,pi-2*pi/n,n)\nr=2+np.sin(np.linspace(0,6*pi,n))\nvx=r*np.cos(ang)\nvy=r*np.sin(ang)\nf1=10*np.sin(np.linspace(0,6*pi,n))\nf = f1 + x\n\n\ndf2 = pd.DataFrame({'x' : vx, 'y' : vy, 'f' : f,'f1':f1})\n\n\n_simul = SIMUL(df2)\n\n\n_simul.get_distance()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:02<00:00, 463.80it/s]\n\n\n\n_simul.D[_simul.D>0].mean()\n\n2.6888234729389295\n\n\n\nplt.hist(_simul.D[_simul.D>0])\n\n(array([ 66308.,  64352.,  68358., 177302., 166964., 114648.,  94344.,\n        111136.,  75508.,  60080.]),\n array([0.00628415, 0.54637775, 1.08647135, 1.62656495, 2.16665855,\n        2.70675214, 3.24684574, 3.78693934, 4.32703294, 4.86712654,\n        5.40722013]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n\n_simul.get_weightmatrix(theta=(2.6984753461932702),kappa=2500) \n\n\n_simul.fit(sd=5,ref=30)\n\n\n\n\n\n#_simul.vis(ref=50)\n\n\n_simul.sub()\n\n\n\n\n\n#_simul.subvis()"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html",
    "title": "Earthquake",
    "section": "",
    "text": "Real analysis"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html#imports",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html#imports",
    "title": "Earthquake",
    "section": "imports",
    "text": "imports\n\nimport tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport plotly.express as px\nimport warnings\nwarnings.simplefilter(\"ignore\", np.ComplexWarning)\nfrom haversine import haversine\nfrom IPython.display import HTML\nimport plotly.graph_objects as go\n\n\nimport rpy2\nimport rpy2.robjects as ro \nfrom rpy2.robjects.vectors import FloatVector \nfrom rpy2.robjects.packages import importr"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html#load-data-and-clean-it",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html#load-data-and-clean-it",
    "title": "Earthquake",
    "section": "load data and clean it",
    "text": "load data and clean it\n- load\n\ndf= pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      Date\n      Latitude\n      Longitude\n      Magnitude\n    \n  \n  \n    \n      0\n      01/02/1965\n      19.2460\n      145.6160\n      6.0\n    \n    \n      1\n      01/04/1965\n      1.8630\n      127.3520\n      5.8\n    \n    \n      2\n      01/05/1965\n      -20.5790\n      -173.9720\n      6.2\n    \n    \n      3\n      01/08/1965\n      -59.0760\n      -23.5570\n      5.8\n    \n    \n      4\n      01/09/1965\n      11.9380\n      126.4270\n      5.8\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      23407\n      12/28/2016\n      38.3917\n      -118.8941\n      5.6\n    \n    \n      23408\n      12/28/2016\n      38.3777\n      -118.8957\n      5.5\n    \n    \n      23409\n      12/28/2016\n      36.9179\n      140.4262\n      5.9\n    \n    \n      23410\n      12/29/2016\n      -9.0283\n      118.6639\n      6.3\n    \n    \n      23411\n      12/30/2016\n      37.3973\n      141.4103\n      5.5\n    \n  \n\n23412 rows Ã— 4 columns\n\n\n\n\ndf_korea= pd.read_csv('earthquake_korea2.csv').iloc[:,[1,2,5,6]].rename(columns={'ê·œëª¨':'Magnitude'})\n\nhttps://www.weather.go.kr/w/eqk-vol/search/korea.do?schOption=&xls=0&startTm=2012-01-02&endTm=2022-06-17&startSize=2&endSize=&startLat=&endLat=&startLon=&endLon=&lat=&lon=&dist=&keyword=&dpType=m\n\ndf_global= pd.concat([pd.read_csv('00_05.csv'),pd.read_csv('05_10.csv'),pd.read_csv('10_15.csv'),pd.read_csv('15_20.csv')]).iloc[:,[0,1,2,4]].rename(columns={'latitude':'Latitude','longitude':'Longitude','mag':'Magnitude'}).reset_index().iloc[:,1:]\n\nhttps://www.usgs.gov/programs/earthquake-hazards/lists-maps-and-statistics\n- cleaning\n\ndf.Date[df.Date == '1975-02-23T02:58:41.000Z']\n\n3378    1975-02-23T02:58:41.000Z\nName: Date, dtype: object\n\n\n\ndf.iloc[3378,0] = '02/03/1975'\n\n\ndf.Date[df.Date == '1985-04-28T02:53:41.530Z']\n\n7512    1985-04-28T02:53:41.530Z\nName: Date, dtype: object\n\n\n\ndf.iloc[7512,0] = '04/28/1985'\n\n\ndf.Date[df.Date == '2011-03-13T02:23:34.520Z']\n\n20650    2011-03-13T02:23:34.520Z\nName: Date, dtype: object\n\n\n\ndf.iloc[20650,0] = '03/13/2011'\n\n\ndf= df.assign(Year=list(map(lambda x: x.split('/')[-1], df.Date))).iloc[:,1:]\ndf\n\n\n\n\n\n  \n    \n      \n      Latitude\n      Longitude\n      Magnitude\n      Year\n    \n  \n  \n    \n      0\n      19.2460\n      145.6160\n      6.0\n      1965\n    \n    \n      1\n      1.8630\n      127.3520\n      5.8\n      1965\n    \n    \n      2\n      -20.5790\n      -173.9720\n      6.2\n      1965\n    \n    \n      3\n      -59.0760\n      -23.5570\n      5.8\n      1965\n    \n    \n      4\n      11.9380\n      126.4270\n      5.8\n      1965\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      23407\n      38.3917\n      -118.8941\n      5.6\n      2016\n    \n    \n      23408\n      38.3777\n      -118.8957\n      5.5\n      2016\n    \n    \n      23409\n      36.9179\n      140.4262\n      5.9\n      2016\n    \n    \n      23410\n      -9.0283\n      118.6639\n      6.3\n      2016\n    \n    \n      23411\n      37.3973\n      141.4103\n      5.5\n      2016\n    \n  \n\n23412 rows Ã— 4 columns\n\n\n\n\ndf.Year = df.Year.astype(np.float64)\n\n\ndf_korea = df_korea.assign(Year=list(map(lambda x: x.split('/')[0], df_korea.ë°œìƒì‹œê°))).iloc[:,1:]\ndf_korea = df_korea.assign(Latitude=list(map(lambda x: x.split(' ')[0], df_korea.ìœ„ë„))).iloc[:,[0,2,3,4]]\ndf_korea = df_korea.assign(Longitude=list(map(lambda x: x.split(' ')[0], df_korea.ê²½ë„))).iloc[:,[0,2,3,4]]\n\n\ndf_global = df_global.assign(Year=list(map(lambda x: x.split('-')[0], df_global.time))).iloc[:,1:]\n\n\ndf_korea.Year = df_korea.Year.astype(np.float64)\ndf_korea.Latitude = df_korea.Latitude.astype(np.float64)\ndf_korea.Longitude = df_korea.Longitude.astype(np.float64)\ndf_global.Year = df_global.Year.astype(np.float64)"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html#define-class",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html#define-class",
    "title": "Earthquake",
    "section": "define class",
    "text": "define class\n\nclass MooYaHo:\n    def __init__(self,df):\n        self.df = df \n        self.f = df.Magnitude.to_numpy()\n        self.year = df.Year.to_numpy()\n        self.lat = df.Latitude.to_numpy()\n        self.long = df.Longitude.to_numpy()\n        self.n = len(self.f)\n        \n        self.theta= None\n    def get_distance(self):\n        self.D = np.zeros([self.n,self.n])\n        locations = np.stack([self.lat, self.long],axis=1)\n        for i in tqdm.tqdm(range(self.n)):\n            for j in range(i,self.n): \n                self.D[i,j]=haversine(locations[i],locations[j])\n        self.D = self.D+self.D.T\n    def get_weightmatrix(self,theta=1,beta=0.5,kappa=4000):\n        self.theta = theta\n        dist = np.where(self.D<kappa,self.D,0)\n        self.W = np.exp(-(dist/self.theta)**2)\n\n    def _eigen(self):\n        d= self.W.sum(axis=1)\n        D= np.diag(d)\n        self.L = np.diag(1/np.sqrt(d)) @ (D-self.W) @ np.diag(1/np.sqrt(d))\n        self.lamb, self.Psi = np.linalg.eigh(self.L)\n        self.Lamb = np.diag(self.lamb)        \n    def fit(self,m):\n        self._eigen()\n        self.fhat = self.Psi[:,0:m]@self.Psi[:,0:m].T@self.f\n        self.df = self.df.assign(MagnitudeHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.Magnitude- self.df.MagnitudeHat)\n        plt.plot(self.f,'.')\n        plt.plot(self.fhat,'x')\n        \n    def vis(self,MagThresh=7,ResThresh=1):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=5,\n                        center=dict(lat=37, lon=160), \n                        zoom=1.5,\n                        height=900,\n                        opacity = 0.4,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 8,\n                      marker_color= 'red',\n                      opacity = 0.6\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= 8,\n                      marker_color= 'blue',\n                      opacity = 0.5\n                      )\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n    def visf(self):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=5,\n                        center=dict(lat=37, lon=160), \n                        zoom=1.5,\n                        height=900,\n                        opacity = 0.7,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n    def visfhat(self):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='MagnitudeHat', \n                        radius=5,\n                        center=dict(lat=37, lon=160), \n                        zoom=1.5,\n                        height=900,\n                        opacity = 0.7,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n    def visres(self,MagThresh=7,ResThresh=1):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z=[0] * len(self.df), \n                        radius=5,\n                        center=dict(lat=37, lon=160), \n                        zoom=1.5,\n                        height=900,\n                        opacity = 0.7,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= 8,\n                      marker_color= 'blue',\n                      opacity = 0.7\n                      )\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n\n\nclass MooYaHo2(MooYaHo): # ebayesthresh ê¸°ëŠ¥ì¶”ê°€\n    def fit2(self,ref=0.5): # fit with ebayesthresh\n        self._eigen()\n        self.fbar = self.Psi.T @ self.f # fbar := graph fourier transform of f\n        self.power = self.fbar**2 \n        ebayesthresh = importr('EbayesThresh').ebayesthresh\n        self.power_threshed=np.array(ebayesthresh(FloatVector(self.fbar**2)))\n        self.fbar_threshed = np.where(self.power_threshed>0,self.fbar,0)\n        self.fhat = self.Psi@self.fbar_threshed\n        self.df = self.df.assign(MagnitudeHat = self.fhat)\n        self.df = self.df.assign(Residual = self.df.Magnitude- self.df.MagnitudeHat)\n        self.con = np.where(self.df.Residual>0.7,1,0)\n        #plt.plot(self.f,'.')\n        #plt.plot(self.fhat,'x')\n\n#         fig, axs = plt.subplots(2,2,figsize=(16,10))\n\n#         axs[0,0].plot(self.f,'b')\n#         axs[0,0].set_title('Magnitude')\n#         axs[0,0].set_ylim([4.5,9])\n\n#         axs[0,1].plot(self.fhat,'k')\n#         axs[0,1].set_title('MagnitudeHat')\n#         axs[0,1].set_ylim([4.5,9])\n\n#         axs[1,0].plot(self.con,'r*')\n#         axs[1,0].set_title('Residual square')\n\n#         axs[1,1].plot(self.f,'b')\n#         axs[1,1].plot(self.fhat,'k')\n#         axs[1,1].plot(self.con,'r*')\n#         axs[1,1].set_title('Graph')\n#         axs[1,1].set_ylim([4.5,9])\n\n#         plt.tight_layout()\n#         plt.show()\n\n\nclass MooYaHo3(MooYaHo2):\n    def vis(self,MagThresh=7,ResThresh=1):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=5,\n                        center=dict(lat=37, lon=126), \n                        zoom=5.7,\n                        height=900,\n                        opacity = 0.3,\n                        mapbox_style=\"stamen-terrain\")\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 8,\n                      marker_color= 'red',\n                      opacity = 0.5\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= 8,\n                      marker_color= 'blue',\n                      opacity = 0.5\n                      )\n        return HTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n\n\n       ebayesthresh = importr('EbayesThresh').ebayesthresh"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html#analysis_df_global20102015",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html#analysis_df_global20102015",
    "title": "Earthquake",
    "section": "analysis_df_global(2010~2015)",
    "text": "analysis_df_global(2010~2015)\n- make instance for analysis\n\nmoo_global=MooYaHo2(df_global.query(\"2010 <= Year < 2015\"))\n\n- get distance\n\nmoo_global.get_distance()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12498/12498 [03:14<00:00, 64.19it/s] \n\n\n\nmoo_global.D[moo_global.D>0].mean()\n\n8810.865423093777\n\n\n\nplt.hist(moo_global.D[moo_global.D>0])\n\n(array([14176290., 16005894., 21186674., 22331128., 19394182., 17548252.,\n        16668048., 13316436., 12973260.,  2582550.]),\n array([8.97930163e-02, 2.00141141e+03, 4.00273303e+03, 6.00405465e+03,\n        8.00537626e+03, 1.00066979e+04, 1.20080195e+04, 1.40093411e+04,\n        1.60106627e+04, 1.80119844e+04, 2.00133060e+04]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n- weight matrix\n\nmoo_global.get_weightmatrix(theta=(8810.865423093777),kappa=2500) \n\n- fit\n\nmoo_global.fit2()\n\n\nmoo_global.df.sort_values(\"Residual\",ascending=False).iloc[:40,:]\n\n\n\n\n\n  \n    \n      \n      Latitude\n      Longitude\n      Magnitude\n      Year\n      MagnitudeHat\n      Residual\n    \n  \n  \n    \n      11094\n      -36.1220\n      -72.8980\n      8.8\n      2010.0\n      7.754649\n      1.045351\n    \n    \n      10727\n      -36.2170\n      -73.2570\n      6.7\n      2010.0\n      5.750555\n      0.949445\n    \n    \n      27513\n      0.8020\n      92.4630\n      8.2\n      2012.0\n      7.253721\n      0.946279\n    \n    \n      26109\n      -10.9280\n      166.0180\n      7.1\n      2013.0\n      6.194467\n      0.905533\n    \n    \n      30291\n      38.0580\n      144.5900\n      7.7\n      2011.0\n      6.804163\n      0.895837\n    \n    \n      31673\n      -17.5410\n      168.0690\n      7.3\n      2010.0\n      6.431399\n      0.868601\n    \n    \n      27527\n      2.3270\n      93.0630\n      8.6\n      2012.0\n      7.736731\n      0.863269\n    \n    \n      30311\n      38.2970\n      142.3730\n      9.1\n      2011.0\n      8.255988\n      0.844012\n    \n    \n      24388\n      -19.6097\n      -70.7691\n      8.2\n      2014.0\n      7.394885\n      0.805115\n    \n    \n      28408\n      -28.9930\n      -176.2380\n      7.4\n      2011.0\n      6.596810\n      0.803190\n    \n    \n      23863\n      -55.4703\n      -28.3669\n      6.9\n      2014.0\n      6.097310\n      0.802690\n    \n    \n      32803\n      -36.1220\n      -72.8980\n      8.8\n      2010.0\n      7.999521\n      0.800479\n    \n    \n      27672\n      -22.1410\n      170.3400\n      6.6\n      2012.0\n      5.830724\n      0.769276\n    \n    \n      24958\n      -60.2738\n      -46.4011\n      7.7\n      2013.0\n      6.938609\n      0.761391\n    \n    \n      31196\n      -56.5860\n      -142.2920\n      6.4\n      2010.0\n      5.641422\n      0.758578\n    \n    \n      26943\n      2.1900\n      126.8370\n      6.6\n      2012.0\n      5.849246\n      0.750754\n    \n    \n      9118\n      -19.7020\n      167.9470\n      7.3\n      2010.0\n      6.550810\n      0.749190\n    \n    \n      32001\n      7.8810\n      91.9360\n      7.5\n      2010.0\n      6.785405\n      0.714595\n    \n    \n      31229\n      -3.4870\n      100.0820\n      7.8\n      2010.0\n      7.086996\n      0.713004\n    \n    \n      29640\n      38.2760\n      141.5880\n      7.1\n      2011.0\n      6.387718\n      0.712282\n    \n    \n      11356\n      18.4430\n      -72.5710\n      7.0\n      2010.0\n      6.288026\n      0.711974\n    \n    \n      30296\n      36.2810\n      141.1110\n      7.9\n      2011.0\n      7.197845\n      0.702155\n    \n    \n      28574\n      -21.6110\n      -179.5280\n      7.3\n      2011.0\n      6.601124\n      0.698876\n    \n    \n      23633\n      -32.6953\n      -71.4416\n      6.4\n      2014.0\n      5.705553\n      0.694447\n    \n    \n      25517\n      10.7010\n      -42.5940\n      6.6\n      2013.0\n      5.908930\n      0.691070\n    \n    \n      28001\n      -10.6170\n      165.1600\n      6.4\n      2012.0\n      5.722428\n      0.677572\n    \n    \n      25773\n      30.3080\n      102.8880\n      6.6\n      2013.0\n      5.922831\n      0.677169\n    \n    \n      29004\n      38.0340\n      143.2640\n      7.0\n      2011.0\n      6.332486\n      0.667514\n    \n    \n      23815\n      14.7240\n      -92.4614\n      6.9\n      2014.0\n      6.237697\n      0.662303\n    \n    \n      24360\n      -20.3113\n      -70.5756\n      6.5\n      2014.0\n      5.839327\n      0.660673\n    \n    \n      25633\n      -23.0090\n      -177.2320\n      7.4\n      2013.0\n      6.743190\n      0.656810\n    \n    \n      30256\n      36.8230\n      141.8240\n      6.1\n      2011.0\n      5.446211\n      0.653789\n    \n    \n      9520\n      -3.4870\n      100.0820\n      7.8\n      2010.0\n      7.146346\n      0.653654\n    \n    \n      29133\n      52.0500\n      -171.8360\n      7.3\n      2011.0\n      6.646652\n      0.653348\n    \n    \n      32492\n      -34.2900\n      -71.8910\n      6.9\n      2010.0\n      6.247025\n      0.652975\n    \n    \n      24066\n      4.2485\n      92.7574\n      6.0\n      2014.0\n      5.353450\n      0.646550\n    \n    \n      24359\n      -20.5709\n      -70.4931\n      7.7\n      2014.0\n      7.056532\n      0.643468\n    \n    \n      26083\n      -10.9940\n      165.7410\n      6.6\n      2013.0\n      5.958927\n      0.641073\n    \n    \n      32768\n      -37.7730\n      -75.0480\n      7.4\n      2010.0\n      6.762054\n      0.637946\n    \n    \n      10845\n      -36.6650\n      -73.3740\n      6.6\n      2010.0\n      5.963286\n      0.636714\n    \n  \n\n\n\n\n(2010~2014 ì‹œë„) - 21ë²ˆì§¸ Ouest Department, Haiti ì•„ì´í‹° ì§€ì§„ 2010ë…„ ì§„ë„ 7.0 - 24ë²ˆì¨° Puchuncavi, ValparaÃ­so, Chile ì¹ ë ˆ ì§€ì§„ 2014ë…„ ì§„ë„ 6.4 - 28ë²ˆì§¸ Baoxing County, Yaan, Sichuan, China ì¤‘êµ­ ì“°ì´¨ì„± ì§€ì§„ 2013ë…„ ì§„ë„ 6.6\n(2010~2015 ì‹œë„_ê²°ê³¼ ì¢‹ì§€ ì•ŠìŒ?!) - 23ë²ˆì§¸ 2010ë…„ West New Britain Province, Papua New Guinea ì§„ë„ 7.3 - 24ë²ˆì§¸ 2011ë…„ Kuzawa Terayama, Tanagura, Higashishirakawa District, Fukushima 963-5671, Japan ì§„ë„ 6.6 - 29ë²ˆì§¸ 2015ë…„ Kishim, Afghanistan ì§„ë„ 7.5\n- vis\n\n#moo_global.visf()\n\n\n#moo_global.visfhat()\n\n\n#moo_global.visres()\n\n\n#moo_global.vis(MagThresh=6.9,ResThresh=0.5)"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html#analysis_df_global20152020",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html#analysis_df_global20152020",
    "title": "Earthquake",
    "section": "analysis_df_global(2015~2020)",
    "text": "analysis_df_global(2015~2020)\n- make instance for analysis\n\nmoo_global=MooYaHo2(df_global.query(\"2015 <= Year <= 2020\"))\n\n- get distance\n\nmoo_global.get_distance()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11239/11239 [02:38<00:00, 71.12it/s] \n\n\n\nmoo_global.D[moo_global.D>0].mean()\n\n8814.318793468068\n\n\n\nplt.hist(moo_global.D[moo_global.D>0])\n\n(array([10894274., 13618924., 16426520., 17583818., 16025000., 15684642.,\n        13794372., 10946494.,  9072574.,  2254138.]),\n array([2.54728455e-02, 2.00123511e+03, 4.00244475e+03, 6.00365439e+03,\n        8.00486402e+03, 1.00060737e+04, 1.20072833e+04, 1.40084929e+04,\n        1.60097026e+04, 1.80109122e+04, 2.00121218e+04]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n- weight matrix\n\nmoo_global.get_weightmatrix(theta=(8814.318793468068),kappa=2500) \n\n- fit\n\nmoo_global.fit2()\n\n\nmoo_global.df.sort_values(\"Residual\",ascending=False).iloc[:30,:]\n\n\n\n\n\n  \n    \n      \n      Latitude\n      Longitude\n      Magnitude\n      Year\n      MagnitudeHat\n      Residual\n    \n  \n  \n    \n      41735\n      -31.5729\n      -71.6744\n      8.3\n      2015.0\n      7.247889\n      1.052111\n    \n    \n      36993\n      -18.1125\n      -178.1530\n      8.2\n      2018.0\n      7.151339\n      1.048661\n    \n    \n      21952\n      -31.5729\n      -71.6744\n      8.3\n      2015.0\n      7.377015\n      0.922985\n    \n    \n      36363\n      -21.9496\n      169.4266\n      7.5\n      2018.0\n      6.641801\n      0.858199\n    \n    \n      39771\n      -10.6812\n      161.3273\n      7.8\n      2016.0\n      6.952781\n      0.847219\n    \n    \n      41015\n      -4.9521\n      94.3299\n      7.8\n      2016.0\n      6.982741\n      0.817259\n    \n    \n      33896\n      -33.2927\n      -177.8571\n      7.4\n      2020.0\n      6.584337\n      0.815663\n    \n    \n      39932\n      -42.7373\n      173.0540\n      7.8\n      2016.0\n      6.994644\n      0.805356\n    \n    \n      21719\n      -8.3381\n      124.8754\n      6.5\n      2015.0\n      5.698561\n      0.801439\n    \n    \n      33404\n      54.6020\n      -159.6258\n      7.6\n      2020.0\n      6.825914\n      0.774086\n    \n    \n      33593\n      -27.9686\n      -71.3062\n      6.8\n      2020.0\n      6.068130\n      0.731870\n    \n    \n      21559\n      38.2107\n      72.7797\n      7.2\n      2015.0\n      6.476211\n      0.723789\n    \n    \n      36263\n      55.0999\n      164.6993\n      7.3\n      2018.0\n      6.591954\n      0.708046\n    \n    \n      42106\n      -36.3601\n      -73.8120\n      6.4\n      2015.0\n      5.712923\n      0.687077\n    \n    \n      37769\n      -6.0699\n      142.7536\n      7.5\n      2018.0\n      6.826692\n      0.673308\n    \n    \n      36848\n      -18.4743\n      179.3502\n      7.9\n      2018.0\n      7.233195\n      0.666805\n    \n    \n      39584\n      -43.4064\n      -73.9413\n      7.6\n      2016.0\n      6.936485\n      0.663515\n    \n    \n      22246\n      -9.3070\n      158.4030\n      6.7\n      2015.0\n      6.036540\n      0.663460\n    \n    \n      36301\n      -58.5446\n      -26.3856\n      7.1\n      2018.0\n      6.448234\n      0.651766\n    \n    \n      42271\n      27.8087\n      86.0655\n      7.3\n      2015.0\n      6.651371\n      0.648629\n    \n    \n      40675\n      -56.2409\n      -26.9353\n      7.2\n      2016.0\n      6.551931\n      0.648069\n    \n    \n      42622\n      -10.7598\n      164.1216\n      6.1\n      2015.0\n      5.469452\n      0.630548\n    \n    \n      35487\n      -30.6441\n      -178.0995\n      7.3\n      2019.0\n      6.673190\n      0.626810\n    \n    \n      22521\n      -7.2175\n      154.5567\n      7.1\n      2015.0\n      6.493164\n      0.606836\n    \n    \n      42475\n      -4.7294\n      152.5623\n      7.5\n      2015.0\n      6.897835\n      0.602165\n    \n    \n      36101\n      -30.0404\n      -71.3815\n      6.7\n      2019.0\n      6.098721\n      0.601279\n    \n    \n      37927\n      56.0039\n      -149.1658\n      7.9\n      2018.0\n      7.307120\n      0.592880\n    \n    \n      38520\n      15.0222\n      -93.8993\n      8.2\n      2017.0\n      7.608529\n      0.591471\n    \n    \n      21912\n      -31.5173\n      -71.8040\n      6.7\n      2015.0\n      6.110265\n      0.589735\n    \n    \n      41887\n      -9.3438\n      158.0525\n      6.6\n      2015.0\n      6.012902\n      0.587098\n    \n  \n\n\n\n\në°”ë‹¤ ì•„ë‹Œ ê±° - 8ë²ˆì§¸ 2016ë…„ Rotherham, New Zealand ë‰´ì§ˆëœë“œ ì¹´ì´ì½”ìš°ë¼ ì§€ì§„ ì§„ë„ 7.8 - 9ë²ˆì§¸ 2015ë…„ Langkuru Utara, Pureman, Alor Regency, East Nusa Tenggara, Indonesia ìˆ˜ë§ˆíŠ¸ë¼ ì§„ë„ 6.5 - 15ë²ˆì§¸ 2018ë…„ Hela Province, Papua New Guinea íŒŒí‘¸ì•„ë‰´ê¸°ë‹ˆ ì§„ë„ 7.5 - 20ë²ˆì§¸ 2015ë…„ Kalinchok, Nepal ë„¤íŒ” ì§„ë„ 7.3 - 26ë²ˆì§¸ 2019ë…„ Coquimbo, Chile ì¹ ë ˆ ì½”í‚´ë³´ì£¼ ì§„ë„ 6.7\n- vis\n\n#moo_global.vis(MagThresh=7,ResThresh=0.3)\n\n\n\npd.read_html('https://en.wikipedia.org/wiki/Lists_of_21st-century_earthquakes',encoding='utf-8')[0].query('Magnitude<=7')# List of deadliest earthquakes\n\n\n\n\n\n  \n    \n      \n      Rank\n      Fatalities\n      Magnitude\n      Location\n      Event\n      Date\n    \n  \n  \n    \n      1\n      2\n      220000\n      7.0\n      Haiti\n      2010 Haiti earthquake\n      January 12, 2010\n    \n    \n      4\n      5\n      26271\n      6.6\n      Iran\n      2003 Bam earthquake\n      December 26, 2003\n    \n    \n      8\n      9\n      5782\n      6.4\n      Indonesia\n      2006 Yogyakarta earthquake\n      May 26, 2006\n    \n    \n      10\n      11\n      2968\n      6.9\n      China\n      2010 Yushu earthquake\n      April 13, 2010\n    \n    \n      11\n      12\n      2266\n      6.8\n      Algeria\n      2003 BoumerdÃ¨s earthquake\n      May 21, 2003\n    \n    \n      14\n      15\n      1163\n      6.0\n      Afghanistan\n      June 2022 Afghanistan earthquake\n      June 21, 2022\n    \n  \n\n\n\n\n\npd.read_html('https://en.wikipedia.org/wiki/Lists_of_21st-century_earthquakes',encoding='utf-8')[3] # Deadliest earthquakes by year\n\n\n\n\n\n  \n    \n      \n      Year\n      Fatalities\n      Magnitude\n      Event\n      Location\n      Date\n      Death toll\n    \n  \n  \n    \n      0\n      2001\n      20085\n      7.7\n      2001 Gujarat earthquake\n      India\n      January 26\n      21357\n    \n    \n      1\n      2002\n      1000\n      6.1\n      2002 Hindu Kush earthquakes\n      Afghanistan\n      March 25\n      1685\n    \n    \n      2\n      2003\n      26271\n      6.6\n      2003 Bam earthquake\n      Iran\n      December 26\n      33819\n    \n    \n      3\n      2004\n      227898\n      9.1\n      2004 Indian Ocean earthquake and tsunami\n      Indonesia, Indian Ocean\n      December 26\n      227898\n    \n    \n      4\n      2005\n      87351\n      7.6\n      2005 Kashmir earthquake\n      Pakistan\n      October 8\n      87992\n    \n    \n      5\n      2006\n      5782\n      6.4\n      2006 Yogyakarta earthquake\n      Indonesia\n      May 26\n      6605\n    \n    \n      6\n      2007\n      519\n      8.0\n      2007 Peru earthquake\n      Peru\n      August 15\n      708\n    \n    \n      7\n      2008\n      87587\n      7.9\n      2008 Sichuan earthquake\n      China\n      May 12\n      88708\n    \n    \n      8\n      2009\n      1115\n      7.6\n      2009 Sumatra earthquakes\n      Indonesia\n      September 30\n      1790\n    \n    \n      9\n      2010\n      160000\n      7.0\n      2010 Haiti earthquake\n      Haiti\n      January 12\n      164627\n    \n    \n      10\n      2011\n      20896\n      9.0\n      2011 TÅhoku earthquake and tsunami\n      Japan\n      March 11\n      21492\n    \n    \n      11\n      2012\n      306\n      6.4\n      2012 East Azerbaijan earthquakes\n      Iran\n      August 11\n      689\n    \n    \n      12\n      2013\n      825\n      7.7\n      2013 Balochistan earthquakes\n      Pakistan\n      September 24\n      1572\n    \n    \n      13\n      2014\n      729\n      6.1\n      2014 Ludian earthquake\n      China\n      August 3\n      756\n    \n    \n      14\n      2015\n      8964\n      7.8\n      2015 Nepal earthquake\n      Nepal\n      April 25\n      9624\n    \n    \n      15\n      2016\n      673\n      7.8\n      2016 Ecuador earthquake\n      Ecuador\n      April 16\n      1339\n    \n    \n      16\n      2017\n      630\n      7.3\n      2017 Iranâ€“Iraq earthquake\n      Iran\n      November 12\n      1232\n    \n    \n      17\n      2018\n      4340\n      7.5\n      2018 Sulawesi earthquake and tsunami\n      Indonesia\n      September 28\n      5239\n    \n    \n      18\n      2019\n      51\n      6.4\n      2019 Albania earthquake\n      Albania\n      November 26\n      288\n    \n    \n      19\n      2020\n      119\n      7.0\n      2020 Aegean Sea earthquake\n      Turkey/Â Greece\n      October 30\n      207\n    \n    \n      20\n      2021\n      2248\n      7.2\n      2021 Haiti earthquake\n      Haiti\n      August 14\n      2476\n    \n    \n      21\n      2022\n      1163\n      6.0\n      June 2022 Afghanistan earthquake\n      Afghanistan\n      June 21\n      1405\n    \n  \n\n\n\n\n\n\nclass eachlocation(MooYaHo2):\n    def haiti(self,MagThresh=7,ResThresh=1,adjzoom=5,adjmarkersize = 40):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=15,\n                        center=dict(lat=18.4430, lon=-72.5710), \n                        zoom= adjzoom,\n                        height=900,\n                        opacity = 0.8,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-3,3])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 5,\n                      marker_color= 'blue',\n                      opacity = 0.1\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= adjmarkersize,\n                      marker_color= 'red',\n                      opacity = 0.8\n                      )\n        fig.add_trace(go.Scattermapbox(\n                    lat=self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                    lon=self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                    mode='markers',\n                    marker=go.scattermapbox.Marker(\n                        size=20,\n                        color='rgb(255, 255, 255)',\n                        opacity=0.4\n                    )\n                ))\n        return fig \n    def lquique(self,MagThresh=7,ResThresh=1,adjzoom=5, adjmarkersize= 40):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=15,\n                        center=dict(lat=-32.6953, lon=-71.4416), \n                        zoom=adjzoom,\n                        height=900,\n                        opacity = 0.8,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 5,\n                      marker_color= 'blue',\n                      opacity = 0.1\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= adjmarkersize,\n                      marker_color= 'red',\n                      opacity = 0.8\n                      )\n        fig.add_trace(go.Scattermapbox(\n                    lat=self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                    lon=self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                    mode='markers',\n                    marker=go.scattermapbox.Marker(\n                        size=20,\n                        color='rgb(255, 255, 255)',\n                        opacity=0.8\n                    )\n                ))\n        return fig \n    def sichuan(self,MagThresh=7,ResThresh=1,adjzoom=5,adjmarkersize=40):\n        fig = px.density_mapbox(self.df, \n                        lat='Latitude', \n                        lon='Longitude', \n                        z='Magnitude', \n                        radius=15,\n                        center=dict(lat=30.3080, lon=102.8880), \n                        zoom=adjzoom,\n                        height=900,\n                        opacity = 0.6,\n                        mapbox_style=\"stamen-terrain\",\n                        range_color=[-7,7])\n        fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n        fig.add_scattermapbox(lat = self.df.query('Magnitude > @MagThresh')['Latitude'],\n                      lon = self.df.query('Magnitude > @MagThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @MagThresh')['Magnitude'],\n                      marker_size= 5,\n                      marker_color= 'blue',\n                      opacity = 0.1\n                      )\n        fig.add_scattermapbox(lat = self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                      lon = self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                      text = self.df.query('Magnitude > @ResThresh')['Magnitude'],\n                      marker_size= adjmarkersize,\n                      marker_color= 'red',\n                      opacity = 0.8\n                      )\n        fig.add_trace(go.Scattermapbox(\n                    lat=self.df.query('Residual**2 > @ResThresh')['Latitude'],\n                    lon=self.df.query('Residual**2 > @ResThresh')['Longitude'],\n                    mode='markers',\n                    marker=go.scattermapbox.Marker(\n                        size=20,\n                        color='rgb(255, 255, 255)',\n                        opacity=0.8\n                    )\n                ))\n        return fig \n\n\neach_location=eachlocation(df_global.query(\"2010 <= Year < 2015\"))\n\n- get distance\n\neach_location.get_distance()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12498/12498 [03:20<00:00, 62.38it/s] \n\n\n\neach_location.D[each_location.D>0].mean()\n\n8810.865423093777\n\n\n\nplt.hist(each_location.D[each_location.D>0])\n\n(array([14176290., 16005894., 21186674., 22331128., 19394182., 17548252.,\n        16668048., 13316436., 12973260.,  2582550.]),\n array([8.97930163e-02, 2.00141141e+03, 4.00273303e+03, 6.00405465e+03,\n        8.00537626e+03, 1.00066979e+04, 1.20080195e+04, 1.40093411e+04,\n        1.60106627e+04, 1.80119844e+04, 2.00133060e+04]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n- weight matrix\n\neach_location.get_weightmatrix(theta=(8810.865423093777),kappa=2500) \n\n- fit\n\neach_location.fit2()\n\n\neach_location.haiti(MagThresh=6.9,ResThresh=0.5)\n\n\n                                                \n\n\n\neach_location.lquique(MagThresh=8,ResThresh=0.4,adjzoom=4.3)\n\n\n                                                \n\n\n\neach_location.sichuan(MagThresh=6.5,ResThresh=0.4)"
  },
  {
    "objectID": "posts/GODE/2022-10-02-Earthquake_real.html#ì¹ ë ˆ",
    "href": "posts/GODE/2022-10-02-Earthquake_real.html#ì¹ ë ˆ",
    "title": "Earthquake",
    "section": "ì¹ ë ˆ",
    "text": "ì¹ ë ˆ\n\ndf_chile_ex= pd.concat([pd.read_csv('05_10.csv'),pd.read_csv('10_15.csv')]).iloc[:,[0,1,2,4]].rename(columns={'latitude':'Latitude','longitude':'Longitude','mag':'Magnitude'}).reset_index().iloc[:,1:]\n\n\ndf_chile = df_chile_ex.assign(Year=list(map(lambda x: x.split('-')[0], df_chile_ex.time))).iloc[:,1:]\n\n\ndf_chile = df_chile.assign(Month=list(map(lambda x: x.split('-')[1], df_chile_ex.time)))\n\n\ndf_chile.Year = df_chile.Year.astype(np.float64)\ndf_chile.Month = df_chile.Month.astype(np.float64)\n\n\nchile_location=eachlocation(df_chile.query(\"2010 <= Year < 2015\"))\n\n\nchile_location.get_distance()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12498/12498 [03:18<00:00, 62.95it/s]  \n\n\n\nchile_location.get_weightmatrix(theta=(chile_location.D[chile_location.D>0].mean()),kappa=2500) \n\n\nchile_location.fit2()\n\nì•„ì´í‹°\n\nchile_location.df.assign(Residual2 = chile_location.df.Residual**2).reset_index().iloc[2324:2330,:]\n\n\n\n\n\n  \n    \n      \n      index\n      Latitude\n      Longitude\n      Magnitude\n      Year\n      Month\n      MagnitudeHat\n      Residual\n      Residual2\n    \n  \n  \n    \n      2324\n      2324\n      18.463\n      -72.626\n      5.0\n      2010.0\n      1.0\n      5.334262\n      -0.334262\n      0.111731\n    \n    \n      2325\n      2325\n      18.387\n      -72.784\n      6.0\n      2010.0\n      1.0\n      5.719565\n      0.280435\n      0.078644\n    \n    \n      2326\n      2326\n      18.443\n      -72.571\n      7.0\n      2010.0\n      1.0\n      6.288026\n      0.711974\n      0.506907\n    \n    \n      2327\n      2327\n      -5.417\n      133.731\n      5.5\n      2010.0\n      1.0\n      5.530625\n      -0.030625\n      0.000938\n    \n    \n      2328\n      2328\n      15.437\n      -88.761\n      5.1\n      2010.0\n      1.0\n      5.125565\n      -0.025565\n      0.000654\n    \n    \n      2329\n      2329\n      -16.861\n      -174.228\n      5.3\n      2010.0\n      1.0\n      5.471571\n      -0.171571\n      0.029437\n    \n  \n\n\n\n\nì¹ ë ˆ\n\nchile_location.df.assign(Residual2 = chile_location.df.Residual**2).reset_index().query(\"-56.5 < Latitude & Latitude <-17.4 & -81.5 < Longitude & Longitude < -61.5 & Year == 2014 & Month == 8\")\n\n\n\n\n\n  \n    \n      \n      index\n      Latitude\n      Longitude\n      Magnitude\n      Year\n      Month\n      MagnitudeHat\n      Residual\n      Residual2\n    \n  \n  \n    \n      2997\n      14603\n      -32.6953\n      -71.4416\n      6.4\n      2014.0\n      8.0\n      5.705553\n      0.694447\n      0.482257\n    \n    \n      2999\n      14605\n      -20.1745\n      -69.0385\n      5.6\n      2014.0\n      8.0\n      5.497368\n      0.102632\n      0.010533\n    \n    \n      3032\n      14638\n      -20.1580\n      -70.0230\n      5.3\n      2014.0\n      8.0\n      5.291126\n      0.008874\n      0.000079\n    \n    \n      3046\n      14652\n      -23.9047\n      -66.7371\n      5.0\n      2014.0\n      8.0\n      4.909951\n      0.090049\n      0.008109\n    \n    \n      3057\n      14663\n      -33.7770\n      -72.2030\n      5.2\n      2014.0\n      8.0\n      5.382720\n      -0.182720\n      0.033387\n    \n  \n\n\n\n\nì¤‘êµ­\n\nchile_location.df.assign(Residual2 = chile_location.df.Residual**2).reset_index().iloc[5136:5142,:]\n\n\n\n\n\n  \n    \n      \n      index\n      Latitude\n      Longitude\n      Magnitude\n      Year\n      Month\n      MagnitudeHat\n      Residual\n      Residual2\n    \n  \n  \n    \n      5136\n      16742\n      30.209\n      102.862\n      5.0\n      2013.0\n      4.0\n      5.027420\n      -0.027420\n      0.000752\n    \n    \n      5137\n      16743\n      30.308\n      102.888\n      6.6\n      2013.0\n      4.0\n      5.922831\n      0.677169\n      0.458558\n    \n    \n      5138\n      16744\n      39.693\n      143.258\n      5.0\n      2013.0\n      4.0\n      4.758333\n      0.241667\n      0.058403\n    \n    \n      5139\n      16745\n      49.965\n      157.652\n      6.1\n      2013.0\n      4.0\n      5.797293\n      0.302707\n      0.091632\n    \n    \n      5140\n      16746\n      -11.976\n      121.632\n      5.8\n      2013.0\n      4.0\n      5.854233\n      -0.054233\n      0.002941\n    \n    \n      5141\n      16747\n      -14.966\n      166.857\n      5.2\n      2013.0\n      4.0\n      5.228670\n      -0.028670\n      0.000822"
  },
  {
    "objectID": "posts/GODE/2022-12-27-DFT_study.html",
    "href": "posts/GODE/2022-12-27-DFT_study.html",
    "title": "Discrete Fourier Transform",
    "section": "",
    "text": "DFT\nhttps://miruetoto.quarto.pub/yechan/posts/CGSP/2022-12-24-CGSP-Chap-8-3-DFT.html#fnref1\nhttps://miruetoto.github.io/yechan/%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88/2019/11/24/(%EB%85%B8%ED%8A%B8)-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88-%EC%B6%94%EB%A1%A0-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88.html"
  },
  {
    "objectID": "posts/GODE/2022-12-27-DFT_study.html#import",
    "href": "posts/GODE/2022-12-27-DFT_study.html#import",
    "title": "Discrete Fourier Transform",
    "section": "import",
    "text": "import\n\nimport numpy as np\n\n\nForward operator A\n\nA = np.array([[0, 0, 0, 0, 1],\n    [1, 0, 0, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, 0, 1, 0, 0],\n    [0, 0, 0, 1, 0]])\nA\n\narray([[0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0]])\n\n\n\nnp.transpose(A)@A\n\narray([[1, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 1]])\n\n\n\nnote: A is orthogonal matrix\n\n\ns = np.array([[1],[22],[333],[4444],[55555]])\ns\n\narray([[    1],\n       [   22],\n       [  333],\n       [ 4444],\n       [55555]])\n\n\n\nA@s\n\narray([[55555],\n       [    1],\n       [   22],\n       [  333],\n       [ 4444]])\n\n\n\nA@A@s\n\narray([[ 4444],\n       [55555],\n       [    1],\n       [   22],\n       [  333]])\n\n\n\nA@A@A@s\n\narray([[  333],\n       [ 4444],\n       [55555],\n       [    1],\n       [   22]])\n\n\n\nnote : thus A is a forward operator,A* is a backward operator.\n\n\n\nDFT\n\\(A = DFT^* \\Lambda DFT\\)\n\nÎ», Ïˆ = np.linalg.eig(A)\nÎ», Ïˆ\n\n(array([-0.80901699+0.58778525j, -0.80901699-0.58778525j,\n         0.30901699+0.95105652j,  0.30901699-0.95105652j,\n         1.        +0.j        ]),\n array([[-0.3618034+0.26286556j, -0.3618034-0.26286556j,\n         -0.3618034-0.26286556j, -0.3618034+0.26286556j,\n          0.4472136+0.j        ],\n        [ 0.4472136+0.j        ,  0.4472136-0.j        ,\n         -0.3618034+0.26286556j, -0.3618034-0.26286556j,\n          0.4472136+0.j        ],\n        [-0.3618034-0.26286556j, -0.3618034+0.26286556j,\n          0.1381966+0.4253254j ,  0.1381966-0.4253254j ,\n          0.4472136+0.j        ],\n        [ 0.1381966+0.4253254j ,  0.1381966-0.4253254j ,\n          0.4472136+0.j        ,  0.4472136-0.j        ,\n          0.4472136+0.j        ],\n        [ 0.1381966-0.4253254j ,  0.1381966+0.4253254j ,\n          0.1381966-0.4253254j ,  0.1381966+0.4253254j ,\n          0.4472136+0.j        ]]))\n\n\n\nÎ».shape, Ïˆ.shape\n\n((5,), (5, 5))\n\n\n\nA \n\narray([[0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0]])\n\n\n\n(Ïˆ @ np.diag(Î») @ Ïˆ.transpose()).round(2)\n\narray([[-0.  +0.j,  0.45+0.j,  0.28+0.j,  0.72+0.j, -0.45+0.j],\n       [ 0.45+0.j,  0.28+0.j,  0.72+0.j, -0.45+0.j, -0.  +0.j],\n       [ 0.28+0.j,  0.72+0.j, -0.45+0.j,  0.  +0.j,  0.45+0.j],\n       [ 0.72+0.j, -0.45+0.j,  0.  +0.j,  0.45+0.j,  0.28+0.j],\n       [-0.45+0.j, -0.  +0.j,  0.45+0.j,  0.28+0.j,  0.72+0.j]])\n\n\n?\ndefine \\(\\psi^* = DFT\\)\n\nDFT = np.transpose(Ïˆ)\nDFT\n\narray([[-0.3618034+0.26286556j,  0.4472136+0.j        ,\n        -0.3618034-0.26286556j,  0.1381966+0.4253254j ,\n         0.1381966-0.4253254j ],\n       [-0.3618034-0.26286556j,  0.4472136-0.j        ,\n        -0.3618034+0.26286556j,  0.1381966-0.4253254j ,\n         0.1381966+0.4253254j ],\n       [-0.3618034-0.26286556j, -0.3618034+0.26286556j,\n         0.1381966+0.4253254j ,  0.4472136+0.j        ,\n         0.1381966-0.4253254j ],\n       [-0.3618034+0.26286556j, -0.3618034-0.26286556j,\n         0.1381966-0.4253254j ,  0.4472136-0.j        ,\n         0.1381966+0.4253254j ],\n       [ 0.4472136+0.j        ,  0.4472136+0.j        ,\n         0.4472136+0.j        ,  0.4472136+0.j        ,\n         0.4472136+0.j        ]])\n\n\n\nÎ»[3,]\n\n(0.30901699437494734-0.9510565162951535j)\n\n\n\na=np.array([1,2,3,4])\nnp.diag(np.diag(a))\n\narray([1, 2, 3, 4])\n\n\n\nÎ»\n\narray([-0.80901699+0.58778525j, -0.80901699-0.58778525j,\n        0.30901699+0.95105652j,  0.30901699-0.95105652j,\n        1.        +0.j        ])\n\n\n\n(np.matrix(Ïˆ)@np.matrix(np.diag(Î»))@np.matrix(Ïˆ).H).round(3)\n\nmatrix([[-0.+0.j,  0.+0.j, -0.+0.j,  0.+0.j,  1.+0.j],\n        [ 1.+0.j, -0.+0.j,  0.+0.j, -0.+0.j, -0.+0.j],\n        [-0.+0.j,  1.+0.j, -0.+0.j,  0.+0.j,  0.+0.j],\n        [ 0.+0.j, -0.+0.j,  1.+0.j,  0.+0.j, -0.+0.j],\n        [-0.+0.j, -0.+0.j,  0.+0.j,  1.+0.j, -0.+0.j]])\n\n\n\nnp.matrix(Ïˆ).H\n\nmatrix([[-0.3618034-0.26286556j,  0.4472136-0.j        ,\n         -0.3618034+0.26286556j,  0.1381966-0.4253254j ,\n          0.1381966+0.4253254j ],\n        [-0.3618034+0.26286556j,  0.4472136+0.j        ,\n         -0.3618034-0.26286556j,  0.1381966+0.4253254j ,\n          0.1381966-0.4253254j ],\n        [-0.3618034+0.26286556j, -0.3618034-0.26286556j,\n          0.1381966-0.4253254j ,  0.4472136-0.j        ,\n          0.1381966+0.4253254j ],\n        [-0.3618034-0.26286556j, -0.3618034+0.26286556j,\n          0.1381966+0.4253254j ,  0.4472136+0.j        ,\n          0.1381966-0.4253254j ],\n        [ 0.4472136-0.j        ,  0.4472136-0.j        ,\n          0.4472136-0.j        ,  0.4472136-0.j        ,\n          0.4472136-0.j        ]])\n\n\n\n\nSpectral components and Frequencies\n\\[\\{ 1,\\psi_1, \\psi_2, \\psi_3,\\dots, \\psi_{N-1} \\}\\]\nThese vectors are called spectral components.\nIn Physics and in operator theory, these eigenvalues are the frequencies of the signal.\nEigenvalues of \\(A\\)"
  },
  {
    "objectID": "posts/GODE/Untitled.html",
    "href": "posts/GODE/Untitled.html",
    "title": "Seoyeon's Blog for study",
    "section": "",
    "text": "import numpy as np\n\n\na=10\nb=20\nn=200\n\n\narr1 = np.array([a+(b-a)/(n-1) * (i-1) for i in range(1,n+1)])\n\n\\[a+\\frac{(b-a)i}{n-1}\\] for \\(i=1,2,3,\\dots, n\\)\n\narr2 = np.linspace(a,b,n)\n\n\narr1[:5]\n\narray([10.        , 10.05025126, 10.10050251, 10.15075377, 10.20100503])\n\n\n\narr2[:5]\n\narray([10.        , 10.05025126, 10.10050251, 10.15075377, 10.20100503])"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, there ;)\nì°¸ê³ \nterminal check\n\ntop\nnvidia-smi"
  }
]